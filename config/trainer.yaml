defaults:
  - _self_
  - tokenizer: default
  - world_model: default
  
wandb:
  mode: online
  project: iris
  entity: null
  name: null
  group: null
  tags: null
  notes: null


initialization:
  path_to_checkpoint: /space/junzheyin/checkpoints_iris8/model_checkpoint_epoch_156.pt
  path_to_checkpoint_trans: /space/junzheyin/check_final_new1/model_checkpoint_epoch_22.pt
  load_tokenizer: True
  load_world_model: True
  load_discriminator: False
  load_discriminator_AENN: True

checkpoint_OPT:
  name_to_checkpoint: optimizer_22.pt
  load_opti: True

common:
  epochs: 200
  device: cuda:1
  batch_size: 512
  do_checkpoint: True
  seed: 0
  obs_time: 3 
  pred_time: 6 
  sequence_length: ${world_model.max_blocks}
  resume: False # set by resume.sh script only.

collection:
  train:
    stop_after_epochs: 200 ###### this should be 100 


training:
  should: True
  learning_rate: 0.0001
  tokenizer:
    batch_num_samples: 1 # batch_size
    grad_acc_steps: 64
    start_after_epochs: 10000 #was 5
    stop_after_epochs: 20000
  world_model:
    batch_num_samples: 2 # batch_size
    grad_acc_steps: 16
    weight_decay: 0.01
    start_after_epochs: 0 # was 25
    stop_after_epochs: 100
  discriminator:
    batch_num_samples: 2 # batch_size
    grad_acc_steps: 16
    start_after_epochs: 0 # was 25
    stop_after_epochs: 100   

evaluation:
  batch: 1
  should: True 
  every: 2
  tokenizer:
    batch_num_samples: 1
    start_after_epochs: ${training.tokenizer.start_after_epochs}
    save_reconstructions: True
  world_model:
    batch_num_samples: 1
    start_after_epochs: ${training.world_model.start_after_epochs}
  discriminator:
    batch_num_samples: 1
    start_after_epochs: ${training.discriminator.start_after_epochs}
