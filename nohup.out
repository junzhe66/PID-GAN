Pysteps configuration file found at: /space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/pysteps/pystepsrc

wandb: Currently logged in as: zeina (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.15.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.7
wandb: Syncing run elated-rain-72
wandb:  View project at https://wandb.ai/zeina/iris
wandb:  View run at https://wandb.ai/zeina/iris/runs/37rye8gw
wandb: Run data is saved locally in /space/zboucher/iris/outputs/2023-09-22/09-09-38/wandb/run-20230922_090939-37rye8gw
wandb: Run `wandb offline` to turn off syncing.
A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)
[Powered by Stella]
/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)

Tokenizer : shape of latent is (512, 4, 4).
91642241 parameters in agent.tokenizer
20358656 parameters in agent.world_model

Epoch 1 / 200

32183 3493 3560
Training tokenizer:   0% 0/8047 [00:00<?, ?it/s]loss_total_epoch 0.4351981580257416
Training tokenizer:   0% 1/8047 [00:01<4:16:45,  1.91s/it]loss_total_epoch 0.7892234921455383
Training tokenizer:   0% 2/8047 [00:03<3:39:13,  1.63s/it]loss_total_epoch 1.0290666669607162
Training tokenizer:   0% 3/8047 [00:04<3:26:36,  1.54s/it]loss_total_epoch 1.2477806955575943
Training tokenizer:   0% 4/8047 [00:06<3:20:52,  1.50s/it]loss_total_epoch 1.4348810762166977
Training tokenizer:   0% 5/8047 [00:07<3:17:51,  1.48s/it]loss_total_epoch 1.621605858206749
Training tokenizer:   0% 6/8047 [00:09<3:16:05,  1.46s/it]loss_total_epoch 1.7846665233373642
Training tokenizer:   0% 7/8047 [00:10<3:14:52,  1.45s/it]loss_total_epoch 1.9726853519678116
Training tokenizer:   0% 8/8047 [00:11<3:13:37,  1.45s/it]loss_total_epoch 2.122775673866272
Training tokenizer:   0% 9/8047 [00:13<3:13:24,  1.44s/it]loss_total_epoch 2.2701973915100098
Training tokenizer:   0% 10/8047 [00:14<3:13:00,  1.44s/it]loss_total_epoch 2.4160076081752777
Training tokenizer:   0% 11/8047 [00:16<3:12:54,  1.44s/it]loss_total_epoch 2.5517517924308777
Training tokenizer:   0% 12/8047 [00:17<3:12:28,  1.44s/it]loss_total_epoch 2.6804655492305756
Training tokenizer:   0% 13/8047 [00:19<3:12:23,  1.44s/it]loss_total_epoch 2.817589044570923
Training tokenizer:   0% 14/8047 [00:20<3:12:43,  1.44s/it]loss_total_epoch 2.9413337484002113
Training tokenizer:   0% 15/8047 [00:22<3:13:02,  1.44s/it]loss_total_epoch 3.0650622248649597
Training tokenizer:   0% 16/8047 [00:23<3:14:14,  1.45s/it]loss_total_epoch 3.1755366399884224
Training tokenizer:   0% 17/8047 [00:24<3:13:47,  1.45s/it]loss_total_epoch 3.294713281095028
Training tokenizer:   0% 18/8047 [00:26<3:13:24,  1.45s/it]loss_total_epoch 3.4114650264382362
Training tokenizer:   0% 19/8047 [00:27<3:12:54,  1.44s/it]loss_total_epoch 3.513114608824253
Training tokenizer:   0% 20/8047 [00:29<3:13:12,  1.44s/it]loss_total_epoch 3.6079321056604385
Training tokenizer:   0% 21/8047 [00:30<3:12:50,  1.44s/it]loss_total_epoch 3.7176553159952164
Training tokenizer:   0% 22/8047 [00:32<3:12:52,  1.44s/it]loss_total_epoch 3.820803038775921
Training tokenizer:   0% 23/8047 [00:33<3:13:24,  1.45s/it]loss_total_epoch 3.9235312789678574
Training tokenizer:   0% 24/8047 [00:35<3:13:36,  1.45s/it]loss_total_epoch 4.038994699716568
Training tokenizer:   0% 25/8047 [00:36<3:12:49,  1.44s/it]loss_total_epoch 4.133711509406567
Training tokenizer:   0% 26/8047 [00:37<3:12:57,  1.44s/it]loss_total_epoch 4.235554918646812
Training tokenizer:   0% 27/8047 [00:39<3:13:03,  1.44s/it]loss_total_epoch 4.337774902582169
Training tokenizer:   0% 28/8047 [00:40<3:13:05,  1.44s/it]loss_total_epoch 4.4313855692744255
Training tokenizer:   0% 29/8047 [00:42<3:13:10,  1.45s/it]loss_total_epoch 4.533239804208279
Training tokenizer:   0% 30/8047 [00:43<3:12:46,  1.44s/it]loss_total_epoch 4.632646806538105
Training tokenizer:   0% 31/8047 [00:45<3:13:41,  1.45s/it]loss_total_epoch 4.7262710854411125
Training tokenizer:   0% 32/8047 [00:46<3:13:23,  1.45s/it]loss_total_epoch 4.817016072571278
Training tokenizer:   0% 33/8047 [00:48<3:13:39,  1.45s/it]loss_total_epoch 4.906657628715038
Training tokenizer:   0% 34/8047 [00:49<3:13:30,  1.45s/it]loss_total_epoch 4.993192799389362
Training tokenizer:   0% 35/8047 [00:50<3:13:51,  1.45s/it]loss_total_epoch 5.082887686789036
Training tokenizer:   0% 36/8047 [00:52<3:13:18,  1.45s/it]loss_total_epoch 5.174204044044018
Training tokenizer:   0% 37/8047 [00:53<3:13:08,  1.45s/it]loss_total_epoch 5.263526730239391
Training tokenizer:   0% 38/8047 [00:55<3:13:13,  1.45s/it]loss_total_epoch 5.3566242307424545
Training tokenizer:   0% 39/8047 [00:56<3:13:25,  1.45s/it]loss_total_epoch 5.441053174436092
Training tokenizer:   0% 40/8047 [00:58<3:21:47,  1.51s/it]loss_total_epoch 5.5421353578567505
Training tokenizer:   1% 41/8047 [01:00<3:27:52,  1.56s/it]loss_total_epoch 5.637348860502243
Training tokenizer:   1% 42/8047 [01:01<3:24:08,  1.53s/it]loss_total_epoch 5.723786897957325
Training tokenizer:   1% 43/8047 [01:03<3:21:25,  1.51s/it]loss_total_epoch 5.8169404193758965
Training tokenizer:   1% 44/8047 [01:04<3:20:26,  1.50s/it]loss_total_epoch 5.890143498778343
Training tokenizer:   1% 45/8047 [01:05<3:18:22,  1.49s/it]loss_total_epoch 5.982176631689072
Training tokenizer:   1% 46/8047 [01:07<3:16:51,  1.48s/it]loss_total_epoch 6.074211925268173
Training tokenizer:   1% 47/8047 [01:08<3:15:58,  1.47s/it]loss_total_epoch 6.1590520069003105
Training tokenizer:   1% 48/8047 [01:10<3:25:40,  1.54s/it]loss_total_epoch 6.241526328027248
Training tokenizer:   1% 49/8047 [01:12<3:22:37,  1.52s/it]loss_total_epoch 6.321099855005741
Training tokenizer:   1% 50/8047 [01:13<3:19:34,  1.50s/it]loss_total_epoch 6.4023972153663635
Training tokenizer:   1% 51/8047 [01:14<3:18:39,  1.49s/it]loss_total_epoch 6.494575463235378
Training tokenizer:   1% 52/8047 [01:16<3:18:39,  1.49s/it]loss_total_epoch 6.579015269875526
Training tokenizer:   1% 53/8047 [01:17<3:19:03,  1.49s/it]loss_total_epoch 6.651452660560608
Training tokenizer:   1% 54/8047 [01:19<3:18:35,  1.49s/it]loss_total_epoch 6.743111111223698
Training tokenizer:   1% 55/8047 [01:20<3:17:51,  1.49s/it]loss_total_epoch 6.8202957808971405
Training tokenizer:   1% 56/8047 [01:22<3:16:43,  1.48s/it]loss_total_epoch 6.900543987751007
Training tokenizer:   1% 57/8047 [01:23<3:17:04,  1.48s/it]loss_total_epoch 6.968484625220299
Training tokenizer:   1% 58/8047 [01:25<3:16:33,  1.48s/it]loss_total_epoch 7.0539800226688385
Training tokenizer:   1% 59/8047 [01:26<3:16:35,  1.48s/it]loss_total_epoch 7.143994964659214
Training tokenizer:   1% 60/8047 [01:28<3:17:09,  1.48s/it]loss_total_epoch 7.216219410300255
Training tokenizer:   1% 61/8047 [01:29<3:16:52,  1.48s/it]loss_total_epoch 7.285980112850666
Training tokenizer:   1% 62/8047 [01:31<3:16:54,  1.48s/it]loss_total_epoch 7.354143917560577
Training tokenizer:   1% 63/8047 [01:32<3:17:16,  1.48s/it]loss_total_epoch 7.423579581081867
Training tokenizer:   1% 64/8047 [01:34<3:16:59,  1.48s/it]loss_total_epoch 7.4758045226335526
Training tokenizer:   1% 65/8047 [01:35<3:16:36,  1.48s/it]loss_total_epoch 7.5590076595544815
Training tokenizer:   1% 66/8047 [01:37<3:16:57,  1.48s/it]loss_total_epoch 7.6199111714959145
Training tokenizer:   1% 67/8047 [01:38<3:16:49,  1.48s/it]loss_total_epoch 7.688542790710926
Training tokenizer:   1% 68/8047 [01:40<3:17:03,  1.48s/it]loss_total_epoch 7.784717217087746
Training tokenizer:   1% 69/8047 [01:41<3:18:10,  1.49s/it]loss_total_epoch 7.867608442902565
Training tokenizer:   1% 70/8047 [01:43<3:18:00,  1.49s/it]loss_total_epoch 7.955711491405964
Training tokenizer:   1% 71/8047 [01:44<3:22:05,  1.52s/it]loss_total_epoch 8.02092519402504
Training tokenizer:   1% 72/8047 [01:46<3:20:31,  1.51s/it]loss_total_epoch 8.081471309065819
Training tokenizer:   1% 73/8047 [01:47<3:19:26,  1.50s/it]loss_total_epoch 8.159358225762844
Training tokenizer:   1% 74/8047 [01:49<3:19:22,  1.50s/it]loss_total_epoch 8.250796556472778
Training tokenizer:   1% 75/8047 [01:50<3:18:56,  1.50s/it]loss_total_epoch 8.317538782954216
Training tokenizer:   1% 76/8047 [01:52<3:18:36,  1.50s/it]loss_total_epoch 8.386870674788952
Training tokenizer:   1% 77/8047 [01:53<3:19:07,  1.50s/it]loss_total_epoch 8.45454577356577
Training tokenizer:   1% 78/8047 [01:55<3:18:01,  1.49s/it]loss_total_epoch 8.544354632496834
Training tokenizer:   1% 79/8047 [01:56<3:17:43,  1.49s/it]loss_total_epoch 8.615234300494194
Training tokenizer:   1% 80/8047 [01:58<3:17:40,  1.49s/it]loss_total_epoch 8.696110934019089
Training tokenizer:   1% 81/8047 [01:59<3:19:00,  1.50s/it]loss_total_epoch 8.779914885759354
Training tokenizer:   1% 82/8047 [02:01<3:19:07,  1.50s/it]loss_total_epoch 8.855473808944225
Training tokenizer:   1% 83/8047 [02:02<3:19:06,  1.50s/it]loss_total_epoch 8.921955838799477
Training tokenizer:   1% 84/8047 [02:04<3:19:07,  1.50s/it]loss_total_epoch 9.005066014826298
Training tokenizer:   1% 85/8047 [02:05<3:18:51,  1.50s/it]loss_total_epoch 9.082864925265312
Training tokenizer:   1% 86/8047 [02:07<3:19:17,  1.50s/it]loss_total_epoch 9.164702020585537
Training tokenizer:   1% 87/8047 [02:08<3:18:58,  1.50s/it]loss_total_epoch 9.25909574329853
Training tokenizer:   1% 88/8047 [02:10<3:18:56,  1.50s/it]loss_total_epoch 9.33676664531231
Training tokenizer:   1% 89/8047 [02:11<3:18:03,  1.49s/it]loss_total_epoch 9.418135777115822
Training tokenizer:   1% 90/8047 [02:13<3:17:29,  1.49s/it]loss_total_epoch 9.50469695776701
Training tokenizer:   1% 91/8047 [02:14<3:18:26,  1.50s/it]loss_total_epoch 9.576158158481121
Training tokenizer:   1% 92/8047 [02:16<3:18:11,  1.49s/it]loss_total_epoch 9.644845813512802
Training tokenizer:   1% 93/8047 [02:17<3:18:14,  1.50s/it]loss_total_epoch 9.692835956811905
Training tokenizer:   1% 94/8047 [02:19<3:18:33,  1.50s/it]loss_total_epoch 9.770502097904682
Training tokenizer:   1% 95/8047 [02:20<3:18:30,  1.50s/it]loss_total_epoch 9.831996992230415
Training tokenizer:   1% 96/8047 [02:22<3:18:32,  1.50s/it]loss_total_epoch 9.903387479484081
Training tokenizer:   1% 97/8047 [02:23<3:18:56,  1.50s/it]loss_total_epoch 9.979344569146633
Training tokenizer:   1% 98/8047 [02:25<3:22:46,  1.53s/it]loss_total_epoch 10.048950649797916
Training tokenizer:   1% 99/8047 [02:26<3:21:51,  1.52s/it]loss_total_epoch 10.11519768834114
Training tokenizer:   1% 100/8047 [02:28<3:21:13,  1.52s/it]loss_total_epoch 10.193056650459766
Training tokenizer:   1% 101/8047 [02:29<3:20:03,  1.51s/it]loss_total_epoch 10.270116180181503
Training tokenizer:   1% 102/8047 [02:31<3:20:05,  1.51s/it]loss_total_epoch 10.34859023988247
Training tokenizer:   1% 103/8047 [02:32<3:19:57,  1.51s/it]loss_total_epoch 10.41898788511753
Training tokenizer:   1% 104/8047 [02:34<3:20:18,  1.51s/it]loss_total_epoch 10.492915704846382
Training tokenizer:   1% 105/8047 [02:35<3:19:44,  1.51s/it]loss_total_epoch 10.544399123638868
Training tokenizer:   1% 106/8047 [02:37<3:20:08,  1.51s/it]loss_total_epoch 10.62427805736661
Training tokenizer:   1% 107/8047 [02:38<3:19:31,  1.51s/it]loss_total_epoch 10.697095114737749
Training tokenizer:   1% 108/8047 [02:40<3:19:03,  1.50s/it]loss_total_epoch 10.761996168643236
Training tokenizer:   1% 109/8047 [02:41<3:18:18,  1.50s/it]loss_total_epoch 10.834642123430967
Training tokenizer:   1% 110/8047 [02:43<3:18:03,  1.50s/it]loss_total_epoch 10.90994082018733
Training tokenizer:   1% 111/8047 [02:44<3:18:06,  1.50s/it]loss_total_epoch 10.976883079856634
Training tokenizer:   1% 112/8047 [02:46<3:19:01,  1.50s/it]loss_total_epoch 11.046354446560144
Training tokenizer:   1% 113/8047 [02:47<3:20:17,  1.51s/it]loss_total_epoch 11.11125124618411
Training tokenizer:   1% 114/8047 [02:49<3:20:43,  1.52s/it]loss_total_epoch 11.188921745866537
Training tokenizer:   1% 115/8047 [02:50<3:20:50,  1.52s/it]loss_total_epoch 11.267234865576029
Training tokenizer:   1% 116/8047 [02:52<3:19:47,  1.51s/it]loss_total_epoch 11.347961064428091
Training tokenizer:   1% 117/8047 [02:53<3:20:31,  1.52s/it]loss_total_epoch 11.405362095683813
Training tokenizer:   1% 118/8047 [02:55<3:20:19,  1.52s/it]loss_total_epoch 11.479958403855562
Training tokenizer:   1% 119/8047 [02:56<3:20:31,  1.52s/it]loss_total_epoch 11.545683298259974
Training tokenizer:   1% 120/8047 [02:58<3:20:28,  1.52s/it]loss_total_epoch 11.627838667482138
Training tokenizer:   2% 121/8047 [02:59<3:20:11,  1.52s/it]loss_total_epoch 11.70651600882411
Training tokenizer:   2% 122/8047 [03:01<3:20:20,  1.52s/it]loss_total_epoch 11.771229084581137
Training tokenizer:   2% 123/8047 [03:02<3:19:50,  1.51s/it]loss_total_epoch 11.848777528852224
Training tokenizer:   2% 124/8047 [03:04<3:19:45,  1.51s/it]loss_total_epoch 11.936420071870089
Training tokenizer:   2% 125/8047 [03:05<3:19:36,  1.51s/it]loss_total_epoch 12.002012249082327
Training tokenizer:   2% 126/8047 [03:07<3:19:03,  1.51s/it]loss_total_epoch 12.075606625527143
Training tokenizer:   2% 127/8047 [03:08<3:19:12,  1.51s/it]loss_total_epoch 12.152162950485945
Training tokenizer:   2% 128/8047 [03:10<3:19:10,  1.51s/it]loss_total_epoch 12.220037546008825
Training tokenizer:   2% 129/8047 [03:12<3:19:46,  1.51s/it]loss_total_epoch 12.314662899821997
Training tokenizer:   2% 130/8047 [03:13<3:20:23,  1.52s/it]loss_total_epoch 12.392237339168787
Training tokenizer:   2% 131/8047 [03:15<3:19:42,  1.51s/it]loss_total_epoch 12.446720581501722
Training tokenizer:   2% 132/8047 [03:16<3:19:21,  1.51s/it]loss_total_epoch 12.519795384258032
Training tokenizer:   2% 133/8047 [03:18<3:19:27,  1.51s/it]loss_total_epoch 12.583535593003035
Training tokenizer:   2% 134/8047 [03:19<3:19:24,  1.51s/it]loss_total_epoch 12.650516893714666
Training tokenizer:   2% 135/8047 [03:21<3:19:33,  1.51s/it]loss_total_epoch 12.719922233372927
Training tokenizer:   2% 136/8047 [03:22<3:19:26,  1.51s/it]loss_total_epoch 12.799238357692957
Training tokenizer:   2% 137/8047 [03:24<3:18:41,  1.51s/it]loss_total_epoch 12.860117297619581
Training tokenizer:   2% 138/8047 [03:25<3:19:06,  1.51s/it]loss_total_epoch 12.936266865581274
Training tokenizer:   2% 139/8047 [03:27<3:19:27,  1.51s/it]loss_total_epoch 13.010728176683187
Training tokenizer:   2% 140/8047 [03:28<3:19:51,  1.52s/it]loss_total_epoch 13.081909690052271
Training tokenizer:   2% 141/8047 [03:30<3:19:41,  1.52s/it]loss_total_epoch 13.163308296352625
Training tokenizer:   2% 142/8047 [03:31<3:19:05,  1.51s/it]loss_total_epoch 13.225714594125748
Training tokenizer:   2% 143/8047 [03:33<3:19:25,  1.51s/it]loss_total_epoch 13.290769822895527
Training tokenizer:   2% 144/8047 [03:34<3:18:37,  1.51s/it]loss_total_epoch 13.352962911128998
Training tokenizer:   2% 145/8047 [03:36<3:19:18,  1.51s/it]loss_total_epoch 13.403445892035961
Training tokenizer:   2% 146/8047 [03:37<3:19:40,  1.52s/it]loss_total_epoch 13.477946311235428
Training tokenizer:   2% 147/8047 [03:39<3:20:39,  1.52s/it]loss_total_epoch 13.545897372066975
Training tokenizer:   2% 148/8047 [03:40<3:21:07,  1.53s/it]loss_total_epoch 13.611286900937557
Training tokenizer:   2% 149/8047 [03:42<3:20:33,  1.52s/it]loss_total_epoch 13.697204105556011
Training tokenizer:   2% 150/8047 [03:43<3:20:27,  1.52s/it]loss_total_epoch 13.760846816003323
Training tokenizer:   2% 151/8047 [03:45<3:20:40,  1.52s/it]loss_total_epoch 13.824931159615517
Training tokenizer:   2% 152/8047 [03:46<3:20:14,  1.52s/it]loss_total_epoch 13.908259220421314
Training tokenizer:   2% 153/8047 [03:48<3:20:35,  1.52s/it]loss_total_epoch 13.982227399945259
Training tokenizer:   2% 154/8047 [03:49<3:21:21,  1.53s/it]loss_total_epoch 14.056518249213696
Training tokenizer:   2% 155/8047 [03:51<3:21:37,  1.53s/it]loss_total_epoch 14.141834437847137
Training tokenizer:   2% 156/8047 [03:53<3:21:54,  1.54s/it]loss_total_epoch 14.217507302761078
Training tokenizer:   2% 157/8047 [03:54<3:21:39,  1.53s/it]loss_total_epoch 14.277237579226494
Training tokenizer:   2% 158/8047 [03:56<3:29:44,  1.60s/it]loss_total_epoch 14.340554237365723
Training tokenizer:   2% 159/8047 [03:57<3:27:06,  1.58s/it]loss_total_epoch 14.40887577086687
Training tokenizer:   2% 160/8047 [03:59<3:25:01,  1.56s/it]loss_total_epoch 14.488073408603668
Training tokenizer:   2% 161/8047 [04:00<3:23:23,  1.55s/it]loss_total_epoch 14.555209033191204
Training tokenizer:   2% 162/8047 [04:02<3:23:13,  1.55s/it]loss_total_epoch 14.625834554433823
Training tokenizer:   2% 163/8047 [04:03<3:22:17,  1.54s/it]loss_total_epoch 14.689374826848507
Training tokenizer:   2% 164/8047 [04:05<3:23:09,  1.55s/it]loss_total_epoch 14.760710507631302
Training tokenizer:   2% 165/8047 [04:07<3:22:26,  1.54s/it]loss_total_epoch 14.841765373945236
Training tokenizer:   2% 166/8047 [04:08<3:21:49,  1.54s/it]loss_total_epoch 14.930521696805954
Training tokenizer:   2% 167/8047 [04:10<3:22:37,  1.54s/it]loss_total_epoch 15.009972780942917
Training tokenizer:   2% 168/8047 [04:11<3:21:59,  1.54s/it]loss_total_epoch 15.098004959523678
Training tokenizer:   2% 169/8047 [04:13<3:20:46,  1.53s/it]loss_total_epoch 15.173259139060974
Training tokenizer:   2% 170/8047 [04:14<3:20:44,  1.53s/it]loss_total_epoch 15.232823885977268
Training tokenizer:   2% 171/8047 [04:16<3:20:52,  1.53s/it]loss_total_epoch 15.302359625697136
Training tokenizer:   2% 172/8047 [04:17<3:21:06,  1.53s/it]loss_total_epoch 15.375434078276157
Training tokenizer:   2% 173/8047 [04:19<3:21:19,  1.53s/it]loss_total_epoch 15.436505410820246
Training tokenizer:   2% 174/8047 [04:20<3:21:19,  1.53s/it]loss_total_epoch 15.507722403854132
Training tokenizer:   2% 175/8047 [04:22<3:20:46,  1.53s/it]loss_total_epoch 15.581207182258368
Training tokenizer:   2% 176/8047 [04:23<3:20:38,  1.53s/it]loss_total_epoch 15.647406112402678
Training tokenizer:   2% 177/8047 [04:25<3:27:36,  1.58s/it]loss_total_epoch 15.720082622021437
Training tokenizer:   2% 178/8047 [04:27<3:25:43,  1.57s/it]loss_total_epoch 15.786206182092428
Training tokenizer:   2% 179/8047 [04:28<3:25:29,  1.57s/it]loss_total_epoch 15.874552201479673
Training tokenizer:   2% 180/8047 [04:30<3:23:38,  1.55s/it]loss_total_epoch 15.962721761316061
Training tokenizer:   2% 181/8047 [04:31<3:22:30,  1.54s/it]loss_total_epoch 16.03530490025878
Training tokenizer:   2% 182/8047 [04:33<3:22:23,  1.54s/it]loss_total_epoch 16.108372014015913
Training tokenizer:   2% 183/8047 [04:34<3:22:37,  1.55s/it]loss_total_epoch 16.171946924179792
Training tokenizer:   2% 184/8047 [04:36<3:23:11,  1.55s/it]loss_total_epoch 16.237916003912687
Training tokenizer:   2% 185/8047 [04:37<3:22:24,  1.54s/it]loss_total_epoch 16.31226584687829
Training tokenizer:   2% 186/8047 [04:39<3:22:32,  1.55s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-exdjxq_v'
loss_total_epoch 16.379694562405348
Training tokenizer:   2% 187/8047 [04:41<3:22:07,  1.54s/it]loss_total_epoch 16.437968868762255
Training tokenizer:   2% 188/8047 [04:42<3:22:00,  1.54s/it]loss_total_epoch 16.509971257299185
Training tokenizer:   2% 189/8047 [04:44<3:22:22,  1.55s/it]loss_total_epoch 16.574824143201113
Training tokenizer:   2% 190/8047 [04:45<3:22:13,  1.54s/it]loss_total_epoch 16.654029171913862
Training tokenizer:   2% 191/8047 [04:47<3:22:03,  1.54s/it]loss_total_epoch 16.72226584330201
Training tokenizer:   2% 192/8047 [04:48<3:22:14,  1.54s/it]loss_total_epoch 16.806170392781496
Training tokenizer:   2% 193/8047 [04:50<3:22:00,  1.54s/it]loss_total_epoch 16.877299401909113
Training tokenizer:   2% 194/8047 [04:51<3:22:10,  1.54s/it]loss_total_epoch 16.955035772174597
Training tokenizer:   2% 195/8047 [04:53<3:21:58,  1.54s/it]loss_total_epoch 17.03251926228404
Training tokenizer:   2% 196/8047 [04:54<3:21:55,  1.54s/it]loss_total_epoch 17.093972496688366
Training tokenizer:   2% 197/8047 [04:56<3:22:33,  1.55s/it]loss_total_epoch 17.16660512983799
Training tokenizer:   2% 198/8047 [04:58<3:22:32,  1.55s/it]loss_total_epoch 17.2358073964715
Training tokenizer:   2% 199/8047 [04:59<3:22:59,  1.55s/it]loss_total_epoch 17.30117529630661
Training tokenizer:   2% 200/8047 [05:01<3:22:39,  1.55s/it]loss_total_epoch 17.372233889997005
Training tokenizer:   2% 201/8047 [05:02<3:22:10,  1.55s/it]loss_total_epoch 17.44628855586052
Training tokenizer:   3% 202/8047 [05:04<3:22:23,  1.55s/it]loss_total_epoch 17.511160582304
Training tokenizer:   3% 203/8047 [05:05<3:22:56,  1.55s/it]loss_total_epoch 17.571337319910526
Training tokenizer:   3% 204/8047 [05:07<3:23:13,  1.55s/it]loss_total_epoch 17.646928429603577
Training tokenizer:   3% 205/8047 [05:08<3:23:30,  1.56s/it]loss_total_epoch 17.714455470442772
Training tokenizer:   3% 206/8047 [05:10<3:23:33,  1.56s/it]loss_total_epoch 17.774485856294632
Training tokenizer:   3% 207/8047 [05:12<3:23:43,  1.56s/it]loss_total_epoch 17.84270030260086
Training tokenizer:   3% 208/8047 [05:13<3:23:21,  1.56s/it]loss_total_epoch 17.919415175914764
Training tokenizer:   3% 209/8047 [05:15<3:23:21,  1.56s/it]loss_total_epoch 17.994890369474888
Training tokenizer:   3% 210/8047 [05:16<3:23:29,  1.56s/it]loss_total_epoch 18.07222879678011
Training tokenizer:   3% 211/8047 [05:18<3:23:54,  1.56s/it]loss_total_epoch 18.14578042924404
Training tokenizer:   3% 212/8047 [05:19<3:24:15,  1.56s/it]loss_total_epoch 18.216368064284325
Training tokenizer:   3% 213/8047 [05:21<3:23:47,  1.56s/it]loss_total_epoch 18.284144088625908
Training tokenizer:   3% 214/8047 [05:22<3:23:23,  1.56s/it]loss_total_epoch 18.355574056506157
Training tokenizer:   3% 215/8047 [05:24<3:23:08,  1.56s/it]loss_total_epoch 18.433980777859688
Training tokenizer:   3% 216/8047 [05:26<3:22:20,  1.55s/it]loss_total_epoch 18.503446087241173
Training tokenizer:   3% 217/8047 [05:27<3:22:25,  1.55s/it]loss_total_epoch 18.567122749984264
Training tokenizer:   3% 218/8047 [05:29<3:22:15,  1.55s/it]loss_total_epoch 18.61742564290762
Training tokenizer:   3% 219/8047 [05:30<3:22:10,  1.55s/it]loss_total_epoch 18.693009167909622
Training tokenizer:   3% 220/8047 [05:32<3:22:24,  1.55s/it]loss_total_epoch 18.764816641807556
Training tokenizer:   3% 221/8047 [05:33<3:22:30,  1.55s/it]loss_total_epoch 18.827345296740532
Training tokenizer:   3% 222/8047 [05:35<3:23:17,  1.56s/it]loss_total_epoch 18.90285937488079
Training tokenizer:   3% 223/8047 [05:36<3:23:26,  1.56s/it]loss_total_epoch 18.975072875618935
Training tokenizer:   3% 224/8047 [05:38<3:23:13,  1.56s/it]loss_total_epoch 19.036970861256123
Training tokenizer:   3% 225/8047 [05:40<3:24:11,  1.57s/it]loss_total_epoch 19.11400316655636
Training tokenizer:   3% 226/8047 [05:41<3:24:13,  1.57s/it]loss_total_epoch 19.190962478518486
Training tokenizer:   3% 227/8047 [05:43<3:23:44,  1.56s/it]loss_total_epoch 19.26958639919758
Training tokenizer:   3% 228/8047 [05:44<3:24:20,  1.57s/it]loss_total_epoch 19.323659617453814
Training tokenizer:   3% 229/8047 [05:46<3:24:41,  1.57s/it]loss_total_epoch 19.394267056137323
Training tokenizer:   3% 230/8047 [05:47<3:24:15,  1.57s/it]loss_total_epoch 19.479069519788027
Training tokenizer:   3% 231/8047 [05:49<3:24:34,  1.57s/it]loss_total_epoch 19.552998173981905
Training tokenizer:   3% 232/8047 [05:51<3:24:54,  1.57s/it]loss_total_epoch 19.616828348487616
Training tokenizer:   3% 233/8047 [05:52<3:24:50,  1.57s/it]loss_total_epoch 19.686819825321436
Training tokenizer:   3% 234/8047 [05:54<3:24:03,  1.57s/it]loss_total_epoch 19.75116639956832
Training tokenizer:   3% 235/8047 [05:55<3:23:46,  1.57s/it]loss_total_epoch 19.835277181118727
Training tokenizer:   3% 236/8047 [05:57<3:24:11,  1.57s/it]loss_total_epoch 19.92156995460391
Training tokenizer:   3% 237/8047 [05:58<3:23:55,  1.57s/it]loss_total_epoch 20.000656884163618
Training tokenizer:   3% 238/8047 [06:00<3:24:05,  1.57s/it]loss_total_epoch 20.064722191542387
Training tokenizer:   3% 239/8047 [06:02<3:24:29,  1.57s/it]loss_total_epoch 20.137859981507063
Training tokenizer:   3% 240/8047 [06:03<3:25:03,  1.58s/it]loss_total_epoch 20.212348636239767
Training tokenizer:   3% 241/8047 [06:05<3:25:43,  1.58s/it]loss_total_epoch 20.277201484888792
Training tokenizer:   3% 242/8047 [06:06<3:24:51,  1.57s/it]loss_total_epoch 20.348096508532763
Training tokenizer:   3% 243/8047 [06:08<3:24:56,  1.58s/it]loss_total_epoch 20.43113337829709
Training tokenizer:   3% 244/8047 [06:09<3:25:08,  1.58s/it]loss_total_epoch 20.509440798312426
Training tokenizer:   3% 245/8047 [06:11<3:24:28,  1.57s/it]loss_total_epoch 20.575509060174227
Training tokenizer:   3% 246/8047 [06:13<3:24:04,  1.57s/it]loss_total_epoch 20.63567978143692
Training tokenizer:   3% 247/8047 [06:14<3:24:19,  1.57s/it]loss_total_epoch 20.71662998944521
Training tokenizer:   3% 248/8047 [06:16<3:24:43,  1.58s/it]loss_total_epoch 20.779866561293602
Training tokenizer:   3% 249/8047 [06:17<3:24:29,  1.57s/it]loss_total_epoch 20.84476823359728
Training tokenizer:   3% 250/8047 [06:19<3:23:47,  1.57s/it]loss_total_epoch 20.92836643010378
Training tokenizer:   3% 251/8047 [06:20<3:23:55,  1.57s/it]loss_total_epoch 21.00434833019972
Training tokenizer:   3% 252/8047 [06:22<3:23:35,  1.57s/it]loss_total_epoch 21.072313383221626
Training tokenizer:   3% 253/8047 [06:24<3:23:25,  1.57s/it]loss_total_epoch 21.13902013003826
Training tokenizer:   3% 254/8047 [06:25<3:23:03,  1.56s/it]loss_total_epoch 21.213762253522873
Training tokenizer:   3% 255/8047 [06:27<3:23:20,  1.57s/it]loss_total_epoch 21.28709538280964
Training tokenizer:   3% 256/8047 [06:28<3:23:33,  1.57s/it]loss_total_epoch 21.352467708289623
Training tokenizer:   3% 257/8047 [06:30<3:23:58,  1.57s/it]loss_total_epoch 21.410440251231194
Training tokenizer:   3% 258/8047 [06:31<3:24:21,  1.57s/it]loss_total_epoch 21.45450257509947
Training tokenizer:   3% 259/8047 [06:33<3:24:36,  1.58s/it]loss_total_epoch 21.516647908836603
Training tokenizer:   3% 260/8047 [06:35<3:24:42,  1.58s/it]loss_total_epoch 21.589111004024744
Training tokenizer:   3% 261/8047 [06:36<3:24:36,  1.58s/it]loss_total_epoch 21.66886407509446
Training tokenizer:   3% 262/8047 [06:38<3:24:24,  1.58s/it]loss_total_epoch 21.736886773258448
Training tokenizer:   3% 263/8047 [06:39<3:24:34,  1.58s/it]loss_total_epoch 21.822759073227644
Training tokenizer:   3% 264/8047 [06:41<3:25:23,  1.58s/it]loss_total_epoch 21.899995002895594
Training tokenizer:   3% 265/8047 [06:42<3:25:34,  1.59s/it]loss_total_epoch 21.97041431441903
Training tokenizer:   3% 266/8047 [06:44<3:26:09,  1.59s/it]loss_total_epoch 22.047206301242113
Training tokenizer:   3% 267/8047 [06:46<3:26:03,  1.59s/it]loss_total_epoch 22.110212352126837
Training tokenizer:   3% 268/8047 [06:47<3:25:27,  1.58s/it]loss_total_epoch 22.191520500928164
Training tokenizer:   3% 269/8047 [06:49<3:25:43,  1.59s/it]loss_total_epoch 22.273373167961836
Training tokenizer:   3% 270/8047 [06:50<3:26:16,  1.59s/it]loss_total_epoch 22.342702616006136
Training tokenizer:   3% 271/8047 [06:52<3:26:13,  1.59s/it]loss_total_epoch 22.420491311699152
Training tokenizer:   3% 272/8047 [06:54<3:25:44,  1.59s/it]loss_total_epoch 22.46460171416402
Training tokenizer:   3% 273/8047 [06:55<3:25:33,  1.59s/it]loss_total_epoch 22.537716943770647
Training tokenizer:   3% 274/8047 [06:57<3:25:30,  1.59s/it]loss_total_epoch 22.617477659136057
Training tokenizer:   3% 275/8047 [06:58<3:25:47,  1.59s/it]loss_total_epoch 22.685343336313963
Training tokenizer:   3% 276/8047 [07:00<3:25:37,  1.59s/it]loss_total_epoch 22.769567023962736
Training tokenizer:   3% 277/8047 [07:02<3:25:59,  1.59s/it]loss_total_epoch 22.838201958686113
Training tokenizer:   3% 278/8047 [07:03<3:25:53,  1.59s/it]loss_total_epoch 22.90650551393628
Training tokenizer:   3% 279/8047 [07:05<3:25:56,  1.59s/it]loss_total_epoch 22.982372265309095
Training tokenizer:   3% 280/8047 [07:06<3:25:41,  1.59s/it]loss_total_epoch 23.05453071370721
Training tokenizer:   3% 281/8047 [07:08<3:25:36,  1.59s/it]loss_total_epoch 23.112961132079363
Training tokenizer:   4% 282/8047 [07:10<3:26:14,  1.59s/it]loss_total_epoch 23.181369591504335
Training tokenizer:   4% 283/8047 [07:11<3:26:30,  1.60s/it]loss_total_epoch 23.25145697966218
Training tokenizer:   4% 284/8047 [07:13<3:26:37,  1.60s/it]loss_total_epoch 23.300613917410374
Training tokenizer:   4% 285/8047 [07:14<3:26:48,  1.60s/it]loss_total_epoch 23.36520018428564
Training tokenizer:   4% 286/8047 [07:16<3:26:15,  1.59s/it]loss_total_epoch 23.427551686763763
Training tokenizer:   4% 287/8047 [07:17<3:25:44,  1.59s/it]loss_total_epoch 23.50618651509285
Training tokenizer:   4% 288/8047 [07:19<3:24:53,  1.58s/it]loss_total_epoch 23.57130406051874
Training tokenizer:   4% 289/8047 [07:21<3:24:56,  1.58s/it]loss_total_epoch 23.643501691520214
Training tokenizer:   4% 290/8047 [07:22<3:25:15,  1.59s/it]loss_total_epoch 23.71833698451519
Training tokenizer:   4% 291/8047 [07:24<3:25:18,  1.59s/it]loss_total_epoch 23.792419753968716
Training tokenizer:   4% 292/8047 [07:25<3:25:00,  1.59s/it]loss_total_epoch 23.847463451325893
Training tokenizer:   4% 293/8047 [07:27<3:25:02,  1.59s/it]loss_total_epoch 23.901420954614878
Training tokenizer:   4% 294/8047 [07:29<3:25:32,  1.59s/it]loss_total_epoch 23.976922903209925
Training tokenizer:   4% 295/8047 [07:30<3:25:17,  1.59s/it]loss_total_epoch 24.051127325743437
Training tokenizer:   4% 296/8047 [07:32<3:26:01,  1.59s/it]loss_total_epoch 24.134388457983732
Training tokenizer:   4% 297/8047 [07:33<3:26:47,  1.60s/it]loss_total_epoch 24.204976823180914
Training tokenizer:   4% 298/8047 [07:35<3:26:23,  1.60s/it]loss_total_epoch 24.278425734490156
Training tokenizer:   4% 299/8047 [07:37<3:26:39,  1.60s/it]loss_total_epoch 24.346739273518324
Training tokenizer:   4% 300/8047 [07:38<3:27:34,  1.61s/it]loss_total_epoch 24.410405289381742
Training tokenizer:   4% 301/8047 [07:40<3:27:14,  1.61s/it]loss_total_epoch 24.483956467360258
Training tokenizer:   4% 302/8047 [07:41<3:27:15,  1.61s/it]loss_total_epoch 24.556634318083525
Training tokenizer:   4% 303/8047 [07:43<3:26:29,  1.60s/it]loss_total_epoch 24.64158957824111
Training tokenizer:   4% 304/8047 [07:45<3:26:03,  1.60s/it]loss_total_epoch 24.70766545459628
Training tokenizer:   4% 305/8047 [07:46<3:25:54,  1.60s/it]loss_total_epoch 24.77083607390523
Training tokenizer:   4% 306/8047 [07:48<3:26:26,  1.60s/it]loss_total_epoch 24.835356924682856
Training tokenizer:   4% 307/8047 [07:49<3:26:44,  1.60s/it]loss_total_epoch 24.8914281912148
Training tokenizer:   4% 308/8047 [07:51<3:26:49,  1.60s/it]loss_total_epoch 24.956046249717474
Training tokenizer:   4% 309/8047 [07:53<3:27:13,  1.61s/it]loss_total_epoch 25.017296444624662
Training tokenizer:   4% 310/8047 [07:54<3:27:36,  1.61s/it]loss_total_epoch 25.094918187707663
Training tokenizer:   4% 311/8047 [07:56<3:27:41,  1.61s/it]loss_total_epoch 25.166670259088278
Training tokenizer:   4% 312/8047 [07:57<3:27:51,  1.61s/it]loss_total_epoch 25.245496418327093
Training tokenizer:   4% 313/8047 [07:59<3:27:30,  1.61s/it]loss_total_epoch 25.32246983423829
Training tokenizer:   4% 314/8047 [08:01<3:26:34,  1.60s/it]loss_total_epoch 25.39329281076789
Training tokenizer:   4% 315/8047 [08:02<3:27:12,  1.61s/it]loss_total_epoch 25.461935129016638
Training tokenizer:   4% 316/8047 [08:04<3:27:28,  1.61s/it]loss_total_epoch 25.53569160029292
Training tokenizer:   4% 317/8047 [08:06<3:27:38,  1.61s/it]loss_total_epoch 25.60298018530011
Training tokenizer:   4% 318/8047 [08:07<3:27:41,  1.61s/it]loss_total_epoch 25.6693198941648
Training tokenizer:   4% 319/8047 [08:09<3:28:10,  1.62s/it]loss_total_epoch 25.74860930070281
Training tokenizer:   4% 320/8047 [08:10<3:28:34,  1.62s/it]loss_total_epoch 25.82004740461707
Training tokenizer:   4% 321/8047 [08:12<3:28:27,  1.62s/it]loss_total_epoch 25.877818502485752
Training tokenizer:   4% 322/8047 [08:14<3:28:42,  1.62s/it]loss_total_epoch 25.96320653706789
Training tokenizer:   4% 323/8047 [08:15<3:28:08,  1.62s/it]loss_total_epoch 26.041703067719936
Training tokenizer:   4% 324/8047 [08:17<3:27:56,  1.62s/it]loss_total_epoch 26.118373848497868
Training tokenizer:   4% 325/8047 [08:18<3:27:40,  1.61s/it]loss_total_epoch 26.180697422474623
Training tokenizer:   4% 326/8047 [08:20<3:27:21,  1.61s/it]loss_total_epoch 26.257492143660784
Training tokenizer:   4% 327/8047 [08:22<3:27:52,  1.62s/it]loss_total_epoch 26.33129570260644
Training tokenizer:   4% 328/8047 [08:23<3:28:19,  1.62s/it]loss_total_epoch 26.395232927054167
Training tokenizer:   4% 329/8047 [08:25<3:27:57,  1.62s/it]loss_total_epoch 26.450020786374807
Training tokenizer:   4% 330/8047 [08:27<3:28:19,  1.62s/it]loss_total_epoch 26.512038256973028
Training tokenizer:   4% 331/8047 [08:28<3:29:08,  1.63s/it]loss_total_epoch 26.582108091562986
Training tokenizer:   4% 332/8047 [08:30<3:28:24,  1.62s/it]loss_total_epoch 26.65321872755885
Training tokenizer:   4% 333/8047 [08:31<3:28:26,  1.62s/it]loss_total_epoch 26.71829469129443
Training tokenizer:   4% 334/8047 [08:33<3:28:17,  1.62s/it]loss_total_epoch 26.794344935566187
Training tokenizer:   4% 335/8047 [08:35<3:28:18,  1.62s/it]loss_total_epoch 26.86231603100896
Training tokenizer:   4% 336/8047 [08:36<3:27:54,  1.62s/it]loss_total_epoch 26.92498493567109
Training tokenizer:   4% 337/8047 [08:38<3:27:48,  1.62s/it]loss_total_epoch 26.989844102412462
Training tokenizer:   4% 338/8047 [08:40<3:28:37,  1.62s/it]loss_total_epoch 27.065558206290007
Training tokenizer:   4% 339/8047 [08:41<3:28:44,  1.62s/it]loss_total_epoch 27.128358658403158
Training tokenizer:   4% 340/8047 [08:43<3:28:35,  1.62s/it]loss_total_epoch 27.187388133257627
Training tokenizer:   4% 341/8047 [08:44<3:28:21,  1.62s/it]loss_total_epoch 27.262145962566137
Training tokenizer:   4% 342/8047 [08:46<3:28:49,  1.63s/it]loss_total_epoch 27.33025887235999
Training tokenizer:   4% 343/8047 [08:48<3:29:21,  1.63s/it]loss_total_epoch 27.40003488585353
Training tokenizer:   4% 344/8047 [08:49<3:29:20,  1.63s/it]loss_total_epoch 27.461931139230728
Training tokenizer:   4% 345/8047 [08:51<3:30:10,  1.64s/it]loss_total_epoch 27.53858219832182
Training tokenizer:   4% 346/8047 [08:53<3:29:46,  1.63s/it]loss_total_epoch 27.588243931531906
Training tokenizer:   4% 347/8047 [08:54<3:28:45,  1.63s/it]loss_total_epoch 27.651726618409157
Training tokenizer:   4% 348/8047 [08:56<3:28:07,  1.62s/it]loss_total_epoch 27.723497167229652
Training tokenizer:   4% 349/8047 [08:57<3:29:28,  1.63s/it]loss_total_epoch 27.79649044573307
Training tokenizer:   4% 350/8047 [08:59<3:29:35,  1.63s/it]loss_total_epoch 27.872469320893288
Training tokenizer:   4% 351/8047 [09:01<3:29:27,  1.63s/it]loss_total_epoch 27.934690918773413
Training tokenizer:   4% 352/8047 [09:02<3:28:50,  1.63s/it]loss_total_epoch 28.0115358941257
Training tokenizer:   4% 353/8047 [09:04<3:27:57,  1.62s/it]loss_total_epoch 28.0800914876163
Training tokenizer:   4% 354/8047 [09:06<3:28:11,  1.62s/it]loss_total_epoch 28.157481875270605
Training tokenizer:   4% 355/8047 [09:07<3:28:40,  1.63s/it]loss_total_epoch 28.22335324063897
Training tokenizer:   4% 356/8047 [09:09<3:28:38,  1.63s/it]loss_total_epoch 28.283516198396683
Training tokenizer:   4% 357/8047 [09:10<3:28:17,  1.63s/it]loss_total_epoch 28.3629712164402
Training tokenizer:   4% 358/8047 [09:12<3:28:37,  1.63s/it]loss_total_epoch 28.433168694376945
Training tokenizer:   4% 359/8047 [09:14<3:28:07,  1.62s/it]loss_total_epoch 28.494295109063387
Training tokenizer:   4% 360/8047 [09:16<3:36:44,  1.69s/it]loss_total_epoch 28.56629839912057
Training tokenizer:   4% 361/8047 [09:17<3:34:24,  1.67s/it]loss_total_epoch 28.616189621388912
Training tokenizer:   4% 362/8047 [09:19<3:33:01,  1.66s/it]loss_total_epoch 28.694378592073917
Training tokenizer:   5% 363/8047 [09:20<3:31:00,  1.65s/it]loss_total_epoch 28.765140905976295
Training tokenizer:   5% 364/8047 [09:22<3:29:58,  1.64s/it]loss_total_epoch 28.837796434760094
Training tokenizer:   5% 365/8047 [09:24<3:30:34,  1.64s/it]loss_total_epoch 28.91233652830124
Training tokenizer:   5% 366/8047 [09:25<3:30:18,  1.64s/it]loss_total_epoch 28.971996154636145
Training tokenizer:   5% 367/8047 [09:27<3:30:45,  1.65s/it]loss_total_epoch 29.05049542710185
Training tokenizer:   5% 368/8047 [09:29<3:30:32,  1.65s/it]loss_total_epoch 29.11888935044408
Training tokenizer:   5% 369/8047 [09:30<3:29:48,  1.64s/it]loss_total_epoch 29.19166001304984
Training tokenizer:   5% 370/8047 [09:32<3:30:04,  1.64s/it]loss_total_epoch 29.26686656102538
Training tokenizer:   5% 371/8047 [09:34<3:30:33,  1.65s/it]loss_total_epoch 29.33709731325507
Training tokenizer:   5% 372/8047 [09:35<3:30:12,  1.64s/it]loss_total_epoch 29.41634763404727
Training tokenizer:   5% 373/8047 [09:37<3:30:00,  1.64s/it]loss_total_epoch 29.482575479894876
Training tokenizer:   5% 374/8047 [09:39<3:30:06,  1.64s/it]loss_total_epoch 29.559942726045847
Training tokenizer:   5% 375/8047 [09:40<3:30:31,  1.65s/it]loss_total_epoch 29.621319729834795
Training tokenizer:   5% 376/8047 [09:42<3:30:00,  1.64s/it]loss_total_epoch 29.69241878017783
Training tokenizer:   5% 377/8047 [09:43<3:29:25,  1.64s/it]loss_total_epoch 29.762870755046606
Training tokenizer:   5% 378/8047 [09:45<3:30:10,  1.64s/it]loss_total_epoch 29.81638276576996
Training tokenizer:   5% 379/8047 [09:47<3:30:17,  1.65s/it]loss_total_epoch 29.882078871130943
Training tokenizer:   5% 380/8047 [09:48<3:30:25,  1.65s/it]loss_total_epoch 29.948701441287994
Training tokenizer:   5% 381/8047 [09:50<3:30:37,  1.65s/it]loss_total_epoch 30.01337229460478
Training tokenizer:   5% 382/8047 [09:52<3:30:24,  1.65s/it]loss_total_epoch 30.077932946383953
Training tokenizer:   5% 383/8047 [09:53<3:29:19,  1.64s/it]loss_total_epoch 30.14996798336506
Training tokenizer:   5% 384/8047 [09:55<3:28:51,  1.64s/it]loss_total_epoch 30.18462362140417
Training tokenizer:   5% 385/8047 [09:57<3:29:43,  1.64s/it]loss_total_epoch 30.248391896486282
Training tokenizer:   5% 386/8047 [09:58<3:29:58,  1.64s/it]loss_total_epoch 30.307590764015913
Training tokenizer:   5% 387/8047 [10:00<3:30:39,  1.65s/it]loss_total_epoch 30.38274286314845
Training tokenizer:   5% 388/8047 [10:02<3:30:13,  1.65s/it]loss_total_epoch 30.44935340806842
Training tokenizer:   5% 389/8047 [10:03<3:30:11,  1.65s/it]loss_total_epoch 30.522795129567385
Training tokenizer:   5% 390/8047 [10:05<3:31:09,  1.65s/it]loss_total_epoch 30.598011892288923
Training tokenizer:   5% 391/8047 [10:06<3:29:58,  1.65s/it]loss_total_epoch 30.66980964317918
Training tokenizer:   5% 392/8047 [10:08<3:29:44,  1.64s/it]loss_total_epoch 30.742013338953257
Training tokenizer:   5% 393/8047 [10:10<3:30:23,  1.65s/it]loss_total_epoch 30.803213518112898
Training tokenizer:   5% 394/8047 [10:11<3:30:31,  1.65s/it]loss_total_epoch 30.8645047172904
Training tokenizer:   5% 395/8047 [10:13<3:31:18,  1.66s/it]loss_total_epoch 30.928434044122696
Training tokenizer:   5% 396/8047 [10:15<3:30:59,  1.65s/it]loss_total_epoch 30.983375571668148
Training tokenizer:   5% 397/8047 [10:16<3:30:54,  1.65s/it]loss_total_epoch 31.0499572083354
Training tokenizer:   5% 398/8047 [10:18<3:30:38,  1.65s/it]loss_total_epoch 31.122739516198635
Training tokenizer:   5% 399/8047 [10:20<3:30:32,  1.65s/it]loss_total_epoch 31.18461660668254
Training tokenizer:   5% 400/8047 [10:21<3:30:28,  1.65s/it]loss_total_epoch 31.23513962700963
Training tokenizer:   5% 401/8047 [10:23<3:30:40,  1.65s/it]loss_total_epoch 31.287368681281805
Training tokenizer:   5% 402/8047 [10:25<3:30:35,  1.65s/it]loss_total_epoch 31.357115674763918
Training tokenizer:   5% 403/8047 [10:26<3:30:16,  1.65s/it]loss_total_epoch 31.4163013510406
Training tokenizer:   5% 404/8047 [10:28<3:29:15,  1.64s/it]loss_total_epoch 31.4992532171309
Training tokenizer:   5% 405/8047 [10:30<3:29:08,  1.64s/it]loss_total_epoch 31.562041234225035
Training tokenizer:   5% 406/8047 [10:31<3:29:42,  1.65s/it]loss_total_epoch 31.6301794834435
Training tokenizer:   5% 407/8047 [10:33<3:29:44,  1.65s/it]loss_total_epoch 31.695045616477728
Training tokenizer:   5% 408/8047 [10:35<3:29:24,  1.64s/it]loss_total_epoch 31.773779820650816
Training tokenizer:   5% 409/8047 [10:36<3:30:08,  1.65s/it]loss_total_epoch 31.856021154671907
Training tokenizer:   5% 410/8047 [10:38<3:31:23,  1.66s/it]loss_total_epoch 31.92229575291276
Training tokenizer:   5% 411/8047 [10:40<3:31:38,  1.66s/it]loss_total_epoch 31.993062894791365
Training tokenizer:   5% 412/8047 [10:41<3:31:38,  1.66s/it]loss_total_epoch 32.06996775791049
Training tokenizer:   5% 413/8047 [10:43<3:30:49,  1.66s/it]loss_total_epoch 32.141639698296785
Training tokenizer:   5% 414/8047 [10:44<3:29:52,  1.65s/it]loss_total_epoch 32.2014757655561
Training tokenizer:   5% 415/8047 [10:46<3:33:15,  1.68s/it]loss_total_epoch 32.268667709082365
Training tokenizer:   5% 416/8047 [10:48<3:32:02,  1.67s/it]loss_total_epoch 32.322231125086546
Training tokenizer:   5% 417/8047 [10:50<3:31:25,  1.66s/it]loss_total_epoch 32.38452050462365
Training tokenizer:   5% 418/8047 [10:51<3:31:30,  1.66s/it]loss_total_epoch 32.44864046946168
Training tokenizer:   5% 419/8047 [10:53<3:31:37,  1.66s/it]loss_total_epoch 32.523195903748274
Training tokenizer:   5% 420/8047 [10:55<3:32:04,  1.67s/it]loss_total_epoch 32.601281110197306
Training tokenizer:   5% 421/8047 [10:56<3:32:26,  1.67s/it]loss_total_epoch 32.6814212910831
Training tokenizer:   5% 422/8047 [10:58<3:32:13,  1.67s/it]loss_total_epoch 32.747983667999506
Training tokenizer:   5% 423/8047 [11:00<3:31:02,  1.66s/it]loss_total_epoch 32.82305500283837
Training tokenizer:   5% 424/8047 [11:01<3:30:44,  1.66s/it]loss_total_epoch 32.89418679103255
Training tokenizer:   5% 425/8047 [11:03<3:30:58,  1.66s/it]loss_total_epoch 32.96667819097638
Training tokenizer:   5% 426/8047 [11:05<3:31:13,  1.66s/it]loss_total_epoch 33.03083308413625
Training tokenizer:   5% 427/8047 [11:06<3:31:16,  1.66s/it]loss_total_epoch 33.105257000774145
Training tokenizer:   5% 428/8047 [11:08<3:31:33,  1.67s/it]loss_total_epoch 33.165090668946505
Training tokenizer:   5% 429/8047 [11:09<3:30:55,  1.66s/it]loss_total_epoch 33.23272966220975
Training tokenizer:   5% 430/8047 [11:11<3:31:59,  1.67s/it]loss_total_epoch 33.29322386160493
Training tokenizer:   5% 431/8047 [11:13<3:31:42,  1.67s/it]loss_total_epoch 33.36237204447389
Training tokenizer:   5% 432/8047 [11:15<3:31:17,  1.66s/it]loss_total_epoch 33.43889486417174
Training tokenizer:   5% 433/8047 [11:16<3:32:10,  1.67s/it]loss_total_epoch 33.51763976737857
Training tokenizer:   5% 434/8047 [11:18<3:32:06,  1.67s/it]loss_total_epoch 33.580257158726454
Training tokenizer:   5% 435/8047 [11:20<3:31:22,  1.67s/it]loss_total_epoch 33.65574662014842
Training tokenizer:   5% 436/8047 [11:21<3:32:21,  1.67s/it]loss_total_epoch 33.72819375619292
Training tokenizer:   5% 437/8047 [11:23<3:32:35,  1.68s/it]loss_total_epoch 33.794873397797346
Training tokenizer:   5% 438/8047 [11:25<3:32:02,  1.67s/it]loss_total_epoch 33.86438385769725
Training tokenizer:   5% 439/8047 [11:26<3:31:14,  1.67s/it]loss_total_epoch 33.92042867094278
Training tokenizer:   5% 440/8047 [11:28<3:31:17,  1.67s/it]loss_total_epoch 33.99879324436188
Training tokenizer:   5% 441/8047 [11:30<3:31:28,  1.67s/it]loss_total_epoch 34.07559923827648
Training tokenizer:   5% 442/8047 [11:31<3:30:44,  1.66s/it]loss_total_epoch 34.135188437998295
Training tokenizer:   6% 443/8047 [11:33<3:30:40,  1.66s/it]loss_total_epoch 34.186741191893816
Training tokenizer:   6% 444/8047 [11:35<3:31:33,  1.67s/it]loss_total_epoch 34.258362617343664
Training tokenizer:   6% 445/8047 [11:36<3:31:51,  1.67s/it]loss_total_epoch 34.32564189657569
Training tokenizer:   6% 446/8047 [11:38<3:31:46,  1.67s/it]loss_total_epoch 34.39020383730531
Training tokenizer:   6% 447/8047 [11:40<3:31:33,  1.67s/it]loss_total_epoch 34.46539254114032
Training tokenizer:   6% 448/8047 [11:41<3:30:51,  1.66s/it]loss_total_epoch 34.52935357764363
Training tokenizer:   6% 449/8047 [11:43<3:30:38,  1.66s/it]loss_total_epoch 34.60078201815486
Training tokenizer:   6% 450/8047 [11:45<3:31:19,  1.67s/it]loss_total_epoch 34.66493597254157
Training tokenizer:   6% 451/8047 [11:46<3:31:11,  1.67s/it]loss_total_epoch 34.73789903149009
Training tokenizer:   6% 452/8047 [11:48<3:31:52,  1.67s/it]loss_total_epoch 34.810435350984335
Training tokenizer:   6% 453/8047 [11:50<3:32:30,  1.68s/it]loss_total_epoch 34.87629185244441
Training tokenizer:   6% 454/8047 [11:51<3:31:27,  1.67s/it]loss_total_epoch 34.9373618401587
Training tokenizer:   6% 455/8047 [11:53<3:31:13,  1.67s/it]loss_total_epoch 35.00668293610215
Training tokenizer:   6% 456/8047 [11:55<3:32:03,  1.68s/it]loss_total_epoch 35.082514237612486
Training tokenizer:   6% 457/8047 [11:56<3:31:49,  1.67s/it]loss_total_epoch 35.15499248728156
Training tokenizer:   6% 458/8047 [11:58<3:31:28,  1.67s/it]loss_total_epoch 35.22205542400479
Training tokenizer:   6% 459/8047 [12:00<3:34:50,  1.70s/it]loss_total_epoch 35.289917919784784
Training tokenizer:   6% 460/8047 [12:01<3:34:56,  1.70s/it]loss_total_epoch 35.36889621987939
Training tokenizer:   6% 461/8047 [12:03<3:33:52,  1.69s/it]loss_total_epoch 35.444965954869986
Training tokenizer:   6% 462/8047 [12:05<3:33:01,  1.69s/it]loss_total_epoch 35.5014631152153
Training tokenizer:   6% 463/8047 [12:06<3:32:09,  1.68s/it]loss_total_epoch 35.566944137215614
Training tokenizer:   6% 464/8047 [12:08<3:31:29,  1.67s/it]loss_total_epoch 35.63892711699009
Training tokenizer:   6% 465/8047 [12:10<3:31:55,  1.68s/it]loss_total_epoch 35.720194950699806
Training tokenizer:   6% 466/8047 [12:11<3:32:04,  1.68s/it]loss_total_epoch 35.792238883674145
Training tokenizer:   6% 467/8047 [12:13<3:31:37,  1.68s/it]loss_total_epoch 35.857806622982025
Training tokenizer:   6% 468/8047 [12:15<3:31:36,  1.68s/it]loss_total_epoch 35.9172546453774
Training tokenizer:   6% 469/8047 [12:17<3:45:53,  1.79s/it]loss_total_epoch 35.99376206472516
Training tokenizer:   6% 470/8047 [12:19<3:42:38,  1.76s/it]loss_total_epoch 36.044763933867216
Training tokenizer:   6% 471/8047 [12:20<3:39:19,  1.74s/it]loss_total_epoch 36.118217188864946
Training tokenizer:   6% 472/8047 [12:22<3:36:39,  1.72s/it]loss_total_epoch 36.18402649834752
Training tokenizer:   6% 473/8047 [12:24<3:35:59,  1.71s/it]loss_total_epoch 36.26282276585698
Training tokenizer:   6% 474/8047 [12:25<3:34:42,  1.70s/it]loss_total_epoch 36.33065838739276
Training tokenizer:   6% 475/8047 [12:27<3:34:26,  1.70s/it]loss_total_epoch 36.394031155854464
Training tokenizer:   6% 476/8047 [12:29<3:34:06,  1.70s/it]loss_total_epoch 36.468828562647104
Training tokenizer:   6% 477/8047 [12:30<3:34:05,  1.70s/it]loss_total_epoch 36.52616365626454
Training tokenizer:   6% 478/8047 [12:32<3:34:19,  1.70s/it]loss_total_epoch 36.60560827329755
Training tokenizer:   6% 479/8047 [12:34<3:34:21,  1.70s/it]loss_total_epoch 36.67654945328832
Training tokenizer:   6% 480/8047 [12:35<3:34:37,  1.70s/it]loss_total_epoch 36.74707989022136
Training tokenizer:   6% 481/8047 [12:37<3:34:37,  1.70s/it]loss_total_epoch 36.801124449819326
Training tokenizer:   6% 482/8047 [12:39<3:34:29,  1.70s/it]loss_total_epoch 36.866311106830835
Training tokenizer:   6% 483/8047 [12:41<3:34:35,  1.70s/it]loss_total_epoch 36.9411028586328
Training tokenizer:   6% 484/8047 [12:42<3:33:46,  1.70s/it]loss_total_epoch 37.01187859848142
Training tokenizer:   6% 485/8047 [12:44<3:34:29,  1.70s/it]loss_total_epoch 37.0866246111691
Training tokenizer:   6% 486/8047 [12:46<3:34:27,  1.70s/it]loss_total_epoch 37.15857193246484
Training tokenizer:   6% 487/8047 [12:47<3:34:33,  1.70s/it]loss_total_epoch 37.23011476173997
Training tokenizer:   6% 488/8047 [12:49<3:34:16,  1.70s/it]loss_total_epoch 37.28928506374359
Training tokenizer:   6% 489/8047 [12:51<3:34:02,  1.70s/it]loss_total_epoch 37.34683418646455
Training tokenizer:   6% 490/8047 [12:53<3:46:07,  1.80s/it]loss_total_epoch 37.41137512400746
Training tokenizer:   6% 491/8047 [12:55<3:43:27,  1.77s/it]loss_total_epoch 37.47825048491359
Training tokenizer:   6% 492/8047 [12:56<3:40:16,  1.75s/it]loss_total_epoch 37.54511355981231
Training tokenizer:   6% 493/8047 [12:58<3:37:46,  1.73s/it]loss_total_epoch 37.61559661850333
Training tokenizer:   6% 494/8047 [13:00<3:36:14,  1.72s/it]loss_total_epoch 37.68158157542348
Training tokenizer:   6% 495/8047 [13:01<3:35:33,  1.71s/it]loss_total_epoch 37.737735360860825
Training tokenizer:   6% 496/8047 [13:03<3:35:01,  1.71s/it]loss_total_epoch 37.790778677910566
Training tokenizer:   6% 497/8047 [13:05<3:35:21,  1.71s/it]loss_total_epoch 37.85805115476251
Training tokenizer:   6% 498/8047 [13:06<3:33:44,  1.70s/it]loss_total_epoch 37.922182995826006
Training tokenizer:   6% 499/8047 [13:08<3:32:55,  1.69s/it]loss_total_epoch 37.97636003047228
Training tokenizer:   6% 500/8047 [13:10<3:32:55,  1.69s/it]loss_total_epoch 38.05139956623316
Training tokenizer:   6% 501/8047 [13:11<3:32:39,  1.69s/it]loss_total_epoch 38.12405254691839
Training tokenizer:   6% 502/8047 [13:13<3:32:42,  1.69s/it]loss_total_epoch 38.19812749326229
Training tokenizer:   6% 503/8047 [13:15<3:32:56,  1.69s/it]loss_total_epoch 38.27253298461437
Training tokenizer:   6% 504/8047 [13:17<3:34:25,  1.71s/it]loss_total_epoch 38.34709745645523
Training tokenizer:   6% 505/8047 [13:18<3:33:59,  1.70s/it]loss_total_epoch 38.42386759072542
Training tokenizer:   6% 506/8047 [13:20<3:32:59,  1.69s/it]loss_total_epoch 38.48601298779249
Training tokenizer:   6% 507/8047 [13:22<3:33:40,  1.70s/it]loss_total_epoch 38.542134176939726
Training tokenizer:   6% 508/8047 [13:23<3:33:54,  1.70s/it]loss_total_epoch 38.61232927069068
Training tokenizer:   6% 509/8047 [13:25<3:34:16,  1.71s/it]loss_total_epoch 38.680819656699896
Training tokenizer:   6% 510/8047 [13:27<3:34:19,  1.71s/it]loss_total_epoch 38.76576331630349
Training tokenizer:   6% 511/8047 [13:28<3:34:56,  1.71s/it]loss_total_epoch 38.82771244645119
Training tokenizer:   6% 512/8047 [13:30<3:34:47,  1.71s/it]loss_total_epoch 38.897049963474274
Training tokenizer:   6% 513/8047 [13:32<3:34:37,  1.71s/it]loss_total_epoch 38.96109143644571
Training tokenizer:   6% 514/8047 [13:34<3:46:53,  1.81s/it]loss_total_epoch 39.031731620430946
Training tokenizer:   6% 515/8047 [13:36<3:43:18,  1.78s/it]loss_total_epoch 39.095305770635605
Training tokenizer:   6% 516/8047 [13:37<3:41:19,  1.76s/it]loss_total_epoch 39.161433435976505
Training tokenizer:   6% 517/8047 [13:39<3:38:37,  1.74s/it]loss_total_epoch 39.23435094952583
Training tokenizer:   6% 518/8047 [13:41<3:37:31,  1.73s/it]loss_total_epoch 39.287928964942694
Training tokenizer:   6% 519/8047 [13:42<3:36:47,  1.73s/it]loss_total_epoch 39.35314064100385
Training tokenizer:   6% 520/8047 [13:44<3:36:25,  1.73s/it]loss_total_epoch 39.42105848714709
Training tokenizer:   6% 521/8047 [13:46<3:34:33,  1.71s/it]loss_total_epoch 39.48788137361407
Training tokenizer:   6% 522/8047 [13:48<3:34:43,  1.71s/it]loss_total_epoch 39.54932814836502
Training tokenizer:   6% 523/8047 [13:49<3:34:53,  1.71s/it]loss_total_epoch 39.607365772128105
Training tokenizer:   7% 524/8047 [13:51<3:34:29,  1.71s/it]loss_total_epoch 39.66121567040682
Training tokenizer:   7% 525/8047 [13:53<3:34:55,  1.71s/it]loss_total_epoch 39.72830732911825
Training tokenizer:   7% 526/8047 [13:54<3:35:09,  1.72s/it]loss_total_epoch 39.7852822765708
Training tokenizer:   7% 527/8047 [13:56<3:34:28,  1.71s/it]loss_total_epoch 39.84974184632301
Training tokenizer:   7% 528/8047 [13:58<3:34:20,  1.71s/it]loss_total_epoch 39.92974345386028
Training tokenizer:   7% 529/8047 [14:00<3:33:27,  1.70s/it]loss_total_epoch 40.00193606317043
Training tokenizer:   7% 530/8047 [14:01<3:33:41,  1.71s/it]loss_total_epoch 40.085351057350636
Training tokenizer:   7% 531/8047 [14:03<3:34:28,  1.71s/it]loss_total_epoch 40.15007107704878
Training tokenizer:   7% 532/8047 [14:05<3:34:45,  1.71s/it]loss_total_epoch 40.21862414479256
Training tokenizer:   7% 533/8047 [14:06<3:34:12,  1.71s/it]loss_total_epoch 40.300933584570885
Training tokenizer:   7% 534/8047 [14:08<3:37:11,  1.73s/it]loss_total_epoch 40.37602877616882
Training tokenizer:   7% 535/8047 [14:10<3:37:03,  1.73s/it]loss_total_epoch 40.459251306951046
Training tokenizer:   7% 536/8047 [14:12<3:36:46,  1.73s/it]loss_total_epoch 40.53776552528143
Training tokenizer:   7% 537/8047 [14:13<3:36:08,  1.73s/it]loss_total_epoch 40.588089633733034
Training tokenizer:   7% 538/8047 [14:15<3:35:30,  1.72s/it]loss_total_epoch 40.65918228402734
Training tokenizer:   7% 539/8047 [14:17<3:34:54,  1.72s/it]loss_total_epoch 40.72074627876282
Training tokenizer:   7% 540/8047 [14:19<3:34:57,  1.72s/it]loss_total_epoch 40.791170574724674
Training tokenizer:   7% 541/8047 [14:21<3:47:06,  1.82s/it]loss_total_epoch 40.8607280254364
Training tokenizer:   7% 542/8047 [14:22<3:43:44,  1.79s/it]loss_total_epoch 40.92608939856291
Training tokenizer:   7% 543/8047 [14:24<3:41:54,  1.77s/it]loss_total_epoch 40.99536656588316
Training tokenizer:   7% 544/8047 [14:26<3:47:19,  1.82s/it]loss_total_epoch 41.05581581965089
Training tokenizer:   7% 545/8047 [14:28<3:43:42,  1.79s/it]loss_total_epoch 41.12368796393275
Training tokenizer:   7% 546/8047 [14:29<3:40:38,  1.76s/it]loss_total_epoch 41.195060815662146
Training tokenizer:   7% 547/8047 [14:31<3:39:04,  1.75s/it]loss_total_epoch 41.263470847159624
Training tokenizer:   7% 548/8047 [14:33<3:37:59,  1.74s/it]loss_total_epoch 41.325978089123964
Training tokenizer:   7% 549/8047 [14:35<3:41:11,  1.77s/it]loss_total_epoch 41.3844963721931
Training tokenizer:   7% 550/8047 [14:36<3:38:35,  1.75s/it]loss_total_epoch 41.435128193348646
Training tokenizer:   7% 551/8047 [14:38<3:37:28,  1.74s/it]loss_total_epoch 41.48720793798566
Training tokenizer:   7% 552/8047 [14:40<3:36:37,  1.73s/it]loss_total_epoch 41.557944025844336
Training tokenizer:   7% 553/8047 [14:42<3:36:10,  1.73s/it]loss_total_epoch 41.621933210641146
Training tokenizer:   7% 554/8047 [14:43<3:36:10,  1.73s/it]loss_total_epoch 41.68683623149991
Training tokenizer:   7% 555/8047 [14:45<3:36:26,  1.73s/it]loss_total_epoch 41.74678444862366
Training tokenizer:   7% 556/8047 [14:47<3:35:43,  1.73s/it]loss_total_epoch 41.824068166315556
Training tokenizer:   7% 557/8047 [14:48<3:35:59,  1.73s/it]loss_total_epoch 41.90165367722511
Training tokenizer:   7% 558/8047 [14:50<3:35:59,  1.73s/it]loss_total_epoch 41.976695500314236
Training tokenizer:   7% 559/8047 [14:52<3:35:06,  1.72s/it]loss_total_epoch 42.05129208415747
Training tokenizer:   7% 560/8047 [14:54<3:35:15,  1.73s/it]loss_total_epoch 42.1150656491518
Training tokenizer:   7% 561/8047 [14:56<3:44:31,  1.80s/it]loss_total_epoch 42.18373562395573
Training tokenizer:   7% 562/8047 [14:57<3:41:36,  1.78s/it]loss_total_epoch 42.253902561962605
Training tokenizer:   7% 563/8047 [14:59<3:39:46,  1.76s/it]loss_total_epoch 42.31817056238651
Training tokenizer:   7% 564/8047 [15:01<3:46:37,  1.82s/it]loss_total_epoch 42.38264632970095
Training tokenizer:   7% 565/8047 [15:03<3:43:03,  1.79s/it]loss_total_epoch 42.454790852963924
Training tokenizer:   7% 566/8047 [15:04<3:40:59,  1.77s/it]loss_total_epoch 42.52159420400858
Training tokenizer:   7% 567/8047 [15:06<3:39:28,  1.76s/it]loss_total_epoch 42.589762665331364
Training tokenizer:   7% 568/8047 [15:08<3:38:59,  1.76s/it]loss_total_epoch 42.65790522843599
Training tokenizer:   7% 569/8047 [15:10<3:37:50,  1.75s/it]loss_total_epoch 42.727538511157036
Training tokenizer:   7% 570/8047 [15:11<3:36:58,  1.74s/it]loss_total_epoch 42.79706031829119
Training tokenizer:   7% 571/8047 [15:13<3:43:30,  1.79s/it]loss_total_epoch 42.866596177220345
Training tokenizer:   7% 572/8047 [15:15<3:41:32,  1.78s/it]loss_total_epoch 42.93627344816923
Training tokenizer:   7% 573/8047 [15:17<3:39:06,  1.76s/it]loss_total_epoch 43.00935413688421
Training tokenizer:   7% 574/8047 [15:19<3:44:13,  1.80s/it]loss_total_epoch 43.084285125136375
Training tokenizer:   7% 575/8047 [15:20<3:42:34,  1.79s/it]loss_total_epoch 43.15301514416933
Training tokenizer:   7% 576/8047 [15:22<3:39:59,  1.77s/it]loss_total_epoch 43.21972664445639
Training tokenizer:   7% 577/8047 [15:24<3:39:21,  1.76s/it]loss_total_epoch 43.28433537483215
Training tokenizer:   7% 578/8047 [15:26<3:38:43,  1.76s/it]loss_total_epoch 43.34801000356674
Training tokenizer:   7% 579/8047 [15:27<3:37:35,  1.75s/it]loss_total_epoch 43.42714260518551
Training tokenizer:   7% 580/8047 [15:29<3:37:28,  1.75s/it]loss_total_epoch 43.50900589674711
Training tokenizer:   7% 581/8047 [15:31<3:36:38,  1.74s/it]loss_total_epoch 43.57702820748091
Training tokenizer:   7% 582/8047 [15:33<3:36:18,  1.74s/it]loss_total_epoch 43.63383090496063
Training tokenizer:   7% 583/8047 [15:34<3:35:48,  1.73s/it]loss_total_epoch 43.71194480359554
Training tokenizer:   7% 584/8047 [15:36<3:35:51,  1.74s/it]loss_total_epoch 43.788951478898525
Training tokenizer:   7% 585/8047 [15:38<3:36:31,  1.74s/it]loss_total_epoch 43.856959745287895
Training tokenizer:   7% 586/8047 [15:40<3:36:41,  1.74s/it]loss_total_epoch 43.92026171833277
Training tokenizer:   7% 587/8047 [15:41<3:36:06,  1.74s/it]loss_total_epoch 43.98948433250189
Training tokenizer:   7% 588/8047 [15:43<3:35:57,  1.74s/it]loss_total_epoch 44.05883326381445
Training tokenizer:   7% 589/8047 [15:45<3:36:01,  1.74s/it]loss_total_epoch 44.112540885806084
Training tokenizer:   7% 590/8047 [15:46<3:36:15,  1.74s/it]loss_total_epoch 44.172350235283375
Training tokenizer:   7% 591/8047 [15:48<3:36:26,  1.74s/it]loss_total_epoch 44.25686653703451
Training tokenizer:   7% 592/8047 [15:50<3:36:01,  1.74s/it]loss_total_epoch 44.325653322041035
Training tokenizer:   7% 593/8047 [15:52<3:36:28,  1.74s/it]loss_total_epoch 44.37805999815464
Training tokenizer:   7% 594/8047 [15:53<3:36:01,  1.74s/it]loss_total_epoch 44.45944208651781
Training tokenizer:   7% 595/8047 [15:55<3:36:57,  1.75s/it]loss_total_epoch 44.530640833079815
Training tokenizer:   7% 596/8047 [15:57<3:36:33,  1.74s/it]loss_total_epoch 44.59770833700895
Training tokenizer:   7% 597/8047 [15:59<3:40:35,  1.78s/it]loss_total_epoch 44.666204676032066
Training tokenizer:   7% 598/8047 [16:01<3:39:02,  1.76s/it]loss_total_epoch 44.72623535618186
Training tokenizer:   7% 599/8047 [16:02<3:39:13,  1.77s/it]loss_total_epoch 44.805976305156946
Training tokenizer:   7% 600/8047 [16:04<3:37:49,  1.76s/it]loss_total_epoch 44.870814230293036
Training tokenizer:   7% 601/8047 [16:06<3:37:54,  1.76s/it]loss_total_epoch 44.94680241122842
Training tokenizer:   7% 602/8047 [16:08<3:38:16,  1.76s/it]loss_total_epoch 45.00756122171879
Training tokenizer:   7% 603/8047 [16:09<3:37:59,  1.76s/it]loss_total_epoch 45.07277858257294
Training tokenizer:   8% 604/8047 [16:11<3:37:23,  1.75s/it]loss_total_epoch 45.15380098670721
Training tokenizer:   8% 605/8047 [16:13<3:37:21,  1.75s/it]loss_total_epoch 45.221743777394295
Training tokenizer:   8% 606/8047 [16:15<3:36:36,  1.75s/it]loss_total_epoch 45.299887128174305
Training tokenizer:   8% 607/8047 [16:16<3:36:37,  1.75s/it]loss_total_epoch 45.370410315692425
Training tokenizer:   8% 608/8047 [16:18<3:36:49,  1.75s/it]loss_total_epoch 45.456177204847336
Training tokenizer:   8% 609/8047 [16:20<3:35:46,  1.74s/it]loss_total_epoch 45.530652828514576
Training tokenizer:   8% 610/8047 [16:21<3:36:23,  1.75s/it]loss_total_epoch 45.609642162919044
Training tokenizer:   8% 611/8047 [16:23<3:36:23,  1.75s/it]loss_total_epoch 45.677593141794205
Training tokenizer:   8% 612/8047 [16:25<3:36:19,  1.75s/it]loss_total_epoch 45.74361598491669
Training tokenizer:   8% 613/8047 [16:27<3:36:53,  1.75s/it]loss_total_epoch 45.814292050898075
Training tokenizer:   8% 614/8047 [16:29<3:37:27,  1.76s/it]loss_total_epoch 45.879195153713226
Training tokenizer:   8% 615/8047 [16:30<3:37:55,  1.76s/it]loss_total_epoch 45.94756031036377
Training tokenizer:   8% 616/8047 [16:32<3:37:35,  1.76s/it]loss_total_epoch 46.01431730389595
Training tokenizer:   8% 617/8047 [16:34<3:37:44,  1.76s/it]loss_total_epoch 46.07622040808201
Training tokenizer:   8% 618/8047 [16:36<3:36:49,  1.75s/it]loss_total_epoch 46.14717882871628
Training tokenizer:   8% 619/8047 [16:37<3:36:30,  1.75s/it]loss_total_epoch 46.21448850631714
Training tokenizer:   8% 620/8047 [16:39<3:36:03,  1.75s/it]loss_total_epoch 46.273425955325365
Training tokenizer:   8% 621/8047 [16:41<3:36:25,  1.75s/it]loss_total_epoch 46.332251381129026
Training tokenizer:   8% 622/8047 [16:43<3:36:39,  1.75s/it]loss_total_epoch 46.40872997418046
Training tokenizer:   8% 623/8047 [16:44<3:36:03,  1.75s/it]loss_total_epoch 46.47482969984412
Training tokenizer:   8% 624/8047 [16:46<3:36:09,  1.75s/it]loss_total_epoch 46.54878505691886
Training tokenizer:   8% 625/8047 [16:48<3:36:28,  1.75s/it]loss_total_epoch 46.61145194992423
Training tokenizer:   8% 626/8047 [16:49<3:35:47,  1.74s/it]loss_total_epoch 46.67513131722808
Training tokenizer:   8% 627/8047 [16:51<3:36:39,  1.75s/it]loss_total_epoch 46.74367402866483
Training tokenizer:   8% 628/8047 [16:53<3:36:58,  1.75s/it]loss_total_epoch 46.80329952016473
Training tokenizer:   8% 629/8047 [16:55<3:36:47,  1.75s/it]loss_total_epoch 46.87620298936963
Training tokenizer:   8% 630/8047 [16:57<3:37:59,  1.76s/it]loss_total_epoch 46.94293646886945
Training tokenizer:   8% 631/8047 [16:58<3:37:24,  1.76s/it]loss_total_epoch 47.00718725845218
Training tokenizer:   8% 632/8047 [17:00<3:37:59,  1.76s/it]loss_total_epoch 47.07459330186248
Training tokenizer:   8% 633/8047 [17:02<3:37:46,  1.76s/it]loss_total_epoch 47.13944540545344
Training tokenizer:   8% 634/8047 [17:04<3:38:02,  1.76s/it]loss_total_epoch 47.20866594836116
Training tokenizer:   8% 635/8047 [17:05<3:37:30,  1.76s/it]loss_total_epoch 47.28169570490718
Training tokenizer:   8% 636/8047 [17:07<3:37:27,  1.76s/it]loss_total_epoch 47.34760098531842
Training tokenizer:   8% 637/8047 [17:09<3:36:30,  1.75s/it]loss_total_epoch 47.41774915531278
Training tokenizer:   8% 638/8047 [17:11<3:36:46,  1.76s/it]loss_total_epoch 47.48795336857438
Training tokenizer:   8% 639/8047 [17:12<3:37:44,  1.76s/it]loss_total_epoch 47.569377329200506
Training tokenizer:   8% 640/8047 [17:14<3:37:52,  1.76s/it]loss_total_epoch 47.638409715145826
Training tokenizer:   8% 641/8047 [17:16<3:37:01,  1.76s/it]loss_total_epoch 47.71098292991519
Training tokenizer:   8% 642/8047 [17:18<3:37:16,  1.76s/it]loss_total_epoch 47.776399698108435
Training tokenizer:   8% 643/8047 [17:19<3:37:36,  1.76s/it]loss_total_epoch 47.83815137669444
Training tokenizer:   8% 644/8047 [17:21<3:37:39,  1.76s/it]loss_total_epoch 47.904551561921835
Training tokenizer:   8% 645/8047 [17:23<3:37:23,  1.76s/it]loss_total_epoch 47.97384321317077
Training tokenizer:   8% 646/8047 [17:25<3:36:49,  1.76s/it]loss_total_epoch 48.038835410028696
Training tokenizer:   8% 647/8047 [17:27<3:41:10,  1.79s/it]loss_total_epoch 48.09514345601201
Training tokenizer:   8% 648/8047 [17:28<3:39:45,  1.78s/it]loss_total_epoch 48.16544895991683
Training tokenizer:   8% 649/8047 [17:30<3:38:46,  1.77s/it]loss_total_epoch 48.23665076121688
Training tokenizer:   8% 650/8047 [17:32<3:37:30,  1.76s/it]loss_total_epoch 48.31157960370183
Training tokenizer:   8% 651/8047 [17:34<3:38:02,  1.77s/it]loss_total_epoch 48.386709701269865
Training tokenizer:   8% 652/8047 [17:35<3:37:17,  1.76s/it]loss_total_epoch 48.457202311605215
Training tokenizer:   8% 653/8047 [17:37<3:37:47,  1.77s/it]loss_total_epoch 48.522113148123026
Training tokenizer:   8% 654/8047 [17:39<3:37:47,  1.77s/it]loss_total_epoch 48.60715759918094
Training tokenizer:   8% 655/8047 [17:41<3:38:27,  1.77s/it]loss_total_epoch 48.673121344298124
Training tokenizer:   8% 656/8047 [17:42<3:38:41,  1.78s/it]loss_total_epoch 48.72929614037275
Training tokenizer:   8% 657/8047 [17:44<3:39:09,  1.78s/it]loss_total_epoch 48.798828303813934
Training tokenizer:   8% 658/8047 [17:46<3:39:19,  1.78s/it]loss_total_epoch 48.86805684864521
Training tokenizer:   8% 659/8047 [17:48<3:39:03,  1.78s/it]loss_total_epoch 48.95015786588192
Training tokenizer:   8% 660/8047 [17:50<3:39:56,  1.79s/it]loss_total_epoch 49.023131638765335
Training tokenizer:   8% 661/8047 [17:51<3:39:40,  1.78s/it]loss_total_epoch 49.08678003400564
Training tokenizer:   8% 662/8047 [17:53<3:38:40,  1.78s/it]loss_total_epoch 49.14635854586959
Training tokenizer:   8% 663/8047 [17:55<3:39:10,  1.78s/it]loss_total_epoch 49.20871187373996
Training tokenizer:   8% 664/8047 [17:57<3:37:53,  1.77s/it]loss_total_epoch 49.276829328387976
Training tokenizer:   8% 665/8047 [17:58<3:38:06,  1.77s/it]loss_total_epoch 49.343021247535944
Training tokenizer:   8% 666/8047 [18:00<3:38:31,  1.78s/it]loss_total_epoch 49.39766178280115
Training tokenizer:   8% 667/8047 [18:02<3:38:49,  1.78s/it]loss_total_epoch 49.454579547047615
Training tokenizer:   8% 668/8047 [18:04<3:39:23,  1.78s/it]loss_total_epoch 49.51773878931999
Training tokenizer:   8% 669/8047 [18:06<3:39:33,  1.79s/it]loss_total_epoch 49.592892214655876
Training tokenizer:   8% 670/8047 [18:07<3:40:10,  1.79s/it]loss_total_epoch 49.66734755039215
Training tokenizer:   8% 671/8047 [18:09<3:39:27,  1.79s/it]loss_total_epoch 49.72035367786884
Training tokenizer:   8% 672/8047 [18:11<3:38:36,  1.78s/it]loss_total_epoch 49.79384592920542
Training tokenizer:   8% 673/8047 [18:13<3:38:24,  1.78s/it]loss_total_epoch 49.87028709799051
Training tokenizer:   8% 674/8047 [18:15<3:38:17,  1.78s/it]loss_total_epoch 49.93675860017538
Training tokenizer:   8% 675/8047 [18:16<3:41:08,  1.80s/it]loss_total_epoch 50.00147466361523
Training tokenizer:   8% 676/8047 [18:18<3:41:08,  1.80s/it]loss_total_epoch 50.07498500496149
Training tokenizer:   8% 677/8047 [18:20<3:40:48,  1.80s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-pp26vdkk'
loss_total_epoch 50.14086626470089
Training tokenizer:   8% 678/8047 [18:22<3:40:43,  1.80s/it]loss_total_epoch 50.20852975547314
Training tokenizer:   8% 679/8047 [18:24<3:39:53,  1.79s/it]loss_total_epoch 50.285677552223206
Training tokenizer:   8% 680/8047 [18:25<3:39:58,  1.79s/it]loss_total_epoch 50.357420951128006
Training tokenizer:   8% 681/8047 [18:27<3:39:31,  1.79s/it]loss_total_epoch 50.43805965781212
Training tokenizer:   8% 682/8047 [18:29<3:40:04,  1.79s/it]loss_total_epoch 50.5057705193758
Training tokenizer:   8% 683/8047 [18:31<3:40:19,  1.80s/it]loss_total_epoch 50.58534283190966
Training tokenizer:   9% 684/8047 [18:32<3:39:07,  1.79s/it]loss_total_epoch 50.6623602733016
Training tokenizer:   9% 685/8047 [18:34<3:39:13,  1.79s/it]loss_total_epoch 50.732905715703964
Training tokenizer:   9% 686/8047 [18:36<3:39:03,  1.79s/it]loss_total_epoch 50.79480803012848
Training tokenizer:   9% 687/8047 [18:38<3:43:57,  1.83s/it]loss_total_epoch 50.86093209683895
Training tokenizer:   9% 688/8047 [18:40<3:42:50,  1.82s/it]loss_total_epoch 50.93353710323572
Training tokenizer:   9% 689/8047 [18:42<3:40:11,  1.80s/it]loss_total_epoch 50.99828727543354
Training tokenizer:   9% 690/8047 [18:43<3:38:59,  1.79s/it]loss_total_epoch 51.05229661241174
Training tokenizer:   9% 691/8047 [18:45<3:38:41,  1.78s/it]loss_total_epoch 51.12095523253083
Training tokenizer:   9% 692/8047 [18:47<3:38:35,  1.78s/it]loss_total_epoch 51.19842200353742
Training tokenizer:   9% 693/8047 [18:49<3:39:24,  1.79s/it]loss_total_epoch 51.269451793283224
Training tokenizer:   9% 694/8047 [18:50<3:39:38,  1.79s/it]loss_total_epoch 51.344032134860754
Training tokenizer:   9% 695/8047 [18:52<3:39:18,  1.79s/it]loss_total_epoch 51.40642582252622
Training tokenizer:   9% 696/8047 [18:54<3:39:48,  1.79s/it]loss_total_epoch 51.47309070453048
Training tokenizer:   9% 697/8047 [18:56<3:40:01,  1.80s/it]loss_total_epoch 51.554365176707506
Training tokenizer:   9% 698/8047 [18:58<3:39:18,  1.79s/it]loss_total_epoch 51.63117541745305
Training tokenizer:   9% 699/8047 [18:59<3:39:37,  1.79s/it]loss_total_epoch 51.69672954455018
Training tokenizer:   9% 700/8047 [19:01<3:39:39,  1.79s/it]loss_total_epoch 51.75046918168664
Training tokenizer:   9% 701/8047 [19:03<3:39:34,  1.79s/it]loss_total_epoch 51.81445033475757
Training tokenizer:   9% 702/8047 [19:05<3:39:41,  1.79s/it]loss_total_epoch 51.873720955103636
Training tokenizer:   9% 703/8047 [19:07<3:39:45,  1.80s/it]loss_total_epoch 51.94092406705022
Training tokenizer:   9% 704/8047 [19:08<3:39:31,  1.79s/it]loss_total_epoch 52.01311283186078
Training tokenizer:   9% 705/8047 [19:10<3:39:38,  1.79s/it]loss_total_epoch 52.07723129913211
Training tokenizer:   9% 706/8047 [19:12<3:39:03,  1.79s/it]loss_total_epoch 52.1448476202786
Training tokenizer:   9% 707/8047 [19:14<3:38:47,  1.79s/it]loss_total_epoch 52.21668869629502
Training tokenizer:   9% 708/8047 [19:16<3:40:12,  1.80s/it]loss_total_epoch 52.2870917879045
Training tokenizer:   9% 709/8047 [19:17<3:40:38,  1.80s/it]loss_total_epoch 52.36549882218242
Training tokenizer:   9% 710/8047 [19:19<3:39:45,  1.80s/it]loss_total_epoch 52.436983946710825
Training tokenizer:   9% 711/8047 [19:21<3:39:55,  1.80s/it]loss_total_epoch 52.49711009114981
Training tokenizer:   9% 712/8047 [19:23<3:40:06,  1.80s/it]loss_total_epoch 52.5665278211236
Training tokenizer:   9% 713/8047 [19:25<3:40:26,  1.80s/it]loss_total_epoch 52.64091018587351
Training tokenizer:   9% 714/8047 [19:26<3:39:59,  1.80s/it]loss_total_epoch 52.70912390202284
Training tokenizer:   9% 715/8047 [19:28<3:39:48,  1.80s/it]loss_total_epoch 52.76462905108929
Training tokenizer:   9% 716/8047 [19:30<3:40:44,  1.81s/it]loss_total_epoch 52.830237843096256
Training tokenizer:   9% 717/8047 [19:32<3:40:55,  1.81s/it]loss_total_epoch 52.898415736854076
Training tokenizer:   9% 718/8047 [19:34<3:40:18,  1.80s/it]loss_total_epoch 52.967580422759056
Training tokenizer:   9% 719/8047 [19:35<3:40:38,  1.81s/it]loss_total_epoch 53.03395511209965
Training tokenizer:   9% 720/8047 [19:37<3:40:45,  1.81s/it]loss_total_epoch 53.09572473913431
Training tokenizer:   9% 721/8047 [19:39<3:40:33,  1.81s/it]loss_total_epoch 53.17260329425335
Training tokenizer:   9% 722/8047 [19:41<3:41:06,  1.81s/it]loss_total_epoch 53.255445286631584
Training tokenizer:   9% 723/8047 [19:43<3:41:38,  1.82s/it]loss_total_epoch 53.31515282392502
Training tokenizer:   9% 724/8047 [19:45<3:41:45,  1.82s/it]loss_total_epoch 53.38706515729427
Training tokenizer:   9% 725/8047 [19:46<3:40:30,  1.81s/it]loss_total_epoch 53.46153349429369
Training tokenizer:   9% 726/8047 [19:48<3:39:39,  1.80s/it]loss_total_epoch 53.52332120388746
Training tokenizer:   9% 727/8047 [19:50<3:38:32,  1.79s/it]loss_total_epoch 53.58949562162161
Training tokenizer:   9% 728/8047 [19:52<3:39:28,  1.80s/it]loss_total_epoch 53.65818237513304
Training tokenizer:   9% 729/8047 [19:53<3:39:58,  1.80s/it]loss_total_epoch 53.73530276119709
Training tokenizer:   9% 730/8047 [19:55<3:39:43,  1.80s/it]loss_total_epoch 53.80638848245144
Training tokenizer:   9% 731/8047 [19:57<3:40:05,  1.81s/it]loss_total_epoch 53.869672045111656
Training tokenizer:   9% 732/8047 [19:59<3:40:24,  1.81s/it]loss_total_epoch 53.93648275732994
Training tokenizer:   9% 733/8047 [20:01<3:41:31,  1.82s/it]loss_total_epoch 53.99282585456967
Training tokenizer:   9% 734/8047 [20:03<3:40:48,  1.81s/it]loss_total_epoch 54.06684337928891
Training tokenizer:   9% 735/8047 [20:04<3:40:53,  1.81s/it]loss_total_epoch 54.14276745542884
Training tokenizer:   9% 736/8047 [20:06<3:41:00,  1.81s/it]loss_total_epoch 54.21517350152135
Training tokenizer:   9% 737/8047 [20:08<3:41:14,  1.82s/it]loss_total_epoch 54.280677784234285
Training tokenizer:   9% 738/8047 [20:10<3:41:00,  1.81s/it]loss_total_epoch 54.34780329093337
Training tokenizer:   9% 739/8047 [20:12<3:40:03,  1.81s/it]loss_total_epoch 54.41438074037433
Training tokenizer:   9% 740/8047 [20:13<3:39:36,  1.80s/it]loss_total_epoch 54.47501727566123
Training tokenizer:   9% 741/8047 [20:15<3:39:52,  1.81s/it]loss_total_epoch 54.53962552919984
Training tokenizer:   9% 742/8047 [20:17<3:40:21,  1.81s/it]loss_total_epoch 54.60544794425368
Training tokenizer:   9% 743/8047 [20:19<3:41:17,  1.82s/it]loss_total_epoch 54.67882799729705
Training tokenizer:   9% 744/8047 [20:21<3:41:53,  1.82s/it]loss_total_epoch 54.75624046102166
Training tokenizer:   9% 745/8047 [20:22<3:41:05,  1.82s/it]loss_total_epoch 54.81575360521674
Training tokenizer:   9% 746/8047 [20:24<3:41:12,  1.82s/it]loss_total_epoch 54.88311816379428
Training tokenizer:   9% 747/8047 [20:26<3:40:41,  1.81s/it]loss_total_epoch 54.95986922457814
Training tokenizer:   9% 748/8047 [20:28<3:39:37,  1.81s/it]loss_total_epoch 55.034091118723154
Training tokenizer:   9% 749/8047 [20:30<3:40:16,  1.81s/it]loss_total_epoch 55.105573911219835
Training tokenizer:   9% 750/8047 [20:32<3:39:02,  1.80s/it]loss_total_epoch 55.17271902784705
Training tokenizer:   9% 751/8047 [20:33<3:39:16,  1.80s/it]loss_total_epoch 55.24686436727643
Training tokenizer:   9% 752/8047 [20:35<3:40:08,  1.81s/it]loss_total_epoch 55.316695380955935
Training tokenizer:   9% 753/8047 [20:37<3:40:28,  1.81s/it]loss_total_epoch 55.39975913986564
Training tokenizer:   9% 754/8047 [20:39<3:41:18,  1.82s/it]loss_total_epoch 55.46304166689515
Training tokenizer:   9% 755/8047 [20:41<3:40:39,  1.82s/it]loss_total_epoch 55.53202925994992
Training tokenizer:   9% 756/8047 [20:42<3:40:26,  1.81s/it]loss_total_epoch 55.59539330378175
Training tokenizer:   9% 757/8047 [20:44<3:39:46,  1.81s/it]loss_total_epoch 55.66276030614972
Training tokenizer:   9% 758/8047 [20:46<3:39:59,  1.81s/it]loss_total_epoch 55.72781864926219
Training tokenizer:   9% 759/8047 [20:48<3:40:41,  1.82s/it]loss_total_epoch 55.7903202585876
Training tokenizer:   9% 760/8047 [20:50<3:40:56,  1.82s/it]loss_total_epoch 55.851319178938866
Training tokenizer:   9% 761/8047 [20:51<3:40:08,  1.81s/it]loss_total_epoch 55.9220155775547
Training tokenizer:   9% 762/8047 [20:53<3:40:29,  1.82s/it]loss_total_epoch 55.9849803224206
Training tokenizer:   9% 763/8047 [20:55<3:41:30,  1.82s/it]loss_total_epoch 56.041161846369505
Training tokenizer:   9% 764/8047 [20:57<3:42:03,  1.83s/it]loss_total_epoch 56.12235763296485
Training tokenizer:  10% 765/8047 [20:59<3:41:37,  1.83s/it]loss_total_epoch 56.198373425751925
Training tokenizer:  10% 766/8047 [21:01<3:40:24,  1.82s/it]loss_total_epoch 56.26629061624408
Training tokenizer:  10% 767/8047 [21:02<3:40:25,  1.82s/it]loss_total_epoch 56.33999848738313
Training tokenizer:  10% 768/8047 [21:04<3:40:34,  1.82s/it]loss_total_epoch 56.411344181746244
Training tokenizer:  10% 769/8047 [21:06<3:40:58,  1.82s/it]loss_total_epoch 56.48313361033797
Training tokenizer:  10% 770/8047 [21:08<3:40:58,  1.82s/it]loss_total_epoch 56.56261857226491
Training tokenizer:  10% 771/8047 [21:10<3:41:25,  1.83s/it]loss_total_epoch 56.63380927965045
Training tokenizer:  10% 772/8047 [21:12<3:42:07,  1.83s/it]loss_total_epoch 56.69053682312369
Training tokenizer:  10% 773/8047 [21:13<3:41:44,  1.83s/it]loss_total_epoch 56.76331324502826
Training tokenizer:  10% 774/8047 [21:15<3:41:25,  1.83s/it]loss_total_epoch 56.831846710294485
Training tokenizer:  10% 775/8047 [21:17<3:41:45,  1.83s/it]loss_total_epoch 56.898484986275434
Training tokenizer:  10% 776/8047 [21:19<3:41:29,  1.83s/it]loss_total_epoch 56.96527264639735
Training tokenizer:  10% 777/8047 [21:21<3:42:08,  1.83s/it]loss_total_epoch 57.030975710600615
Training tokenizer:  10% 778/8047 [21:23<3:42:42,  1.84s/it]loss_total_epoch 57.10127129033208
Training tokenizer:  10% 779/8047 [21:24<3:42:23,  1.84s/it]loss_total_epoch 57.16788838431239
Training tokenizer:  10% 780/8047 [21:26<3:42:31,  1.84s/it]loss_total_epoch 57.2332919575274
Training tokenizer:  10% 781/8047 [21:28<3:40:58,  1.82s/it]loss_total_epoch 57.29881788417697
Training tokenizer:  10% 782/8047 [21:30<3:40:39,  1.82s/it]loss_total_epoch 57.373572792857885
Training tokenizer:  10% 783/8047 [21:32<3:39:51,  1.82s/it]loss_total_epoch 57.44213956966996
Training tokenizer:  10% 784/8047 [21:33<3:39:39,  1.81s/it]loss_total_epoch 57.49932166188955
Training tokenizer:  10% 785/8047 [21:35<3:40:40,  1.82s/it]loss_total_epoch 57.561153538525105
Training tokenizer:  10% 786/8047 [21:37<3:42:01,  1.83s/it]loss_total_epoch 57.62800920009613
Training tokenizer:  10% 787/8047 [21:39<3:42:07,  1.84s/it]loss_total_epoch 57.69662320613861
Training tokenizer:  10% 788/8047 [21:41<3:42:56,  1.84s/it]loss_total_epoch 57.77880126982927
Training tokenizer:  10% 789/8047 [21:43<3:42:34,  1.84s/it]loss_total_epoch 57.85230343788862
Training tokenizer:  10% 790/8047 [21:45<3:43:30,  1.85s/it]loss_total_epoch 57.917173974215984
Training tokenizer:  10% 791/8047 [21:46<3:44:16,  1.85s/it]loss_total_epoch 57.98695292323828
Training tokenizer:  10% 792/8047 [21:48<3:43:59,  1.85s/it]loss_total_epoch 58.069879814982414
Training tokenizer:  10% 793/8047 [21:50<3:43:18,  1.85s/it]loss_total_epoch 58.133681528270245
Training tokenizer:  10% 794/8047 [21:52<3:43:41,  1.85s/it]loss_total_epoch 58.20605421066284
Training tokenizer:  10% 795/8047 [21:54<3:43:55,  1.85s/it]loss_total_epoch 58.2826811671257
Training tokenizer:  10% 796/8047 [21:56<3:43:58,  1.85s/it]loss_total_epoch 58.34927495568991
Training tokenizer:  10% 797/8047 [21:58<3:43:56,  1.85s/it]loss_total_epoch 58.420826844871044
Training tokenizer:  10% 798/8047 [21:59<3:43:27,  1.85s/it]loss_total_epoch 58.50161740928888
Training tokenizer:  10% 799/8047 [22:01<3:43:04,  1.85s/it]loss_total_epoch 58.56484352052212
Training tokenizer:  10% 800/8047 [22:03<3:42:32,  1.84s/it]loss_total_epoch 58.629338942468166
Training tokenizer:  10% 801/8047 [22:05<3:42:16,  1.84s/it]loss_total_epoch 58.7088091596961
Training tokenizer:  10% 802/8047 [22:07<3:43:33,  1.85s/it]loss_total_epoch 58.77192196249962
Training tokenizer:  10% 803/8047 [22:09<3:44:14,  1.86s/it]loss_total_epoch 58.836996890604496
Training tokenizer:  10% 804/8047 [22:10<3:43:14,  1.85s/it]loss_total_epoch 58.893555887043476
Training tokenizer:  10% 805/8047 [22:12<3:43:11,  1.85s/it]loss_total_epoch 58.95779490470886
Training tokenizer:  10% 806/8047 [22:14<3:43:15,  1.85s/it]loss_total_epoch 59.016821064054966
Training tokenizer:  10% 807/8047 [22:16<3:43:00,  1.85s/it]loss_total_epoch 59.09575857967138
Training tokenizer:  10% 808/8047 [22:18<3:42:58,  1.85s/it]loss_total_epoch 59.164867371320724
Training tokenizer:  10% 809/8047 [22:20<3:42:56,  1.85s/it]loss_total_epoch 59.229014448821545
Training tokenizer:  10% 810/8047 [22:22<3:42:42,  1.85s/it]loss_total_epoch 59.292701199650764
Training tokenizer:  10% 811/8047 [22:23<3:42:11,  1.84s/it]loss_total_epoch 59.349790655076504
Training tokenizer:  10% 812/8047 [22:25<3:41:29,  1.84s/it]loss_total_epoch 59.43165444582701
Training tokenizer:  10% 813/8047 [22:27<3:41:51,  1.84s/it]loss_total_epoch 59.49975856393576
Training tokenizer:  10% 814/8047 [22:29<3:42:03,  1.84s/it]loss_total_epoch 59.574676029384136
Training tokenizer:  10% 815/8047 [22:31<3:42:04,  1.84s/it]loss_total_epoch 59.63644713163376
Training tokenizer:  10% 816/8047 [22:33<3:42:04,  1.84s/it]loss_total_epoch 59.70702751725912
Training tokenizer:  10% 817/8047 [22:34<3:42:29,  1.85s/it]loss_total_epoch 59.779358960688114
Training tokenizer:  10% 818/8047 [22:36<3:42:22,  1.85s/it]loss_total_epoch 59.85515297949314
Training tokenizer:  10% 819/8047 [22:38<3:42:09,  1.84s/it]loss_total_epoch 59.91878072172403
Training tokenizer:  10% 820/8047 [22:40<3:42:26,  1.85s/it]loss_total_epoch 59.97444887459278
Training tokenizer:  10% 821/8047 [22:42<3:42:27,  1.85s/it]loss_total_epoch 60.0345505438745
Training tokenizer:  10% 822/8047 [22:44<3:43:23,  1.86s/it]loss_total_epoch 60.09664952382445
Training tokenizer:  10% 823/8047 [22:46<3:42:55,  1.85s/it]loss_total_epoch 60.16896592453122
Training tokenizer:  10% 824/8047 [22:47<3:43:46,  1.86s/it]loss_total_epoch 60.23993030562997
Training tokenizer:  10% 825/8047 [22:49<3:44:01,  1.86s/it]loss_total_epoch 60.30734558030963
Training tokenizer:  10% 826/8047 [22:51<3:43:34,  1.86s/it]loss_total_epoch 60.377483893185854
Training tokenizer:  10% 827/8047 [22:53<3:42:38,  1.85s/it]loss_total_epoch 60.44209252670407
Training tokenizer:  10% 828/8047 [22:55<3:42:34,  1.85s/it]loss_total_epoch 60.51099454984069
Training tokenizer:  10% 829/8047 [22:57<3:42:48,  1.85s/it]loss_total_epoch 60.57391059026122
Training tokenizer:  10% 830/8047 [22:59<3:42:41,  1.85s/it]loss_total_epoch 60.66556091979146
Training tokenizer:  10% 831/8047 [23:00<3:41:51,  1.84s/it]loss_total_epoch 60.75157745555043
Training tokenizer:  10% 832/8047 [23:02<3:41:55,  1.85s/it]loss_total_epoch 60.817774165421724
Training tokenizer:  10% 833/8047 [23:04<3:42:37,  1.85s/it]loss_total_epoch 60.907271046191454
Training tokenizer:  10% 834/8047 [23:06<3:43:01,  1.86s/it]loss_total_epoch 60.985376324504614
Training tokenizer:  10% 835/8047 [23:08<3:43:05,  1.86s/it]loss_total_epoch 61.07139104232192
Training tokenizer:  10% 836/8047 [23:10<3:42:38,  1.85s/it]loss_total_epoch 61.15158760175109
Training tokenizer:  10% 837/8047 [23:11<3:42:48,  1.85s/it]loss_total_epoch 61.23302474245429
Training tokenizer:  10% 838/8047 [23:13<3:42:22,  1.85s/it]loss_total_epoch 61.31716452911496
Training tokenizer:  10% 839/8047 [23:15<3:42:41,  1.85s/it]loss_total_epoch 61.393820624798536
Training tokenizer:  10% 840/8047 [23:17<3:41:54,  1.85s/it]loss_total_epoch 61.470978390425444
Training tokenizer:  10% 841/8047 [23:19<3:42:19,  1.85s/it]loss_total_epoch 61.538225915282965
Training tokenizer:  10% 842/8047 [23:21<3:42:43,  1.85s/it]loss_total_epoch 61.59524151310325
Training tokenizer:  10% 843/8047 [23:23<3:42:44,  1.86s/it]loss_total_epoch 61.67445074394345
Training tokenizer:  10% 844/8047 [23:24<3:43:02,  1.86s/it]loss_total_epoch 61.747038166970015
Training tokenizer:  11% 845/8047 [23:26<3:43:42,  1.86s/it]loss_total_epoch 61.821579817682505
Training tokenizer:  11% 846/8047 [23:28<3:43:39,  1.86s/it]loss_total_epoch 61.899496894329786
Training tokenizer:  11% 847/8047 [23:30<3:43:00,  1.86s/it]loss_total_epoch 61.96777027472854
Training tokenizer:  11% 848/8047 [23:32<3:43:13,  1.86s/it]loss_total_epoch 62.02968559414148
Training tokenizer:  11% 849/8047 [23:34<3:43:26,  1.86s/it]loss_total_epoch 62.10324755311012
Training tokenizer:  11% 850/8047 [23:36<3:43:49,  1.87s/it]loss_total_epoch 62.164920534938574
Training tokenizer:  11% 851/8047 [23:38<3:42:49,  1.86s/it]loss_total_epoch 62.229815784841776
Training tokenizer:  11% 852/8047 [23:39<3:42:44,  1.86s/it]loss_total_epoch 62.28874381259084
Training tokenizer:  11% 853/8047 [23:41<3:42:49,  1.86s/it]loss_total_epoch 62.35530896112323
Training tokenizer:  11% 854/8047 [23:43<3:42:54,  1.86s/it]loss_total_epoch 62.42316859588027
Training tokenizer:  11% 855/8047 [23:45<3:43:36,  1.87s/it]loss_total_epoch 62.51076181605458
Training tokenizer:  11% 856/8047 [23:47<3:43:29,  1.86s/it]loss_total_epoch 62.57551201060414
Training tokenizer:  11% 857/8047 [23:49<3:43:20,  1.86s/it]loss_total_epoch 62.649171229451895
Training tokenizer:  11% 858/8047 [23:51<3:43:28,  1.87s/it]loss_total_epoch 62.71669161692262
Training tokenizer:  11% 859/8047 [23:52<3:43:09,  1.86s/it]loss_total_epoch 62.790257800370455
Training tokenizer:  11% 860/8047 [23:54<3:42:44,  1.86s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-qy2bhq65'
loss_total_epoch 62.84817877411842
Training tokenizer:  11% 861/8047 [23:56<3:43:06,  1.86s/it]loss_total_epoch 62.918866604566574
Training tokenizer:  11% 862/8047 [23:58<3:42:44,  1.86s/it]loss_total_epoch 62.97807344421744
Training tokenizer:  11% 863/8047 [24:00<3:43:41,  1.87s/it]loss_total_epoch 63.043840389698744
Training tokenizer:  11% 864/8047 [24:02<3:43:59,  1.87s/it]loss_total_epoch 63.105048421770334
Training tokenizer:  11% 865/8047 [24:04<3:44:25,  1.87s/it]loss_total_epoch 63.17362954095006
Training tokenizer:  11% 866/8047 [24:06<3:44:26,  1.88s/it]loss_total_epoch 63.22832267731428
Training tokenizer:  11% 867/8047 [24:07<3:44:34,  1.88s/it]loss_total_epoch 63.28068142384291
Training tokenizer:  11% 868/8047 [24:09<3:43:31,  1.87s/it]loss_total_epoch 63.34421340376139
Training tokenizer:  11% 869/8047 [24:11<3:44:06,  1.87s/it]loss_total_epoch 63.41380414366722
Training tokenizer:  11% 870/8047 [24:13<3:42:50,  1.86s/it]loss_total_epoch 63.49555364996195
Training tokenizer:  11% 871/8047 [24:15<3:42:37,  1.86s/it]loss_total_epoch 63.57431799918413
Training tokenizer:  11% 872/8047 [24:17<3:43:06,  1.87s/it]loss_total_epoch 63.62846816703677
Training tokenizer:  11% 873/8047 [24:19<3:43:10,  1.87s/it]loss_total_epoch 63.70277601853013
Training tokenizer:  11% 874/8047 [24:20<3:42:33,  1.86s/it]loss_total_epoch 63.77851362153888
Training tokenizer:  11% 875/8047 [24:22<3:42:53,  1.86s/it]loss_total_epoch 63.85103302076459
Training tokenizer:  11% 876/8047 [24:24<3:44:01,  1.87s/it]loss_total_epoch 63.91650145128369
Training tokenizer:  11% 877/8047 [24:26<3:44:11,  1.88s/it]loss_total_epoch 63.97294920682907
Training tokenizer:  11% 878/8047 [24:28<3:44:15,  1.88s/it]loss_total_epoch 64.0330580919981
Training tokenizer:  11% 879/8047 [24:30<3:45:11,  1.89s/it]loss_total_epoch 64.09616388380527
Training tokenizer:  11% 880/8047 [24:32<3:44:16,  1.88s/it]loss_total_epoch 64.16186804324389
Training tokenizer:  11% 881/8047 [24:34<3:43:08,  1.87s/it]loss_total_epoch 64.24640174210072
Training tokenizer:  11% 882/8047 [24:35<3:43:32,  1.87s/it]loss_total_epoch 64.30595238134265
Training tokenizer:  11% 883/8047 [24:37<3:43:51,  1.87s/it]loss_total_epoch 64.38906225189567
Training tokenizer:  11% 884/8047 [24:39<3:44:08,  1.88s/it]loss_total_epoch 64.45007381215692
Training tokenizer:  11% 885/8047 [24:41<3:44:00,  1.88s/it]loss_total_epoch 64.51085998862982
Training tokenizer:  11% 886/8047 [24:43<3:44:42,  1.88s/it]loss_total_epoch 64.5725043527782
Training tokenizer:  11% 887/8047 [24:45<3:44:52,  1.88s/it]loss_total_epoch 64.63101083040237
Training tokenizer:  11% 888/8047 [24:47<3:44:18,  1.88s/it]loss_total_epoch 64.69174395874143
Training tokenizer:  11% 889/8047 [24:49<3:44:55,  1.89s/it]loss_total_epoch 64.75571983680129
Training tokenizer:  11% 890/8047 [24:51<3:45:06,  1.89s/it]loss_total_epoch 64.82510343566537
Training tokenizer:  11% 891/8047 [24:52<3:44:40,  1.88s/it]loss_total_epoch 64.91058135405183
Training tokenizer:  11% 892/8047 [24:54<3:45:04,  1.89s/it]loss_total_epoch 64.97703500464559
Training tokenizer:  11% 893/8047 [24:56<3:44:33,  1.88s/it]loss_total_epoch 65.04967885836959
Training tokenizer:  11% 894/8047 [24:58<3:44:06,  1.88s/it]loss_total_epoch 65.1091316267848
Training tokenizer:  11% 895/8047 [25:00<3:44:17,  1.88s/it]loss_total_epoch 65.16661944240332
Training tokenizer:  11% 896/8047 [25:02<3:44:36,  1.88s/it]loss_total_epoch 65.23202683031559
Training tokenizer:  11% 897/8047 [25:04<3:45:14,  1.89s/it]loss_total_epoch 65.29332564026117
Training tokenizer:  11% 898/8047 [25:06<3:45:01,  1.89s/it]loss_total_epoch 65.3522812128067
Training tokenizer:  11% 899/8047 [25:08<3:45:08,  1.89s/it]loss_total_epoch 65.41048672050238
Training tokenizer:  11% 900/8047 [25:09<3:46:21,  1.90s/it]loss_total_epoch 65.47825784236193
Training tokenizer:  11% 901/8047 [25:11<3:46:13,  1.90s/it]loss_total_epoch 65.54209653288126
Training tokenizer:  11% 902/8047 [25:13<3:46:12,  1.90s/it]loss_total_epoch 65.62051226198673
Training tokenizer:  11% 903/8047 [25:15<3:46:41,  1.90s/it]loss_total_epoch 65.69459081441164
Training tokenizer:  11% 904/8047 [25:17<3:47:09,  1.91s/it]loss_total_epoch 65.75911653786898
Training tokenizer:  11% 905/8047 [25:19<3:46:55,  1.91s/it]loss_total_epoch 65.83327663689852
Training tokenizer:  11% 906/8047 [25:21<3:46:07,  1.90s/it]loss_total_epoch 65.89842037111521
Training tokenizer:  11% 907/8047 [25:23<3:46:03,  1.90s/it]loss_total_epoch 65.96369028836489
Training tokenizer:  11% 908/8047 [25:25<3:46:17,  1.90s/it]loss_total_epoch 66.02460165321827
Training tokenizer:  11% 909/8047 [25:27<3:46:54,  1.91s/it]loss_total_epoch 66.08999998867512
Training tokenizer:  11% 910/8047 [25:28<3:46:13,  1.90s/it]loss_total_epoch 66.13311531022191
Training tokenizer:  11% 911/8047 [25:30<3:46:13,  1.90s/it]loss_total_epoch 66.20926072075963
Training tokenizer:  11% 912/8047 [25:32<3:46:27,  1.90s/it]loss_total_epoch 66.28568201139569
Training tokenizer:  11% 913/8047 [25:34<3:46:33,  1.91s/it]loss_total_epoch 66.36004280671477
Training tokenizer:  11% 914/8047 [25:36<3:45:25,  1.90s/it]loss_total_epoch 66.41407952457666
Training tokenizer:  11% 915/8047 [25:38<3:45:01,  1.89s/it]loss_total_epoch 66.47363267838955
Training tokenizer:  11% 916/8047 [25:40<3:45:35,  1.90s/it]loss_total_epoch 66.52838327735662
Training tokenizer:  11% 917/8047 [25:42<3:45:35,  1.90s/it]loss_total_epoch 66.59183903038502
Training tokenizer:  11% 918/8047 [25:44<3:45:32,  1.90s/it]loss_total_epoch 66.65511126071215
Training tokenizer:  11% 919/8047 [25:46<3:44:56,  1.89s/it]loss_total_epoch 66.72115335613489
Training tokenizer:  11% 920/8047 [25:47<3:45:17,  1.90s/it]loss_total_epoch 66.77226100489497
Training tokenizer:  11% 921/8047 [25:49<3:45:53,  1.90s/it]loss_total_epoch 66.8308760561049
Training tokenizer:  11% 922/8047 [25:51<3:46:30,  1.91s/it]loss_total_epoch 66.89151180163026
Training tokenizer:  11% 923/8047 [25:53<3:47:19,  1.91s/it]loss_total_epoch 66.95680702850223
Training tokenizer:  11% 924/8047 [25:55<3:46:25,  1.91s/it]loss_total_epoch 67.02387781813741
Training tokenizer:  11% 925/8047 [25:57<3:47:01,  1.91s/it]loss_total_epoch 67.10176526382565
Training tokenizer:  12% 926/8047 [25:59<3:46:21,  1.91s/it]loss_total_epoch 67.16785851493478
Training tokenizer:  12% 927/8047 [26:01<3:46:46,  1.91s/it]loss_total_epoch 67.22803767025471
Training tokenizer:  12% 928/8047 [26:03<3:45:47,  1.90s/it]loss_total_epoch 67.30292776226997
Training tokenizer:  12% 929/8047 [26:05<3:45:40,  1.90s/it]loss_total_epoch 67.36195947229862
Training tokenizer:  12% 930/8047 [26:07<3:45:23,  1.90s/it]loss_total_epoch 67.41736528649926
Training tokenizer:  12% 931/8047 [26:08<3:45:36,  1.90s/it]loss_total_epoch 67.47635708004236
Training tokenizer:  12% 932/8047 [26:10<3:45:04,  1.90s/it]loss_total_epoch 67.54312688857317
Training tokenizer:  12% 933/8047 [26:12<3:45:43,  1.90s/it]loss_total_epoch 67.60347924008965
Training tokenizer:  12% 934/8047 [26:14<3:44:40,  1.90s/it]loss_total_epoch 67.66672195121646
Training tokenizer:  12% 935/8047 [26:16<3:45:23,  1.90s/it]loss_total_epoch 67.73146916553378
Training tokenizer:  12% 936/8047 [26:18<3:44:48,  1.90s/it]loss_total_epoch 67.78576032072306
Training tokenizer:  12% 937/8047 [26:20<3:45:35,  1.90s/it]loss_total_epoch 67.85274337232113
Training tokenizer:  12% 938/8047 [26:22<3:46:02,  1.91s/it]loss_total_epoch 67.91701637208462
Training tokenizer:  12% 939/8047 [26:24<3:46:48,  1.91s/it]loss_total_epoch 67.98625143617392
Training tokenizer:  12% 940/8047 [26:26<3:46:02,  1.91s/it]loss_total_epoch 68.04763062670827
Training tokenizer:  12% 941/8047 [26:27<3:45:50,  1.91s/it]loss_total_epoch 68.11000097170472
Training tokenizer:  12% 942/8047 [26:29<3:45:45,  1.91s/it]loss_total_epoch 68.18072691187263
Training tokenizer:  12% 943/8047 [26:31<3:45:53,  1.91s/it]loss_total_epoch 68.2266146056354
Training tokenizer:  12% 944/8047 [26:33<3:46:39,  1.91s/it]loss_total_epoch 68.2970585487783
Training tokenizer:  12% 945/8047 [26:35<3:47:04,  1.92s/it]loss_total_epoch 68.35833091288805
Training tokenizer:  12% 946/8047 [26:37<3:46:05,  1.91s/it]loss_total_epoch 68.43116584420204
Training tokenizer:  12% 947/8047 [26:39<3:46:09,  1.91s/it]loss_total_epoch 68.4895560964942
Training tokenizer:  12% 948/8047 [26:41<3:46:02,  1.91s/it]loss_total_epoch 68.54979215189815
Training tokenizer:  12% 949/8047 [26:43<3:46:19,  1.91s/it]loss_total_epoch 68.62279150262475
Training tokenizer:  12% 950/8047 [26:45<3:46:28,  1.91s/it]loss_total_epoch 68.68089521676302
Training tokenizer:  12% 951/8047 [26:47<3:45:45,  1.91s/it]loss_total_epoch 68.75505236536264
Training tokenizer:  12% 952/8047 [26:48<3:45:46,  1.91s/it]loss_total_epoch 68.82190811634064
Training tokenizer:  12% 953/8047 [26:50<3:45:18,  1.91s/it]loss_total_epoch 68.88707807660103
Training tokenizer:  12% 954/8047 [26:52<3:43:47,  1.89s/it]loss_total_epoch 68.9549090564251
Training tokenizer:  12% 955/8047 [26:54<3:45:21,  1.91s/it]loss_total_epoch 69.02167976647615
Training tokenizer:  12% 956/8047 [26:56<3:45:27,  1.91s/it]loss_total_epoch 69.07841105014086
Training tokenizer:  12% 957/8047 [26:58<3:46:05,  1.91s/it]loss_total_epoch 69.14730621874332
Training tokenizer:  12% 958/8047 [27:00<3:45:34,  1.91s/it]loss_total_epoch 69.22783516347408
Training tokenizer:  12% 959/8047 [27:02<3:45:10,  1.91s/it]loss_total_epoch 69.28551126644015
Training tokenizer:  12% 960/8047 [27:04<3:46:14,  1.92s/it]loss_total_epoch 69.35567348822951
Training tokenizer:  12% 961/8047 [27:06<3:46:56,  1.92s/it]loss_total_epoch 69.41446153819561
Training tokenizer:  12% 962/8047 [27:08<3:46:53,  1.92s/it]loss_total_epoch 69.47580025345087
Training tokenizer:  12% 963/8047 [27:10<3:46:37,  1.92s/it]loss_total_epoch 69.55429976433516
Training tokenizer:  12% 964/8047 [27:11<3:46:50,  1.92s/it]loss_total_epoch 69.628269135952
Training tokenizer:  12% 965/8047 [27:13<3:46:51,  1.92s/it]loss_total_epoch 69.69432316720486
Training tokenizer:  12% 966/8047 [27:15<3:47:08,  1.92s/it]loss_total_epoch 69.76631096750498
Training tokenizer:  12% 967/8047 [27:17<3:46:16,  1.92s/it]loss_total_epoch 69.82881740480661
Training tokenizer:  12% 968/8047 [27:19<3:46:40,  1.92s/it]loss_total_epoch 69.9083204716444
Training tokenizer:  12% 969/8047 [27:21<3:46:07,  1.92s/it]loss_total_epoch 69.97112560272217
Training tokenizer:  12% 970/8047 [27:23<3:46:12,  1.92s/it]loss_total_epoch 70.03325578942895
Training tokenizer:  12% 971/8047 [27:25<3:46:22,  1.92s/it]loss_total_epoch 70.09051594883204
Training tokenizer:  12% 972/8047 [27:27<3:46:14,  1.92s/it]loss_total_epoch 70.1665757521987
Training tokenizer:  12% 973/8047 [27:29<3:45:02,  1.91s/it]loss_total_epoch 70.22791223600507
Training tokenizer:  12% 974/8047 [27:31<3:45:32,  1.91s/it]loss_total_epoch 70.28537023812532
Training tokenizer:  12% 975/8047 [27:33<3:45:56,  1.92s/it]loss_total_epoch 70.35520628839731
Training tokenizer:  12% 976/8047 [27:34<3:45:56,  1.92s/it]loss_total_epoch 70.41163286939263
Training tokenizer:  12% 977/8047 [27:36<3:46:23,  1.92s/it]loss_total_epoch 70.47879381850362
Training tokenizer:  12% 978/8047 [27:38<3:46:32,  1.92s/it]loss_total_epoch 70.54195380583405
Training tokenizer:  12% 979/8047 [27:40<3:47:24,  1.93s/it]loss_total_epoch 70.6104358099401
Training tokenizer:  12% 980/8047 [27:42<3:46:44,  1.93s/it]loss_total_epoch 70.66663732379675
Training tokenizer:  12% 981/8047 [27:44<3:46:28,  1.92s/it]loss_total_epoch 70.75734335184097
Training tokenizer:  12% 982/8047 [27:46<3:46:37,  1.92s/it]loss_total_epoch 70.83643751591444
Training tokenizer:  12% 983/8047 [27:48<3:46:42,  1.93s/it]loss_total_epoch 70.890530731529
Training tokenizer:  12% 984/8047 [27:50<3:47:24,  1.93s/it]loss_total_epoch 70.96238470450044
Training tokenizer:  12% 985/8047 [27:52<3:47:16,  1.93s/it]loss_total_epoch 71.0430741943419
Training tokenizer:  12% 986/8047 [27:54<3:47:22,  1.93s/it]loss_total_epoch 71.10972451046109
Training tokenizer:  12% 987/8047 [27:56<3:47:32,  1.93s/it]loss_total_epoch 71.17316791042686
Training tokenizer:  12% 988/8047 [27:58<3:46:51,  1.93s/it]loss_total_epoch 71.23646868392825
Training tokenizer:  12% 989/8047 [28:00<3:46:33,  1.93s/it]loss_total_epoch 71.31015838310122
Training tokenizer:  12% 990/8047 [28:01<3:46:53,  1.93s/it]loss_total_epoch 71.38055134192109
Training tokenizer:  12% 991/8047 [28:03<3:47:24,  1.93s/it]loss_total_epoch 71.44876750186086
Training tokenizer:  12% 992/8047 [28:05<3:47:36,  1.94s/it]loss_total_epoch 71.52652091160417
Training tokenizer:  12% 993/8047 [28:07<3:47:38,  1.94s/it]loss_total_epoch 71.59303577616811
Training tokenizer:  12% 994/8047 [28:09<3:48:21,  1.94s/it]loss_total_epoch 71.66215008124709
Training tokenizer:  12% 995/8047 [28:11<3:47:34,  1.94s/it]loss_total_epoch 71.72027504444122
Training tokenizer:  12% 996/8047 [28:13<3:47:21,  1.93s/it]loss_total_epoch 71.7747052013874
Training tokenizer:  12% 997/8047 [28:15<3:47:05,  1.93s/it]loss_total_epoch 71.85358350723982
Training tokenizer:  12% 998/8047 [28:17<3:47:12,  1.93s/it]loss_total_epoch 71.90630822256207
Training tokenizer:  12% 999/8047 [28:19<3:46:44,  1.93s/it]loss_total_epoch 71.96773337945342
Training tokenizer:  12% 1000/8047 [28:21<3:46:00,  1.92s/it]loss_total_epoch 72.03479425236583
Training tokenizer:  12% 1001/8047 [28:23<3:47:28,  1.94s/it]loss_total_epoch 72.1051114834845
Training tokenizer:  12% 1002/8047 [28:25<3:48:27,  1.95s/it]loss_total_epoch 72.1678141541779
Training tokenizer:  12% 1003/8047 [28:27<3:48:14,  1.94s/it]loss_total_epoch 72.23339277133346
Training tokenizer:  12% 1004/8047 [28:29<3:47:26,  1.94s/it]loss_total_epoch 72.30032438412309
Training tokenizer:  12% 1005/8047 [28:31<3:47:53,  1.94s/it]loss_total_epoch 72.36315717920661
Training tokenizer:  13% 1006/8047 [28:33<3:47:56,  1.94s/it]loss_total_epoch 72.43127362802625
Training tokenizer:  13% 1007/8047 [28:34<3:47:48,  1.94s/it]loss_total_epoch 72.50357296690345
Training tokenizer:  13% 1008/8047 [28:36<3:48:34,  1.95s/it]loss_total_epoch 72.56753275170922
Training tokenizer:  13% 1009/8047 [28:38<3:48:42,  1.95s/it]loss_total_epoch 72.6258032694459
Training tokenizer:  13% 1010/8047 [28:40<3:48:36,  1.95s/it]loss_total_epoch 72.70018299669027
Training tokenizer:  13% 1011/8047 [28:42<3:47:50,  1.94s/it]loss_total_epoch 72.75380135327578
Training tokenizer:  13% 1012/8047 [28:44<3:47:06,  1.94s/it]loss_total_epoch 72.82345869392157
Training tokenizer:  13% 1013/8047 [28:46<3:47:03,  1.94s/it]loss_total_epoch 72.89154268801212
Training tokenizer:  13% 1014/8047 [28:48<3:47:28,  1.94s/it]loss_total_epoch 72.95406921207905
Training tokenizer:  13% 1015/8047 [28:50<3:48:01,  1.95s/it]loss_total_epoch 73.01628823205829
Training tokenizer:  13% 1016/8047 [28:52<3:49:11,  1.96s/it]loss_total_epoch 73.08721270784736
Training tokenizer:  13% 1017/8047 [28:54<3:49:26,  1.96s/it]loss_total_epoch 73.1467764750123
Training tokenizer:  13% 1018/8047 [28:56<3:49:06,  1.96s/it]loss_total_epoch 73.22252536565065
Training tokenizer:  13% 1019/8047 [28:58<3:49:29,  1.96s/it]loss_total_epoch 73.28586754947901
Training tokenizer:  13% 1020/8047 [29:00<3:48:44,  1.95s/it]loss_total_epoch 73.3509319499135
Training tokenizer:  13% 1021/8047 [29:02<3:48:18,  1.95s/it]loss_total_epoch 73.4228890389204
Training tokenizer:  13% 1022/8047 [29:04<3:48:05,  1.95s/it]loss_total_epoch 73.48426755145192
Training tokenizer:  13% 1023/8047 [29:06<3:46:30,  1.93s/it]loss_total_epoch 73.54698402062058
Training tokenizer:  13% 1024/8047 [29:08<3:48:07,  1.95s/it]loss_total_epoch 73.60370061919093
Training tokenizer:  13% 1025/8047 [29:10<3:49:37,  1.96s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-r70h8px4'
loss_total_epoch 73.66115836426616
Training tokenizer:  13% 1026/8047 [29:12<3:50:17,  1.97s/it]loss_total_epoch 73.73146203532815
Training tokenizer:  13% 1027/8047 [29:14<3:49:56,  1.97s/it]loss_total_epoch 73.79183803498745
Training tokenizer:  13% 1028/8047 [29:15<3:48:49,  1.96s/it]loss_total_epoch 73.86200052499771
Training tokenizer:  13% 1029/8047 [29:17<3:49:04,  1.96s/it]loss_total_epoch 73.92596960067749
Training tokenizer:  13% 1030/8047 [29:19<3:48:51,  1.96s/it]loss_total_epoch 74.00257512927055
Training tokenizer:  13% 1031/8047 [29:21<3:47:50,  1.95s/it]loss_total_epoch 74.06559788435698
Training tokenizer:  13% 1032/8047 [29:23<3:47:50,  1.95s/it]loss_total_epoch 74.12343025580049
Training tokenizer:  13% 1033/8047 [29:25<3:48:29,  1.95s/it]loss_total_epoch 74.18992106243968
Training tokenizer:  13% 1034/8047 [29:27<3:49:20,  1.96s/it]loss_total_epoch 74.25556407496333
Training tokenizer:  13% 1035/8047 [29:29<3:49:15,  1.96s/it]loss_total_epoch 74.32140147313476
Training tokenizer:  13% 1036/8047 [29:31<3:48:12,  1.95s/it]loss_total_epoch 74.38403644785285
Training tokenizer:  13% 1037/8047 [29:33<3:48:10,  1.95s/it]loss_total_epoch 74.46156707033515
Training tokenizer:  13% 1038/8047 [29:35<3:47:14,  1.95s/it]loss_total_epoch 74.53109504655004
Training tokenizer:  13% 1039/8047 [29:37<3:46:43,  1.94s/it]loss_total_epoch 74.60168671235442
Training tokenizer:  13% 1040/8047 [29:39<3:47:31,  1.95s/it]loss_total_epoch 74.66987375542521
Training tokenizer:  13% 1041/8047 [29:41<3:48:09,  1.95s/it]loss_total_epoch 74.73214175179601
Training tokenizer:  13% 1042/8047 [29:43<3:47:59,  1.95s/it]loss_total_epoch 74.79604914411902
Training tokenizer:  13% 1043/8047 [29:45<3:48:31,  1.96s/it]loss_total_epoch 74.87078519538045
Training tokenizer:  13% 1044/8047 [29:47<3:48:07,  1.95s/it]loss_total_epoch 74.94091187044978
Training tokenizer:  13% 1045/8047 [29:49<3:47:08,  1.95s/it]loss_total_epoch 75.00529197975993
Training tokenizer:  13% 1046/8047 [29:51<3:47:53,  1.95s/it]loss_total_epoch 75.06583238765597
Training tokenizer:  13% 1047/8047 [29:53<3:48:25,  1.96s/it]loss_total_epoch 75.13687809929252
Training tokenizer:  13% 1048/8047 [29:55<3:49:20,  1.97s/it]loss_total_epoch 75.2092197202146
Training tokenizer:  13% 1049/8047 [29:57<3:49:31,  1.97s/it]loss_total_epoch 75.27970012649894
Training tokenizer:  13% 1050/8047 [29:58<3:48:11,  1.96s/it]loss_total_epoch 75.36568703129888
Training tokenizer:  13% 1051/8047 [30:00<3:46:40,  1.94s/it]loss_total_epoch 75.42228357493877
Training tokenizer:  13% 1052/8047 [30:02<3:51:33,  1.99s/it]loss_total_epoch 75.49299240857363
Training tokenizer:  13% 1053/8047 [30:04<3:50:24,  1.98s/it]loss_total_epoch 75.55970332026482
Training tokenizer:  13% 1054/8047 [30:06<3:49:22,  1.97s/it]loss_total_epoch 75.62390074133873
Training tokenizer:  13% 1055/8047 [30:08<3:48:30,  1.96s/it]loss_total_epoch 75.69087202101946
Training tokenizer:  13% 1056/8047 [30:10<3:48:30,  1.96s/it]loss_total_epoch 75.75651042163372
Training tokenizer:  13% 1057/8047 [30:12<3:48:31,  1.96s/it]loss_total_epoch 75.83469245582819
Training tokenizer:  13% 1058/8047 [30:14<3:49:22,  1.97s/it]loss_total_epoch 75.89888328313828
Training tokenizer:  13% 1059/8047 [30:16<3:48:21,  1.96s/it]loss_total_epoch 75.97845783829689
Training tokenizer:  13% 1060/8047 [30:18<3:47:57,  1.96s/it]loss_total_epoch 76.04701597243547
Training tokenizer:  13% 1061/8047 [30:20<3:48:32,  1.96s/it]loss_total_epoch 76.11876682192087
Training tokenizer:  13% 1062/8047 [30:22<3:48:23,  1.96s/it]loss_total_epoch 76.1761165857315
Training tokenizer:  13% 1063/8047 [30:24<3:48:49,  1.97s/it]loss_total_epoch 76.24421402812004
Training tokenizer:  13% 1064/8047 [30:26<3:49:14,  1.97s/it]loss_total_epoch 76.30824313312769
Training tokenizer:  13% 1065/8047 [30:28<3:49:29,  1.97s/it]loss_total_epoch 76.37547964602709
Training tokenizer:  13% 1066/8047 [30:30<3:49:52,  1.98s/it]loss_total_epoch 76.4430713802576
Training tokenizer:  13% 1067/8047 [30:32<3:49:35,  1.97s/it]loss_total_epoch 76.50711454451084
Training tokenizer:  13% 1068/8047 [30:34<3:49:31,  1.97s/it]loss_total_epoch 76.56748096644878
Training tokenizer:  13% 1069/8047 [30:36<3:50:20,  1.98s/it]loss_total_epoch 76.63076485693455
Training tokenizer:  13% 1070/8047 [30:38<3:49:55,  1.98s/it]loss_total_epoch 76.70416217297316
Training tokenizer:  13% 1071/8047 [30:40<3:49:49,  1.98s/it]loss_total_epoch 76.77107317000628
Training tokenizer:  13% 1072/8047 [30:42<3:49:35,  1.97s/it]loss_total_epoch 76.84452220797539
Training tokenizer:  13% 1073/8047 [30:44<3:50:39,  1.98s/it]loss_total_epoch 76.92258484661579
Training tokenizer:  13% 1074/8047 [30:46<3:49:47,  1.98s/it]loss_total_epoch 76.9812977090478
Training tokenizer:  13% 1075/8047 [30:48<3:50:15,  1.98s/it]loss_total_epoch 77.04329485818744
Training tokenizer:  13% 1076/8047 [30:50<3:49:54,  1.98s/it]loss_total_epoch 77.10501414164901
Training tokenizer:  13% 1077/8047 [30:52<3:50:29,  1.98s/it]loss_total_epoch 77.15316182374954
Training tokenizer:  13% 1078/8047 [30:54<3:49:45,  1.98s/it]loss_total_epoch 77.21565211564302
Training tokenizer:  13% 1079/8047 [30:56<3:49:50,  1.98s/it]loss_total_epoch 77.28161291778088
Training tokenizer:  13% 1080/8047 [30:58<3:49:46,  1.98s/it]loss_total_epoch 77.34715286642313
Training tokenizer:  13% 1081/8047 [31:00<3:48:48,  1.97s/it]loss_total_epoch 77.40250534936786
Training tokenizer:  13% 1082/8047 [31:02<3:48:39,  1.97s/it]loss_total_epoch 77.46852060034871
Training tokenizer:  13% 1083/8047 [31:04<3:48:22,  1.97s/it]loss_total_epoch 77.54035667702556
Training tokenizer:  13% 1084/8047 [31:06<3:48:48,  1.97s/it]loss_total_epoch 77.61074761673808
Training tokenizer:  13% 1085/8047 [31:07<3:48:56,  1.97s/it]loss_total_epoch 77.67370734736323
Training tokenizer:  13% 1086/8047 [31:09<3:48:32,  1.97s/it]loss_total_epoch 77.74216007068753
Training tokenizer:  14% 1087/8047 [31:11<3:47:37,  1.96s/it]loss_total_epoch 77.80795092508197
Training tokenizer:  14% 1088/8047 [31:13<3:47:58,  1.97s/it]loss_total_epoch 77.86708349362016
Training tokenizer:  14% 1089/8047 [31:15<3:48:02,  1.97s/it]loss_total_epoch 77.94125202670693
Training tokenizer:  14% 1090/8047 [31:17<3:48:14,  1.97s/it]loss_total_epoch 78.00266939774156
Training tokenizer:  14% 1091/8047 [31:19<3:48:10,  1.97s/it]loss_total_epoch 78.06156112253666
Training tokenizer:  14% 1092/8047 [31:21<3:48:48,  1.97s/it]loss_total_epoch 78.11669135838747
Training tokenizer:  14% 1093/8047 [31:23<3:48:29,  1.97s/it]loss_total_epoch 78.17342945188284
Training tokenizer:  14% 1094/8047 [31:25<3:48:24,  1.97s/it]loss_total_epoch 78.23007253557444
Training tokenizer:  14% 1095/8047 [31:27<3:48:56,  1.98s/it]loss_total_epoch 78.27513838931918
Training tokenizer:  14% 1096/8047 [31:29<3:48:35,  1.97s/it]loss_total_epoch 78.33072720468044
Training tokenizer:  14% 1097/8047 [31:31<3:48:30,  1.97s/it]loss_total_epoch 78.38759176805615
Training tokenizer:  14% 1098/8047 [31:33<3:48:40,  1.97s/it]loss_total_epoch 78.45948416367173
Training tokenizer:  14% 1099/8047 [31:35<3:48:46,  1.98s/it]loss_total_epoch 78.52799847349524
Training tokenizer:  14% 1100/8047 [31:37<3:49:42,  1.98s/it]loss_total_epoch 78.6086501441896
Training tokenizer:  14% 1101/8047 [31:39<3:50:03,  1.99s/it]loss_total_epoch 78.67025512084365
Training tokenizer:  14% 1102/8047 [31:41<3:49:56,  1.99s/it]loss_total_epoch 78.73145545274019
Training tokenizer:  14% 1103/8047 [31:43<3:48:40,  1.98s/it]loss_total_epoch 78.77855094149709
Training tokenizer:  14% 1104/8047 [31:45<3:48:16,  1.97s/it]loss_total_epoch 78.84919011220336
Training tokenizer:  14% 1105/8047 [31:47<3:48:51,  1.98s/it]loss_total_epoch 78.91117887571454
Training tokenizer:  14% 1106/8047 [31:49<3:49:38,  1.99s/it]loss_total_epoch 78.98063754662871
Training tokenizer:  14% 1107/8047 [31:51<3:49:48,  1.99s/it]loss_total_epoch 79.0525652654469
Training tokenizer:  14% 1108/8047 [31:53<3:49:53,  1.99s/it]loss_total_epoch 79.11840957775712
Training tokenizer:  14% 1109/8047 [31:55<3:50:17,  1.99s/it]loss_total_epoch 79.1616110689938
Training tokenizer:  14% 1110/8047 [31:57<3:50:12,  1.99s/it]loss_total_epoch 79.23246474191546
Training tokenizer:  14% 1111/8047 [31:59<3:50:15,  1.99s/it]loss_total_epoch 79.31225076690316
Training tokenizer:  14% 1112/8047 [32:01<3:50:28,  1.99s/it]loss_total_epoch 79.3824654109776
Training tokenizer:  14% 1113/8047 [32:03<3:51:27,  2.00s/it]loss_total_epoch 79.45475629344583
Training tokenizer:  14% 1114/8047 [32:05<3:51:43,  2.01s/it]loss_total_epoch 79.5344371162355
Training tokenizer:  14% 1115/8047 [32:07<3:51:35,  2.00s/it]loss_total_epoch 79.60540768131614
Training tokenizer:  14% 1116/8047 [32:09<3:51:45,  2.01s/it]loss_total_epoch 79.67037166282535
Training tokenizer:  14% 1117/8047 [32:11<3:51:15,  2.00s/it]loss_total_epoch 79.74405730888247
Training tokenizer:  14% 1118/8047 [32:13<3:50:32,  2.00s/it]loss_total_epoch 79.80225086212158
Training tokenizer:  14% 1119/8047 [32:15<3:50:27,  2.00s/it]loss_total_epoch 79.86766710877419
Training tokenizer:  14% 1120/8047 [32:17<3:49:53,  1.99s/it]loss_total_epoch 79.92335268855095
Training tokenizer:  14% 1121/8047 [32:19<3:48:53,  1.98s/it]loss_total_epoch 79.98674157261848
Training tokenizer:  14% 1122/8047 [32:21<3:49:49,  1.99s/it]loss_total_epoch 80.06070787459612
Training tokenizer:  14% 1123/8047 [32:23<3:50:32,  2.00s/it]loss_total_epoch 80.13199573010206
Training tokenizer:  14% 1124/8047 [32:25<3:49:34,  1.99s/it]loss_total_epoch 80.20081663131714
Training tokenizer:  14% 1125/8047 [32:27<3:48:33,  1.98s/it]loss_total_epoch 80.25291749089956
Training tokenizer:  14% 1126/8047 [32:29<3:48:58,  1.99s/it]loss_total_epoch 80.30778583139181
Training tokenizer:  14% 1127/8047 [32:31<3:49:16,  1.99s/it]loss_total_epoch 80.3585355207324
Training tokenizer:  14% 1128/8047 [32:33<3:49:37,  1.99s/it]loss_total_epoch 80.42956908792257
Training tokenizer:  14% 1129/8047 [32:35<3:51:26,  2.01s/it]loss_total_epoch 80.48859968036413
Training tokenizer:  14% 1130/8047 [32:37<3:50:25,  2.00s/it]loss_total_epoch 80.56536710262299
Training tokenizer:  14% 1131/8047 [32:39<3:49:26,  1.99s/it]loss_total_epoch 80.62507575005293
Training tokenizer:  14% 1132/8047 [32:41<3:49:39,  1.99s/it]loss_total_epoch 80.6951637789607
Training tokenizer:  14% 1133/8047 [32:43<3:50:30,  2.00s/it]loss_total_epoch 80.76125028729439
Training tokenizer:  14% 1134/8047 [32:45<3:50:55,  2.00s/it]loss_total_epoch 80.83566676080227
Training tokenizer:  14% 1135/8047 [32:47<3:50:37,  2.00s/it]loss_total_epoch 80.89898444712162
Training tokenizer:  14% 1136/8047 [32:49<3:50:19,  2.00s/it]loss_total_epoch 80.96348650753498
Training tokenizer:  14% 1137/8047 [32:51<3:49:49,  2.00s/it]loss_total_epoch 81.01459058746696
Training tokenizer:  14% 1138/8047 [32:53<3:51:35,  2.01s/it]loss_total_epoch 81.07045036554337
Training tokenizer:  14% 1139/8047 [32:55<3:51:07,  2.01s/it]loss_total_epoch 81.14277213066816
Training tokenizer:  14% 1140/8047 [32:57<3:51:07,  2.01s/it]loss_total_epoch 81.21555332094431
Training tokenizer:  14% 1141/8047 [32:59<3:52:15,  2.02s/it]loss_total_epoch 81.28235784173012
Training tokenizer:  14% 1142/8047 [33:01<3:51:36,  2.01s/it]loss_total_epoch 81.34784832596779
Training tokenizer:  14% 1143/8047 [33:03<3:51:40,  2.01s/it]loss_total_epoch 81.40323374420404
Training tokenizer:  14% 1144/8047 [33:05<3:52:03,  2.02s/it]loss_total_epoch 81.46365851163864
Training tokenizer:  14% 1145/8047 [33:07<3:52:13,  2.02s/it]loss_total_epoch 81.51591989770532
Training tokenizer:  14% 1146/8047 [33:09<3:52:09,  2.02s/it]loss_total_epoch 81.5842181108892
Training tokenizer:  14% 1147/8047 [33:11<3:51:28,  2.01s/it]loss_total_epoch 81.6478840149939
Training tokenizer:  14% 1148/8047 [33:13<3:50:38,  2.01s/it]loss_total_epoch 81.71945187821984
Training tokenizer:  14% 1149/8047 [33:15<3:49:41,  2.00s/it]loss_total_epoch 81.7956097535789
Training tokenizer:  14% 1150/8047 [33:17<3:49:08,  1.99s/it]loss_total_epoch 81.8539007641375
Training tokenizer:  14% 1151/8047 [33:19<3:48:46,  1.99s/it]loss_total_epoch 81.92303039506078
Training tokenizer:  14% 1152/8047 [33:21<3:49:11,  1.99s/it]loss_total_epoch 81.98221763595939
Training tokenizer:  14% 1153/8047 [33:23<3:49:36,  2.00s/it]loss_total_epoch 82.04266842827201
Training tokenizer:  14% 1154/8047 [33:25<3:49:59,  2.00s/it]loss_total_epoch 82.10350236296654
Training tokenizer:  14% 1155/8047 [33:27<3:49:43,  2.00s/it]loss_total_epoch 82.1699088215828
Training tokenizer:  14% 1156/8047 [33:29<3:50:00,  2.00s/it]loss_total_epoch 82.23838224261999
Training tokenizer:  14% 1157/8047 [33:31<3:50:34,  2.01s/it]loss_total_epoch 82.3149612993002
Training tokenizer:  14% 1158/8047 [33:33<3:50:59,  2.01s/it]loss_total_epoch 82.39641243219376
Training tokenizer:  14% 1159/8047 [33:35<3:50:07,  2.00s/it]loss_total_epoch 82.45518684014678
Training tokenizer:  14% 1160/8047 [33:37<3:49:48,  2.00s/it]loss_total_epoch 82.50350408628583
Training tokenizer:  14% 1161/8047 [33:39<3:50:35,  2.01s/it]loss_total_epoch 82.57351337745786
Training tokenizer:  14% 1162/8047 [33:41<3:50:42,  2.01s/it]loss_total_epoch 82.65642119571567
Training tokenizer:  14% 1163/8047 [33:43<3:51:09,  2.01s/it]loss_total_epoch 82.72784082964063
Training tokenizer:  14% 1164/8047 [33:45<3:51:12,  2.02s/it]loss_total_epoch 82.79297490790486
Training tokenizer:  14% 1165/8047 [33:47<3:51:23,  2.02s/it]loss_total_epoch 82.85619333013892
Training tokenizer:  14% 1166/8047 [33:49<3:51:38,  2.02s/it]loss_total_epoch 82.91550938040018
Training tokenizer:  15% 1167/8047 [33:51<3:51:07,  2.02s/it]loss_total_epoch 82.97164818644524
Training tokenizer:  15% 1168/8047 [33:53<3:50:20,  2.01s/it]loss_total_epoch 83.04745604097843
Training tokenizer:  15% 1169/8047 [33:55<3:49:19,  2.00s/it]loss_total_epoch 83.1111623197794
Training tokenizer:  15% 1170/8047 [33:57<3:48:48,  2.00s/it]loss_total_epoch 83.17067959159613
Training tokenizer:  15% 1171/8047 [33:59<3:50:22,  2.01s/it]loss_total_epoch 83.25113096833229
Training tokenizer:  15% 1172/8047 [34:01<3:49:27,  2.00s/it]loss_total_epoch 83.30883036926389
Training tokenizer:  15% 1173/8047 [34:03<3:50:16,  2.01s/it]loss_total_epoch 83.36622705310583
Training tokenizer:  15% 1174/8047 [34:05<3:50:01,  2.01s/it]loss_total_epoch 83.42542467266321
Training tokenizer:  15% 1175/8047 [34:07<3:49:03,  2.00s/it]loss_total_epoch 83.5024099573493
Training tokenizer:  15% 1176/8047 [34:09<3:47:29,  1.99s/it]loss_total_epoch 83.5559294000268
Training tokenizer:  15% 1177/8047 [34:11<3:46:59,  1.98s/it]loss_total_epoch 83.61921037733555
Training tokenizer:  15% 1178/8047 [34:13<3:45:51,  1.97s/it]loss_total_epoch 83.68520699441433
Training tokenizer:  15% 1179/8047 [34:15<3:47:11,  1.98s/it]loss_total_epoch 83.75336761027575
Training tokenizer:  15% 1180/8047 [34:17<3:48:50,  2.00s/it]loss_total_epoch 83.81633219122887
Training tokenizer:  15% 1181/8047 [34:19<3:49:32,  2.01s/it]loss_total_epoch 83.89480277895927
Training tokenizer:  15% 1182/8047 [34:21<3:49:05,  2.00s/it]loss_total_epoch 83.96015809476376
Training tokenizer:  15% 1183/8047 [34:23<3:49:35,  2.01s/it]loss_total_epoch 84.02198441326618
Training tokenizer:  15% 1184/8047 [34:25<3:54:26,  2.05s/it]loss_total_epoch 84.08997349441051
Training tokenizer:  15% 1185/8047 [34:27<3:53:40,  2.04s/it]loss_total_epoch 84.1584072932601
Training tokenizer:  15% 1186/8047 [34:29<3:52:54,  2.04s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-hd48voxs'
loss_total_epoch 84.22517370432615
Training tokenizer:  15% 1187/8047 [34:31<3:52:22,  2.03s/it]loss_total_epoch 84.30139610916376
Training tokenizer:  15% 1188/8047 [34:33<3:51:52,  2.03s/it]loss_total_epoch 84.38376086205244
Training tokenizer:  15% 1189/8047 [34:35<3:51:04,  2.02s/it]loss_total_epoch 84.45672350376844
Training tokenizer:  15% 1190/8047 [34:37<3:51:07,  2.02s/it]loss_total_epoch 84.51650597900152
Training tokenizer:  15% 1191/8047 [34:39<3:51:52,  2.03s/it]loss_total_epoch 84.57385830581188
Training tokenizer:  15% 1192/8047 [34:41<3:51:38,  2.03s/it]loss_total_epoch 84.64323338121176
Training tokenizer:  15% 1193/8047 [34:43<3:49:25,  2.01s/it]loss_total_epoch 84.7024562060833
Training tokenizer:  15% 1194/8047 [34:45<3:49:45,  2.01s/it]loss_total_epoch 84.76004701852798
Training tokenizer:  15% 1195/8047 [34:47<3:49:29,  2.01s/it]loss_total_epoch 84.83985122293234
Training tokenizer:  15% 1196/8047 [34:49<3:50:08,  2.02s/it]loss_total_epoch 84.9174123108387
Training tokenizer:  15% 1197/8047 [34:51<3:50:17,  2.02s/it]loss_total_epoch 84.9840838611126
Training tokenizer:  15% 1198/8047 [34:54<3:50:42,  2.02s/it]loss_total_epoch 85.04571360349655
Training tokenizer:  15% 1199/8047 [34:56<3:51:12,  2.03s/it]loss_total_epoch 85.10446568578482
Training tokenizer:  15% 1200/8047 [34:58<3:51:29,  2.03s/it]loss_total_epoch 85.15232795476913
Training tokenizer:  15% 1201/8047 [35:00<3:52:01,  2.03s/it]loss_total_epoch 85.22157268226147
Training tokenizer:  15% 1202/8047 [35:02<3:51:32,  2.03s/it]loss_total_epoch 85.30390314757824
Training tokenizer:  15% 1203/8047 [35:04<3:51:30,  2.03s/it]loss_total_epoch 85.37685170769691
Training tokenizer:  15% 1204/8047 [35:06<3:51:52,  2.03s/it]loss_total_epoch 85.43705987930298
Training tokenizer:  15% 1205/8047 [35:08<3:52:10,  2.04s/it]loss_total_epoch 85.50437236577272
Training tokenizer:  15% 1206/8047 [35:10<3:52:46,  2.04s/it]loss_total_epoch 85.57491163909435
Training tokenizer:  15% 1207/8047 [35:12<3:52:51,  2.04s/it]loss_total_epoch 85.64314697682858
Training tokenizer:  15% 1208/8047 [35:14<3:52:12,  2.04s/it]loss_total_epoch 85.6996874883771
Training tokenizer:  15% 1209/8047 [35:16<3:52:18,  2.04s/it]loss_total_epoch 85.76664542406797
Training tokenizer:  15% 1210/8047 [35:18<3:52:06,  2.04s/it]loss_total_epoch 85.82080396264791
Training tokenizer:  15% 1211/8047 [35:20<3:51:32,  2.03s/it]loss_total_epoch 85.88801927119493
Training tokenizer:  15% 1212/8047 [35:22<3:52:12,  2.04s/it]loss_total_epoch 85.94676987826824
Training tokenizer:  15% 1213/8047 [35:24<3:53:42,  2.05s/it]loss_total_epoch 86.01859998703003
Training tokenizer:  15% 1214/8047 [35:26<3:53:34,  2.05s/it]loss_total_epoch 86.08436423540115
Training tokenizer:  15% 1215/8047 [35:28<3:53:31,  2.05s/it]loss_total_epoch 86.14644289016724
Training tokenizer:  15% 1216/8047 [35:30<3:51:42,  2.04s/it]loss_total_epoch 86.2071077786386
Training tokenizer:  15% 1217/8047 [35:32<3:50:13,  2.02s/it]loss_total_epoch 86.27218149974942
Training tokenizer:  15% 1218/8047 [35:34<3:50:56,  2.03s/it]loss_total_epoch 86.31426464393735
Training tokenizer:  15% 1219/8047 [35:36<3:51:47,  2.04s/it]loss_total_epoch 86.37768684700131
Training tokenizer:  15% 1220/8047 [35:38<3:51:53,  2.04s/it]loss_total_epoch 86.44786826148629
Training tokenizer:  15% 1221/8047 [35:40<3:50:48,  2.03s/it]loss_total_epoch 86.5078480951488
Training tokenizer:  15% 1222/8047 [35:42<3:50:09,  2.02s/it]loss_total_epoch 86.57898195460439
Training tokenizer:  15% 1223/8047 [35:44<3:49:49,  2.02s/it]loss_total_epoch 86.644675206393
Training tokenizer:  15% 1224/8047 [35:46<3:50:10,  2.02s/it]loss_total_epoch 86.71350594982505
Training tokenizer:  15% 1225/8047 [35:48<3:50:08,  2.02s/it]loss_total_epoch 86.77316519990563
Training tokenizer:  15% 1226/8047 [35:50<3:50:00,  2.02s/it]loss_total_epoch 86.84183823689818
Training tokenizer:  15% 1227/8047 [35:52<3:49:50,  2.02s/it]loss_total_epoch 86.92134493216872
Training tokenizer:  15% 1228/8047 [35:55<3:51:04,  2.03s/it]loss_total_epoch 86.98030902445316
Training tokenizer:  15% 1229/8047 [35:57<3:53:33,  2.06s/it]loss_total_epoch 87.04693756997585
Training tokenizer:  15% 1230/8047 [35:59<3:52:45,  2.05s/it]loss_total_epoch 87.10508063435555
Training tokenizer:  15% 1231/8047 [36:01<3:52:57,  2.05s/it]loss_total_epoch 87.17669320851564
Training tokenizer:  15% 1232/8047 [36:03<3:53:38,  2.06s/it]loss_total_epoch 87.24733544141054
Training tokenizer:  15% 1233/8047 [36:05<3:52:35,  2.05s/it]loss_total_epoch 87.31731751561165
Training tokenizer:  15% 1234/8047 [36:07<3:52:06,  2.04s/it]loss_total_epoch 87.36589558422565
Training tokenizer:  15% 1235/8047 [36:09<3:51:34,  2.04s/it]loss_total_epoch 87.41309178248048
Training tokenizer:  15% 1236/8047 [36:11<3:51:42,  2.04s/it]loss_total_epoch 87.46937157213688
Training tokenizer:  15% 1237/8047 [36:13<3:52:11,  2.05s/it]loss_total_epoch 87.5272836536169
Training tokenizer:  15% 1238/8047 [36:15<3:52:35,  2.05s/it]loss_total_epoch 87.60093279927969
Training tokenizer:  15% 1239/8047 [36:17<3:52:15,  2.05s/it]loss_total_epoch 87.65971442312002
Training tokenizer:  15% 1240/8047 [36:19<3:51:53,  2.04s/it]loss_total_epoch 87.73227156698704
Training tokenizer:  15% 1241/8047 [36:21<3:51:43,  2.04s/it]loss_total_epoch 87.79894905537367
Training tokenizer:  15% 1242/8047 [36:23<3:51:17,  2.04s/it]loss_total_epoch 87.85650837793946
Training tokenizer:  15% 1243/8047 [36:25<3:51:34,  2.04s/it]loss_total_epoch 87.9304777868092
Training tokenizer:  15% 1244/8047 [36:27<3:51:37,  2.04s/it]loss_total_epoch 87.98863379657269
Training tokenizer:  15% 1245/8047 [36:29<3:52:13,  2.05s/it]loss_total_epoch 88.04070902615786
Training tokenizer:  15% 1246/8047 [36:31<3:51:48,  2.05s/it]loss_total_epoch 88.10257550701499
Training tokenizer:  15% 1247/8047 [36:33<3:52:01,  2.05s/it]loss_total_epoch 88.16160871088505
Training tokenizer:  16% 1248/8047 [36:35<3:51:23,  2.04s/it]loss_total_epoch 88.24091459810734
Training tokenizer:  16% 1249/8047 [36:38<3:50:56,  2.04s/it]loss_total_epoch 88.31125642359257
Training tokenizer:  16% 1250/8047 [36:40<3:50:12,  2.03s/it]loss_total_epoch 88.3650244474411
Training tokenizer:  16% 1251/8047 [36:42<3:51:04,  2.04s/it]loss_total_epoch 88.42333478853106
Training tokenizer:  16% 1252/8047 [36:44<3:52:03,  2.05s/it]loss_total_epoch 88.47495332360268
Training tokenizer:  16% 1253/8047 [36:46<3:52:29,  2.05s/it]loss_total_epoch 88.53570153564215
Training tokenizer:  16% 1254/8047 [36:48<3:52:18,  2.05s/it]loss_total_epoch 88.59572359919548
Training tokenizer:  16% 1255/8047 [36:50<3:52:37,  2.05s/it]loss_total_epoch 88.63929610699415
Training tokenizer:  16% 1256/8047 [36:52<3:52:36,  2.06s/it]loss_total_epoch 88.69800781831145
Training tokenizer:  16% 1257/8047 [36:54<3:52:13,  2.05s/it]loss_total_epoch 88.76400200650096
Training tokenizer:  16% 1258/8047 [36:56<3:52:17,  2.05s/it]loss_total_epoch 88.83044726774096
Training tokenizer:  16% 1259/8047 [36:58<3:52:33,  2.06s/it]loss_total_epoch 88.88662424311042
Training tokenizer:  16% 1260/8047 [37:01<4:14:37,  2.25s/it]loss_total_epoch 88.94483371078968
Training tokenizer:  16% 1261/8047 [37:03<4:07:29,  2.19s/it]loss_total_epoch 89.01483512669802
Training tokenizer:  16% 1262/8047 [37:05<4:04:01,  2.16s/it]loss_total_epoch 89.07933013141155
Training tokenizer:  16% 1263/8047 [37:07<4:00:57,  2.13s/it]loss_total_epoch 89.14921603351831
Training tokenizer:  16% 1264/8047 [37:09<3:57:38,  2.10s/it]loss_total_epoch 89.21509239822626
Training tokenizer:  16% 1265/8047 [37:11<3:56:21,  2.09s/it]loss_total_epoch 89.28107958287
Training tokenizer:  16% 1266/8047 [37:13<3:55:22,  2.08s/it]loss_total_epoch 89.33940610289574
Training tokenizer:  16% 1267/8047 [37:15<3:54:54,  2.08s/it]loss_total_epoch 89.41297229379416
Training tokenizer:  16% 1268/8047 [37:17<3:55:15,  2.08s/it]loss_total_epoch 89.48599161952734
Training tokenizer:  16% 1269/8047 [37:19<3:55:20,  2.08s/it]loss_total_epoch 89.56379415094852
Training tokenizer:  16% 1270/8047 [37:21<3:55:21,  2.08s/it]loss_total_epoch 89.62523080781102
Training tokenizer:  16% 1271/8047 [37:24<3:55:26,  2.08s/it]loss_total_epoch 89.68999630585313
Training tokenizer:  16% 1272/8047 [37:26<3:54:44,  2.08s/it]loss_total_epoch 89.75318375602365
Training tokenizer:  16% 1273/8047 [37:28<3:54:35,  2.08s/it]loss_total_epoch 89.80836744233966
Training tokenizer:  16% 1274/8047 [37:30<3:53:29,  2.07s/it]loss_total_epoch 89.87349320575595
Training tokenizer:  16% 1275/8047 [37:32<3:53:58,  2.07s/it]loss_total_epoch 89.93757986649871
Training tokenizer:  16% 1276/8047 [37:34<3:54:06,  2.07s/it]loss_total_epoch 89.99416881799698
Training tokenizer:  16% 1277/8047 [37:36<3:54:14,  2.08s/it]loss_total_epoch 90.05975790321827
Training tokenizer:  16% 1278/8047 [37:38<3:53:46,  2.07s/it]loss_total_epoch 90.11215313524008
Training tokenizer:  16% 1279/8047 [37:40<3:53:17,  2.07s/it]loss_total_epoch 90.17038243636489
Training tokenizer:  16% 1280/8047 [37:42<3:53:28,  2.07s/it]loss_total_epoch 90.23611925914884
Training tokenizer:  16% 1281/8047 [37:44<3:52:19,  2.06s/it]loss_total_epoch 90.30830858275294
Training tokenizer:  16% 1282/8047 [37:46<3:52:08,  2.06s/it]loss_total_epoch 90.3626247048378
Training tokenizer:  16% 1283/8047 [37:48<3:51:54,  2.06s/it]loss_total_epoch 90.41359348222613
Training tokenizer:  16% 1284/8047 [37:50<3:51:07,  2.05s/it]loss_total_epoch 90.46648904308677
Training tokenizer:  16% 1285/8047 [37:53<4:00:19,  2.13s/it]loss_total_epoch 90.53191547468305
Training tokenizer:  16% 1286/8047 [37:55<3:58:56,  2.12s/it]loss_total_epoch 90.58645822107792
Training tokenizer:  16% 1287/8047 [37:57<3:56:51,  2.10s/it]loss_total_epoch 90.63487654179335
Training tokenizer:  16% 1288/8047 [37:59<3:56:21,  2.10s/it]loss_total_epoch 90.70624304562807
Training tokenizer:  16% 1289/8047 [38:01<3:56:24,  2.10s/it]loss_total_epoch 90.78018297255039
Training tokenizer:  16% 1290/8047 [38:03<3:55:53,  2.09s/it]loss_total_epoch 90.83887354657054
Training tokenizer:  16% 1291/8047 [38:05<3:54:44,  2.08s/it]loss_total_epoch 90.90405472740531
Training tokenizer:  16% 1292/8047 [38:07<3:54:39,  2.08s/it]loss_total_epoch 90.97165075317025
Training tokenizer:  16% 1293/8047 [38:09<3:54:21,  2.08s/it]loss_total_epoch 91.04639976844192
Training tokenizer:  16% 1294/8047 [38:11<3:52:39,  2.07s/it]loss_total_epoch 91.10948009416461
Training tokenizer:  16% 1295/8047 [38:14<4:00:45,  2.14s/it]loss_total_epoch 91.16250735148787
Training tokenizer:  16% 1296/8047 [38:16<3:57:54,  2.11s/it]loss_total_epoch 91.22794734314084
Training tokenizer:  16% 1297/8047 [38:18<3:57:19,  2.11s/it]loss_total_epoch 91.26992845907807
Training tokenizer:  16% 1298/8047 [38:20<3:56:39,  2.10s/it]loss_total_epoch 91.33344163373113
Training tokenizer:  16% 1299/8047 [38:22<3:55:33,  2.09s/it]loss_total_epoch 91.38042606785893
Training tokenizer:  16% 1300/8047 [38:24<3:53:30,  2.08s/it]loss_total_epoch 91.44852840527892
Training tokenizer:  16% 1301/8047 [38:26<3:51:33,  2.06s/it]loss_total_epoch 91.50087148323655
Training tokenizer:  16% 1302/8047 [38:28<3:51:15,  2.06s/it]loss_total_epoch 91.55699329078197
Training tokenizer:  16% 1303/8047 [38:30<3:51:59,  2.06s/it]loss_total_epoch 91.63467289507389
Training tokenizer:  16% 1304/8047 [38:32<3:52:27,  2.07s/it]loss_total_epoch 91.71059376746416
Training tokenizer:  16% 1305/8047 [38:34<3:52:47,  2.07s/it]loss_total_epoch 91.77199012041092
Training tokenizer:  16% 1306/8047 [38:36<3:53:14,  2.08s/it]loss_total_epoch 91.83258543536067
Training tokenizer:  16% 1307/8047 [38:38<3:52:48,  2.07s/it]loss_total_epoch 91.89004611223936
Training tokenizer:  16% 1308/8047 [38:41<3:52:55,  2.07s/it]loss_total_epoch 91.95859660953283
Training tokenizer:  16% 1309/8047 [38:43<3:52:07,  2.07s/it]loss_total_epoch 92.01456834003329
Training tokenizer:  16% 1310/8047 [38:45<3:52:03,  2.07s/it]loss_total_epoch 92.08634525164962
Training tokenizer:  16% 1311/8047 [38:47<3:52:23,  2.07s/it]loss_total_epoch 92.1419368237257
Training tokenizer:  16% 1312/8047 [38:49<3:52:58,  2.08s/it]loss_total_epoch 92.19569996371865
Training tokenizer:  16% 1313/8047 [38:51<3:52:48,  2.07s/it]loss_total_epoch 92.25010700523853
Training tokenizer:  16% 1314/8047 [38:53<3:53:18,  2.08s/it]loss_total_epoch 92.32421231269836
Training tokenizer:  16% 1315/8047 [38:55<3:53:15,  2.08s/it]loss_total_epoch 92.37915500998497
Training tokenizer:  16% 1316/8047 [38:57<3:54:28,  2.09s/it]loss_total_epoch 92.44332857429981
Training tokenizer:  16% 1317/8047 [38:59<3:55:24,  2.10s/it]loss_total_epoch 92.51416075229645
Training tokenizer:  16% 1318/8047 [39:01<3:55:57,  2.10s/it]loss_total_epoch 92.56980751827359
Training tokenizer:  16% 1319/8047 [39:04<3:55:36,  2.10s/it]loss_total_epoch 92.62715619802475
Training tokenizer:  16% 1320/8047 [39:06<3:55:33,  2.10s/it]loss_total_epoch 92.6934452354908
Training tokenizer:  16% 1321/8047 [39:08<3:55:07,  2.10s/it]loss_total_epoch 92.75998064875603
Training tokenizer:  16% 1322/8047 [39:10<3:54:48,  2.10s/it]loss_total_epoch 92.8125011138618
Training tokenizer:  16% 1323/8047 [39:12<4:02:17,  2.16s/it]loss_total_epoch 92.87866181507707
Training tokenizer:  16% 1324/8047 [39:14<4:00:49,  2.15s/it]loss_total_epoch 92.94316285476089
Training tokenizer:  16% 1325/8047 [39:16<3:59:05,  2.13s/it]loss_total_epoch 92.9994481280446
Training tokenizer:  16% 1326/8047 [39:18<3:58:05,  2.13s/it]loss_total_epoch 93.06148833781481
Training tokenizer:  16% 1327/8047 [39:20<3:55:20,  2.10s/it]loss_total_epoch 93.11932185664773
Training tokenizer:  17% 1328/8047 [39:23<3:54:42,  2.10s/it]loss_total_epoch 93.18391002342105
Training tokenizer:  17% 1329/8047 [39:25<3:55:13,  2.10s/it]loss_total_epoch 93.25395657494664
Training tokenizer:  17% 1330/8047 [39:27<3:54:23,  2.09s/it]loss_total_epoch 93.31980926170945
Training tokenizer:  17% 1331/8047 [39:29<3:54:31,  2.10s/it]loss_total_epoch 93.38982916995883
Training tokenizer:  17% 1332/8047 [39:31<3:54:00,  2.09s/it]loss_total_epoch 93.45058136060834
Training tokenizer:  17% 1333/8047 [39:33<3:53:45,  2.09s/it]loss_total_epoch 93.50763353705406
Training tokenizer:  17% 1334/8047 [39:35<3:54:23,  2.09s/it]loss_total_epoch 93.55997293442488
Training tokenizer:  17% 1335/8047 [39:37<3:54:52,  2.10s/it]loss_total_epoch 93.6234170421958
Training tokenizer:  17% 1336/8047 [39:39<3:55:30,  2.11s/it]loss_total_epoch 93.67106924578547
Training tokenizer:  17% 1337/8047 [39:41<3:53:18,  2.09s/it]loss_total_epoch 93.73453703895211
Training tokenizer:  17% 1338/8047 [39:44<3:54:53,  2.10s/it]loss_total_epoch 93.80438705906272
Training tokenizer:  17% 1339/8047 [39:46<3:55:42,  2.11s/it]loss_total_epoch 93.86123143136501
Training tokenizer:  17% 1340/8047 [39:48<3:54:50,  2.10s/it]loss_total_epoch 93.91425393894315
Training tokenizer:  17% 1341/8047 [39:50<3:54:02,  2.09s/it]loss_total_epoch 93.97354293987155
Training tokenizer:  17% 1342/8047 [39:52<3:54:44,  2.10s/it]loss_total_epoch 94.0326540581882
Training tokenizer:  17% 1343/8047 [39:54<3:55:18,  2.11s/it]loss_total_epoch 94.08412786573172
Training tokenizer:  17% 1344/8047 [39:56<3:55:32,  2.11s/it]loss_total_epoch 94.14121583849192
Training tokenizer:  17% 1345/8047 [39:58<3:56:23,  2.12s/it]loss_total_epoch 94.19906647503376
Training tokenizer:  17% 1346/8047 [40:00<3:56:59,  2.12s/it]loss_total_epoch 94.24961894378066
Training tokenizer:  17% 1347/8047 [40:03<3:56:08,  2.11s/it]loss_total_epoch 94.30873008072376
Training tokenizer:  17% 1348/8047 [40:05<3:54:52,  2.10s/it]loss_total_epoch 94.3628463409841
Training tokenizer:  17% 1349/8047 [40:07<3:55:18,  2.11s/it]loss_total_epoch 94.4314875639975
Training tokenizer:  17% 1350/8047 [40:09<3:54:38,  2.10s/it]loss_total_epoch 94.48794128745794
Training tokenizer:  17% 1351/8047 [40:11<3:53:29,  2.09s/it]loss_total_epoch 94.55337606370449
Training tokenizer:  17% 1352/8047 [40:13<3:54:22,  2.10s/it]loss_total_epoch 94.61432276666164
Training tokenizer:  17% 1353/8047 [40:15<3:55:51,  2.11s/it]loss_total_epoch 94.67075523734093
Training tokenizer:  17% 1354/8047 [40:17<3:56:28,  2.12s/it]loss_total_epoch 94.74482247978449
Training tokenizer:  17% 1355/8047 [40:19<3:56:22,  2.12s/it]loss_total_epoch 94.80263162404299
Training tokenizer:  17% 1356/8047 [40:21<3:55:12,  2.11s/it]loss_total_epoch 94.86350290104747
Training tokenizer:  17% 1357/8047 [40:24<3:53:49,  2.10s/it]loss_total_epoch 94.92859091237187
Training tokenizer:  17% 1358/8047 [40:26<3:54:30,  2.10s/it]loss_total_epoch 94.98497988656163
Training tokenizer:  17% 1359/8047 [40:28<3:54:37,  2.10s/it]loss_total_epoch 95.0430382117629
Training tokenizer:  17% 1360/8047 [40:30<3:55:14,  2.11s/it]loss_total_epoch 95.10134657472372
Training tokenizer:  17% 1361/8047 [40:32<3:55:25,  2.11s/it]loss_total_epoch 95.16385738551617
Training tokenizer:  17% 1362/8047 [40:34<3:54:54,  2.11s/it]loss_total_epoch 95.22281159088016
Training tokenizer:  17% 1363/8047 [40:36<3:54:45,  2.11s/it]loss_total_epoch 95.27182169258595
Training tokenizer:  17% 1364/8047 [40:38<3:55:22,  2.11s/it]loss_total_epoch 95.32988381385803
Training tokenizer:  17% 1365/8047 [40:40<3:55:08,  2.11s/it]loss_total_epoch 95.38960900902748
Training tokenizer:  17% 1366/8047 [40:43<3:55:12,  2.11s/it]loss_total_epoch 95.44413733109832
Training tokenizer:  17% 1367/8047 [40:45<3:55:43,  2.12s/it]loss_total_epoch 95.50274616852403
Training tokenizer:  17% 1368/8047 [40:47<3:56:14,  2.12s/it]loss_total_epoch 95.56454759836197
Training tokenizer:  17% 1369/8047 [40:49<3:55:57,  2.12s/it]loss_total_epoch 95.62069374322891
Training tokenizer:  17% 1370/8047 [40:51<3:56:29,  2.13s/it]loss_total_epoch 95.68345922231674
Training tokenizer:  17% 1371/8047 [40:53<3:56:13,  2.12s/it]loss_total_epoch 95.74613247066736
Training tokenizer:  17% 1372/8047 [40:55<3:56:00,  2.12s/it]loss_total_epoch 95.81289256364107
Training tokenizer:  17% 1373/8047 [40:57<3:56:00,  2.12s/it]loss_total_epoch 95.87859240174294
Training tokenizer:  17% 1374/8047 [41:00<3:55:50,  2.12s/it]loss_total_epoch 95.9238571934402
Training tokenizer:  17% 1375/8047 [41:02<3:55:19,  2.12s/it]loss_total_epoch 95.9719323515892
Training tokenizer:  17% 1376/8047 [41:04<3:55:03,  2.11s/it]loss_total_epoch 96.0356437638402
Training tokenizer:  17% 1377/8047 [41:06<3:55:18,  2.12s/it]loss_total_epoch 96.08359243348241
Training tokenizer:  17% 1378/8047 [41:08<3:54:42,  2.11s/it]loss_total_epoch 96.14401566237211
Training tokenizer:  17% 1379/8047 [41:10<3:53:38,  2.10s/it]loss_total_epoch 96.19413828849792
Training tokenizer:  17% 1380/8047 [41:12<3:54:44,  2.11s/it]loss_total_epoch 96.25705802440643
Training tokenizer:  17% 1381/8047 [41:14<3:54:53,  2.11s/it]loss_total_epoch 96.33414658904076
Training tokenizer:  17% 1382/8047 [41:16<3:54:04,  2.11s/it]loss_total_epoch 96.39279938489199
Training tokenizer:  17% 1383/8047 [41:19<3:54:08,  2.11s/it]loss_total_epoch 96.43946344032884
Training tokenizer:  17% 1384/8047 [41:21<3:53:59,  2.11s/it]loss_total_epoch 96.4986161403358
Training tokenizer:  17% 1385/8047 [41:23<3:54:41,  2.11s/it]loss_total_epoch 96.56171352043748
Training tokenizer:  17% 1386/8047 [41:25<3:55:16,  2.12s/it]loss_total_epoch 96.60688105970621
Training tokenizer:  17% 1387/8047 [41:27<3:55:40,  2.12s/it]loss_total_epoch 96.66979539394379
Training tokenizer:  17% 1388/8047 [41:29<3:54:28,  2.11s/it]loss_total_epoch 96.73668652772903
Training tokenizer:  17% 1389/8047 [41:31<3:54:37,  2.11s/it]loss_total_epoch 96.81090012192726
Training tokenizer:  17% 1390/8047 [41:33<3:54:36,  2.11s/it]loss_total_epoch 96.88196974992752
Training tokenizer:  17% 1391/8047 [41:35<3:54:25,  2.11s/it]loss_total_epoch 96.9507349357009
Training tokenizer:  17% 1392/8047 [41:38<3:53:55,  2.11s/it]loss_total_epoch 97.00548893213272
Training tokenizer:  17% 1393/8047 [41:40<3:55:26,  2.12s/it]loss_total_epoch 97.06398548558354
Training tokenizer:  17% 1394/8047 [41:42<3:55:57,  2.13s/it]loss_total_epoch 97.13637846335769
Training tokenizer:  17% 1395/8047 [41:44<3:55:31,  2.12s/it]loss_total_epoch 97.18578639999032
Training tokenizer:  17% 1396/8047 [41:46<3:56:40,  2.14s/it]loss_total_epoch 97.232978887856
Training tokenizer:  17% 1397/8047 [41:48<3:56:15,  2.13s/it]loss_total_epoch 97.29988361895084
Training tokenizer:  17% 1398/8047 [41:50<3:56:45,  2.14s/it]loss_total_epoch 97.35322301462293
Training tokenizer:  17% 1399/8047 [41:53<3:56:09,  2.13s/it]loss_total_epoch 97.42189933732152
Training tokenizer:  17% 1400/8047 [41:55<3:56:09,  2.13s/it]loss_total_epoch 97.47624338045716
Training tokenizer:  17% 1401/8047 [41:57<3:56:39,  2.14s/it]loss_total_epoch 97.54344196990132
Training tokenizer:  17% 1402/8047 [41:59<3:56:23,  2.13s/it]loss_total_epoch 97.60199199989438
Training tokenizer:  17% 1403/8047 [42:01<3:56:42,  2.14s/it]loss_total_epoch 97.65722912922502
Training tokenizer:  17% 1404/8047 [42:03<3:57:33,  2.15s/it]loss_total_epoch 97.71872278302908
Training tokenizer:  17% 1405/8047 [42:05<3:56:55,  2.14s/it]loss_total_epoch 97.77575642615557
Training tokenizer:  17% 1406/8047 [42:08<3:57:00,  2.14s/it]loss_total_epoch 97.82525355368853
Training tokenizer:  17% 1407/8047 [42:10<3:57:22,  2.15s/it]loss_total_epoch 97.90554478019476
Training tokenizer:  17% 1408/8047 [42:12<3:57:08,  2.14s/it]loss_total_epoch 97.9669337272644
Training tokenizer:  18% 1409/8047 [42:14<3:55:59,  2.13s/it]loss_total_epoch 98.02371319383383
Training tokenizer:  18% 1410/8047 [42:16<3:56:43,  2.14s/it]loss_total_epoch 98.08133997395635
Training tokenizer:  18% 1411/8047 [42:18<3:57:19,  2.15s/it]loss_total_epoch 98.14594724401832
Training tokenizer:  18% 1412/8047 [42:20<3:57:16,  2.15s/it]loss_total_epoch 98.2030108012259
Training tokenizer:  18% 1413/8047 [42:22<3:55:48,  2.13s/it]loss_total_epoch 98.25913707911968
Training tokenizer:  18% 1414/8047 [42:25<3:56:16,  2.14s/it]loss_total_epoch 98.32349456101656
Training tokenizer:  18% 1415/8047 [42:27<3:56:45,  2.14s/it]loss_total_epoch 98.37929526716471
Training tokenizer:  18% 1416/8047 [42:29<3:57:01,  2.14s/it]loss_total_epoch 98.4526238143444
Training tokenizer:  18% 1417/8047 [42:31<3:56:27,  2.14s/it]loss_total_epoch 98.50144490227103
Training tokenizer:  18% 1418/8047 [42:33<3:56:19,  2.14s/it]loss_total_epoch 98.5568421445787
Training tokenizer:  18% 1419/8047 [42:35<3:56:42,  2.14s/it]loss_total_epoch 98.61202904209495
Training tokenizer:  18% 1420/8047 [42:37<3:56:27,  2.14s/it]loss_total_epoch 98.67541037127376
Training tokenizer:  18% 1421/8047 [42:40<3:56:27,  2.14s/it]loss_total_epoch 98.73175586387515
Training tokenizer:  18% 1422/8047 [42:42<3:57:03,  2.15s/it]loss_total_epoch 98.79363610595465
Training tokenizer:  18% 1423/8047 [42:44<3:56:49,  2.15s/it]loss_total_epoch 98.85432325676084
Training tokenizer:  18% 1424/8047 [42:46<3:56:31,  2.14s/it]loss_total_epoch 98.92437388375401
Training tokenizer:  18% 1425/8047 [42:48<3:56:26,  2.14s/it]loss_total_epoch 98.9812599606812
Training tokenizer:  18% 1426/8047 [42:50<3:55:31,  2.13s/it]loss_total_epoch 99.03090004250407
Training tokenizer:  18% 1427/8047 [42:52<3:55:45,  2.14s/it]loss_total_epoch 99.08869935199618
Training tokenizer:  18% 1428/8047 [42:55<3:55:53,  2.14s/it]loss_total_epoch 99.15390785411
Training tokenizer:  18% 1429/8047 [42:57<3:56:35,  2.14s/it]loss_total_epoch 99.18281781859696
Training tokenizer:  18% 1430/8047 [42:59<3:56:47,  2.15s/it]loss_total_epoch 99.23405090160668
Training tokenizer:  18% 1431/8047 [43:01<3:56:41,  2.15s/it]loss_total_epoch 99.29290831647813
Training tokenizer:  18% 1432/8047 [43:03<3:54:45,  2.13s/it]loss_total_epoch 99.35426038689911
Training tokenizer:  18% 1433/8047 [43:05<3:54:45,  2.13s/it]loss_total_epoch 99.42288294620812
Training tokenizer:  18% 1434/8047 [43:07<3:54:33,  2.13s/it]loss_total_epoch 99.47570532374084
Training tokenizer:  18% 1435/8047 [43:10<3:54:46,  2.13s/it]loss_total_epoch 99.54165814630687
Training tokenizer:  18% 1436/8047 [43:12<3:55:53,  2.14s/it]loss_total_epoch 99.60678473673761
Training tokenizer:  18% 1437/8047 [43:14<3:56:50,  2.15s/it]loss_total_epoch 99.68008470349014
Training tokenizer:  18% 1438/8047 [43:16<3:56:16,  2.15s/it]loss_total_epoch 99.74167215637863
Training tokenizer:  18% 1439/8047 [43:18<3:55:36,  2.14s/it]loss_total_epoch 99.79988567344844
Training tokenizer:  18% 1440/8047 [43:20<3:55:45,  2.14s/it]loss_total_epoch 99.86252767406404
Training tokenizer:  18% 1441/8047 [43:22<3:56:44,  2.15s/it]loss_total_epoch 99.92049521394074
Training tokenizer:  18% 1442/8047 [43:25<3:57:04,  2.15s/it]loss_total_epoch 99.99437920190394
Training tokenizer:  18% 1443/8047 [43:27<3:56:31,  2.15s/it]loss_total_epoch 100.05642019025981
Training tokenizer:  18% 1444/8047 [43:29<3:56:48,  2.15s/it]loss_total_epoch 100.12444397620857
Training tokenizer:  18% 1445/8047 [43:31<3:56:50,  2.15s/it]loss_total_epoch 100.19351804070175
Training tokenizer:  18% 1446/8047 [43:33<3:56:53,  2.15s/it]loss_total_epoch 100.25724404118955
Training tokenizer:  18% 1447/8047 [43:35<3:56:16,  2.15s/it]loss_total_epoch 100.31291036866605
Training tokenizer:  18% 1448/8047 [43:37<3:55:18,  2.14s/it]loss_total_epoch 100.3617444653064
Training tokenizer:  18% 1449/8047 [43:40<3:54:59,  2.14s/it]loss_total_epoch 100.41311547346413
Training tokenizer:  18% 1450/8047 [43:42<3:55:20,  2.14s/it]loss_total_epoch 100.47369360364974
Training tokenizer:  18% 1451/8047 [43:44<4:00:27,  2.19s/it]loss_total_epoch 100.51703854091465
Training tokenizer:  18% 1452/8047 [43:46<3:58:09,  2.17s/it]loss_total_epoch 100.58749274350703
Training tokenizer:  18% 1453/8047 [43:48<3:57:30,  2.16s/it]loss_total_epoch 100.65800275094807
Training tokenizer:  18% 1454/8047 [43:50<3:56:49,  2.16s/it]loss_total_epoch 100.72196890600026
Training tokenizer:  18% 1455/8047 [43:53<3:56:16,  2.15s/it]loss_total_epoch 100.78798053599894
Training tokenizer:  18% 1456/8047 [43:55<3:55:42,  2.15s/it]loss_total_epoch 100.85129360295832
Training tokenizer:  18% 1457/8047 [43:57<3:54:26,  2.13s/it]loss_total_epoch 100.91753921844065
Training tokenizer:  18% 1458/8047 [43:59<3:55:11,  2.14s/it]loss_total_epoch 100.96860536001623
Training tokenizer:  18% 1459/8047 [44:01<3:55:47,  2.15s/it]loss_total_epoch 101.03063420392573
Training tokenizer:  18% 1460/8047 [44:03<3:56:24,  2.15s/it]loss_total_epoch 101.07561393640935
Training tokenizer:  18% 1461/8047 [44:06<3:57:45,  2.17s/it]loss_total_epoch 101.11860214732587
Training tokenizer:  18% 1462/8047 [44:08<3:57:08,  2.16s/it]loss_total_epoch 101.17250256799161
Training tokenizer:  18% 1463/8047 [44:10<3:56:52,  2.16s/it]loss_total_epoch 101.23507185839117
Training tokenizer:  18% 1464/8047 [44:12<3:56:52,  2.16s/it]loss_total_epoch 101.28904533945024
Training tokenizer:  18% 1465/8047 [44:14<3:55:49,  2.15s/it]loss_total_epoch 101.35529148392379
Training tokenizer:  18% 1466/8047 [44:16<3:55:39,  2.15s/it]loss_total_epoch 101.4071689043194
Training tokenizer:  18% 1467/8047 [44:18<3:55:46,  2.15s/it]loss_total_epoch 101.4552414920181
Training tokenizer:  18% 1468/8047 [44:21<3:56:04,  2.15s/it]loss_total_epoch 101.51302883587778
Training tokenizer:  18% 1469/8047 [44:23<3:56:42,  2.16s/it]loss_total_epoch 101.58948305808008
Training tokenizer:  18% 1470/8047 [44:25<3:54:35,  2.14s/it]loss_total_epoch 101.65098170377314
Training tokenizer:  18% 1471/8047 [44:27<3:55:15,  2.15s/it]loss_total_epoch 101.71715522743762
Training tokenizer:  18% 1472/8047 [44:29<3:55:53,  2.15s/it]loss_total_epoch 101.77679153718054
Training tokenizer:  18% 1473/8047 [44:31<3:56:08,  2.16s/it]loss_total_epoch 101.83158766664565
Training tokenizer:  18% 1474/8047 [44:34<3:56:21,  2.16s/it]loss_total_epoch 101.88878669403493
Training tokenizer:  18% 1475/8047 [44:36<3:56:22,  2.16s/it]loss_total_epoch 101.95230745337903
Training tokenizer:  18% 1476/8047 [44:38<3:56:24,  2.16s/it]loss_total_epoch 102.00112840346992
Training tokenizer:  18% 1477/8047 [44:40<3:57:10,  2.17s/it]loss_total_epoch 102.06165027432144
Training tokenizer:  18% 1478/8047 [44:42<3:57:36,  2.17s/it]loss_total_epoch 102.11295298673213
Training tokenizer:  18% 1479/8047 [44:44<3:56:52,  2.16s/it]loss_total_epoch 102.16718457825482
Training tokenizer:  18% 1480/8047 [44:46<3:55:22,  2.15s/it]loss_total_epoch 102.22242774255574
Training tokenizer:  18% 1481/8047 [44:49<3:55:42,  2.15s/it]loss_total_epoch 102.28567367233336
Training tokenizer:  18% 1482/8047 [44:51<3:55:43,  2.15s/it]loss_total_epoch 102.34830196388066
Training tokenizer:  18% 1483/8047 [44:53<3:55:47,  2.16s/it]loss_total_epoch 102.4131076503545
Training tokenizer:  18% 1484/8047 [44:55<3:56:18,  2.16s/it]loss_total_epoch 102.46574334613979
Training tokenizer:  18% 1485/8047 [44:57<3:56:41,  2.16s/it]loss_total_epoch 102.52635476179421
Training tokenizer:  18% 1486/8047 [44:59<3:56:57,  2.17s/it]loss_total_epoch 102.58854587562382
Training tokenizer:  18% 1487/8047 [45:02<3:57:21,  2.17s/it]loss_total_epoch 102.64832909591496
Training tokenizer:  18% 1488/8047 [45:04<3:57:50,  2.18s/it]loss_total_epoch 102.70920198969543
Training tokenizer:  19% 1489/8047 [45:06<3:57:48,  2.18s/it]loss_total_epoch 102.76385023258626
Training tokenizer:  19% 1490/8047 [45:08<3:57:05,  2.17s/it]loss_total_epoch 102.82945621572435
Training tokenizer:  19% 1491/8047 [45:10<3:57:34,  2.17s/it]loss_total_epoch 102.87272617779672
Training tokenizer:  19% 1492/8047 [45:13<3:58:05,  2.18s/it]loss_total_epoch 102.93071554787457
Training tokenizer:  19% 1493/8047 [45:15<3:57:08,  2.17s/it]loss_total_epoch 103.00083036907017
Training tokenizer:  19% 1494/8047 [45:17<3:56:37,  2.17s/it]loss_total_epoch 103.06669016368687
Training tokenizer:  19% 1495/8047 [45:19<3:58:01,  2.18s/it]loss_total_epoch 103.11696507968009
Training tokenizer:  19% 1496/8047 [45:21<3:57:46,  2.18s/it]loss_total_epoch 103.17424272187054
Training tokenizer:  19% 1497/8047 [45:23<3:58:24,  2.18s/it]loss_total_epoch 103.224076455459
Training tokenizer:  19% 1498/8047 [45:26<3:57:54,  2.18s/it]loss_total_epoch 103.28239228390157
Training tokenizer:  19% 1499/8047 [45:28<3:56:28,  2.17s/it]loss_total_epoch 103.33598337508738
Training tokenizer:  19% 1500/8047 [45:30<3:55:30,  2.16s/it]loss_total_epoch 103.39805836044252
Training tokenizer:  19% 1501/8047 [45:32<3:55:10,  2.16s/it]loss_total_epoch 103.44980770535767
Training tokenizer:  19% 1502/8047 [45:34<3:55:40,  2.16s/it]loss_total_epoch 103.50383714027703
Training tokenizer:  19% 1503/8047 [45:36<3:56:17,  2.17s/it]loss_total_epoch 103.56040319614112
Training tokenizer:  19% 1504/8047 [45:39<3:57:23,  2.18s/it]loss_total_epoch 103.62682667933404
Training tokenizer:  19% 1505/8047 [45:41<3:58:03,  2.18s/it]loss_total_epoch 103.68796556256711
Training tokenizer:  19% 1506/8047 [45:43<3:57:29,  2.18s/it]loss_total_epoch 103.75273167155683
Training tokenizer:  19% 1507/8047 [45:45<3:57:08,  2.18s/it]loss_total_epoch 103.82230625115335
Training tokenizer:  19% 1508/8047 [45:47<3:55:48,  2.16s/it]loss_total_epoch 103.87738215364516
Training tokenizer:  19% 1509/8047 [45:49<3:55:14,  2.16s/it]loss_total_epoch 103.93467095308006
Training tokenizer:  19% 1510/8047 [45:52<3:55:45,  2.16s/it]loss_total_epoch 103.98902049474418
Training tokenizer:  19% 1511/8047 [45:54<3:56:40,  2.17s/it]loss_total_epoch 104.03776492364705
Training tokenizer:  19% 1512/8047 [45:56<3:57:01,  2.18s/it]loss_total_epoch 104.09779725037515
Training tokenizer:  19% 1513/8047 [45:58<3:56:17,  2.17s/it]loss_total_epoch 104.15901644341648
Training tokenizer:  19% 1514/8047 [46:00<3:56:25,  2.17s/it]loss_total_epoch 104.20971948839724
Training tokenizer:  19% 1515/8047 [46:02<3:57:18,  2.18s/it]loss_total_epoch 104.27583424933255
Training tokenizer:  19% 1516/8047 [46:05<3:57:21,  2.18s/it]loss_total_epoch 104.3466865438968
Training tokenizer:  19% 1517/8047 [46:07<3:57:49,  2.19s/it]loss_total_epoch 104.3976271841675
Training tokenizer:  19% 1518/8047 [46:09<3:56:05,  2.17s/it]loss_total_epoch 104.46449506469071
Training tokenizer:  19% 1519/8047 [46:11<3:55:36,  2.17s/it]loss_total_epoch 104.5012389216572
Training tokenizer:  19% 1520/8047 [46:13<3:56:38,  2.18s/it]loss_total_epoch 104.56139481253922
Training tokenizer:  19% 1521/8047 [46:16<3:57:13,  2.18s/it]loss_total_epoch 104.62447843886912
Training tokenizer:  19% 1522/8047 [46:18<3:56:42,  2.18s/it]loss_total_epoch 104.6781341675669
Training tokenizer:  19% 1523/8047 [46:20<3:56:27,  2.17s/it]loss_total_epoch 104.74143417365849
Training tokenizer:  19% 1524/8047 [46:22<3:56:31,  2.18s/it]loss_total_epoch 104.79666137881577
Training tokenizer:  19% 1525/8047 [46:24<3:56:08,  2.17s/it]loss_total_epoch 104.8644789699465
Training tokenizer:  19% 1526/8047 [46:26<3:56:00,  2.17s/it]loss_total_epoch 104.91124297119677
Training tokenizer:  19% 1527/8047 [46:29<3:55:50,  2.17s/it]loss_total_epoch 104.97005104459822
Training tokenizer:  19% 1528/8047 [46:31<3:56:04,  2.17s/it]loss_total_epoch 105.02763439528644
Training tokenizer:  19% 1529/8047 [46:33<3:55:47,  2.17s/it]loss_total_epoch 105.0826365519315
Training tokenizer:  19% 1530/8047 [46:35<3:55:55,  2.17s/it]loss_total_epoch 105.12721751444042
Training tokenizer:  19% 1531/8047 [46:37<3:56:51,  2.18s/it]loss_total_epoch 105.18775760941207
Training tokenizer:  19% 1532/8047 [46:39<3:57:27,  2.19s/it]loss_total_epoch 105.25094648264349
Training tokenizer:  19% 1533/8047 [46:42<3:57:26,  2.19s/it]loss_total_epoch 105.30146384052932
Training tokenizer:  19% 1534/8047 [46:44<3:57:35,  2.19s/it]loss_total_epoch 105.36255240626633
Training tokenizer:  19% 1535/8047 [46:46<3:57:43,  2.19s/it]loss_total_epoch 105.42290900833905
Training tokenizer:  19% 1536/8047 [46:48<3:57:47,  2.19s/it]loss_total_epoch 105.47443830780685
Training tokenizer:  19% 1537/8047 [46:50<3:58:10,  2.20s/it]loss_total_epoch 105.53581889905035
Training tokenizer:  19% 1538/8047 [46:53<3:57:47,  2.19s/it]loss_total_epoch 105.60170192457736
Training tokenizer:  19% 1539/8047 [46:55<3:57:31,  2.19s/it]loss_total_epoch 105.67239222116768
Training tokenizer:  19% 1540/8047 [46:57<3:57:00,  2.19s/it]loss_total_epoch 105.73028324358165
Training tokenizer:  19% 1541/8047 [46:59<3:56:48,  2.18s/it]loss_total_epoch 105.7795826587826
Training tokenizer:  19% 1542/8047 [47:01<3:55:47,  2.17s/it]loss_total_epoch 105.8326279129833
Training tokenizer:  19% 1543/8047 [47:03<3:55:38,  2.17s/it]loss_total_epoch 105.88917660526931
Training tokenizer:  19% 1544/8047 [47:06<3:56:20,  2.18s/it]loss_total_epoch 105.94475310109556
Training tokenizer:  19% 1545/8047 [47:08<3:56:53,  2.19s/it]loss_total_epoch 106.00412919931114
Training tokenizer:  19% 1546/8047 [47:10<3:57:04,  2.19s/it]loss_total_epoch 106.06555394269526
Training tokenizer:  19% 1547/8047 [47:12<3:57:51,  2.20s/it]loss_total_epoch 106.12119590304792
Training tokenizer:  19% 1548/8047 [47:14<3:57:26,  2.19s/it]loss_total_epoch 106.18535934947431
Training tokenizer:  19% 1549/8047 [47:17<3:58:03,  2.20s/it]loss_total_epoch 106.24087578617036
Training tokenizer:  19% 1550/8047 [47:19<3:55:54,  2.18s/it]loss_total_epoch 106.29651535861194
Training tokenizer:  19% 1551/8047 [47:21<3:55:24,  2.17s/it]loss_total_epoch 106.3537089470774
Training tokenizer:  19% 1552/8047 [47:23<3:55:37,  2.18s/it]loss_total_epoch 106.4168248269707
Training tokenizer:  19% 1553/8047 [47:25<3:56:19,  2.18s/it]loss_total_epoch 106.48861058987677
Training tokenizer:  19% 1554/8047 [47:28<3:57:35,  2.20s/it]loss_total_epoch 106.54536260478199
Training tokenizer:  19% 1555/8047 [47:30<3:56:50,  2.19s/it]loss_total_epoch 106.60058152489364
Training tokenizer:  19% 1556/8047 [47:32<3:57:09,  2.19s/it]loss_total_epoch 106.66988716833293
Training tokenizer:  19% 1557/8047 [47:34<3:57:21,  2.19s/it]loss_total_epoch 106.72376896254718
Training tokenizer:  19% 1558/8047 [47:36<3:58:31,  2.21s/it]loss_total_epoch 106.79669856838882
Training tokenizer:  19% 1559/8047 [47:39<3:58:52,  2.21s/it]loss_total_epoch 106.851167967543
Training tokenizer:  19% 1560/8047 [47:41<3:58:29,  2.21s/it]loss_total_epoch 106.9013645965606
Training tokenizer:  19% 1561/8047 [47:43<3:57:24,  2.20s/it]loss_total_epoch 106.94635204784572
Training tokenizer:  19% 1562/8047 [47:45<3:56:37,  2.19s/it]loss_total_epoch 107.0235750284046
Training tokenizer:  19% 1563/8047 [47:47<3:57:47,  2.20s/it]loss_total_epoch 107.08318172954023
Training tokenizer:  19% 1564/8047 [47:50<3:57:58,  2.20s/it]loss_total_epoch 107.1442143600434
Training tokenizer:  19% 1565/8047 [47:52<3:57:15,  2.20s/it]loss_total_epoch 107.20500348694623
Training tokenizer:  19% 1566/8047 [47:54<3:57:57,  2.20s/it]loss_total_epoch 107.27183366008103
Training tokenizer:  19% 1567/8047 [47:56<3:58:45,  2.21s/it]loss_total_epoch 107.33814348466694
Training tokenizer:  19% 1568/8047 [47:58<3:59:20,  2.22s/it]loss_total_epoch 107.39127388037741
Training tokenizer:  19% 1569/8047 [48:01<3:59:26,  2.22s/it]loss_total_epoch 107.43876451812685
Training tokenizer:  20% 1570/8047 [48:03<3:58:21,  2.21s/it]loss_total_epoch 107.48871269635856
Training tokenizer:  20% 1571/8047 [48:05<3:58:33,  2.21s/it]loss_total_epoch 107.55576087348163
Training tokenizer:  20% 1572/8047 [48:07<3:58:44,  2.21s/it]loss_total_epoch 107.60705640725791
Training tokenizer:  20% 1573/8047 [48:09<3:58:25,  2.21s/it]loss_total_epoch 107.66437374986708
Training tokenizer:  20% 1574/8047 [48:12<3:57:35,  2.20s/it]loss_total_epoch 107.72403446398675
Training tokenizer:  20% 1575/8047 [48:14<3:58:10,  2.21s/it]loss_total_epoch 107.78611754439771
Training tokenizer:  20% 1576/8047 [48:16<3:59:25,  2.22s/it]loss_total_epoch 107.83884052745998
Training tokenizer:  20% 1577/8047 [48:18<3:59:07,  2.22s/it]loss_total_epoch 107.89609897695482
Training tokenizer:  20% 1578/8047 [48:21<3:58:24,  2.21s/it]loss_total_epoch 107.96257874183357
Training tokenizer:  20% 1579/8047 [48:23<3:58:04,  2.21s/it]loss_total_epoch 108.02102803625166
Training tokenizer:  20% 1580/8047 [48:25<3:58:07,  2.21s/it]loss_total_epoch 108.06851573847234
Training tokenizer:  20% 1581/8047 [48:27<3:58:24,  2.21s/it]loss_total_epoch 108.13236565701663
Training tokenizer:  20% 1582/8047 [48:29<3:58:15,  2.21s/it]loss_total_epoch 108.1976516675204
Training tokenizer:  20% 1583/8047 [48:32<3:59:17,  2.22s/it]loss_total_epoch 108.26714927516878
Training tokenizer:  20% 1584/8047 [48:34<3:59:11,  2.22s/it]loss_total_epoch 108.33861351571977
Training tokenizer:  20% 1585/8047 [48:36<3:58:53,  2.22s/it]loss_total_epoch 108.38798047415912
Training tokenizer:  20% 1586/8047 [48:38<3:59:00,  2.22s/it]loss_total_epoch 108.45310561172664
Training tokenizer:  20% 1587/8047 [48:40<3:58:25,  2.21s/it]loss_total_epoch 108.50144135765731
Training tokenizer:  20% 1588/8047 [48:43<3:58:45,  2.22s/it]loss_total_epoch 108.5529031958431
Training tokenizer:  20% 1589/8047 [48:45<3:58:54,  2.22s/it]loss_total_epoch 108.61785506270826
Training tokenizer:  20% 1590/8047 [48:47<3:58:23,  2.22s/it]loss_total_epoch 108.67765485681593
Training tokenizer:  20% 1591/8047 [48:49<3:59:14,  2.22s/it]loss_total_epoch 108.73585032857955
Training tokenizer:  20% 1592/8047 [48:52<3:59:47,  2.23s/it]loss_total_epoch 108.79052185080945
Training tokenizer:  20% 1593/8047 [48:54<3:59:37,  2.23s/it]loss_total_epoch 108.850351838395
Training tokenizer:  20% 1594/8047 [48:56<3:59:51,  2.23s/it]loss_total_epoch 108.90205580554903
Training tokenizer:  20% 1595/8047 [48:58<4:00:14,  2.23s/it]loss_total_epoch 108.95188874192536
Training tokenizer:  20% 1596/8047 [49:01<3:59:59,  2.23s/it]loss_total_epoch 109.00694167800248
Training tokenizer:  20% 1597/8047 [49:03<3:59:23,  2.23s/it]loss_total_epoch 109.06657198630273
Training tokenizer:  20% 1598/8047 [49:05<3:59:22,  2.23s/it]loss_total_epoch 109.12872637622058
Training tokenizer:  20% 1599/8047 [49:07<3:58:15,  2.22s/it]loss_total_epoch 109.18236480094492
Training tokenizer:  20% 1600/8047 [49:09<3:58:17,  2.22s/it]loss_total_epoch 109.25065256841481
Training tokenizer:  20% 1601/8047 [49:12<3:58:10,  2.22s/it]loss_total_epoch 109.31002287007868
Training tokenizer:  20% 1602/8047 [49:14<3:58:37,  2.22s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-chmigxjz'
loss_total_epoch 109.36283311061561
Training tokenizer:  20% 1603/8047 [49:16<3:59:37,  2.23s/it]loss_total_epoch 109.43106975965202
Training tokenizer:  20% 1604/8047 [49:18<3:59:32,  2.23s/it]loss_total_epoch 109.48685656674206
Training tokenizer:  20% 1605/8047 [49:21<4:00:25,  2.24s/it]loss_total_epoch 109.54558629356325
Training tokenizer:  20% 1606/8047 [49:23<3:59:41,  2.23s/it]loss_total_epoch 109.60527497716248
Training tokenizer:  20% 1607/8047 [49:25<3:59:30,  2.23s/it]loss_total_epoch 109.66307429410517
Training tokenizer:  20% 1608/8047 [49:27<3:59:02,  2.23s/it]loss_total_epoch 109.72200388275087
Training tokenizer:  20% 1609/8047 [49:29<3:58:53,  2.23s/it]loss_total_epoch 109.77275525219738
Training tokenizer:  20% 1610/8047 [49:32<3:58:23,  2.22s/it]loss_total_epoch 109.83746745623648
Training tokenizer:  20% 1611/8047 [49:34<3:58:25,  2.22s/it]loss_total_epoch 109.90350809879601
Training tokenizer:  20% 1612/8047 [49:36<3:58:30,  2.22s/it]loss_total_epoch 109.94874879904091
Training tokenizer:  20% 1613/8047 [49:38<3:58:36,  2.23s/it]loss_total_epoch 110.01006340794265
Training tokenizer:  20% 1614/8047 [49:41<3:58:29,  2.22s/it]loss_total_epoch 110.05664719082415
Training tokenizer:  20% 1615/8047 [49:43<3:58:26,  2.22s/it]loss_total_epoch 110.11887698061764
Training tokenizer:  20% 1616/8047 [49:45<3:59:00,  2.23s/it]loss_total_epoch 110.16082157753408
Training tokenizer:  20% 1617/8047 [49:47<3:58:51,  2.23s/it]loss_total_epoch 110.21599810384214
Training tokenizer:  20% 1618/8047 [49:50<3:59:42,  2.24s/it]loss_total_epoch 110.27314650826156
Training tokenizer:  20% 1619/8047 [49:52<3:59:25,  2.23s/it]loss_total_epoch 110.3291922416538
Training tokenizer:  20% 1620/8047 [49:54<3:59:28,  2.24s/it]loss_total_epoch 110.38857140950859
Training tokenizer:  20% 1621/8047 [49:56<3:59:34,  2.24s/it]loss_total_epoch 110.44088289700449
Training tokenizer:  20% 1622/8047 [49:58<3:58:49,  2.23s/it]loss_total_epoch 110.49723086692393
Training tokenizer:  20% 1623/8047 [50:01<3:59:01,  2.23s/it]loss_total_epoch 110.56973908282816
Training tokenizer:  20% 1624/8047 [50:03<3:59:24,  2.24s/it]loss_total_epoch 110.62631722353399
Training tokenizer:  20% 1625/8047 [50:05<3:59:14,  2.24s/it]loss_total_epoch 110.68323340080678
Training tokenizer:  20% 1626/8047 [50:07<3:59:58,  2.24s/it]loss_total_epoch 110.73523888550699
Training tokenizer:  20% 1627/8047 [50:10<4:00:18,  2.25s/it]loss_total_epoch 110.79822710715234
Training tokenizer:  20% 1628/8047 [50:12<4:00:19,  2.25s/it]loss_total_epoch 110.86435299180448
Training tokenizer:  20% 1629/8047 [50:14<4:00:40,  2.25s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-0bcyhju5'
loss_total_epoch 110.92758868448436
Training tokenizer:  20% 1630/8047 [50:16<4:00:38,  2.25s/it]loss_total_epoch 110.99632634036243
Training tokenizer:  20% 1631/8047 [50:19<3:59:41,  2.24s/it]loss_total_epoch 111.04802951030433
Training tokenizer:  20% 1632/8047 [50:21<3:59:48,  2.24s/it]loss_total_epoch 111.10627826489508
Training tokenizer:  20% 1633/8047 [50:23<3:58:49,  2.23s/it]loss_total_epoch 111.17723281122744
Training tokenizer:  20% 1634/8047 [50:25<3:57:51,  2.23s/it]loss_total_epoch 111.23922734893858
Training tokenizer:  20% 1635/8047 [50:28<3:58:13,  2.23s/it]loss_total_epoch 111.3117788080126
Training tokenizer:  20% 1636/8047 [50:30<3:59:03,  2.24s/it]loss_total_epoch 111.3583786059171
Training tokenizer:  20% 1637/8047 [50:32<3:59:00,  2.24s/it]loss_total_epoch 111.41590363718569
Training tokenizer:  20% 1638/8047 [50:34<3:58:37,  2.23s/it]loss_total_epoch 111.47336410172284
Training tokenizer:  20% 1639/8047 [50:37<3:58:11,  2.23s/it]loss_total_epoch 111.53775130398571
Training tokenizer:  20% 1640/8047 [50:39<3:59:27,  2.24s/it]loss_total_epoch 111.59809047169983
Training tokenizer:  20% 1641/8047 [50:41<3:59:29,  2.24s/it]loss_total_epoch 111.65656351856887
Training tokenizer:  20% 1642/8047 [50:43<3:59:03,  2.24s/it]loss_total_epoch 111.72268509678543
Training tokenizer:  20% 1643/8047 [50:46<3:59:05,  2.24s/it]loss_total_epoch 111.77589499764144
Training tokenizer:  20% 1644/8047 [50:48<4:00:06,  2.25s/it]loss_total_epoch 111.82855314202607
Training tokenizer:  20% 1645/8047 [50:50<4:00:36,  2.26s/it]loss_total_epoch 111.88779980130494
Training tokenizer:  20% 1646/8047 [50:52<4:03:05,  2.28s/it]loss_total_epoch 111.95048727281392
Training tokenizer:  20% 1647/8047 [50:55<4:01:10,  2.26s/it]loss_total_epoch 112.00940772332251
Training tokenizer:  20% 1648/8047 [50:57<3:59:34,  2.25s/it]loss_total_epoch 112.06501307152212
Training tokenizer:  20% 1649/8047 [50:59<3:59:25,  2.25s/it]loss_total_epoch 112.1177569758147
Training tokenizer:  21% 1650/8047 [51:01<3:59:14,  2.24s/it]loss_total_epoch 112.16877056844532
Training tokenizer:  21% 1651/8047 [51:04<3:59:00,  2.24s/it]loss_total_epoch 112.21505111642182
Training tokenizer:  21% 1652/8047 [51:06<3:59:24,  2.25s/it]loss_total_epoch 112.27244414575398
Training tokenizer:  21% 1653/8047 [51:08<4:00:24,  2.26s/it]loss_total_epoch 112.32384611479938
Training tokenizer:  21% 1654/8047 [51:10<4:00:36,  2.26s/it]loss_total_epoch 112.37259613908827
Training tokenizer:  21% 1655/8047 [51:13<3:59:47,  2.25s/it]loss_total_epoch 112.42950743623078
Training tokenizer:  21% 1656/8047 [51:15<4:00:02,  2.25s/it]loss_total_epoch 112.49277719296515
Training tokenizer:  21% 1657/8047 [51:17<3:59:19,  2.25s/it]loss_total_epoch 112.5511922519654
Training tokenizer:  21% 1658/8047 [51:19<3:59:13,  2.25s/it]loss_total_epoch 112.60963979922235
Training tokenizer:  21% 1659/8047 [51:22<3:58:23,  2.24s/it]loss_total_epoch 112.66469947807491
Training tokenizer:  21% 1660/8047 [51:24<3:59:13,  2.25s/it]loss_total_epoch 112.72147813625634
Training tokenizer:  21% 1661/8047 [51:26<3:59:31,  2.25s/it]loss_total_epoch 112.78799856640399
Training tokenizer:  21% 1662/8047 [51:28<4:01:00,  2.26s/it]loss_total_epoch 112.83908967860043
Training tokenizer:  21% 1663/8047 [51:31<4:01:51,  2.27s/it]loss_total_epoch 112.89085190184414
Training tokenizer:  21% 1664/8047 [51:33<4:01:27,  2.27s/it]loss_total_epoch 112.95254509709775
Training tokenizer:  21% 1665/8047 [51:35<4:01:19,  2.27s/it]loss_total_epoch 113.01052050106227
Training tokenizer:  21% 1666/8047 [51:37<4:00:43,  2.26s/it]loss_total_epoch 113.06799178756773
Training tokenizer:  21% 1667/8047 [51:40<4:01:51,  2.27s/it]loss_total_epoch 113.13122355379164
Training tokenizer:  21% 1668/8047 [51:42<4:01:29,  2.27s/it]loss_total_epoch 113.19447110034525
Training tokenizer:  21% 1669/8047 [51:44<4:02:01,  2.28s/it]loss_total_epoch 113.25853667594492
Training tokenizer:  21% 1670/8047 [51:47<4:01:21,  2.27s/it]loss_total_epoch 113.30293456278741
Training tokenizer:  21% 1671/8047 [51:49<4:01:34,  2.27s/it]loss_total_epoch 113.36757807992399
Training tokenizer:  21% 1672/8047 [51:51<4:01:52,  2.28s/it]loss_total_epoch 113.42760021053255
Training tokenizer:  21% 1673/8047 [51:53<4:01:43,  2.28s/it]loss_total_epoch 113.47792020998895
Training tokenizer:  21% 1674/8047 [51:56<4:02:48,  2.29s/it]loss_total_epoch 113.52953784354031
Training tokenizer:  21% 1675/8047 [51:58<4:02:29,  2.28s/it]loss_total_epoch 113.58112778700888
Training tokenizer:  21% 1676/8047 [52:00<4:01:40,  2.28s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-vjroogj7'
loss_total_epoch 113.6410269010812
Training tokenizer:  21% 1677/8047 [52:03<4:02:43,  2.29s/it]loss_total_epoch 113.6970365922898
Training tokenizer:  21% 1678/8047 [52:05<4:02:53,  2.29s/it]loss_total_epoch 113.75923223607242
Training tokenizer:  21% 1679/8047 [52:07<4:03:25,  2.29s/it]loss_total_epoch 113.8268220257014
Training tokenizer:  21% 1680/8047 [52:09<4:01:48,  2.28s/it]loss_total_epoch 113.87956918589771
Training tokenizer:  21% 1681/8047 [52:12<4:00:38,  2.27s/it]loss_total_epoch 113.92677545361221
Training tokenizer:  21% 1682/8047 [52:14<4:00:28,  2.27s/it]loss_total_epoch 113.9846226926893
Training tokenizer:  21% 1683/8047 [52:16<4:01:30,  2.28s/it]loss_total_epoch 114.04137394018471
Training tokenizer:  21% 1684/8047 [52:18<4:01:25,  2.28s/it]loss_total_epoch 114.11081630922854
Training tokenizer:  21% 1685/8047 [52:21<3:59:29,  2.26s/it]loss_total_epoch 114.16504892893136
Training tokenizer:  21% 1686/8047 [52:23<4:00:46,  2.27s/it]loss_total_epoch 114.21765609271824
Training tokenizer:  21% 1687/8047 [52:25<4:01:07,  2.27s/it]loss_total_epoch 114.27403479255736
Training tokenizer:  21% 1688/8047 [52:28<4:01:39,  2.28s/it]loss_total_epoch 114.33848665095866
Training tokenizer:  21% 1689/8047 [52:30<4:00:42,  2.27s/it]loss_total_epoch 114.3980705384165
Training tokenizer:  21% 1690/8047 [52:32<4:00:55,  2.27s/it]loss_total_epoch 114.45787875168025
Training tokenizer:  21% 1691/8047 [52:34<4:01:19,  2.28s/it]loss_total_epoch 114.53050749562681
Training tokenizer:  21% 1692/8047 [52:37<4:01:41,  2.28s/it]loss_total_epoch 114.58780419267714
Training tokenizer:  21% 1693/8047 [52:39<4:00:46,  2.27s/it]loss_total_epoch 114.64903396554291
Training tokenizer:  21% 1694/8047 [52:41<4:01:45,  2.28s/it]loss_total_epoch 114.70711942948401
Training tokenizer:  21% 1695/8047 [52:44<4:02:03,  2.29s/it]loss_total_epoch 114.76899020932615
Training tokenizer:  21% 1696/8047 [52:46<4:01:33,  2.28s/it]loss_total_epoch 114.8281188737601
Training tokenizer:  21% 1697/8047 [52:48<4:02:27,  2.29s/it]loss_total_epoch 114.8800009842962
Training tokenizer:  21% 1698/8047 [52:50<4:03:08,  2.30s/it]loss_total_epoch 114.93505599163473
Training tokenizer:  21% 1699/8047 [52:53<4:03:31,  2.30s/it]loss_total_epoch 114.9831571970135
Training tokenizer:  21% 1700/8047 [52:55<4:01:58,  2.29s/it]loss_total_epoch 115.038091911003
Training tokenizer:  21% 1701/8047 [52:57<4:01:20,  2.28s/it]loss_total_epoch 115.08932632394135
Training tokenizer:  21% 1702/8047 [53:00<4:02:47,  2.30s/it]loss_total_epoch 115.1522687766701
Training tokenizer:  21% 1703/8047 [53:02<4:02:45,  2.30s/it]loss_total_epoch 115.21142228133976
Training tokenizer:  21% 1704/8047 [53:04<4:02:29,  2.29s/it]loss_total_epoch 115.27317036502063
Training tokenizer:  21% 1705/8047 [53:06<4:02:38,  2.30s/it]loss_total_epoch 115.32353112660348
Training tokenizer:  21% 1706/8047 [53:09<4:01:05,  2.28s/it]loss_total_epoch 115.37920444272459
Training tokenizer:  21% 1707/8047 [53:11<4:05:29,  2.32s/it]loss_total_epoch 115.42268099449575
Training tokenizer:  21% 1708/8047 [53:13<4:03:10,  2.30s/it]loss_total_epoch 115.48554704897106
Training tokenizer:  21% 1709/8047 [53:16<4:02:37,  2.30s/it]loss_total_epoch 115.53216308541596
Training tokenizer:  21% 1710/8047 [53:18<4:02:08,  2.29s/it]loss_total_epoch 115.59934503026307
Training tokenizer:  21% 1711/8047 [53:20<4:02:01,  2.29s/it]loss_total_epoch 115.66754871048033
Training tokenizer:  21% 1712/8047 [53:23<4:02:22,  2.30s/it]loss_total_epoch 115.71751305274665
Training tokenizer:  21% 1713/8047 [53:25<4:02:07,  2.29s/it]loss_total_epoch 115.77489635162055
Training tokenizer:  21% 1714/8047 [53:27<4:02:18,  2.30s/it]loss_total_epoch 115.83821460418403
Training tokenizer:  21% 1715/8047 [53:29<4:01:20,  2.29s/it]loss_total_epoch 115.8954139854759
Training tokenizer:  21% 1716/8047 [53:32<4:02:12,  2.30s/it]loss_total_epoch 115.94368243031204
Training tokenizer:  21% 1717/8047 [53:34<4:02:52,  2.30s/it]loss_total_epoch 116.00619394145906
Training tokenizer:  21% 1718/8047 [53:36<4:01:39,  2.29s/it]loss_total_epoch 116.06200925819576
Training tokenizer:  21% 1719/8047 [53:39<4:02:13,  2.30s/it]loss_total_epoch 116.12149450741708
Training tokenizer:  21% 1720/8047 [53:41<4:00:51,  2.28s/it]loss_total_epoch 116.17547578923404
Training tokenizer:  21% 1721/8047 [53:43<4:01:12,  2.29s/it]loss_total_epoch 116.23094516061246
Training tokenizer:  21% 1722/8047 [53:45<4:01:08,  2.29s/it]loss_total_epoch 116.30180796794593
Training tokenizer:  21% 1723/8047 [53:48<4:01:58,  2.30s/it]loss_total_epoch 116.35822519473732
Training tokenizer:  21% 1724/8047 [53:50<4:02:18,  2.30s/it]loss_total_epoch 116.4128158185631
Training tokenizer:  21% 1725/8047 [53:52<4:03:06,  2.31s/it]loss_total_epoch 116.46622368507087
Training tokenizer:  21% 1726/8047 [53:55<4:03:15,  2.31s/it]loss_total_epoch 116.52327811159194
Training tokenizer:  21% 1727/8047 [53:57<4:03:22,  2.31s/it]loss_total_epoch 116.58030293695629
Training tokenizer:  21% 1728/8047 [53:59<4:02:37,  2.30s/it]loss_total_epoch 116.6426689568907
Training tokenizer:  21% 1729/8047 [54:02<4:01:58,  2.30s/it]loss_total_epoch 116.70646292157471
Training tokenizer:  21% 1730/8047 [54:04<4:01:20,  2.29s/it]loss_total_epoch 116.76131771318614
Training tokenizer:  22% 1731/8047 [54:06<4:01:10,  2.29s/it]loss_total_epoch 116.81528411619365
Training tokenizer:  22% 1732/8047 [54:08<4:01:38,  2.30s/it]loss_total_epoch 116.87114525400102
Training tokenizer:  22% 1733/8047 [54:11<4:02:34,  2.31s/it]loss_total_epoch 116.93375551514328
Training tokenizer:  22% 1734/8047 [54:13<4:02:10,  2.30s/it]loss_total_epoch 116.99909928254783
Training tokenizer:  22% 1735/8047 [54:15<4:02:58,  2.31s/it]loss_total_epoch 117.0466531906277
Training tokenizer:  22% 1736/8047 [54:18<4:02:21,  2.30s/it]loss_total_epoch 117.10149438120425
Training tokenizer:  22% 1737/8047 [54:20<4:02:47,  2.31s/it]loss_total_epoch 117.15831313468516
Training tokenizer:  22% 1738/8047 [54:22<4:00:35,  2.29s/it]loss_total_epoch 117.21998126246035
Training tokenizer:  22% 1739/8047 [54:25<3:59:34,  2.28s/it]loss_total_epoch 117.27622461505234
Training tokenizer:  22% 1740/8047 [54:27<3:59:40,  2.28s/it]loss_total_epoch 117.34150822646916
Training tokenizer:  22% 1741/8047 [54:29<3:59:43,  2.28s/it]loss_total_epoch 117.39055328629911
Training tokenizer:  22% 1742/8047 [54:31<3:59:13,  2.28s/it]loss_total_epoch 117.44415686093271
Training tokenizer:  22% 1743/8047 [54:34<3:59:05,  2.28s/it]loss_total_epoch 117.4992732014507
Training tokenizer:  22% 1744/8047 [54:36<4:00:36,  2.29s/it]loss_total_epoch 117.55280428566039
Training tokenizer:  22% 1745/8047 [54:38<4:00:46,  2.29s/it]loss_total_epoch 117.60537704639137
Training tokenizer:  22% 1746/8047 [54:41<4:00:32,  2.29s/it]loss_total_epoch 117.65467371232808
Training tokenizer:  22% 1747/8047 [54:43<3:59:55,  2.28s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-2s_clppn'
loss_total_epoch 117.71117748133838
Training tokenizer:  22% 1748/8047 [54:45<4:00:07,  2.29s/it]loss_total_epoch 117.77441305480897
Training tokenizer:  22% 1749/8047 [54:47<3:59:40,  2.28s/it]loss_total_epoch 117.83242001570761
Training tokenizer:  22% 1750/8047 [54:50<3:59:00,  2.28s/it]loss_total_epoch 117.89200592599809
Training tokenizer:  22% 1751/8047 [54:52<3:59:02,  2.28s/it]loss_total_epoch 117.94528097473085
Training tokenizer:  22% 1752/8047 [54:54<3:59:45,  2.29s/it]loss_total_epoch 118.00188676826656
Training tokenizer:  22% 1753/8047 [54:56<3:59:18,  2.28s/it]loss_total_epoch 118.05464851669967
Training tokenizer:  22% 1754/8047 [54:59<4:00:31,  2.29s/it]loss_total_epoch 118.11606230400503
Training tokenizer:  22% 1755/8047 [55:01<4:01:51,  2.31s/it]loss_total_epoch 118.16373122297227
Training tokenizer:  22% 1756/8047 [55:03<4:01:58,  2.31s/it]loss_total_epoch 118.22461839206517
Training tokenizer:  22% 1757/8047 [55:06<4:02:26,  2.31s/it]loss_total_epoch 118.26675685681403
Training tokenizer:  22% 1758/8047 [55:08<4:02:18,  2.31s/it]loss_total_epoch 118.32190460152924
Training tokenizer:  22% 1759/8047 [55:10<4:00:52,  2.30s/it]loss_total_epoch 118.3692216295749
Training tokenizer:  22% 1760/8047 [55:13<4:00:34,  2.30s/it]loss_total_epoch 118.4335372094065
Training tokenizer:  22% 1761/8047 [55:15<4:00:00,  2.29s/it]loss_total_epoch 118.49144630692899
Training tokenizer:  22% 1762/8047 [55:17<4:01:03,  2.30s/it]loss_total_epoch 118.5490194838494
Training tokenizer:  22% 1763/8047 [55:20<4:01:11,  2.30s/it]loss_total_epoch 118.59786157123744
Training tokenizer:  22% 1764/8047 [55:22<4:00:52,  2.30s/it]loss_total_epoch 118.65946081466973
Training tokenizer:  22% 1765/8047 [55:24<3:59:13,  2.28s/it]loss_total_epoch 118.70647597499192
Training tokenizer:  22% 1766/8047 [55:26<4:00:01,  2.29s/it]loss_total_epoch 118.76128962822258
Training tokenizer:  22% 1767/8047 [55:29<4:00:14,  2.30s/it]loss_total_epoch 118.81140911020339
Training tokenizer:  22% 1768/8047 [55:31<4:00:52,  2.30s/it]loss_total_epoch 118.8734454009682
Training tokenizer:  22% 1769/8047 [55:33<4:00:34,  2.30s/it]loss_total_epoch 118.92022938840091
Training tokenizer:  22% 1770/8047 [55:36<4:01:20,  2.31s/it]loss_total_epoch 118.99225906096399
Training tokenizer:  22% 1771/8047 [55:38<4:01:47,  2.31s/it]loss_total_epoch 119.05000187642872
Training tokenizer:  22% 1772/8047 [55:40<4:01:48,  2.31s/it]loss_total_epoch 119.11512758024037
Training tokenizer:  22% 1773/8047 [55:43<4:03:03,  2.32s/it]loss_total_epoch 119.17027827911079
Training tokenizer:  22% 1774/8047 [55:45<4:03:01,  2.32s/it]loss_total_epoch 119.2268826905638
Training tokenizer:  22% 1775/8047 [55:47<4:02:53,  2.32s/it]loss_total_epoch 119.28657472319901
Training tokenizer:  22% 1776/8047 [55:50<4:03:22,  2.33s/it]loss_total_epoch 119.33864703215659
Training tokenizer:  22% 1777/8047 [55:52<4:03:00,  2.33s/it]loss_total_epoch 119.38283979706466
Training tokenizer:  22% 1778/8047 [55:54<4:01:48,  2.31s/it]loss_total_epoch 119.4484285544604
Training tokenizer:  22% 1779/8047 [55:57<4:01:07,  2.31s/it]loss_total_epoch 119.50140334106982
Training tokenizer:  22% 1780/8047 [55:59<4:00:32,  2.30s/it]loss_total_epoch 119.55328139476478
Training tokenizer:  22% 1781/8047 [56:01<4:01:35,  2.31s/it]loss_total_epoch 119.61152495257556
Training tokenizer:  22% 1782/8047 [56:03<4:00:51,  2.31s/it]loss_total_epoch 119.66821454279125
Training tokenizer:  22% 1783/8047 [56:06<4:01:48,  2.32s/it]loss_total_epoch 119.72531408630311
Training tokenizer:  22% 1784/8047 [56:08<4:01:46,  2.32s/it]loss_total_epoch 119.78287741355598
Training tokenizer:  22% 1785/8047 [56:10<4:02:18,  2.32s/it]loss_total_epoch 119.83544657565653
Training tokenizer:  22% 1786/8047 [56:13<4:01:47,  2.32s/it]loss_total_epoch 119.8915611114353
Training tokenizer:  22% 1787/8047 [56:15<4:00:30,  2.31s/it]loss_total_epoch 119.94734744541347
Training tokenizer:  22% 1788/8047 [56:17<4:00:34,  2.31s/it]loss_total_epoch 120.01158265583217
Training tokenizer:  22% 1789/8047 [56:20<4:00:54,  2.31s/it]loss_total_epoch 120.06344082020223
Training tokenizer:  22% 1790/8047 [56:22<4:01:29,  2.32s/it]loss_total_epoch 120.12370143644512
Training tokenizer:  22% 1791/8047 [56:24<4:01:20,  2.31s/it]loss_total_epoch 120.19137991033494
Training tokenizer:  22% 1792/8047 [56:27<4:01:52,  2.32s/it]loss_total_epoch 120.25604166649282
Training tokenizer:  22% 1793/8047 [56:29<4:02:15,  2.32s/it]loss_total_epoch 120.31009143404663
Training tokenizer:  22% 1794/8047 [56:31<4:02:58,  2.33s/it]loss_total_epoch 120.37062815763056
Training tokenizer:  22% 1795/8047 [56:34<4:03:22,  2.34s/it]loss_total_epoch 120.43945769406855
Training tokenizer:  22% 1796/8047 [56:36<4:03:37,  2.34s/it]loss_total_epoch 120.49222145043314
Training tokenizer:  22% 1797/8047 [56:38<4:02:21,  2.33s/it]loss_total_epoch 120.54942419193685
Training tokenizer:  22% 1798/8047 [56:41<4:02:22,  2.33s/it]loss_total_epoch 120.5968047324568
Training tokenizer:  22% 1799/8047 [56:43<4:02:00,  2.32s/it]loss_total_epoch 120.64953634701669
Training tokenizer:  22% 1800/8047 [56:45<4:03:17,  2.34s/it]loss_total_epoch 120.70320522971451
Training tokenizer:  22% 1801/8047 [56:48<4:02:46,  2.33s/it]loss_total_epoch 120.75799384899437
Training tokenizer:  22% 1802/8047 [56:50<4:01:31,  2.32s/it]loss_total_epoch 120.81283889897168
Training tokenizer:  22% 1803/8047 [56:52<4:01:10,  2.32s/it]loss_total_epoch 120.87091418541968
Training tokenizer:  22% 1804/8047 [56:55<4:01:43,  2.32s/it]loss_total_epoch 120.92436107806861
Training tokenizer:  22% 1805/8047 [56:57<4:02:19,  2.33s/it]loss_total_epoch 120.97529432363808
Training tokenizer:  22% 1806/8047 [56:59<4:02:03,  2.33s/it]loss_total_epoch 121.035295156762
Training tokenizer:  22% 1807/8047 [57:02<4:02:40,  2.33s/it]loss_total_epoch 121.0867700446397
Training tokenizer:  22% 1808/8047 [57:04<4:02:46,  2.33s/it]loss_total_epoch 121.13796322233975
Training tokenizer:  22% 1809/8047 [57:06<4:01:12,  2.32s/it]loss_total_epoch 121.18754960410297
Training tokenizer:  22% 1810/8047 [57:09<4:01:11,  2.32s/it]loss_total_epoch 121.24707419238985
Training tokenizer:  23% 1811/8047 [57:11<4:01:55,  2.33s/it]loss_total_epoch 121.31546464376152
Training tokenizer:  23% 1812/8047 [57:13<4:02:39,  2.34s/it]loss_total_epoch 121.37739103846252
Training tokenizer:  23% 1813/8047 [57:16<4:01:51,  2.33s/it]loss_total_epoch 121.43199243955314
Training tokenizer:  23% 1814/8047 [57:18<4:01:36,  2.33s/it]loss_total_epoch 121.48556655459106
Training tokenizer:  23% 1815/8047 [57:20<4:02:10,  2.33s/it]loss_total_epoch 121.5457758847624
Training tokenizer:  23% 1816/8047 [57:23<4:02:17,  2.33s/it]loss_total_epoch 121.58954398520291
Training tokenizer:  23% 1817/8047 [57:25<4:02:05,  2.33s/it]loss_total_epoch 121.63658885844052
Training tokenizer:  23% 1818/8047 [57:27<4:03:33,  2.35s/it]loss_total_epoch 121.68605150841177
Training tokenizer:  23% 1819/8047 [57:30<4:03:57,  2.35s/it]loss_total_epoch 121.74035088531673
Training tokenizer:  23% 1820/8047 [57:32<4:03:55,  2.35s/it]loss_total_epoch 121.81273341737688
Training tokenizer:  23% 1821/8047 [57:34<4:03:43,  2.35s/it]loss_total_epoch 121.86610752902925
Training tokenizer:  23% 1822/8047 [57:37<4:04:41,  2.36s/it]loss_total_epoch 121.91590702347457
Training tokenizer:  23% 1823/8047 [57:39<4:04:25,  2.36s/it]loss_total_epoch 121.97199827246368
Training tokenizer:  23% 1824/8047 [57:41<4:03:44,  2.35s/it]loss_total_epoch 122.0189295951277
Training tokenizer:  23% 1825/8047 [57:44<4:01:54,  2.33s/it]loss_total_epoch 122.0735956672579
Training tokenizer:  23% 1826/8047 [57:46<4:00:24,  2.32s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-pdz1vhhp'
loss_total_epoch 122.13333442993462
Training tokenizer:  23% 1827/8047 [57:48<4:00:22,  2.32s/it]loss_total_epoch 122.19226575829089
Training tokenizer:  23% 1828/8047 [57:51<4:00:04,  2.32s/it]loss_total_epoch 122.25293775089085
Training tokenizer:  23% 1829/8047 [57:53<4:01:15,  2.33s/it]loss_total_epoch 122.29952590726316
Training tokenizer:  23% 1830/8047 [57:55<4:01:32,  2.33s/it]loss_total_epoch 122.35990328900516
Training tokenizer:  23% 1831/8047 [57:58<4:02:39,  2.34s/it]loss_total_epoch 122.40878439508379
Training tokenizer:  23% 1832/8047 [58:00<4:03:05,  2.35s/it]loss_total_epoch 122.45471374876797
Training tokenizer:  23% 1833/8047 [58:02<4:01:10,  2.33s/it]loss_total_epoch 122.51538304798305
Training tokenizer:  23% 1834/8047 [58:05<4:01:00,  2.33s/it]loss_total_epoch 122.57733473367989
Training tokenizer:  23% 1835/8047 [58:07<4:01:33,  2.33s/it]loss_total_epoch 122.6212917137891
Training tokenizer:  23% 1836/8047 [58:09<4:04:14,  2.36s/it]loss_total_epoch 122.68592140264809
Training tokenizer:  23% 1837/8047 [58:12<4:03:54,  2.36s/it]loss_total_epoch 122.74130523018539
Training tokenizer:  23% 1838/8047 [58:14<4:03:19,  2.35s/it]loss_total_epoch 122.79233316145837
Training tokenizer:  23% 1839/8047 [58:16<4:02:15,  2.34s/it]loss_total_epoch 122.8501538913697
Training tokenizer:  23% 1840/8047 [58:19<4:01:55,  2.34s/it]loss_total_epoch 122.90003584511578
Training tokenizer:  23% 1841/8047 [58:21<4:01:55,  2.34s/it]loss_total_epoch 122.95790768228471
Training tokenizer:  23% 1842/8047 [58:23<4:01:59,  2.34s/it]loss_total_epoch 123.01115542091429
Training tokenizer:  23% 1843/8047 [58:26<4:02:14,  2.34s/it]loss_total_epoch 123.07599473185837
Training tokenizer:  23% 1844/8047 [58:28<4:03:08,  2.35s/it]loss_total_epoch 123.12956869788468
Training tokenizer:  23% 1845/8047 [58:30<4:02:36,  2.35s/it]loss_total_epoch 123.18624816276133
Training tokenizer:  23% 1846/8047 [58:33<4:01:59,  2.34s/it]loss_total_epoch 123.2324113342911
Training tokenizer:  23% 1847/8047 [58:35<3:59:36,  2.32s/it]loss_total_epoch 123.28245238028467
Training tokenizer:  23% 1848/8047 [58:37<3:59:14,  2.32s/it]loss_total_epoch 123.33530635945499
Training tokenizer:  23% 1849/8047 [58:40<3:59:57,  2.32s/it]loss_total_epoch 123.38606105186045
Training tokenizer:  23% 1850/8047 [58:42<4:00:36,  2.33s/it]loss_total_epoch 123.43682156689465
Training tokenizer:  23% 1851/8047 [58:44<4:02:02,  2.34s/it]loss_total_epoch 123.5035067293793
Training tokenizer:  23% 1852/8047 [58:47<4:01:40,  2.34s/it]loss_total_epoch 123.56707257218659
Training tokenizer:  23% 1853/8047 [58:49<4:01:40,  2.34s/it]loss_total_epoch 123.62571694515646
Training tokenizer:  23% 1854/8047 [58:51<4:01:52,  2.34s/it]loss_total_epoch 123.68506006337702
Training tokenizer:  23% 1855/8047 [58:54<4:02:28,  2.35s/it]loss_total_epoch 123.75181129015982
Training tokenizer:  23% 1856/8047 [58:56<4:03:45,  2.36s/it]loss_total_epoch 123.80962836183608
Training tokenizer:  23% 1857/8047 [58:59<4:04:03,  2.37s/it]loss_total_epoch 123.87302594818175
Training tokenizer:  23% 1858/8047 [59:01<4:03:53,  2.36s/it]loss_total_epoch 123.93110712058842
Training tokenizer:  23% 1859/8047 [59:03<4:04:04,  2.37s/it]loss_total_epoch 123.98988088779151
Training tokenizer:  23% 1860/8047 [59:06<4:03:06,  2.36s/it]loss_total_epoch 124.04446907900274
Training tokenizer:  23% 1861/8047 [59:08<4:03:23,  2.36s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-tzydi6zk'
loss_total_epoch 124.1091721560806
Training tokenizer:  23% 1862/8047 [59:10<4:02:37,  2.35s/it]loss_total_epoch 124.17476817034185
Training tokenizer:  23% 1863/8047 [59:13<4:02:09,  2.35s/it]loss_total_epoch 124.2291721496731
Training tokenizer:  23% 1864/8047 [59:15<4:02:49,  2.36s/it]loss_total_epoch 124.28808784671128
Training tokenizer:  23% 1865/8047 [59:17<4:03:16,  2.36s/it]loss_total_epoch 124.33652405999601
Training tokenizer:  23% 1866/8047 [59:20<4:02:19,  2.35s/it]loss_total_epoch 124.38433939032257
Training tokenizer:  23% 1867/8047 [59:22<4:01:56,  2.35s/it]loss_total_epoch 124.42518998868763
Training tokenizer:  23% 1868/8047 [59:24<4:01:15,  2.34s/it]loss_total_epoch 124.4766093250364
Training tokenizer:  23% 1869/8047 [59:27<4:00:16,  2.33s/it]loss_total_epoch 124.5285962652415
Training tokenizer:  23% 1870/8047 [59:29<4:00:43,  2.34s/it]loss_total_epoch 124.58108386211097
Training tokenizer:  23% 1871/8047 [59:31<4:01:49,  2.35s/it]loss_total_epoch 124.63376270793378
Training tokenizer:  23% 1872/8047 [59:34<4:02:58,  2.36s/it]loss_total_epoch 124.69330939464271
Training tokenizer:  23% 1873/8047 [59:36<4:03:43,  2.37s/it]loss_total_epoch 124.75398366339505
Training tokenizer:  23% 1874/8047 [59:39<4:03:18,  2.36s/it]loss_total_epoch 124.80177007429302
Training tokenizer:  23% 1875/8047 [59:41<4:03:30,  2.37s/it]loss_total_epoch 124.84456731565297
Training tokenizer:  23% 1876/8047 [59:43<4:03:30,  2.37s/it]loss_total_epoch 124.8951197359711
Training tokenizer:  23% 1877/8047 [59:46<4:00:45,  2.34s/it]loss_total_epoch 124.94710838608444
Training tokenizer:  23% 1878/8047 [59:48<3:59:54,  2.33s/it]loss_total_epoch 125.00216838158667
Training tokenizer:  23% 1879/8047 [59:50<4:00:47,  2.34s/it]loss_total_epoch 125.06362076662481
Training tokenizer:  23% 1880/8047 [59:53<4:00:46,  2.34s/it]loss_total_epoch 125.13129245303571
Training tokenizer:  23% 1881/8047 [59:55<4:00:40,  2.34s/it]loss_total_epoch 125.17405063472688
Training tokenizer:  23% 1882/8047 [59:57<4:01:07,  2.35s/it]loss_total_epoch 125.2254637721926
Training tokenizer:  23% 1883/8047 [1:00:00<4:01:45,  2.35s/it]loss_total_epoch 125.28447845764458
Training tokenizer:  23% 1884/8047 [1:00:02<4:02:37,  2.36s/it]loss_total_epoch 125.34369723685086
Training tokenizer:  23% 1885/8047 [1:00:04<4:02:37,  2.36s/it]loss_total_epoch 125.40199919231236
Training tokenizer:  23% 1886/8047 [1:00:07<4:03:11,  2.37s/it]loss_total_epoch 125.45567799918354
Training tokenizer:  23% 1887/8047 [1:00:09<4:01:30,  2.35s/it]loss_total_epoch 125.50610026530921
Training tokenizer:  23% 1888/8047 [1:00:12<4:03:07,  2.37s/it]loss_total_epoch 125.56154078058898
Training tokenizer:  23% 1889/8047 [1:00:14<4:02:39,  2.36s/it]loss_total_epoch 125.60055807419121
Training tokenizer:  23% 1890/8047 [1:00:16<4:07:47,  2.41s/it]loss_total_epoch 125.64974670298398
Training tokenizer:  23% 1891/8047 [1:00:19<4:21:29,  2.55s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-hah0c0ln'
loss_total_epoch 125.70713655091822
Training tokenizer:  24% 1892/8047 [1:00:22<4:24:23,  2.58s/it]loss_total_epoch 125.75211359746754
Training tokenizer:  24% 1893/8047 [1:00:25<4:27:06,  2.60s/it]loss_total_epoch 125.80819264985621
Training tokenizer:  24% 1894/8047 [1:00:27<4:19:52,  2.53s/it]loss_total_epoch 125.85946534015238
Training tokenizer:  24% 1895/8047 [1:00:29<4:14:26,  2.48s/it]loss_total_epoch 125.92371077276766
Training tokenizer:  24% 1896/8047 [1:00:32<4:10:58,  2.45s/it]loss_total_epoch 125.97222359664738
Training tokenizer:  24% 1897/8047 [1:00:34<4:09:13,  2.43s/it]loss_total_epoch 126.01990330405533
Training tokenizer:  24% 1898/8047 [1:00:37<4:08:45,  2.43s/it]loss_total_epoch 126.07934241183102
Training tokenizer:  24% 1899/8047 [1:00:39<4:07:25,  2.41s/it]loss_total_epoch 126.13391989655793
Training tokenizer:  24% 1900/8047 [1:00:41<4:05:24,  2.40s/it]loss_total_epoch 126.19757807441056
Training tokenizer:  24% 1901/8047 [1:00:44<4:05:01,  2.39s/it]loss_total_epoch 126.25117515958846
Training tokenizer:  24% 1902/8047 [1:00:46<4:05:02,  2.39s/it]loss_total_epoch 126.29728231020272
Training tokenizer:  24% 1903/8047 [1:00:48<4:03:14,  2.38s/it]loss_total_epoch 126.35059246979654
Training tokenizer:  24% 1904/8047 [1:00:51<4:03:26,  2.38s/it]loss_total_epoch 126.40280564688146
Training tokenizer:  24% 1905/8047 [1:00:53<4:03:45,  2.38s/it]loss_total_epoch 126.44623881019652
Training tokenizer:  24% 1906/8047 [1:00:56<4:20:06,  2.54s/it]loss_total_epoch 126.5031190533191
Training tokenizer:  24% 1907/8047 [1:00:59<4:24:05,  2.58s/it]loss_total_epoch 126.55385895632207
Training tokenizer:  24% 1908/8047 [1:01:02<4:36:06,  2.70s/it]loss_total_epoch 126.61134040541947
Training tokenizer:  24% 1909/8047 [1:01:05<4:41:59,  2.76s/it]loss_total_epoch 126.66070515103638
Training tokenizer:  24% 1910/8047 [1:01:07<4:38:29,  2.72s/it]loss_total_epoch 126.71057347394526
Training tokenizer:  24% 1911/8047 [1:01:10<4:44:16,  2.78s/it]loss_total_epoch 126.75549637340009
Training tokenizer:  24% 1912/8047 [1:01:13<4:50:39,  2.84s/it]loss_total_epoch 126.81160940416157
Training tokenizer:  24% 1913/8047 [1:01:16<4:53:33,  2.87s/it]loss_total_epoch 126.86507283337414
Training tokenizer:  24% 1914/8047 [1:01:19<4:46:33,  2.80s/it]loss_total_epoch 126.9086054097861
Training tokenizer:  24% 1915/8047 [1:01:22<4:49:53,  2.84s/it]loss_total_epoch 126.96523007191718
Training tokenizer:  24% 1916/8047 [1:01:24<4:39:53,  2.74s/it]loss_total_epoch 127.0205694604665
Training tokenizer:  24% 1917/8047 [1:01:27<4:44:50,  2.79s/it]loss_total_epoch 127.07300273887813
Training tokenizer:  24% 1918/8047 [1:01:30<4:37:52,  2.72s/it]loss_total_epoch 127.1197148989886
Training tokenizer:  24% 1919/8047 [1:01:32<4:35:29,  2.70s/it]loss_total_epoch 127.17245491035283
Training tokenizer:  24% 1920/8047 [1:01:35<4:34:52,  2.69s/it]loss_total_epoch 127.21246988885105
Training tokenizer:  24% 1921/8047 [1:01:38<4:40:05,  2.74s/it]loss_total_epoch 127.26404034160078
Training tokenizer:  24% 1922/8047 [1:01:41<4:40:59,  2.75s/it]loss_total_epoch 127.3145196530968
Training tokenizer:  24% 1923/8047 [1:01:44<4:47:10,  2.81s/it]loss_total_epoch 127.36656448431313
Training tokenizer:  24% 1924/8047 [1:01:46<4:40:27,  2.75s/it]loss_total_epoch 127.42830745689571
Training tokenizer:  24% 1925/8047 [1:01:49<4:45:16,  2.80s/it]loss_total_epoch 127.48957163281739
Training tokenizer:  24% 1926/8047 [1:01:52<4:39:36,  2.74s/it]loss_total_epoch 127.54558290354908
Training tokenizer:  24% 1927/8047 [1:01:54<4:37:19,  2.72s/it]loss_total_epoch 127.59872093610466
Training tokenizer:  24% 1928/8047 [1:01:57<4:37:20,  2.72s/it]loss_total_epoch 127.6496740039438
Training tokenizer:  24% 1929/8047 [1:02:00<4:35:00,  2.70s/it]loss_total_epoch 127.6978994552046
Training tokenizer:  24% 1930/8047 [1:02:03<4:43:29,  2.78s/it]loss_total_epoch 127.75881016813219
Training tokenizer:  24% 1931/8047 [1:02:06<4:47:01,  2.82s/it]loss_total_epoch 127.8079568054527
Training tokenizer:  24% 1932/8047 [1:02:08<4:48:39,  2.83s/it]loss_total_epoch 127.86557303927839
Training tokenizer:  24% 1933/8047 [1:02:11<4:50:15,  2.85s/it]loss_total_epoch 127.93625025264919
Training tokenizer:  24% 1934/8047 [1:02:14<4:52:32,  2.87s/it]loss_total_epoch 127.99887956492603
Training tokenizer:  24% 1935/8047 [1:02:17<4:47:43,  2.82s/it]loss_total_epoch 128.07322405092418
Training tokenizer:  24% 1936/8047 [1:02:20<4:40:39,  2.76s/it]loss_total_epoch 128.12907832674682
Training tokenizer:  24% 1937/8047 [1:02:22<4:35:26,  2.70s/it]loss_total_epoch 128.18707220070064
Training tokenizer:  24% 1938/8047 [1:02:25<4:37:04,  2.72s/it]loss_total_epoch 128.2410392742604
Training tokenizer:  24% 1939/8047 [1:02:28<4:43:16,  2.78s/it]loss_total_epoch 128.29338444583118
Training tokenizer:  24% 1940/8047 [1:02:30<4:34:24,  2.70s/it]loss_total_epoch 128.34134579263628
Training tokenizer:  24% 1941/8047 [1:02:33<4:32:26,  2.68s/it]loss_total_epoch 128.40208677761257
Training tokenizer:  24% 1942/8047 [1:02:36<4:29:49,  2.65s/it]loss_total_epoch 128.47144280187786
Training tokenizer:  24% 1943/8047 [1:02:38<4:27:24,  2.63s/it]loss_total_epoch 128.5300445239991
Training tokenizer:  24% 1944/8047 [1:02:41<4:33:31,  2.69s/it]loss_total_epoch 128.57925781048834
Training tokenizer:  24% 1945/8047 [1:02:44<4:30:24,  2.66s/it]loss_total_epoch 128.63088383339345
Training tokenizer:  24% 1946/8047 [1:02:46<4:28:26,  2.64s/it]loss_total_epoch 128.6863379497081
Training tokenizer:  24% 1947/8047 [1:02:49<4:35:04,  2.71s/it]loss_total_epoch 128.74009712971747
Training tokenizer:  24% 1948/8047 [1:02:52<4:39:40,  2.75s/it]loss_total_epoch 128.79174884222448
Training tokenizer:  24% 1949/8047 [1:02:55<4:40:56,  2.76s/it]loss_total_epoch 128.84659183584154
Training tokenizer:  24% 1950/8047 [1:02:58<4:45:07,  2.81s/it]loss_total_epoch 128.91024897806346
Training tokenizer:  24% 1951/8047 [1:03:00<4:40:36,  2.76s/it]loss_total_epoch 128.97403318993747
Training tokenizer:  24% 1952/8047 [1:03:03<4:47:36,  2.83s/it]loss_total_epoch 129.02957633696496
Training tokenizer:  24% 1953/8047 [1:03:06<4:51:56,  2.87s/it]loss_total_epoch 129.09736642800272
Training tokenizer:  24% 1954/8047 [1:03:09<4:52:17,  2.88s/it]loss_total_epoch 129.1475933920592
Training tokenizer:  24% 1955/8047 [1:03:12<4:54:58,  2.91s/it]loss_total_epoch 129.20088418386877
Training tokenizer:  24% 1956/8047 [1:03:15<4:56:45,  2.92s/it]loss_total_epoch 129.25831985287368
Training tokenizer:  24% 1957/8047 [1:03:18<4:52:41,  2.88s/it]loss_total_epoch 129.3260974418372
Training tokenizer:  24% 1958/8047 [1:03:21<4:54:02,  2.90s/it]loss_total_epoch 129.38249005191028
Training tokenizer:  24% 1959/8047 [1:03:23<4:43:50,  2.80s/it]loss_total_epoch 129.4281570482999
Training tokenizer:  24% 1960/8047 [1:03:26<4:49:58,  2.86s/it]loss_total_epoch 129.4907085839659
Training tokenizer:  24% 1961/8047 [1:03:29<4:50:46,  2.87s/it]loss_total_epoch 129.54701401107013
Training tokenizer:  24% 1962/8047 [1:03:32<4:51:21,  2.87s/it]loss_total_epoch 129.60921138711274
Training tokenizer:  24% 1963/8047 [1:03:35<4:54:08,  2.90s/it]loss_total_epoch 129.66511976532638
Training tokenizer:  24% 1964/8047 [1:03:38<4:41:47,  2.78s/it]loss_total_epoch 129.71880145929754
Training tokenizer:  24% 1965/8047 [1:03:40<4:31:04,  2.67s/it]loss_total_epoch 129.76869305036962
Training tokenizer:  24% 1966/8047 [1:03:43<4:31:07,  2.68s/it]loss_total_epoch 129.83286734484136
Training tokenizer:  24% 1967/8047 [1:03:45<4:29:16,  2.66s/it]loss_total_epoch 129.88859199918807
Training tokenizer:  24% 1968/8047 [1:03:48<4:26:47,  2.63s/it]loss_total_epoch 129.94650399498641
Training tokenizer:  24% 1969/8047 [1:03:51<4:31:42,  2.68s/it]loss_total_epoch 130.01214413903654
Training tokenizer:  24% 1970/8047 [1:03:53<4:37:49,  2.74s/it]loss_total_epoch 130.06809892691672
Training tokenizer:  24% 1971/8047 [1:03:56<4:33:33,  2.70s/it]loss_total_epoch 130.12659203074872
Training tokenizer:  25% 1972/8047 [1:03:59<4:32:53,  2.70s/it]loss_total_epoch 130.17830318771303
Training tokenizer:  25% 1973/8047 [1:04:02<4:38:41,  2.75s/it]loss_total_epoch 130.24243252538145
Training tokenizer:  25% 1974/8047 [1:04:04<4:33:20,  2.70s/it]loss_total_epoch 130.30220254324377
Training tokenizer:  25% 1975/8047 [1:04:07<4:39:57,  2.77s/it]loss_total_epoch 130.3534698765725
Training tokenizer:  25% 1976/8047 [1:04:10<4:42:21,  2.79s/it]loss_total_epoch 130.39773763529956
Training tokenizer:  25% 1977/8047 [1:04:13<4:47:13,  2.84s/it]loss_total_epoch 130.469075685367
Training tokenizer:  25% 1978/8047 [1:04:15<4:37:50,  2.75s/it]loss_total_epoch 130.5273645091802
Training tokenizer:  25% 1979/8047 [1:04:18<4:32:33,  2.70s/it]loss_total_epoch 130.58245403133333
Training tokenizer:  25% 1980/8047 [1:04:21<4:39:47,  2.77s/it]loss_total_epoch 130.6241579707712
Training tokenizer:  25% 1981/8047 [1:04:24<4:44:22,  2.81s/it]loss_total_epoch 130.67887989990413
Training tokenizer:  25% 1982/8047 [1:04:27<4:39:27,  2.76s/it]loss_total_epoch 130.731194389984
Training tokenizer:  25% 1983/8047 [1:04:30<4:45:39,  2.83s/it]loss_total_epoch 130.77634047530591
Training tokenizer:  25% 1984/8047 [1:04:32<4:47:27,  2.84s/it]loss_total_epoch 130.8288244139403
Training tokenizer:  25% 1985/8047 [1:04:35<4:52:34,  2.90s/it]loss_total_epoch 130.88349881209433
Training tokenizer:  25% 1986/8047 [1:04:38<4:42:51,  2.80s/it]loss_total_epoch 130.95142104662955
Training tokenizer:  25% 1987/8047 [1:04:41<4:42:10,  2.79s/it]loss_total_epoch 131.0122990105301
Training tokenizer:  25% 1988/8047 [1:04:44<4:46:42,  2.84s/it]loss_total_epoch 131.06771525926888
Training tokenizer:  25% 1989/8047 [1:04:46<4:42:31,  2.80s/it]loss_total_epoch 131.12831912003458
Training tokenizer:  25% 1990/8047 [1:04:49<4:49:10,  2.86s/it]loss_total_epoch 131.18555984832346
Training tokenizer:  25% 1991/8047 [1:04:52<4:50:48,  2.88s/it]loss_total_epoch 131.24049467779696
Training tokenizer:  25% 1992/8047 [1:04:55<4:53:27,  2.91s/it]loss_total_epoch 131.29612312279642
Training tokenizer:  25% 1993/8047 [1:04:58<4:55:59,  2.93s/it]loss_total_epoch 131.34657409973443
Training tokenizer:  25% 1994/8047 [1:05:01<4:57:57,  2.95s/it]loss_total_epoch 131.41069468297064
Training tokenizer:  25% 1995/8047 [1:05:04<4:49:58,  2.87s/it]loss_total_epoch 131.4529069866985
Training tokenizer:  25% 1996/8047 [1:05:07<4:47:40,  2.85s/it]loss_total_epoch 131.51142841391265
Training tokenizer:  25% 1997/8047 [1:05:10<4:51:49,  2.89s/it]loss_total_epoch 131.5563786830753
Training tokenizer:  25% 1998/8047 [1:05:13<4:49:19,  2.87s/it]loss_total_epoch 131.60991292633116
Training tokenizer:  25% 1999/8047 [1:05:15<4:43:02,  2.81s/it]loss_total_epoch 131.6683042962104
Training tokenizer:  25% 2000/8047 [1:05:18<4:46:14,  2.84s/it]loss_total_epoch 131.72695021145046
Training tokenizer:  25% 2001/8047 [1:05:21<4:38:17,  2.76s/it]loss_total_epoch 131.78175516985357
Training tokenizer:  25% 2002/8047 [1:05:23<4:31:40,  2.70s/it]loss_total_epoch 131.83588557876647
Training tokenizer:  25% 2003/8047 [1:05:26<4:28:46,  2.67s/it]loss_total_epoch 131.88541576825082
Training tokenizer:  25% 2004/8047 [1:05:29<4:25:35,  2.64s/it]loss_total_epoch 131.9460589941591
Training tokenizer:  25% 2005/8047 [1:05:31<4:34:55,  2.73s/it]loss_total_epoch 132.00482282601297
Training tokenizer:  25% 2006/8047 [1:05:34<4:42:24,  2.80s/it]loss_total_epoch 132.0471971128136
Training tokenizer:  25% 2007/8047 [1:05:37<4:39:07,  2.77s/it]loss_total_epoch 132.10402670688927
Training tokenizer:  25% 2008/8047 [1:05:40<4:40:41,  2.79s/it]loss_total_epoch 132.16438132710755
Training tokenizer:  25% 2009/8047 [1:05:43<4:48:12,  2.86s/it]loss_total_epoch 132.2120772395283
Training tokenizer:  25% 2010/8047 [1:05:46<4:42:44,  2.81s/it]loss_total_epoch 132.27473052032292
Training tokenizer:  25% 2011/8047 [1:05:48<4:37:11,  2.76s/it]loss_total_epoch 132.32623155973852
Training tokenizer:  25% 2012/8047 [1:05:51<4:44:45,  2.83s/it]loss_total_epoch 132.37740376405418
Training tokenizer:  25% 2013/8047 [1:05:54<4:38:50,  2.77s/it]loss_total_epoch 132.4330728147179
Training tokenizer:  25% 2014/8047 [1:05:57<4:39:24,  2.78s/it]loss_total_epoch 132.48106010071933
Training tokenizer:  25% 2015/8047 [1:05:59<4:34:58,  2.74s/it]loss_total_epoch 132.54827295057476
Training tokenizer:  25% 2016/8047 [1:06:02<4:30:20,  2.69s/it]loss_total_epoch 132.60958109609783
Training tokenizer:  25% 2017/8047 [1:06:05<4:31:13,  2.70s/it]loss_total_epoch 132.6592206824571
Training tokenizer:  25% 2018/8047 [1:06:08<4:35:55,  2.75s/it]loss_total_epoch 132.70454316027462
Training tokenizer:  25% 2019/8047 [1:06:10<4:35:49,  2.75s/it]loss_total_epoch 132.75912945903838
Training tokenizer:  25% 2020/8047 [1:06:13<4:36:26,  2.75s/it]loss_total_epoch 132.80915910936892
Training tokenizer:  25% 2021/8047 [1:06:16<4:32:57,  2.72s/it]loss_total_epoch 132.87547987885773
Training tokenizer:  25% 2022/8047 [1:06:18<4:30:31,  2.69s/it]loss_total_epoch 132.93327153287828
Training tokenizer:  25% 2023/8047 [1:06:21<4:25:47,  2.65s/it]loss_total_epoch 132.99886082671583
Training tokenizer:  25% 2024/8047 [1:06:24<4:38:00,  2.77s/it]loss_total_epoch 133.06208309344947
Training tokenizer:  25% 2025/8047 [1:06:27<4:47:11,  2.86s/it]loss_total_epoch 133.1092839743942
Training tokenizer:  25% 2026/8047 [1:06:30<4:49:59,  2.89s/it]loss_total_epoch 133.15515437163413
Training tokenizer:  25% 2027/8047 [1:06:33<4:42:32,  2.82s/it]loss_total_epoch 133.2091213222593
Training tokenizer:  25% 2028/8047 [1:06:36<4:47:37,  2.87s/it]loss_total_epoch 133.26133607141674
Training tokenizer:  25% 2029/8047 [1:06:38<4:41:50,  2.81s/it]loss_total_epoch 133.32687818817794
Training tokenizer:  25% 2030/8047 [1:06:41<4:46:06,  2.85s/it]loss_total_epoch 133.39236090891063
Training tokenizer:  25% 2031/8047 [1:06:44<4:50:06,  2.89s/it]loss_total_epoch 133.44648589007556
Training tokenizer:  25% 2032/8047 [1:06:47<4:42:58,  2.82s/it]loss_total_epoch 133.48629287444055
Training tokenizer:  25% 2033/8047 [1:06:50<4:39:23,  2.79s/it]loss_total_epoch 133.53514902479947
Training tokenizer:  25% 2034/8047 [1:06:53<4:45:06,  2.84s/it]loss_total_epoch 133.60328772105277
Training tokenizer:  25% 2035/8047 [1:06:55<4:37:31,  2.77s/it]loss_total_epoch 133.6623963918537
Training tokenizer:  25% 2036/8047 [1:06:58<4:34:10,  2.74s/it]loss_total_epoch 133.7075551506132
Training tokenizer:  25% 2037/8047 [1:07:01<4:34:18,  2.74s/it]loss_total_epoch 133.75375905819237
Training tokenizer:  25% 2038/8047 [1:07:03<4:31:42,  2.71s/it]loss_total_epoch 133.80766864307225
Training tokenizer:  25% 2039/8047 [1:07:06<4:40:23,  2.80s/it]loss_total_epoch 133.86853938363492
Training tokenizer:  25% 2040/8047 [1:07:09<4:45:10,  2.85s/it]loss_total_epoch 133.9157210122794
Training tokenizer:  25% 2041/8047 [1:07:12<4:47:20,  2.87s/it]loss_total_epoch 133.96624810807407
Training tokenizer:  25% 2042/8047 [1:07:15<4:50:54,  2.91s/it]loss_total_epoch 134.0146435070783
Training tokenizer:  25% 2043/8047 [1:07:18<4:50:35,  2.90s/it]loss_total_epoch 134.062647620216
Training tokenizer:  25% 2044/8047 [1:07:21<4:42:10,  2.82s/it]loss_total_epoch 134.11154216341674
Training tokenizer:  25% 2045/8047 [1:07:23<4:40:35,  2.80s/it]loss_total_epoch 134.1583328191191
Training tokenizer:  25% 2046/8047 [1:07:26<4:40:42,  2.81s/it]loss_total_epoch 134.2053972389549
Training tokenizer:  25% 2047/8047 [1:07:29<4:44:39,  2.85s/it]loss_total_epoch 134.25854531116784
Training tokenizer:  25% 2048/8047 [1:07:32<4:40:00,  2.80s/it]loss_total_epoch 134.32615432329476
Training tokenizer:  25% 2049/8047 [1:07:35<4:46:27,  2.87s/it]loss_total_epoch 134.38561612926424
Training tokenizer:  25% 2050/8047 [1:07:38<4:44:26,  2.85s/it]loss_total_epoch 134.43206400237978
Training tokenizer:  25% 2051/8047 [1:07:41<4:47:09,  2.87s/it]loss_total_epoch 134.49367898888886
Training tokenizer:  26% 2052/8047 [1:07:43<4:48:13,  2.88s/it]loss_total_epoch 134.5525050777942
Training tokenizer:  26% 2053/8047 [1:07:46<4:50:45,  2.91s/it]loss_total_epoch 134.59657963179052
Training tokenizer:  26% 2054/8047 [1:07:49<4:43:06,  2.83s/it]loss_total_epoch 134.6586125921458
Training tokenizer:  26% 2055/8047 [1:07:52<4:45:16,  2.86s/it]loss_total_epoch 134.71676643751562
Training tokenizer:  26% 2056/8047 [1:07:55<4:50:12,  2.91s/it]loss_total_epoch 134.77502579055727
Training tokenizer:  26% 2057/8047 [1:07:58<4:42:24,  2.83s/it]loss_total_epoch 134.82078758068383
Training tokenizer:  26% 2058/8047 [1:08:01<4:48:54,  2.89s/it]loss_total_epoch 134.8881789240986
Training tokenizer:  26% 2059/8047 [1:08:04<4:46:01,  2.87s/it]loss_total_epoch 134.94749029539526
Training tokenizer:  26% 2060/8047 [1:08:06<4:39:57,  2.81s/it]loss_total_epoch 134.99842361547053
Training tokenizer:  26% 2061/8047 [1:08:09<4:33:19,  2.74s/it]loss_total_epoch 135.04868690110743
Training tokenizer:  26% 2062/8047 [1:08:12<4:37:27,  2.78s/it]loss_total_epoch 135.10139350034297
Training tokenizer:  26% 2063/8047 [1:08:14<4:34:59,  2.76s/it]loss_total_epoch 135.14701512642205
Training tokenizer:  26% 2064/8047 [1:08:17<4:42:39,  2.83s/it]loss_total_epoch 135.19960148073733
Training tokenizer:  26% 2065/8047 [1:08:20<4:43:55,  2.85s/it]loss_total_epoch 135.25002350844443
Training tokenizer:  26% 2066/8047 [1:08:23<4:49:21,  2.90s/it]loss_total_epoch 135.2959557119757
Training tokenizer:  26% 2067/8047 [1:08:26<4:51:13,  2.92s/it]loss_total_epoch 135.3418477680534
Training tokenizer:  26% 2068/8047 [1:08:29<4:40:55,  2.82s/it]loss_total_epoch 135.4001798760146
Training tokenizer:  26% 2069/8047 [1:08:32<4:45:40,  2.87s/it]loss_total_epoch 135.4479162041098
Training tokenizer:  26% 2070/8047 [1:08:34<4:38:37,  2.80s/it]loss_total_epoch 135.4954725895077
Training tokenizer:  26% 2071/8047 [1:08:37<4:39:46,  2.81s/it]loss_total_epoch 135.55195511318743
Training tokenizer:  26% 2072/8047 [1:08:40<4:34:42,  2.76s/it]loss_total_epoch 135.60507799126208
Training tokenizer:  26% 2073/8047 [1:08:43<4:29:19,  2.70s/it]loss_total_epoch 135.6712937746197
Training tokenizer:  26% 2074/8047 [1:08:45<4:37:12,  2.78s/it]loss_total_epoch 135.72357744164765
Training tokenizer:  26% 2075/8047 [1:08:48<4:35:03,  2.76s/it]loss_total_epoch 135.7869848627597
Training tokenizer:  26% 2076/8047 [1:08:51<4:38:07,  2.79s/it]loss_total_epoch 135.83378951065242
Training tokenizer:  26% 2077/8047 [1:08:54<4:45:32,  2.87s/it]loss_total_epoch 135.89521221630275
Training tokenizer:  26% 2078/8047 [1:08:57<4:43:36,  2.85s/it]loss_total_epoch 135.94084228388965
Training tokenizer:  26% 2079/8047 [1:09:00<4:38:12,  2.80s/it]loss_total_epoch 136.00060793943703
Training tokenizer:  26% 2080/8047 [1:09:03<4:44:47,  2.86s/it]loss_total_epoch 136.05991309322417
Training tokenizer:  26% 2081/8047 [1:09:06<4:45:59,  2.88s/it]loss_total_epoch 136.11408184282482
Training tokenizer:  26% 2082/8047 [1:09:08<4:42:12,  2.84s/it]loss_total_epoch 136.17683091573417
Training tokenizer:  26% 2083/8047 [1:09:11<4:42:45,  2.84s/it]loss_total_epoch 136.22645513527095
Training tokenizer:  26% 2084/8047 [1:09:14<4:46:44,  2.89s/it]loss_total_epoch 136.2812182214111
Training tokenizer:  26% 2085/8047 [1:09:17<4:38:48,  2.81s/it]loss_total_epoch 136.33533101342618
Training tokenizer:  26% 2086/8047 [1:09:20<4:44:22,  2.86s/it]loss_total_epoch 136.38278401084244
Training tokenizer:  26% 2087/8047 [1:09:23<4:49:13,  2.91s/it]loss_total_epoch 136.43870698846877
Training tokenizer:  26% 2088/8047 [1:09:25<4:42:01,  2.84s/it]loss_total_epoch 136.49550304748118
Training tokenizer:  26% 2089/8047 [1:09:28<4:36:50,  2.79s/it]loss_total_epoch 136.5609980020672
Training tokenizer:  26% 2090/8047 [1:09:31<4:37:19,  2.79s/it]loss_total_epoch 136.6134415473789
Training tokenizer:  26% 2091/8047 [1:09:34<4:42:05,  2.84s/it]loss_total_epoch 136.67632278613746
Training tokenizer:  26% 2092/8047 [1:09:37<4:44:41,  2.87s/it]loss_total_epoch 136.71894077397883
Training tokenizer:  26% 2093/8047 [1:09:40<4:40:31,  2.83s/it]loss_total_epoch 136.78015532530844
Training tokenizer:  26% 2094/8047 [1:09:42<4:35:14,  2.77s/it]loss_total_epoch 136.83368943445385
Training tokenizer:  26% 2095/8047 [1:09:45<4:34:21,  2.77s/it]loss_total_epoch 136.8762274403125
Training tokenizer:  26% 2096/8047 [1:09:48<4:42:46,  2.85s/it]loss_total_epoch 136.9228981602937
Training tokenizer:  26% 2097/8047 [1:09:51<4:50:15,  2.93s/it]loss_total_epoch 136.97014586813748
Training tokenizer:  26% 2098/8047 [1:09:54<4:40:07,  2.83s/it]loss_total_epoch 137.0194280873984
Training tokenizer:  26% 2099/8047 [1:09:56<4:35:50,  2.78s/it]loss_total_epoch 137.06664324365556
Training tokenizer:  26% 2100/8047 [1:09:59<4:34:04,  2.77s/it]loss_total_epoch 137.1220537032932
Training tokenizer:  26% 2101/8047 [1:10:02<4:36:07,  2.79s/it]loss_total_epoch 137.1795235890895
Training tokenizer:  26% 2102/8047 [1:10:05<4:43:36,  2.86s/it]loss_total_epoch 137.2392561007291
Training tokenizer:  26% 2103/8047 [1:10:08<4:49:55,  2.93s/it]loss_total_epoch 137.29527368955314
Training tokenizer:  26% 2104/8047 [1:10:11<4:53:29,  2.96s/it]loss_total_epoch 137.34814768098295
Training tokenizer:  26% 2105/8047 [1:10:14<4:59:14,  3.02s/it]loss_total_epoch 137.3954174760729
Training tokenizer:  26% 2106/8047 [1:10:17<4:59:39,  3.03s/it]loss_total_epoch 137.44242723844945
Training tokenizer:  26% 2107/8047 [1:10:20<4:52:40,  2.96s/it]loss_total_epoch 137.50131055153906
Training tokenizer:  26% 2108/8047 [1:10:23<4:45:02,  2.88s/it]loss_total_epoch 137.55143011920154
Training tokenizer:  26% 2109/8047 [1:10:26<4:51:13,  2.94s/it]loss_total_epoch 137.59994010441005
Training tokenizer:  26% 2110/8047 [1:10:29<4:45:41,  2.89s/it]loss_total_epoch 137.66089077480137
Training tokenizer:  26% 2111/8047 [1:10:32<4:48:23,  2.92s/it]loss_total_epoch 137.71596835367382
Training tokenizer:  26% 2112/8047 [1:10:34<4:43:16,  2.86s/it]loss_total_epoch 137.76230695657432
Training tokenizer:  26% 2113/8047 [1:10:37<4:38:02,  2.81s/it]loss_total_epoch 137.82143045775592
Training tokenizer:  26% 2114/8047 [1:10:40<4:31:43,  2.75s/it]loss_total_epoch 137.86862877942622
Training tokenizer:  26% 2115/8047 [1:10:43<4:41:50,  2.85s/it]loss_total_epoch 137.9220324177295
Training tokenizer:  26% 2116/8047 [1:10:46<4:48:12,  2.92s/it]loss_total_epoch 137.96343152411282
Training tokenizer:  26% 2117/8047 [1:10:49<4:50:19,  2.94s/it]loss_total_epoch 138.0215198416263
Training tokenizer:  26% 2118/8047 [1:10:52<4:48:05,  2.92s/it]loss_total_epoch 138.07355029322207
Training tokenizer:  26% 2119/8047 [1:10:54<4:42:47,  2.86s/it]loss_total_epoch 138.12823729403317
Training tokenizer:  26% 2120/8047 [1:10:57<4:46:39,  2.90s/it]loss_total_epoch 138.18204255215824
Training tokenizer:  26% 2121/8047 [1:11:00<4:49:54,  2.94s/it]loss_total_epoch 138.2472231220454
Training tokenizer:  26% 2122/8047 [1:11:03<4:39:54,  2.83s/it]loss_total_epoch 138.28812525607646
Training tokenizer:  26% 2123/8047 [1:11:06<4:35:44,  2.79s/it]loss_total_epoch 138.33941299282014
Training tokenizer:  26% 2124/8047 [1:11:08<4:34:08,  2.78s/it]loss_total_epoch 138.39733660407364
Training tokenizer:  26% 2125/8047 [1:11:11<4:43:51,  2.88s/it]loss_total_epoch 138.4526843111962
Training tokenizer:  26% 2126/8047 [1:11:15<4:49:27,  2.93s/it]loss_total_epoch 138.51448366604745
Training tokenizer:  26% 2127/8047 [1:11:18<4:55:27,  2.99s/it]loss_total_epoch 138.56517113558948
Training tokenizer:  26% 2128/8047 [1:11:21<4:52:08,  2.96s/it]loss_total_epoch 138.6117039974779
Training tokenizer:  26% 2129/8047 [1:11:23<4:45:26,  2.89s/it]loss_total_epoch 138.662843240425
Training tokenizer:  26% 2130/8047 [1:11:26<4:50:58,  2.95s/it]loss_total_epoch 138.7243562694639
Training tokenizer:  26% 2131/8047 [1:11:29<4:42:52,  2.87s/it]loss_total_epoch 138.7851455900818
Training tokenizer:  26% 2132/8047 [1:11:32<4:37:36,  2.82s/it]loss_total_epoch 138.84256233088672
Training tokenizer:  27% 2133/8047 [1:11:35<4:38:50,  2.83s/it]loss_total_epoch 138.90005513839424
Training tokenizer:  27% 2134/8047 [1:11:37<4:37:26,  2.82s/it]loss_total_epoch 138.960828544572
Training tokenizer:  27% 2135/8047 [1:11:40<4:33:54,  2.78s/it]loss_total_epoch 139.00965340994298
Training tokenizer:  27% 2136/8047 [1:11:43<4:35:13,  2.79s/it]loss_total_epoch 139.06554217077792
Training tokenizer:  27% 2137/8047 [1:11:46<4:37:32,  2.82s/it]loss_total_epoch 139.11706043593585
Training tokenizer:  27% 2138/8047 [1:11:48<4:32:25,  2.77s/it]loss_total_epoch 139.16662157140672
Training tokenizer:  27% 2139/8047 [1:11:52<4:41:21,  2.86s/it]loss_total_epoch 139.20968917943537
Training tokenizer:  27% 2140/8047 [1:11:54<4:36:19,  2.81s/it]loss_total_epoch 139.2593820821494
Training tokenizer:  27% 2141/8047 [1:11:57<4:43:08,  2.88s/it]loss_total_epoch 139.31652631051838
Training tokenizer:  27% 2142/8047 [1:12:00<4:38:38,  2.83s/it]loss_total_epoch 139.36902319826186
Training tokenizer:  27% 2143/8047 [1:12:03<4:35:36,  2.80s/it]loss_total_epoch 139.41338883899152
Training tokenizer:  27% 2144/8047 [1:12:06<4:44:09,  2.89s/it]loss_total_epoch 139.47246996872127
Training tokenizer:  27% 2145/8047 [1:12:08<4:36:13,  2.81s/it]loss_total_epoch 139.52802958898246
Training tokenizer:  27% 2146/8047 [1:12:11<4:36:32,  2.81s/it]loss_total_epoch 139.56899410299957
Training tokenizer:  27% 2147/8047 [1:12:14<4:32:14,  2.77s/it]loss_total_epoch 139.6260530371219
Training tokenizer:  27% 2148/8047 [1:12:17<4:43:39,  2.89s/it]loss_total_epoch 139.67979870177805
Training tokenizer:  27% 2149/8047 [1:12:20<4:50:26,  2.95s/it]loss_total_epoch 139.73263154365122
Training tokenizer:  27% 2150/8047 [1:12:23<4:47:36,  2.93s/it]loss_total_epoch 139.79170289449394
Training tokenizer:  27% 2151/8047 [1:12:26<4:54:14,  2.99s/it]loss_total_epoch 139.85123519785702
Training tokenizer:  27% 2152/8047 [1:12:29<4:57:19,  3.03s/it]loss_total_epoch 139.89729885198176
Training tokenizer:  27% 2153/8047 [1:12:32<4:53:05,  2.98s/it]loss_total_epoch 139.95690529607236
Training tokenizer:  27% 2154/8047 [1:12:35<4:53:29,  2.99s/it]loss_total_epoch 140.0041748750955
Training tokenizer:  27% 2155/8047 [1:12:38<4:47:09,  2.92s/it]loss_total_epoch 140.0549343097955
Training tokenizer:  27% 2156/8047 [1:12:41<4:52:41,  2.98s/it]loss_total_epoch 140.09422246925533
Training tokenizer:  27% 2157/8047 [1:12:44<4:48:02,  2.93s/it]loss_total_epoch 140.1464096661657
Training tokenizer:  27% 2158/8047 [1:12:47<4:54:20,  3.00s/it]loss_total_epoch 140.20040782354772
Training tokenizer:  27% 2159/8047 [1:12:50<4:58:16,  3.04s/it]loss_total_epoch 140.24991109408438
Training tokenizer:  27% 2160/8047 [1:12:53<4:53:45,  2.99s/it]loss_total_epoch 140.3009351361543
Training tokenizer:  27% 2161/8047 [1:12:56<4:51:17,  2.97s/it]loss_total_epoch 140.34940203092992
Training tokenizer:  27% 2162/8047 [1:12:59<4:53:10,  2.99s/it]loss_total_epoch 140.39757578261197
Training tokenizer:  27% 2163/8047 [1:13:02<4:55:35,  3.01s/it]loss_total_epoch 140.4469357598573
Training tokenizer:  27% 2164/8047 [1:13:05<4:58:11,  3.04s/it]loss_total_epoch 140.5036736894399
Training tokenizer:  27% 2165/8047 [1:13:08<5:00:45,  3.07s/it]loss_total_epoch 140.5645064432174
Training tokenizer:  27% 2166/8047 [1:13:11<5:01:54,  3.08s/it]loss_total_epoch 140.6189426537603
Training tokenizer:  27% 2167/8047 [1:13:15<5:03:42,  3.10s/it]loss_total_epoch 140.6707591470331
Training tokenizer:  27% 2168/8047 [1:13:18<5:03:02,  3.09s/it]loss_total_epoch 140.71577707119286
Training tokenizer:  27% 2169/8047 [1:13:21<5:04:27,  3.11s/it]loss_total_epoch 140.77719045616686
Training tokenizer:  27% 2170/8047 [1:13:24<5:04:01,  3.10s/it]loss_total_epoch 140.83309776894748
Training tokenizer:  27% 2171/8047 [1:13:27<4:54:38,  3.01s/it]loss_total_epoch 140.9066787417978
Training tokenizer:  27% 2172/8047 [1:13:30<4:56:08,  3.02s/it]loss_total_epoch 140.96190234832466
Training tokenizer:  27% 2173/8047 [1:13:33<4:58:32,  3.05s/it]loss_total_epoch 141.01100368611515
Training tokenizer:  27% 2174/8047 [1:13:36<5:02:53,  3.09s/it]loss_total_epoch 141.05591521970928
Training tokenizer:  27% 2175/8047 [1:13:39<5:07:30,  3.14s/it]loss_total_epoch 141.112517202273
Training tokenizer:  27% 2176/8047 [1:13:43<5:09:32,  3.16s/it]loss_total_epoch 141.15700067020953
Training tokenizer:  27% 2177/8047 [1:13:46<5:08:25,  3.15s/it]loss_total_epoch 141.21562821231782
Training tokenizer:  27% 2178/8047 [1:13:48<4:55:07,  3.02s/it]loss_total_epoch 141.27316029183567
Training tokenizer:  27% 2179/8047 [1:13:51<4:52:45,  2.99s/it]loss_total_epoch 141.3305900786072
Training tokenizer:  27% 2180/8047 [1:13:54<4:55:12,  3.02s/it]loss_total_epoch 141.38086256571114
Training tokenizer:  27% 2181/8047 [1:13:57<4:55:37,  3.02s/it]loss_total_epoch 141.432274883613
Training tokenizer:  27% 2182/8047 [1:14:00<4:48:23,  2.95s/it]loss_total_epoch 141.48518718592823
Training tokenizer:  27% 2183/8047 [1:14:03<4:53:33,  3.00s/it]loss_total_epoch 141.53574767522514
Training tokenizer:  27% 2184/8047 [1:14:06<4:57:34,  3.05s/it]loss_total_epoch 141.60041995160282
Training tokenizer:  27% 2185/8047 [1:14:09<4:45:51,  2.93s/it]loss_total_epoch 141.65130437724292
Training tokenizer:  27% 2186/8047 [1:14:12<4:38:25,  2.85s/it]loss_total_epoch 141.70710854791105
Training tokenizer:  27% 2187/8047 [1:14:15<4:47:37,  2.94s/it]loss_total_epoch 141.76311401091516
Training tokenizer:  27% 2188/8047 [1:14:18<4:49:08,  2.96s/it]loss_total_epoch 141.82099307514727
Training tokenizer:  27% 2189/8047 [1:14:21<4:53:53,  3.01s/it]loss_total_epoch 141.86859247647226
Training tokenizer:  27% 2190/8047 [1:14:24<4:44:15,  2.91s/it]loss_total_epoch 141.9001973401755
Training tokenizer:  27% 2191/8047 [1:14:27<4:49:54,  2.97s/it]loss_total_epoch 141.9497669097036
Training tokenizer:  27% 2192/8047 [1:14:30<4:55:10,  3.02s/it]loss_total_epoch 141.99646714143455
Training tokenizer:  27% 2193/8047 [1:14:33<4:58:15,  3.06s/it]loss_total_epoch 142.0505470726639
Training tokenizer:  27% 2194/8047 [1:14:36<5:02:44,  3.10s/it]loss_total_epoch 142.1013258229941
Training tokenizer:  27% 2195/8047 [1:14:39<5:04:16,  3.12s/it]loss_total_epoch 142.156132215634
Training tokenizer:  27% 2196/8047 [1:14:43<5:04:18,  3.12s/it]loss_total_epoch 142.2199612427503
Training tokenizer:  27% 2197/8047 [1:14:46<5:01:56,  3.10s/it]loss_total_epoch 142.2723098900169
Training tokenizer:  27% 2198/8047 [1:14:49<4:54:48,  3.02s/it]loss_total_epoch 142.32381603308022
Training tokenizer:  27% 2199/8047 [1:14:51<4:53:15,  3.01s/it]loss_total_epoch 142.38012011162937
Training tokenizer:  27% 2200/8047 [1:14:54<4:53:08,  3.01s/it]loss_total_epoch 142.4318982604891
Training tokenizer:  27% 2201/8047 [1:14:58<4:58:07,  3.06s/it]loss_total_epoch 142.48247812129557
Training tokenizer:  27% 2202/8047 [1:15:01<4:57:34,  3.05s/it]loss_total_epoch 142.53194168396294
Training tokenizer:  27% 2203/8047 [1:15:04<4:51:54,  3.00s/it]loss_total_epoch 142.58339375071228
Training tokenizer:  27% 2204/8047 [1:15:07<4:49:44,  2.98s/it]loss_total_epoch 142.63165956921875
Training tokenizer:  27% 2205/8047 [1:15:10<4:55:20,  3.03s/it]loss_total_epoch 142.69369535706937
Training tokenizer:  27% 2206/8047 [1:15:13<4:58:37,  3.07s/it]loss_total_epoch 142.75078605301678
Training tokenizer:  27% 2207/8047 [1:15:16<4:50:54,  2.99s/it]loss_total_epoch 142.80029243044555
Training tokenizer:  27% 2208/8047 [1:15:18<4:42:51,  2.91s/it]loss_total_epoch 142.85707305185497
Training tokenizer:  27% 2209/8047 [1:15:21<4:49:55,  2.98s/it]loss_total_epoch 142.9082292523235
Training tokenizer:  27% 2210/8047 [1:15:24<4:40:11,  2.88s/it]loss_total_epoch 142.96496610157192
Training tokenizer:  27% 2211/8047 [1:15:27<4:36:54,  2.85s/it]loss_total_epoch 143.00940147973597
Training tokenizer:  27% 2212/8047 [1:15:30<4:33:33,  2.81s/it]loss_total_epoch 143.06031584180892
Training tokenizer:  28% 2213/8047 [1:15:32<4:30:02,  2.78s/it]loss_total_epoch 143.1128541920334
Training tokenizer:  28% 2214/8047 [1:15:36<4:41:19,  2.89s/it]loss_total_epoch 143.17265929467976
Training tokenizer:  28% 2215/8047 [1:15:38<4:34:13,  2.82s/it]loss_total_epoch 143.20573932491243
Training tokenizer:  28% 2216/8047 [1:15:41<4:35:35,  2.84s/it]loss_total_epoch 143.25550678931177
Training tokenizer:  28% 2217/8047 [1:15:44<4:46:22,  2.95s/it]loss_total_epoch 143.3203443493694
Training tokenizer:  28% 2218/8047 [1:15:47<4:53:15,  3.02s/it]loss_total_epoch 143.37080444581807
Training tokenizer:  28% 2219/8047 [1:15:51<4:56:01,  3.05s/it]loss_total_epoch 143.43125666864216
Training tokenizer:  28% 2220/8047 [1:15:54<5:00:41,  3.10s/it]loss_total_epoch 143.4826859626919
Training tokenizer:  28% 2221/8047 [1:15:57<4:52:57,  3.02s/it]loss_total_epoch 143.5420426633209
Training tokenizer:  28% 2222/8047 [1:15:59<4:45:02,  2.94s/it]loss_total_epoch 143.60591382347047
Training tokenizer:  28% 2223/8047 [1:16:02<4:42:51,  2.91s/it]loss_total_epoch 143.65954509563744
Training tokenizer:  28% 2224/8047 [1:16:05<4:41:58,  2.91s/it]loss_total_epoch 143.70886698551476
Training tokenizer:  28% 2225/8047 [1:16:08<4:50:15,  2.99s/it]loss_total_epoch 143.77799867279828
Training tokenizer:  28% 2226/8047 [1:16:11<4:52:04,  3.01s/it]loss_total_epoch 143.82819657586515
Training tokenizer:  28% 2227/8047 [1:16:14<4:49:36,  2.99s/it]loss_total_epoch 143.87645370326936
Training tokenizer:  28% 2228/8047 [1:16:17<4:54:48,  3.04s/it]loss_total_epoch 143.9323188494891
Training tokenizer:  28% 2229/8047 [1:16:21<4:58:17,  3.08s/it]loss_total_epoch 143.98102892003953
Training tokenizer:  28% 2230/8047 [1:16:23<4:51:45,  3.01s/it]loss_total_epoch 144.0392365437001
Training tokenizer:  28% 2231/8047 [1:16:27<4:56:29,  3.06s/it]loss_total_epoch 144.09247550927103
Training tokenizer:  28% 2232/8047 [1:16:30<4:59:28,  3.09s/it]loss_total_epoch 144.14173253439367
Training tokenizer:  28% 2233/8047 [1:16:33<4:57:03,  3.07s/it]loss_total_epoch 144.1987131331116
Training tokenizer:  28% 2234/8047 [1:16:36<4:57:42,  3.07s/it]loss_total_epoch 144.25295477546751
Training tokenizer:  28% 2235/8047 [1:16:39<5:00:58,  3.11s/it]loss_total_epoch 144.29657388664782
Training tokenizer:  28% 2236/8047 [1:16:42<4:55:09,  3.05s/it]loss_total_epoch 144.35003115423024
Training tokenizer:  28% 2237/8047 [1:16:45<4:55:42,  3.05s/it]loss_total_epoch 144.3990117367357
Training tokenizer:  28% 2238/8047 [1:16:48<4:52:34,  3.02s/it]loss_total_epoch 144.4356361385435
Training tokenizer:  28% 2239/8047 [1:16:51<4:56:22,  3.06s/it]loss_total_epoch 144.48148555867374
Training tokenizer:  28% 2240/8047 [1:16:54<4:45:50,  2.95s/it]loss_total_epoch 144.53543430753052
Training tokenizer:  28% 2241/8047 [1:16:57<4:47:31,  2.97s/it]loss_total_epoch 144.58944963477552
Training tokenizer:  28% 2242/8047 [1:17:00<4:39:35,  2.89s/it]loss_total_epoch 144.64876766316593
Training tokenizer:  28% 2243/8047 [1:17:03<4:48:55,  2.99s/it]loss_total_epoch 144.69378483854234
Training tokenizer:  28% 2244/8047 [1:17:06<4:44:20,  2.94s/it]loss_total_epoch 144.73824376426637
Training tokenizer:  28% 2245/8047 [1:17:09<4:53:04,  3.03s/it]loss_total_epoch 144.79927813075483
Training tokenizer:  28% 2246/8047 [1:17:12<4:44:44,  2.95s/it]loss_total_epoch 144.8526136111468
Training tokenizer:  28% 2247/8047 [1:17:14<4:41:47,  2.92s/it]loss_total_epoch 144.9073204305023
Training tokenizer:  28% 2248/8047 [1:17:18<4:49:13,  2.99s/it]loss_total_epoch 144.96239162795246
Training tokenizer:  28% 2249/8047 [1:17:20<4:42:43,  2.93s/it]loss_total_epoch 145.0177003685385
Training tokenizer:  28% 2250/8047 [1:17:24<4:51:24,  3.02s/it]loss_total_epoch 145.05231590010226
Training tokenizer:  28% 2251/8047 [1:17:27<4:54:32,  3.05s/it]loss_total_epoch 145.09092801250517
Training tokenizer:  28% 2252/8047 [1:17:30<4:59:08,  3.10s/it]loss_total_epoch 145.14915471710265
Training tokenizer:  28% 2253/8047 [1:17:33<4:49:23,  3.00s/it]loss_total_epoch 145.2030717190355
Training tokenizer:  28% 2254/8047 [1:17:36<4:53:46,  3.04s/it]loss_total_epoch 145.25530184991658
Training tokenizer:  28% 2255/8047 [1:17:39<4:55:17,  3.06s/it]loss_total_epoch 145.31737197376788
Training tokenizer:  28% 2256/8047 [1:17:42<4:47:51,  2.98s/it]loss_total_epoch 145.37736568786204
Training tokenizer:  28% 2257/8047 [1:17:44<4:37:44,  2.88s/it]loss_total_epoch 145.42519358359277
Training tokenizer:  28% 2258/8047 [1:17:47<4:43:47,  2.94s/it]loss_total_epoch 145.48330147005618
Training tokenizer:  28% 2259/8047 [1:17:50<4:37:07,  2.87s/it]loss_total_epoch 145.5286100562662
Training tokenizer:  28% 2260/8047 [1:17:53<4:39:16,  2.90s/it]loss_total_epoch 145.58240588195622
Training tokenizer:  28% 2261/8047 [1:17:56<4:45:49,  2.96s/it]loss_total_epoch 145.63007869385183
Training tokenizer:  28% 2262/8047 [1:18:00<4:54:26,  3.05s/it]loss_total_epoch 145.68204769305885
Training tokenizer:  28% 2263/8047 [1:18:03<4:54:52,  3.06s/it]loss_total_epoch 145.74073928035796
Training tokenizer:  28% 2264/8047 [1:18:06<4:55:53,  3.07s/it]loss_total_epoch 145.78920176811516
Training tokenizer:  28% 2265/8047 [1:18:09<4:50:30,  3.01s/it]loss_total_epoch 145.84148968197405
Training tokenizer:  28% 2266/8047 [1:18:12<4:54:49,  3.06s/it]loss_total_epoch 145.89890933968127
Training tokenizer:  28% 2267/8047 [1:18:15<4:59:01,  3.10s/it]loss_total_epoch 145.94631616584957
Training tokenizer:  28% 2268/8047 [1:18:18<5:00:44,  3.12s/it]loss_total_epoch 145.99136652238667
Training tokenizer:  28% 2269/8047 [1:18:21<4:53:34,  3.05s/it]loss_total_epoch 146.0345903094858
Training tokenizer:  28% 2270/8047 [1:18:24<4:52:44,  3.04s/it]loss_total_epoch 146.08621571026742
Training tokenizer:  28% 2271/8047 [1:18:27<4:57:40,  3.09s/it]loss_total_epoch 146.13970991410315
Training tokenizer:  28% 2272/8047 [1:18:30<5:02:04,  3.14s/it]loss_total_epoch 146.19146311096847
Training tokenizer:  28% 2273/8047 [1:18:33<4:54:56,  3.06s/it]loss_total_epoch 146.2332284618169
Training tokenizer:  28% 2274/8047 [1:18:36<4:50:43,  3.02s/it]loss_total_epoch 146.28554756753147
Training tokenizer:  28% 2275/8047 [1:18:39<4:49:19,  3.01s/it]loss_total_epoch 146.32071680016816
Training tokenizer:  28% 2276/8047 [1:18:42<4:42:54,  2.94s/it]loss_total_epoch 146.37466826103628
Training tokenizer:  28% 2277/8047 [1:18:45<4:51:46,  3.03s/it]loss_total_epoch 146.43427798338234
Training tokenizer:  28% 2278/8047 [1:18:49<4:58:59,  3.11s/it]loss_total_epoch 146.48862903751433
Training tokenizer:  28% 2279/8047 [1:18:52<4:55:31,  3.07s/it]loss_total_epoch 146.5535482559353
Training tokenizer:  28% 2280/8047 [1:18:55<4:59:54,  3.12s/it]loss_total_epoch 146.6040112953633
Training tokenizer:  28% 2281/8047 [1:18:58<5:02:50,  3.15s/it]loss_total_epoch 146.65510720200837
Training tokenizer:  28% 2282/8047 [1:19:01<5:03:25,  3.16s/it]loss_total_epoch 146.71080935187638
Training tokenizer:  28% 2283/8047 [1:19:04<5:06:44,  3.19s/it]loss_total_epoch 146.7696770671755
Training tokenizer:  28% 2284/8047 [1:19:08<5:04:09,  3.17s/it]loss_total_epoch 146.82495520822704
Training tokenizer:  28% 2285/8047 [1:19:11<5:06:22,  3.19s/it]loss_total_epoch 146.87794206105173
Training tokenizer:  28% 2286/8047 [1:19:14<5:05:51,  3.19s/it]loss_total_epoch 146.927000047639
Training tokenizer:  28% 2287/8047 [1:19:17<5:04:24,  3.17s/it]loss_total_epoch 146.9828291181475
Training tokenizer:  28% 2288/8047 [1:19:20<5:02:39,  3.15s/it]loss_total_epoch 147.03726941160858
Training tokenizer:  28% 2289/8047 [1:19:23<5:03:55,  3.17s/it]loss_total_epoch 147.09497285075486
Training tokenizer:  28% 2290/8047 [1:19:26<4:59:28,  3.12s/it]loss_total_epoch 147.16583838500082
Training tokenizer:  28% 2291/8047 [1:19:29<4:49:10,  3.01s/it]loss_total_epoch 147.2246910650283
Training tokenizer:  28% 2292/8047 [1:19:32<4:51:47,  3.04s/it]loss_total_epoch 147.27722824923694
Training tokenizer:  28% 2293/8047 [1:19:35<4:44:54,  2.97s/it]loss_total_epoch 147.32848927937448
Training tokenizer:  29% 2294/8047 [1:19:38<4:37:46,  2.90s/it]loss_total_epoch 147.3847898710519
Training tokenizer:  29% 2295/8047 [1:19:41<4:39:32,  2.92s/it]loss_total_epoch 147.44293057732284
Training tokenizer:  29% 2296/8047 [1:19:44<4:36:01,  2.88s/it]loss_total_epoch 147.49296651221812
Training tokenizer:  29% 2297/8047 [1:19:47<4:47:53,  3.00s/it]loss_total_epoch 147.54212494380772
Training tokenizer:  29% 2298/8047 [1:19:50<4:50:38,  3.03s/it]loss_total_epoch 147.5878653805703
Training tokenizer:  29% 2299/8047 [1:19:53<4:57:06,  3.10s/it]loss_total_epoch 147.64218973182142
Training tokenizer:  29% 2300/8047 [1:19:56<4:57:32,  3.11s/it]loss_total_epoch 147.70305068604648
Training tokenizer:  29% 2301/8047 [1:19:59<4:50:10,  3.03s/it]loss_total_epoch 147.75291886366904
Training tokenizer:  29% 2302/8047 [1:20:02<4:43:24,  2.96s/it]loss_total_epoch 147.7980850469321
Training tokenizer:  29% 2303/8047 [1:20:05<4:48:55,  3.02s/it]loss_total_epoch 147.85897531174123
Training tokenizer:  29% 2304/8047 [1:20:08<4:54:37,  3.08s/it]loss_total_epoch 147.90177148021758
Training tokenizer:  29% 2305/8047 [1:20:12<4:58:09,  3.12s/it]loss_total_epoch 147.9644220713526
Training tokenizer:  29% 2306/8047 [1:20:15<4:57:22,  3.11s/it]loss_total_epoch 148.0128751900047
Training tokenizer:  29% 2307/8047 [1:20:18<5:00:00,  3.14s/it]loss_total_epoch 148.06113670952618
Training tokenizer:  29% 2308/8047 [1:20:21<4:59:17,  3.13s/it]loss_total_epoch 148.1180978063494
Training tokenizer:  29% 2309/8047 [1:20:24<5:01:49,  3.16s/it]loss_total_epoch 148.17720544151962
Training tokenizer:  29% 2310/8047 [1:20:27<5:03:46,  3.18s/it]loss_total_epoch 148.23295186273754
Training tokenizer:  29% 2311/8047 [1:20:31<5:03:04,  3.17s/it]loss_total_epoch 148.2763533499092
Training tokenizer:  29% 2312/8047 [1:20:34<5:03:06,  3.17s/it]loss_total_epoch 148.32627922855318
Training tokenizer:  29% 2313/8047 [1:20:37<4:53:17,  3.07s/it]loss_total_epoch 148.37025085650384
Training tokenizer:  29% 2314/8047 [1:20:40<4:50:53,  3.04s/it]loss_total_epoch 148.42349024303257
Training tokenizer:  29% 2315/8047 [1:20:43<4:48:20,  3.02s/it]loss_total_epoch 148.47073768265545
Training tokenizer:  29% 2316/8047 [1:20:46<4:53:31,  3.07s/it]loss_total_epoch 148.51768256165087
Training tokenizer:  29% 2317/8047 [1:20:49<4:55:21,  3.09s/it]loss_total_epoch 148.57046286948025
Training tokenizer:  29% 2318/8047 [1:20:52<4:50:40,  3.04s/it]loss_total_epoch 148.63270528428257
Training tokenizer:  29% 2319/8047 [1:20:55<4:54:33,  3.09s/it]loss_total_epoch 148.6727500911802
Training tokenizer:  29% 2320/8047 [1:20:58<4:44:07,  2.98s/it]loss_total_epoch 148.72047323919833
Training tokenizer:  29% 2321/8047 [1:21:01<4:39:16,  2.93s/it]loss_total_epoch 148.77252298034728
Training tokenizer:  29% 2322/8047 [1:21:04<4:43:12,  2.97s/it]loss_total_epoch 148.83059682883322
Training tokenizer:  29% 2323/8047 [1:21:07<4:44:06,  2.98s/it]loss_total_epoch 148.8802630249411
Training tokenizer:  29% 2324/8047 [1:21:10<4:46:51,  3.01s/it]loss_total_epoch 148.93690186180174
Training tokenizer:  29% 2325/8047 [1:21:12<4:39:05,  2.93s/it]loss_total_epoch 148.98002235032618
Training tokenizer:  29% 2326/8047 [1:21:16<4:44:41,  2.99s/it]loss_total_epoch 149.03925770707428
Training tokenizer:  29% 2327/8047 [1:21:19<4:49:44,  3.04s/it]loss_total_epoch 149.0799257773906
Training tokenizer:  29% 2328/8047 [1:21:22<4:56:49,  3.11s/it]loss_total_epoch 149.12296045012772
Training tokenizer:  29% 2329/8047 [1:21:25<4:47:34,  3.02s/it]loss_total_epoch 149.18240748532116
Training tokenizer:  29% 2330/8047 [1:21:28<4:48:41,  3.03s/it]loss_total_epoch 149.23663655854762
Training tokenizer:  29% 2331/8047 [1:21:31<4:47:13,  3.01s/it]loss_total_epoch 149.2995281238109
Training tokenizer:  29% 2332/8047 [1:21:34<4:47:53,  3.02s/it]loss_total_epoch 149.3493877183646
Training tokenizer:  29% 2333/8047 [1:21:37<4:48:18,  3.03s/it]loss_total_epoch 149.39836150594056
Training tokenizer:  29% 2334/8047 [1:21:40<4:55:18,  3.10s/it]loss_total_epoch 149.4579168986529
Training tokenizer:  29% 2335/8047 [1:21:43<4:59:00,  3.14s/it]loss_total_epoch 149.50070586241782
Training tokenizer:  29% 2336/8047 [1:21:47<4:59:23,  3.15s/it]loss_total_epoch 149.54008722491562
Training tokenizer:  29% 2337/8047 [1:21:50<4:58:34,  3.14s/it]loss_total_epoch 149.58827154524624
Training tokenizer:  29% 2338/8047 [1:21:53<4:59:19,  3.15s/it]loss_total_epoch 149.64846388064325
Training tokenizer:  29% 2339/8047 [1:21:56<4:56:38,  3.12s/it]loss_total_epoch 149.70561815612018
Training tokenizer:  29% 2340/8047 [1:21:59<4:49:11,  3.04s/it]loss_total_epoch 149.75626858882606
Training tokenizer:  29% 2341/8047 [1:22:02<4:53:39,  3.09s/it]loss_total_epoch 149.81244239397347
Training tokenizer:  29% 2342/8047 [1:22:05<4:59:45,  3.15s/it]loss_total_epoch 149.87244330905378
Training tokenizer:  29% 2343/8047 [1:22:08<4:59:38,  3.15s/it]loss_total_epoch 149.92542639560997
Training tokenizer:  29% 2344/8047 [1:22:11<4:56:09,  3.12s/it]loss_total_epoch 149.98254762403667
Training tokenizer:  29% 2345/8047 [1:22:14<4:45:47,  3.01s/it]loss_total_epoch 150.0384442154318
Training tokenizer:  29% 2346/8047 [1:22:17<4:39:06,  2.94s/it]loss_total_epoch 150.08457979001105
Training tokenizer:  29% 2347/8047 [1:22:20<4:32:30,  2.87s/it]loss_total_epoch 150.14488340727985
Training tokenizer:  29% 2348/8047 [1:22:23<4:43:30,  2.98s/it]loss_total_epoch 150.20102569274604
Training tokenizer:  29% 2349/8047 [1:22:26<4:38:46,  2.94s/it]loss_total_epoch 150.2531116064638
Training tokenizer:  29% 2350/8047 [1:22:29<4:39:21,  2.94s/it]loss_total_epoch 150.2962443884462
Training tokenizer:  29% 2351/8047 [1:22:32<4:37:49,  2.93s/it]loss_total_epoch 150.35901438258588
Training tokenizer:  29% 2352/8047 [1:22:34<4:33:49,  2.88s/it]loss_total_epoch 150.40261358581483
Training tokenizer:  29% 2353/8047 [1:22:37<4:37:21,  2.92s/it]loss_total_epoch 150.45734569244087
Training tokenizer:  29% 2354/8047 [1:22:41<4:47:22,  3.03s/it]loss_total_epoch 150.5048415120691
Training tokenizer:  29% 2355/8047 [1:22:44<4:51:42,  3.07s/it]loss_total_epoch 150.55024118162692
Training tokenizer:  29% 2356/8047 [1:22:47<4:50:51,  3.07s/it]loss_total_epoch 150.61290943808854
Training tokenizer:  29% 2357/8047 [1:22:50<4:45:57,  3.02s/it]loss_total_epoch 150.65976365841925
Training tokenizer:  29% 2358/8047 [1:22:53<4:38:59,  2.94s/it]loss_total_epoch 150.71136690862477
Training tokenizer:  29% 2359/8047 [1:22:56<4:44:07,  3.00s/it]loss_total_epoch 150.760472105816
Training tokenizer:  29% 2360/8047 [1:22:59<4:50:50,  3.07s/it]loss_total_epoch 150.821362150833
Training tokenizer:  29% 2361/8047 [1:23:02<4:49:43,  3.06s/it]loss_total_epoch 150.87882004864514
Training tokenizer:  29% 2362/8047 [1:23:05<4:56:53,  3.13s/it]loss_total_epoch 150.92680355347693
Training tokenizer:  29% 2363/8047 [1:23:09<5:01:13,  3.18s/it]loss_total_epoch 150.98502385430038
Training tokenizer:  29% 2364/8047 [1:23:12<5:01:02,  3.18s/it]loss_total_epoch 151.03833718784153
Training tokenizer:  29% 2365/8047 [1:23:15<4:59:48,  3.17s/it]loss_total_epoch 151.09100686572492
Training tokenizer:  29% 2366/8047 [1:23:18<5:02:40,  3.20s/it]loss_total_epoch 151.14332103542984
Training tokenizer:  29% 2367/8047 [1:23:21<4:53:56,  3.10s/it]loss_total_epoch 151.1974315326661
Training tokenizer:  29% 2368/8047 [1:23:24<4:45:02,  3.01s/it]loss_total_epoch 151.2358071114868
Training tokenizer:  29% 2369/8047 [1:23:27<4:51:55,  3.08s/it]loss_total_epoch 151.29480426944792
Training tokenizer:  29% 2370/8047 [1:23:30<4:52:29,  3.09s/it]loss_total_epoch 151.3483972940594
Training tokenizer:  29% 2371/8047 [1:23:33<4:50:28,  3.07s/it]loss_total_epoch 151.39105279929936
Training tokenizer:  29% 2372/8047 [1:23:36<4:41:20,  2.97s/it]loss_total_epoch 151.4437651988119
Training tokenizer:  29% 2373/8047 [1:23:39<4:35:05,  2.91s/it]loss_total_epoch 151.48814762197435
Training tokenizer:  30% 2374/8047 [1:23:42<4:33:41,  2.89s/it]loss_total_epoch 151.54115799628198
Training tokenizer:  30% 2375/8047 [1:23:45<4:41:11,  2.97s/it]loss_total_epoch 151.59323912672698
Training tokenizer:  30% 2376/8047 [1:23:48<4:48:29,  3.05s/it]loss_total_epoch 151.64618233032525
Training tokenizer:  30% 2377/8047 [1:23:51<4:50:32,  3.07s/it]loss_total_epoch 151.69438470713794
Training tokenizer:  30% 2378/8047 [1:23:54<4:43:24,  3.00s/it]loss_total_epoch 151.7490801755339
Training tokenizer:  30% 2379/8047 [1:23:57<4:41:12,  2.98s/it]loss_total_epoch 151.80480345897377
Training tokenizer:  30% 2380/8047 [1:24:00<4:43:44,  3.00s/it]loss_total_epoch 151.8623213749379
Training tokenizer:  30% 2381/8047 [1:24:03<4:49:28,  3.07s/it]loss_total_epoch 151.91596070863307
Training tokenizer:  30% 2382/8047 [1:24:06<4:55:25,  3.13s/it]loss_total_epoch 151.96674860827625
Training tokenizer:  30% 2383/8047 [1:24:09<4:53:24,  3.11s/it]loss_total_epoch 152.01818831078708
Training tokenizer:  30% 2384/8047 [1:24:12<4:48:21,  3.06s/it]loss_total_epoch 152.06979652307928
Training tokenizer:  30% 2385/8047 [1:24:15<4:41:57,  2.99s/it]loss_total_epoch 152.1250143367797
Training tokenizer:  30% 2386/8047 [1:24:18<4:37:44,  2.94s/it]loss_total_epoch 152.1771119441837
Training tokenizer:  30% 2387/8047 [1:24:21<4:35:02,  2.92s/it]loss_total_epoch 152.2266305666417
Training tokenizer:  30% 2388/8047 [1:24:24<4:40:54,  2.98s/it]loss_total_epoch 152.28011024557054
Training tokenizer:  30% 2389/8047 [1:24:27<4:38:47,  2.96s/it]loss_total_epoch 152.3286030869931
Training tokenizer:  30% 2390/8047 [1:24:30<4:45:43,  3.03s/it]loss_total_epoch 152.36930341087282
Training tokenizer:  30% 2391/8047 [1:24:34<4:55:16,  3.13s/it]loss_total_epoch 152.42465807683766
Training tokenizer:  30% 2392/8047 [1:24:37<5:02:30,  3.21s/it]loss_total_epoch 152.48586490936577
Training tokenizer:  30% 2393/8047 [1:24:40<5:04:45,  3.23s/it]loss_total_epoch 152.53904922492802
Training tokenizer:  30% 2394/8047 [1:24:43<5:00:57,  3.19s/it]loss_total_epoch 152.58129662834108
Training tokenizer:  30% 2395/8047 [1:24:47<5:02:28,  3.21s/it]loss_total_epoch 152.6320777516812
Training tokenizer:  30% 2396/8047 [1:24:50<4:59:09,  3.18s/it]loss_total_epoch 152.69104565493762
Training tokenizer:  30% 2397/8047 [1:24:53<4:57:48,  3.16s/it]loss_total_epoch 152.74005903489888
Training tokenizer:  30% 2398/8047 [1:24:56<4:59:59,  3.19s/it]loss_total_epoch 152.80037505738437
Training tokenizer:  30% 2399/8047 [1:24:59<4:58:49,  3.17s/it]loss_total_epoch 152.85505197010934
Training tokenizer:  30% 2400/8047 [1:25:02<5:00:02,  3.19s/it]loss_total_epoch 152.901881178841
Training tokenizer:  30% 2401/8047 [1:25:05<4:53:00,  3.11s/it]loss_total_epoch 152.9536313060671
Training tokenizer:  30% 2402/8047 [1:25:09<4:56:59,  3.16s/it]loss_total_epoch 153.02184375561774
Training tokenizer:  30% 2403/8047 [1:25:12<4:58:32,  3.17s/it]loss_total_epoch 153.0647829156369
Training tokenizer:  30% 2404/8047 [1:25:15<5:01:51,  3.21s/it]loss_total_epoch 153.1209386792034
Training tokenizer:  30% 2405/8047 [1:25:18<4:57:30,  3.16s/it]loss_total_epoch 153.16774261929095
Training tokenizer:  30% 2406/8047 [1:25:21<4:50:35,  3.09s/it]loss_total_epoch 153.2097788285464
Training tokenizer:  30% 2407/8047 [1:25:24<4:42:39,  3.01s/it]loss_total_epoch 153.2475090045482
Training tokenizer:  30% 2408/8047 [1:25:27<4:43:49,  3.02s/it]loss_total_epoch 153.29922839440405
Training tokenizer:  30% 2409/8047 [1:25:30<4:44:48,  3.03s/it]loss_total_epoch 153.34805178456008
Training tokenizer:  30% 2410/8047 [1:25:33<4:48:13,  3.07s/it]loss_total_epoch 153.39723665453494
Training tokenizer:  30% 2411/8047 [1:25:36<4:41:37,  3.00s/it]loss_total_epoch 153.4560733642429
Training tokenizer:  30% 2412/8047 [1:25:39<4:42:58,  3.01s/it]loss_total_epoch 153.5177482496947
Training tokenizer:  30% 2413/8047 [1:25:42<4:37:21,  2.95s/it]loss_total_epoch 153.58037476800382
Training tokenizer:  30% 2414/8047 [1:25:45<4:37:14,  2.95s/it]loss_total_epoch 153.6459902431816
Training tokenizer:  30% 2415/8047 [1:25:48<4:34:59,  2.93s/it]loss_total_epoch 153.70664088241756
Training tokenizer:  30% 2416/8047 [1:25:51<4:41:09,  3.00s/it]loss_total_epoch 153.76084527559578
Training tokenizer:  30% 2417/8047 [1:25:54<4:47:52,  3.07s/it]loss_total_epoch 153.80802034400403
Training tokenizer:  30% 2418/8047 [1:25:57<4:43:29,  3.02s/it]loss_total_epoch 153.86201835237443
Training tokenizer:  30% 2419/8047 [1:26:00<4:51:45,  3.11s/it]loss_total_epoch 153.91267575882375
Training tokenizer:  30% 2420/8047 [1:26:03<4:46:55,  3.06s/it]loss_total_epoch 153.97847492061555
Training tokenizer:  30% 2421/8047 [1:26:06<4:47:05,  3.06s/it]loss_total_epoch 154.02476179413497
Training tokenizer:  30% 2422/8047 [1:26:10<4:52:08,  3.12s/it]loss_total_epoch 154.08633043430746
Training tokenizer:  30% 2423/8047 [1:26:12<4:44:20,  3.03s/it]loss_total_epoch 154.14609868265688
Training tokenizer:  30% 2424/8047 [1:26:15<4:38:27,  2.97s/it]loss_total_epoch 154.20395143143833
Training tokenizer:  30% 2425/8047 [1:26:18<4:40:27,  2.99s/it]loss_total_epoch 154.26167738623917
Training tokenizer:  30% 2426/8047 [1:26:21<4:35:18,  2.94s/it]loss_total_epoch 154.31528862006962
Training tokenizer:  30% 2427/8047 [1:26:24<4:43:48,  3.03s/it]loss_total_epoch 154.35621727444232
Training tokenizer:  30% 2428/8047 [1:26:27<4:47:43,  3.07s/it]loss_total_epoch 154.39520080946386
Training tokenizer:  30% 2429/8047 [1:26:31<4:52:06,  3.12s/it]loss_total_epoch 154.43599306233227
Training tokenizer:  30% 2430/8047 [1:26:34<4:55:31,  3.16s/it]loss_total_epoch 154.48729781992733
Training tokenizer:  30% 2431/8047 [1:26:37<4:54:43,  3.15s/it]loss_total_epoch 154.53721627779305
Training tokenizer:  30% 2432/8047 [1:26:40<4:55:30,  3.16s/it]loss_total_epoch 154.59884658642113
Training tokenizer:  30% 2433/8047 [1:26:44<4:59:54,  3.21s/it]loss_total_epoch 154.65081385709345
Training tokenizer:  30% 2434/8047 [1:26:47<5:01:15,  3.22s/it]loss_total_epoch 154.69483427144587
Training tokenizer:  30% 2435/8047 [1:26:50<4:57:39,  3.18s/it]loss_total_epoch 154.7503375839442
Training tokenizer:  30% 2436/8047 [1:26:53<4:49:57,  3.10s/it]loss_total_epoch 154.79306464083493
Training tokenizer:  30% 2437/8047 [1:26:56<4:51:49,  3.12s/it]loss_total_epoch 154.84172348864377
Training tokenizer:  30% 2438/8047 [1:26:59<4:50:46,  3.11s/it]loss_total_epoch 154.91376221366227
Training tokenizer:  30% 2439/8047 [1:27:02<4:48:54,  3.09s/it]loss_total_epoch 154.9695281330496
Training tokenizer:  30% 2440/8047 [1:27:05<4:49:58,  3.10s/it]loss_total_epoch 155.02680880390108
Training tokenizer:  30% 2441/8047 [1:27:09<4:54:54,  3.16s/it]loss_total_epoch 155.08025517873466
Training tokenizer:  30% 2442/8047 [1:27:12<4:58:02,  3.19s/it]loss_total_epoch 155.13463299162686
Training tokenizer:  30% 2443/8047 [1:27:15<4:58:57,  3.20s/it]loss_total_epoch 155.18925725109875
Training tokenizer:  30% 2444/8047 [1:27:18<4:47:42,  3.08s/it]loss_total_epoch 155.24196613021195
Training tokenizer:  30% 2445/8047 [1:27:21<4:41:27,  3.01s/it]loss_total_epoch 155.3005983028561
Training tokenizer:  30% 2446/8047 [1:27:24<4:44:51,  3.05s/it]loss_total_epoch 155.36921338178217
Training tokenizer:  30% 2447/8047 [1:27:27<4:51:44,  3.13s/it]loss_total_epoch 155.41780222393572
Training tokenizer:  30% 2448/8047 [1:27:30<4:54:04,  3.15s/it]loss_total_epoch 155.48692400194705
Training tokenizer:  30% 2449/8047 [1:27:34<4:58:23,  3.20s/it]loss_total_epoch 155.54020003415644
Training tokenizer:  30% 2450/8047 [1:27:37<4:53:40,  3.15s/it]loss_total_epoch 155.59253294579685
Training tokenizer:  30% 2451/8047 [1:27:40<4:56:43,  3.18s/it]loss_total_epoch 155.63160312362015
Training tokenizer:  30% 2452/8047 [1:27:43<4:57:21,  3.19s/it]loss_total_epoch 155.6750347930938
Training tokenizer:  30% 2453/8047 [1:27:46<4:57:16,  3.19s/it]loss_total_epoch 155.71552187018096
Training tokenizer:  30% 2454/8047 [1:27:49<4:51:19,  3.13s/it]loss_total_epoch 155.7616966869682
Training tokenizer:  31% 2455/8047 [1:27:53<4:57:14,  3.19s/it]loss_total_epoch 155.81275933422148
Training tokenizer:  31% 2456/8047 [1:27:56<5:01:11,  3.23s/it]loss_total_epoch 155.86177414096892
Training tokenizer:  31% 2457/8047 [1:27:59<4:51:16,  3.13s/it]loss_total_epoch 155.91460523195565
Training tokenizer:  31% 2458/8047 [1:28:02<4:55:33,  3.17s/it]loss_total_epoch 155.96140942163765
Training tokenizer:  31% 2459/8047 [1:28:05<4:52:42,  3.14s/it]loss_total_epoch 156.0247191581875
Training tokenizer:  31% 2460/8047 [1:28:08<4:46:59,  3.08s/it]loss_total_epoch 156.07921108789742
Training tokenizer:  31% 2461/8047 [1:28:11<4:38:33,  2.99s/it]loss_total_epoch 156.13054131157696
Training tokenizer:  31% 2462/8047 [1:28:14<4:37:38,  2.98s/it]loss_total_epoch 156.19227459095418
Training tokenizer:  31% 2463/8047 [1:28:17<4:43:38,  3.05s/it]loss_total_epoch 156.2509054262191
Training tokenizer:  31% 2464/8047 [1:28:20<4:35:00,  2.96s/it]loss_total_epoch 156.30464787967503
Training tokenizer:  31% 2465/8047 [1:28:23<4:45:40,  3.07s/it]loss_total_epoch 156.35586620680988
Training tokenizer:  31% 2466/8047 [1:28:26<4:48:20,  3.10s/it]loss_total_epoch 156.40697678364813
Training tokenizer:  31% 2467/8047 [1:28:30<4:54:20,  3.16s/it]loss_total_epoch 156.45502620376647
Training tokenizer:  31% 2468/8047 [1:28:33<4:57:55,  3.20s/it]loss_total_epoch 156.4929397199303
Training tokenizer:  31% 2469/8047 [1:28:36<4:52:46,  3.15s/it]loss_total_epoch 156.55609048344195
Training tokenizer:  31% 2470/8047 [1:28:39<4:58:05,  3.21s/it]loss_total_epoch 156.6005616877228
Training tokenizer:  31% 2471/8047 [1:28:42<4:51:25,  3.14s/it]loss_total_epoch 156.6544883903116
Training tokenizer:  31% 2472/8047 [1:28:46<4:54:21,  3.17s/it]loss_total_epoch 156.7155448179692
Training tokenizer:  31% 2473/8047 [1:28:49<5:00:30,  3.23s/it]loss_total_epoch 156.7677248660475
Training tokenizer:  31% 2474/8047 [1:28:52<4:50:46,  3.13s/it]loss_total_epoch 156.8098017964512
Training tokenizer:  31% 2475/8047 [1:28:55<4:57:08,  3.20s/it]loss_total_epoch 156.86216936819255
Training tokenizer:  31% 2476/8047 [1:28:58<4:45:10,  3.07s/it]loss_total_epoch 156.90494727157056
Training tokenizer:  31% 2477/8047 [1:29:01<4:43:01,  3.05s/it]loss_total_epoch 156.95699734054506
Training tokenizer:  31% 2478/8047 [1:29:04<4:37:55,  2.99s/it]loss_total_epoch 156.9900403637439
Training tokenizer:  31% 2479/8047 [1:29:07<4:34:20,  2.96s/it]loss_total_epoch 157.04895472712815
Training tokenizer:  31% 2480/8047 [1:29:10<4:34:12,  2.96s/it]loss_total_epoch 157.08718345873058
Training tokenizer:  31% 2481/8047 [1:29:13<4:44:16,  3.06s/it]loss_total_epoch 157.1365505401045
Training tokenizer:  31% 2482/8047 [1:29:16<4:36:37,  2.98s/it]loss_total_epoch 157.1892024744302
Training tokenizer:  31% 2483/8047 [1:29:19<4:42:04,  3.04s/it]loss_total_epoch 157.2340693678707
Training tokenizer:  31% 2484/8047 [1:29:22<4:32:59,  2.94s/it]loss_total_epoch 157.28644148819149
Training tokenizer:  31% 2485/8047 [1:29:24<4:25:53,  2.87s/it]loss_total_epoch 157.335305551067
Training tokenizer:  31% 2486/8047 [1:29:27<4:20:57,  2.82s/it]loss_total_epoch 157.384613243863
Training tokenizer:  31% 2487/8047 [1:29:30<4:16:52,  2.77s/it]loss_total_epoch 157.4381238911301
Training tokenizer:  31% 2488/8047 [1:29:33<4:36:21,  2.98s/it]loss_total_epoch 157.49536168016493
Training tokenizer:  31% 2489/8047 [1:29:36<4:27:42,  2.89s/it]loss_total_epoch 157.54861039854586
Training tokenizer:  31% 2490/8047 [1:29:39<4:26:20,  2.88s/it]loss_total_epoch 157.6040123384446
Training tokenizer:  31% 2491/8047 [1:29:41<4:19:03,  2.80s/it]loss_total_epoch 157.65409344248474
Training tokenizer:  31% 2492/8047 [1:29:44<4:14:53,  2.75s/it]loss_total_epoch 157.7097329441458
Training tokenizer:  31% 2493/8047 [1:29:47<4:12:29,  2.73s/it]loss_total_epoch 157.75467999093235
Training tokenizer:  31% 2494/8047 [1:29:49<4:09:51,  2.70s/it]loss_total_epoch 157.80104688368738
Training tokenizer:  31% 2495/8047 [1:29:52<4:08:14,  2.68s/it]loss_total_epoch 157.85065512545407
Training tokenizer:  31% 2496/8047 [1:29:55<4:08:20,  2.68s/it]loss_total_epoch 157.89810203947127
Training tokenizer:  31% 2497/8047 [1:29:57<4:07:20,  2.67s/it]loss_total_epoch 157.95297020860016
Training tokenizer:  31% 2498/8047 [1:30:00<4:07:28,  2.68s/it]loss_total_epoch 157.9899043496698
Training tokenizer:  31% 2499/8047 [1:30:03<4:06:53,  2.67s/it]loss_total_epoch 158.04447215981781
Training tokenizer:  31% 2500/8047 [1:30:05<4:05:47,  2.66s/it]loss_total_epoch 158.09472475014627
Training tokenizer:  31% 2501/8047 [1:30:08<4:04:49,  2.65s/it]loss_total_epoch 158.1410152260214
Training tokenizer:  31% 2502/8047 [1:30:11<4:05:35,  2.66s/it]loss_total_epoch 158.1879449877888
Training tokenizer:  31% 2503/8047 [1:30:13<4:06:18,  2.67s/it]loss_total_epoch 158.24416043050587
Training tokenizer:  31% 2504/8047 [1:30:16<4:04:56,  2.65s/it]loss_total_epoch 158.2994087021798
Training tokenizer:  31% 2505/8047 [1:30:18<4:04:40,  2.65s/it]loss_total_epoch 158.35295458324254
Training tokenizer:  31% 2506/8047 [1:30:21<4:04:55,  2.65s/it]loss_total_epoch 158.40832294337451
Training tokenizer:  31% 2507/8047 [1:30:24<4:05:12,  2.66s/it]loss_total_epoch 158.4505577813834
Training tokenizer:  31% 2508/8047 [1:30:26<4:04:50,  2.65s/it]loss_total_epoch 158.50681352429092
Training tokenizer:  31% 2509/8047 [1:30:29<4:04:52,  2.65s/it]loss_total_epoch 158.55531906150281
Training tokenizer:  31% 2510/8047 [1:30:32<4:05:25,  2.66s/it]loss_total_epoch 158.61006860993803
Training tokenizer:  31% 2511/8047 [1:30:34<4:05:46,  2.66s/it]loss_total_epoch 158.6466873344034
Training tokenizer:  31% 2512/8047 [1:30:37<4:06:27,  2.67s/it]loss_total_epoch 158.68969442509115
Training tokenizer:  31% 2513/8047 [1:30:40<4:05:23,  2.66s/it]loss_total_epoch 158.74080324359238
Training tokenizer:  31% 2514/8047 [1:30:42<4:05:51,  2.67s/it]loss_total_epoch 158.79160734452307
Training tokenizer:  31% 2515/8047 [1:30:45<4:06:09,  2.67s/it]loss_total_epoch 158.8400050010532
Training tokenizer:  31% 2516/8047 [1:30:48<4:05:21,  2.66s/it]loss_total_epoch 158.87949414364994
Training tokenizer:  31% 2517/8047 [1:30:50<4:05:13,  2.66s/it]loss_total_epoch 158.92621223442256
Training tokenizer:  31% 2518/8047 [1:30:53<4:06:01,  2.67s/it]loss_total_epoch 158.97725260816514
Training tokenizer:  31% 2519/8047 [1:30:56<4:06:07,  2.67s/it]loss_total_epoch 159.02710936032236
Training tokenizer:  31% 2520/8047 [1:30:58<4:06:11,  2.67s/it]loss_total_epoch 159.07665293104947
Training tokenizer:  31% 2521/8047 [1:31:01<4:06:38,  2.68s/it]loss_total_epoch 159.11921841092408
Training tokenizer:  31% 2522/8047 [1:31:04<4:06:20,  2.68s/it]loss_total_epoch 159.16700674779713
Training tokenizer:  31% 2523/8047 [1:31:06<4:05:10,  2.66s/it]loss_total_epoch 159.21611951477826
Training tokenizer:  31% 2524/8047 [1:31:09<4:05:24,  2.67s/it]loss_total_epoch 159.2686140332371
Training tokenizer:  31% 2525/8047 [1:31:12<4:05:45,  2.67s/it]loss_total_epoch 159.31859870813787
Training tokenizer:  31% 2526/8047 [1:31:14<4:05:04,  2.66s/it]loss_total_epoch 159.3733695205301
Training tokenizer:  31% 2527/8047 [1:31:17<4:04:24,  2.66s/it]loss_total_epoch 159.42062903381884
Training tokenizer:  31% 2528/8047 [1:31:20<4:04:31,  2.66s/it]loss_total_epoch 159.47356101311743
Training tokenizer:  31% 2529/8047 [1:31:22<4:05:21,  2.67s/it]loss_total_epoch 159.53208053298295
Training tokenizer:  31% 2530/8047 [1:31:25<4:06:04,  2.68s/it]loss_total_epoch 159.5889570210129
Training tokenizer:  31% 2531/8047 [1:31:28<4:06:49,  2.68s/it]loss_total_epoch 159.64969613589346
Training tokenizer:  31% 2532/8047 [1:31:30<4:05:53,  2.68s/it]loss_total_epoch 159.7045142520219
Training tokenizer:  31% 2533/8047 [1:31:33<4:05:59,  2.68s/it]loss_total_epoch 159.7654014956206
Training tokenizer:  31% 2534/8047 [1:31:36<4:05:47,  2.67s/it]loss_total_epoch 159.81668764166534
Training tokenizer:  32% 2535/8047 [1:31:39<4:05:27,  2.67s/it]loss_total_epoch 159.8652636501938
Training tokenizer:  32% 2536/8047 [1:31:41<4:05:21,  2.67s/it]loss_total_epoch 159.91686961986125
Training tokenizer:  32% 2537/8047 [1:31:44<4:05:54,  2.68s/it]loss_total_epoch 159.9658238980919
Training tokenizer:  32% 2538/8047 [1:31:46<4:04:28,  2.66s/it]loss_total_epoch 160.02413231693208
Training tokenizer:  32% 2539/8047 [1:31:49<4:06:01,  2.68s/it]loss_total_epoch 160.07192099280655
Training tokenizer:  32% 2540/8047 [1:31:52<4:05:13,  2.67s/it]loss_total_epoch 160.12938977219164
Training tokenizer:  32% 2541/8047 [1:31:55<4:06:03,  2.68s/it]loss_total_epoch 160.17775637097657
Training tokenizer:  32% 2542/8047 [1:31:57<4:06:21,  2.69s/it]loss_total_epoch 160.22771441750228
Training tokenizer:  32% 2543/8047 [1:32:00<4:07:15,  2.70s/it]loss_total_epoch 160.28948689810932
Training tokenizer:  32% 2544/8047 [1:32:03<4:05:52,  2.68s/it]loss_total_epoch 160.33769401721656
Training tokenizer:  32% 2545/8047 [1:32:05<4:05:56,  2.68s/it]loss_total_epoch 160.39710414223373
Training tokenizer:  32% 2546/8047 [1:32:08<4:05:17,  2.68s/it]loss_total_epoch 160.46308287791908
Training tokenizer:  32% 2547/8047 [1:32:11<4:04:41,  2.67s/it]loss_total_epoch 160.51845564506948
Training tokenizer:  32% 2548/8047 [1:32:13<4:05:04,  2.67s/it]loss_total_epoch 160.5681137125939
Training tokenizer:  32% 2549/8047 [1:32:16<4:05:24,  2.68s/it]loss_total_epoch 160.6127990204841
Training tokenizer:  32% 2550/8047 [1:32:19<4:05:08,  2.68s/it]loss_total_epoch 160.65788242034614
Training tokenizer:  32% 2551/8047 [1:32:21<4:05:22,  2.68s/it]loss_total_epoch 160.71827839128673
Training tokenizer:  32% 2552/8047 [1:32:24<4:05:23,  2.68s/it]loss_total_epoch 160.7727372702211
Training tokenizer:  32% 2553/8047 [1:32:27<4:05:29,  2.68s/it]loss_total_epoch 160.81561990641057
Training tokenizer:  32% 2554/8047 [1:32:29<4:04:56,  2.68s/it]loss_total_epoch 160.86022999323905
Training tokenizer:  32% 2555/8047 [1:32:32<4:04:37,  2.67s/it]loss_total_epoch 160.9186337571591
Training tokenizer:  32% 2556/8047 [1:32:35<4:05:10,  2.68s/it]loss_total_epoch 160.97482544369996
Training tokenizer:  32% 2557/8047 [1:32:37<4:04:28,  2.67s/it]loss_total_epoch 161.01494962535799
Training tokenizer:  32% 2558/8047 [1:32:40<4:04:34,  2.67s/it]loss_total_epoch 161.08009302429855
Training tokenizer:  32% 2559/8047 [1:32:43<4:05:29,  2.68s/it]loss_total_epoch 161.1286610569805
Training tokenizer:  32% 2560/8047 [1:32:45<4:05:14,  2.68s/it]loss_total_epoch 161.18592949025333
Training tokenizer:  32% 2561/8047 [1:32:48<4:05:19,  2.68s/it]loss_total_epoch 161.23150637187064
Training tokenizer:  32% 2562/8047 [1:32:51<4:06:57,  2.70s/it]loss_total_epoch 161.2753187213093
Training tokenizer:  32% 2563/8047 [1:32:54<4:06:27,  2.70s/it]loss_total_epoch 161.32007036544383
Training tokenizer:  32% 2564/8047 [1:32:56<4:05:32,  2.69s/it]loss_total_epoch 161.36833990179002
Training tokenizer:  32% 2565/8047 [1:32:59<4:05:24,  2.69s/it]loss_total_epoch 161.4237624090165
Training tokenizer:  32% 2566/8047 [1:33:02<4:05:47,  2.69s/it]loss_total_epoch 161.48787972144783
Training tokenizer:  32% 2567/8047 [1:33:04<4:05:22,  2.69s/it]loss_total_epoch 161.54114498756826
Training tokenizer:  32% 2568/8047 [1:33:07<4:05:16,  2.69s/it]loss_total_epoch 161.58802088163793
Training tokenizer:  32% 2569/8047 [1:33:10<4:05:32,  2.69s/it]loss_total_epoch 161.63596628792584
Training tokenizer:  32% 2570/8047 [1:33:12<4:04:47,  2.68s/it]loss_total_epoch 161.67523022182286
Training tokenizer:  32% 2571/8047 [1:33:15<4:05:22,  2.69s/it]loss_total_epoch 161.7421291191131
Training tokenizer:  32% 2572/8047 [1:33:18<4:04:59,  2.68s/it]loss_total_epoch 161.7919713947922
Training tokenizer:  32% 2573/8047 [1:33:20<4:04:17,  2.68s/it]loss_total_epoch 161.83916190080345
Training tokenizer:  32% 2574/8047 [1:33:23<4:04:58,  2.69s/it]loss_total_epoch 161.8918823916465
Training tokenizer:  32% 2575/8047 [1:33:26<4:04:55,  2.69s/it]loss_total_epoch 161.93776254542172
Training tokenizer:  32% 2576/8047 [1:33:28<4:05:17,  2.69s/it]loss_total_epoch 161.9940255600959
Training tokenizer:  32% 2577/8047 [1:33:31<4:05:03,  2.69s/it]loss_total_epoch 162.0318530704826
Training tokenizer:  32% 2578/8047 [1:33:34<4:05:31,  2.69s/it]loss_total_epoch 162.08705071173608
Training tokenizer:  32% 2579/8047 [1:33:37<4:05:11,  2.69s/it]loss_total_epoch 162.1491915974766
Training tokenizer:  32% 2580/8047 [1:33:39<4:05:44,  2.70s/it]loss_total_epoch 162.20173984579742
Training tokenizer:  32% 2581/8047 [1:33:42<4:05:31,  2.70s/it]loss_total_epoch 162.2540907729417
Training tokenizer:  32% 2582/8047 [1:33:45<4:05:41,  2.70s/it]loss_total_epoch 162.2940638344735
Training tokenizer:  32% 2583/8047 [1:33:47<4:05:40,  2.70s/it]loss_total_epoch 162.35922959260643
Training tokenizer:  32% 2584/8047 [1:33:50<4:05:27,  2.70s/it]loss_total_epoch 162.42534730397165
Training tokenizer:  32% 2585/8047 [1:33:53<4:06:07,  2.70s/it]loss_total_epoch 162.49164089374244
Training tokenizer:  32% 2586/8047 [1:33:55<4:05:08,  2.69s/it]loss_total_epoch 162.54224046878517
Training tokenizer:  32% 2587/8047 [1:33:58<4:05:33,  2.70s/it]loss_total_epoch 162.60339573957026
Training tokenizer:  32% 2588/8047 [1:34:01<4:05:29,  2.70s/it]loss_total_epoch 162.65056747756898
Training tokenizer:  32% 2589/8047 [1:34:04<4:05:58,  2.70s/it]loss_total_epoch 162.70115097425878
Training tokenizer:  32% 2590/8047 [1:34:06<4:05:41,  2.70s/it]loss_total_epoch 162.76763805560768
Training tokenizer:  32% 2591/8047 [1:34:09<4:05:33,  2.70s/it]loss_total_epoch 162.81738552637398
Training tokenizer:  32% 2592/8047 [1:34:12<4:05:10,  2.70s/it]loss_total_epoch 162.86630797944963
Training tokenizer:  32% 2593/8047 [1:34:14<4:04:54,  2.69s/it]loss_total_epoch 162.91719104908407
Training tokenizer:  32% 2594/8047 [1:34:17<4:04:52,  2.69s/it]loss_total_epoch 162.95674497820437
Training tokenizer:  32% 2595/8047 [1:34:20<4:04:41,  2.69s/it]loss_total_epoch 163.00613784976304
Training tokenizer:  32% 2596/8047 [1:34:22<4:04:44,  2.69s/it]loss_total_epoch 163.05732743255794
Training tokenizer:  32% 2597/8047 [1:34:25<4:05:25,  2.70s/it]loss_total_epoch 163.11591539345682
Training tokenizer:  32% 2598/8047 [1:34:28<4:05:07,  2.70s/it]loss_total_epoch 163.16646342910826
Training tokenizer:  32% 2599/8047 [1:34:31<4:04:40,  2.69s/it]loss_total_epoch 163.22290996275842
Training tokenizer:  32% 2600/8047 [1:34:33<4:03:45,  2.69s/it]loss_total_epoch 163.2771880645305
Training tokenizer:  32% 2601/8047 [1:34:36<4:04:34,  2.69s/it]loss_total_epoch 163.32006840594113
Training tokenizer:  32% 2602/8047 [1:34:39<4:03:41,  2.69s/it]loss_total_epoch 163.37552207894623
Training tokenizer:  32% 2603/8047 [1:34:41<4:04:46,  2.70s/it]loss_total_epoch 163.42454756237566
Training tokenizer:  32% 2604/8047 [1:34:44<4:05:48,  2.71s/it]loss_total_epoch 163.46770993433893
Training tokenizer:  32% 2605/8047 [1:34:47<4:05:19,  2.70s/it]loss_total_epoch 163.5206325147301
Training tokenizer:  32% 2606/8047 [1:34:49<4:05:03,  2.70s/it]loss_total_epoch 163.5790079701692
Training tokenizer:  32% 2607/8047 [1:34:52<4:04:33,  2.70s/it]loss_total_epoch 163.62663586996496
Training tokenizer:  32% 2608/8047 [1:34:55<4:04:07,  2.69s/it]loss_total_epoch 163.679821645841
Training tokenizer:  32% 2609/8047 [1:34:57<4:03:46,  2.69s/it]loss_total_epoch 163.7319561559707
Training tokenizer:  32% 2610/8047 [1:35:00<4:04:27,  2.70s/it]loss_total_epoch 163.7888398785144
Training tokenizer:  32% 2611/8047 [1:35:03<4:05:23,  2.71s/it]loss_total_epoch 163.84531745128334
Training tokenizer:  32% 2612/8047 [1:35:06<4:06:10,  2.72s/it]loss_total_epoch 163.89818383939564
Training tokenizer:  32% 2613/8047 [1:35:08<4:04:56,  2.70s/it]loss_total_epoch 163.94713232852519
Training tokenizer:  32% 2614/8047 [1:35:11<4:03:41,  2.69s/it]loss_total_epoch 163.992499621585
Training tokenizer:  32% 2615/8047 [1:35:14<4:03:41,  2.69s/it]loss_total_epoch 164.05467006005347
Training tokenizer:  33% 2616/8047 [1:35:16<4:03:42,  2.69s/it]loss_total_epoch 164.1029605511576
Training tokenizer:  33% 2617/8047 [1:35:19<4:04:59,  2.71s/it]loss_total_epoch 164.15551931969821
Training tokenizer:  33% 2618/8047 [1:35:22<4:05:10,  2.71s/it]loss_total_epoch 164.20005240477622
Training tokenizer:  33% 2619/8047 [1:35:25<4:05:43,  2.72s/it]loss_total_epoch 164.26452075131238
Training tokenizer:  33% 2620/8047 [1:35:27<4:04:57,  2.71s/it]loss_total_epoch 164.31615269370377
Training tokenizer:  33% 2621/8047 [1:35:30<4:04:20,  2.70s/it]loss_total_epoch 164.35207786224782
Training tokenizer:  33% 2622/8047 [1:35:33<4:04:52,  2.71s/it]loss_total_epoch 164.4063726272434
Training tokenizer:  33% 2623/8047 [1:35:35<4:05:13,  2.71s/it]loss_total_epoch 164.4578378368169
Training tokenizer:  33% 2624/8047 [1:35:38<4:04:53,  2.71s/it]loss_total_epoch 164.52590723522007
Training tokenizer:  33% 2625/8047 [1:35:41<4:04:49,  2.71s/it]loss_total_epoch 164.57247269712389
Training tokenizer:  33% 2626/8047 [1:35:43<4:03:37,  2.70s/it]loss_total_epoch 164.62426979281008
Training tokenizer:  33% 2627/8047 [1:35:46<4:03:34,  2.70s/it]loss_total_epoch 164.67990372888744
Training tokenizer:  33% 2628/8047 [1:35:49<4:03:33,  2.70s/it]loss_total_epoch 164.74273660592735
Training tokenizer:  33% 2629/8047 [1:35:52<4:03:54,  2.70s/it]loss_total_epoch 164.788476170972
Training tokenizer:  33% 2630/8047 [1:35:54<4:04:26,  2.71s/it]loss_total_epoch 164.83036777563393
Training tokenizer:  33% 2631/8047 [1:35:57<4:04:39,  2.71s/it]loss_total_epoch 164.8871825542301
Training tokenizer:  33% 2632/8047 [1:36:00<4:03:32,  2.70s/it]loss_total_epoch 164.93158944509923
Training tokenizer:  33% 2633/8047 [1:36:02<4:03:48,  2.70s/it]loss_total_epoch 164.98451802693307
Training tokenizer:  33% 2634/8047 [1:36:05<4:04:20,  2.71s/it]loss_total_epoch 165.0208630170673
Training tokenizer:  33% 2635/8047 [1:36:08<4:04:35,  2.71s/it]loss_total_epoch 165.06378002278507
Training tokenizer:  33% 2636/8047 [1:36:11<4:04:58,  2.72s/it]loss_total_epoch 165.11096528358757
Training tokenizer:  33% 2637/8047 [1:36:13<4:05:07,  2.72s/it]loss_total_epoch 165.15990788303316
Training tokenizer:  33% 2638/8047 [1:36:16<4:05:15,  2.72s/it]loss_total_epoch 165.2101108711213
Training tokenizer:  33% 2639/8047 [1:36:19<4:05:06,  2.72s/it]loss_total_epoch 165.25474584288895
Training tokenizer:  33% 2640/8047 [1:36:21<4:05:09,  2.72s/it]loss_total_epoch 165.30251779966056
Training tokenizer:  33% 2641/8047 [1:36:24<4:04:10,  2.71s/it]loss_total_epoch 165.3526596110314
Training tokenizer:  33% 2642/8047 [1:36:27<4:04:04,  2.71s/it]loss_total_epoch 165.41144856624305
Training tokenizer:  33% 2643/8047 [1:36:30<4:04:24,  2.71s/it]loss_total_epoch 165.46085456572473
Training tokenizer:  33% 2644/8047 [1:36:32<4:04:28,  2.71s/it]loss_total_epoch 165.4987556617707
Training tokenizer:  33% 2645/8047 [1:36:35<4:05:41,  2.73s/it]loss_total_epoch 165.5527120847255
Training tokenizer:  33% 2646/8047 [1:36:38<4:05:38,  2.73s/it]loss_total_epoch 165.60304147563875
Training tokenizer:  33% 2647/8047 [1:36:41<4:07:17,  2.75s/it]loss_total_epoch 165.64805340580642
Training tokenizer:  33% 2648/8047 [1:36:43<4:07:04,  2.75s/it]loss_total_epoch 165.70844863168895
Training tokenizer:  33% 2649/8047 [1:36:46<4:06:21,  2.74s/it]loss_total_epoch 165.75784971378744
Training tokenizer:  33% 2650/8047 [1:36:49<4:04:56,  2.72s/it]loss_total_epoch 165.8036363106221
Training tokenizer:  33% 2651/8047 [1:36:51<4:06:12,  2.74s/it]loss_total_epoch 165.85918378643692
Training tokenizer:  33% 2652/8047 [1:36:54<4:05:44,  2.73s/it]loss_total_epoch 165.91251377947628
Training tokenizer:  33% 2653/8047 [1:36:57<4:05:56,  2.74s/it]loss_total_epoch 165.96456821076572
Training tokenizer:  33% 2654/8047 [1:37:00<4:05:51,  2.74s/it]loss_total_epoch 166.02523637376726
Training tokenizer:  33% 2655/8047 [1:37:02<4:05:08,  2.73s/it]loss_total_epoch 166.06593847088516
Training tokenizer:  33% 2656/8047 [1:37:05<4:05:20,  2.73s/it]loss_total_epoch 166.111336318776
Training tokenizer:  33% 2657/8047 [1:37:08<4:06:05,  2.74s/it]loss_total_epoch 166.14751815237105
Training tokenizer:  33% 2658/8047 [1:37:11<4:05:45,  2.74s/it]loss_total_epoch 166.1904652286321
Training tokenizer:  33% 2659/8047 [1:37:13<4:04:11,  2.72s/it]loss_total_epoch 166.23454644717276
Training tokenizer:  33% 2660/8047 [1:37:16<4:04:06,  2.72s/it]loss_total_epoch 166.28052633069456
Training tokenizer:  33% 2661/8047 [1:37:19<4:03:26,  2.71s/it]loss_total_epoch 166.34927289001644
Training tokenizer:  33% 2662/8047 [1:37:21<4:04:06,  2.72s/it]loss_total_epoch 166.39874941296875
Training tokenizer:  33% 2663/8047 [1:37:24<4:04:26,  2.72s/it]loss_total_epoch 166.4599043969065
Training tokenizer:  33% 2664/8047 [1:37:27<4:03:53,  2.72s/it]loss_total_epoch 166.5203842688352
Training tokenizer:  33% 2665/8047 [1:37:30<4:05:28,  2.74s/it]loss_total_epoch 166.57882417552173
Training tokenizer:  33% 2666/8047 [1:37:32<4:04:09,  2.72s/it]loss_total_epoch 166.6332734618336
Training tokenizer:  33% 2667/8047 [1:37:35<4:05:10,  2.73s/it]loss_total_epoch 166.68634388409555
Training tokenizer:  33% 2668/8047 [1:37:38<4:08:42,  2.77s/it]loss_total_epoch 166.73075992427766
Training tokenizer:  33% 2669/8047 [1:37:41<4:07:26,  2.76s/it]loss_total_epoch 166.7856448572129
Training tokenizer:  33% 2670/8047 [1:37:43<4:06:45,  2.75s/it]loss_total_epoch 166.84775955416262
Training tokenizer:  33% 2671/8047 [1:37:46<4:07:13,  2.76s/it]loss_total_epoch 166.90156890638173
Training tokenizer:  33% 2672/8047 [1:37:49<4:06:36,  2.75s/it]loss_total_epoch 166.94719143398106
Training tokenizer:  33% 2673/8047 [1:37:52<4:06:10,  2.75s/it]loss_total_epoch 167.01456742174923
Training tokenizer:  33% 2674/8047 [1:37:54<4:04:25,  2.73s/it]loss_total_epoch 167.07747432030737
Training tokenizer:  33% 2675/8047 [1:37:57<4:05:06,  2.74s/it]loss_total_epoch 167.1248343307525
Training tokenizer:  33% 2676/8047 [1:38:00<4:05:36,  2.74s/it]loss_total_epoch 167.17998715303838
Training tokenizer:  33% 2677/8047 [1:38:03<4:06:51,  2.76s/it]loss_total_epoch 167.23978834412992
Training tokenizer:  33% 2678/8047 [1:38:05<4:06:28,  2.75s/it]loss_total_epoch 167.29415208287537
Training tokenizer:  33% 2679/8047 [1:38:08<4:06:21,  2.75s/it]loss_total_epoch 167.3383492100984
Training tokenizer:  33% 2680/8047 [1:38:11<4:08:26,  2.78s/it]loss_total_epoch 167.37947251833975
Training tokenizer:  33% 2681/8047 [1:38:14<4:07:25,  2.77s/it]loss_total_epoch 167.43189675919712
Training tokenizer:  33% 2682/8047 [1:38:16<4:06:11,  2.75s/it]loss_total_epoch 167.49225758202374
Training tokenizer:  33% 2683/8047 [1:38:19<4:06:11,  2.75s/it]loss_total_epoch 167.5366403926164
Training tokenizer:  33% 2684/8047 [1:38:22<4:05:52,  2.75s/it]loss_total_epoch 167.5873657297343
Training tokenizer:  33% 2685/8047 [1:38:25<4:05:07,  2.74s/it]loss_total_epoch 167.64443386904895
Training tokenizer:  33% 2686/8047 [1:38:27<4:04:12,  2.73s/it]loss_total_epoch 167.6994970459491
Training tokenizer:  33% 2687/8047 [1:38:30<4:04:38,  2.74s/it]loss_total_epoch 167.7409021575004
Training tokenizer:  33% 2688/8047 [1:38:33<4:04:03,  2.73s/it]loss_total_epoch 167.79270225949585
Training tokenizer:  33% 2689/8047 [1:38:36<4:05:03,  2.74s/it]loss_total_epoch 167.8462530951947
Training tokenizer:  33% 2690/8047 [1:38:38<4:05:08,  2.75s/it]loss_total_epoch 167.9105401020497
Training tokenizer:  33% 2691/8047 [1:38:41<4:04:08,  2.74s/it]loss_total_epoch 167.9666893798858
Training tokenizer:  33% 2692/8047 [1:38:44<4:03:27,  2.73s/it]loss_total_epoch 168.01444200240076
Training tokenizer:  33% 2693/8047 [1:38:47<4:04:49,  2.74s/it]loss_total_epoch 168.0689618382603
Training tokenizer:  33% 2694/8047 [1:38:49<4:05:19,  2.75s/it]loss_total_epoch 168.12087363936007
Training tokenizer:  33% 2695/8047 [1:38:52<4:04:40,  2.74s/it]loss_total_epoch 168.17096331901848
Training tokenizer:  34% 2696/8047 [1:38:55<4:04:10,  2.74s/it]loss_total_epoch 168.21498307026923
Training tokenizer:  34% 2697/8047 [1:38:58<4:03:29,  2.73s/it]loss_total_epoch 168.27709251455963
Training tokenizer:  34% 2698/8047 [1:39:00<4:04:26,  2.74s/it]loss_total_epoch 168.33111412264407
Training tokenizer:  34% 2699/8047 [1:39:03<4:03:59,  2.74s/it]loss_total_epoch 168.38566136173904
Training tokenizer:  34% 2700/8047 [1:39:06<4:03:41,  2.73s/it]loss_total_epoch 168.45157955400646
Training tokenizer:  34% 2701/8047 [1:39:09<4:03:55,  2.74s/it]loss_total_epoch 168.50983156822622
Training tokenizer:  34% 2702/8047 [1:39:11<4:04:27,  2.74s/it]loss_total_epoch 168.5670354757458
Training tokenizer:  34% 2703/8047 [1:39:14<4:04:03,  2.74s/it]loss_total_epoch 168.61816839314997
Training tokenizer:  34% 2704/8047 [1:39:17<4:04:33,  2.75s/it]loss_total_epoch 168.66101208515465
Training tokenizer:  34% 2705/8047 [1:39:20<4:04:20,  2.74s/it]loss_total_epoch 168.71062116883695
Training tokenizer:  34% 2706/8047 [1:39:22<4:04:00,  2.74s/it]loss_total_epoch 168.7622442636639
Training tokenizer:  34% 2707/8047 [1:39:25<4:07:26,  2.78s/it]loss_total_epoch 168.80444186367095
Training tokenizer:  34% 2708/8047 [1:39:28<4:06:37,  2.77s/it]loss_total_epoch 168.84931110404432
Training tokenizer:  34% 2709/8047 [1:39:31<4:07:21,  2.78s/it]loss_total_epoch 168.8958320710808
Training tokenizer:  34% 2710/8047 [1:39:33<4:05:47,  2.76s/it]loss_total_epoch 168.95399807207286
Training tokenizer:  34% 2711/8047 [1:39:36<4:05:19,  2.76s/it]loss_total_epoch 169.0158375967294
Training tokenizer:  34% 2712/8047 [1:39:39<4:05:31,  2.76s/it]loss_total_epoch 169.05415706522763
Training tokenizer:  34% 2713/8047 [1:39:42<4:07:12,  2.78s/it]loss_total_epoch 169.10095096193254
Training tokenizer:  34% 2714/8047 [1:39:45<4:07:12,  2.78s/it]loss_total_epoch 169.1537028197199
Training tokenizer:  34% 2715/8047 [1:39:47<4:06:24,  2.77s/it]loss_total_epoch 169.19571872241795
Training tokenizer:  34% 2716/8047 [1:39:50<4:06:27,  2.77s/it]loss_total_epoch 169.2520653884858
Training tokenizer:  34% 2717/8047 [1:39:53<4:05:49,  2.77s/it]loss_total_epoch 169.3047926146537
Training tokenizer:  34% 2718/8047 [1:39:56<4:04:54,  2.76s/it]loss_total_epoch 169.35762420855463
Training tokenizer:  34% 2719/8047 [1:39:59<4:15:32,  2.88s/it]loss_total_epoch 169.41083273105323
Training tokenizer:  34% 2720/8047 [1:40:01<4:12:20,  2.84s/it]loss_total_epoch 169.47157627530396
Training tokenizer:  34% 2721/8047 [1:40:04<4:09:58,  2.82s/it]loss_total_epoch 169.50752981938422
Training tokenizer:  34% 2722/8047 [1:40:07<4:08:26,  2.80s/it]loss_total_epoch 169.55156348086894
Training tokenizer:  34% 2723/8047 [1:40:10<4:07:01,  2.78s/it]loss_total_epoch 169.60689680092037
Training tokenizer:  34% 2724/8047 [1:40:12<4:06:17,  2.78s/it]loss_total_epoch 169.6723024379462
Training tokenizer:  34% 2725/8047 [1:40:15<4:06:11,  2.78s/it]loss_total_epoch 169.7265171725303
Training tokenizer:  34% 2726/8047 [1:40:18<4:05:29,  2.77s/it]loss_total_epoch 169.78035091422498
Training tokenizer:  34% 2727/8047 [1:40:21<4:04:45,  2.76s/it]loss_total_epoch 169.83186079002917
Training tokenizer:  34% 2728/8047 [1:40:24<4:05:19,  2.77s/it]loss_total_epoch 169.87954444997013
Training tokenizer:  34% 2729/8047 [1:40:26<4:03:59,  2.75s/it]loss_total_epoch 169.9253517407924
Training tokenizer:  34% 2730/8047 [1:40:29<4:03:19,  2.75s/it]loss_total_epoch 169.9752764608711
Training tokenizer:  34% 2731/8047 [1:40:32<4:03:58,  2.75s/it]loss_total_epoch 170.02932341955602
Training tokenizer:  34% 2732/8047 [1:40:34<4:02:18,  2.74s/it]loss_total_epoch 170.0740820337087
Training tokenizer:  34% 2733/8047 [1:40:37<4:03:01,  2.74s/it]loss_total_epoch 170.13009877316654
Training tokenizer:  34% 2734/8047 [1:40:40<4:03:10,  2.75s/it]loss_total_epoch 170.18613713793457
Training tokenizer:  34% 2735/8047 [1:40:43<4:04:41,  2.76s/it]loss_total_epoch 170.25008190982044
Training tokenizer:  34% 2736/8047 [1:40:46<4:04:32,  2.76s/it]loss_total_epoch 170.29591296426952
Training tokenizer:  34% 2737/8047 [1:40:48<4:04:27,  2.76s/it]loss_total_epoch 170.33719456382096
Training tokenizer:  34% 2738/8047 [1:40:51<4:04:55,  2.77s/it]loss_total_epoch 170.37355419434607
Training tokenizer:  34% 2739/8047 [1:40:54<4:05:29,  2.77s/it]loss_total_epoch 170.41893901862204
Training tokenizer:  34% 2740/8047 [1:40:57<4:05:02,  2.77s/it]loss_total_epoch 170.46082138828933
Training tokenizer:  34% 2741/8047 [1:40:59<4:05:27,  2.78s/it]loss_total_epoch 170.50674112699926
Training tokenizer:  34% 2742/8047 [1:41:02<4:05:21,  2.77s/it]loss_total_epoch 170.55918151326478
Training tokenizer:  34% 2743/8047 [1:41:05<4:05:20,  2.78s/it]loss_total_epoch 170.60351350717247
Training tokenizer:  34% 2744/8047 [1:41:08<4:05:18,  2.78s/it]loss_total_epoch 170.66875032149255
Training tokenizer:  34% 2745/8047 [1:41:10<4:04:57,  2.77s/it]loss_total_epoch 170.71830230392516
Training tokenizer:  34% 2746/8047 [1:41:13<4:04:35,  2.77s/it]loss_total_epoch 170.75893797539175
Training tokenizer:  34% 2747/8047 [1:41:16<4:03:51,  2.76s/it]loss_total_epoch 170.80325863696635
Training tokenizer:  34% 2748/8047 [1:41:19<4:03:38,  2.76s/it]loss_total_epoch 170.85548023320735
Training tokenizer:  34% 2749/8047 [1:41:21<4:03:06,  2.75s/it]loss_total_epoch 170.917508745566
Training tokenizer:  34% 2750/8047 [1:41:24<4:02:54,  2.75s/it]loss_total_epoch 170.9754468370229
Training tokenizer:  34% 2751/8047 [1:41:27<4:05:05,  2.78s/it]loss_total_epoch 171.02970595099032
Training tokenizer:  34% 2752/8047 [1:41:30<4:05:24,  2.78s/it]loss_total_epoch 171.07570591382682
Training tokenizer:  34% 2753/8047 [1:41:33<4:05:47,  2.79s/it]loss_total_epoch 171.13433398865163
Training tokenizer:  34% 2754/8047 [1:41:35<4:04:41,  2.77s/it]loss_total_epoch 171.18913488276303
Training tokenizer:  34% 2755/8047 [1:41:38<4:05:24,  2.78s/it]loss_total_epoch 171.23498672805727
Training tokenizer:  34% 2756/8047 [1:41:41<4:05:40,  2.79s/it]loss_total_epoch 171.29378153197467
Training tokenizer:  34% 2757/8047 [1:41:44<4:05:13,  2.78s/it]loss_total_epoch 171.3426143694669
Training tokenizer:  34% 2758/8047 [1:41:47<4:05:53,  2.79s/it]loss_total_epoch 171.38892396353185
Training tokenizer:  34% 2759/8047 [1:41:49<4:03:36,  2.76s/it]loss_total_epoch 171.4374580141157
Training tokenizer:  34% 2760/8047 [1:41:52<4:04:16,  2.77s/it]loss_total_epoch 171.49507233686745
Training tokenizer:  34% 2761/8047 [1:41:55<4:04:56,  2.78s/it]loss_total_epoch 171.54248733259737
Training tokenizer:  34% 2762/8047 [1:41:58<4:04:27,  2.78s/it]loss_total_epoch 171.60161310620606
Training tokenizer:  34% 2763/8047 [1:42:00<4:04:48,  2.78s/it]loss_total_epoch 171.65852621011436
Training tokenizer:  34% 2764/8047 [1:42:03<4:05:32,  2.79s/it]loss_total_epoch 171.7027825433761
Training tokenizer:  34% 2765/8047 [1:42:06<4:06:58,  2.81s/it]loss_total_epoch 171.74528061784804
Training tokenizer:  34% 2766/8047 [1:42:09<4:06:02,  2.80s/it]loss_total_epoch 171.7832644339651
Training tokenizer:  34% 2767/8047 [1:42:12<4:06:19,  2.80s/it]loss_total_epoch 171.8439031969756
Training tokenizer:  34% 2768/8047 [1:42:14<4:04:57,  2.78s/it]loss_total_epoch 171.89508147351444
Training tokenizer:  34% 2769/8047 [1:42:17<4:05:09,  2.79s/it]loss_total_epoch 171.95787636749446
Training tokenizer:  34% 2770/8047 [1:42:20<4:04:46,  2.78s/it]loss_total_epoch 172.00854245759547
Training tokenizer:  34% 2771/8047 [1:42:23<4:05:12,  2.79s/it]loss_total_epoch 172.0638058166951
Training tokenizer:  34% 2772/8047 [1:42:26<4:04:52,  2.79s/it]loss_total_epoch 172.11939548514783
Training tokenizer:  34% 2773/8047 [1:42:28<4:04:28,  2.78s/it]loss_total_epoch 172.1766496244818
Training tokenizer:  34% 2774/8047 [1:42:31<4:03:39,  2.77s/it]loss_total_epoch 172.23134550638497
Training tokenizer:  34% 2775/8047 [1:42:34<4:04:36,  2.78s/it]loss_total_epoch 172.27846819348633
Training tokenizer:  34% 2776/8047 [1:42:37<4:05:13,  2.79s/it]loss_total_epoch 172.325014622882
Training tokenizer:  35% 2777/8047 [1:42:39<4:04:08,  2.78s/it]loss_total_epoch 172.3788460623473
Training tokenizer:  35% 2778/8047 [1:42:42<4:04:18,  2.78s/it]loss_total_epoch 172.4246673937887
Training tokenizer:  35% 2779/8047 [1:42:45<4:04:55,  2.79s/it]loss_total_epoch 172.48380037210882
Training tokenizer:  35% 2780/8047 [1:42:48<4:04:44,  2.79s/it]loss_total_epoch 172.537835938856
Training tokenizer:  35% 2781/8047 [1:42:51<4:03:35,  2.78s/it]loss_total_epoch 172.58318536169827
Training tokenizer:  35% 2782/8047 [1:42:53<4:03:35,  2.78s/it]loss_total_epoch 172.62877237983048
Training tokenizer:  35% 2783/8047 [1:42:56<4:03:15,  2.77s/it]loss_total_epoch 172.6847894024104
Training tokenizer:  35% 2784/8047 [1:42:59<4:03:09,  2.77s/it]loss_total_epoch 172.7354307193309
Training tokenizer:  35% 2785/8047 [1:43:02<4:04:00,  2.78s/it]loss_total_epoch 172.79332777298987
Training tokenizer:  35% 2786/8047 [1:43:05<4:04:44,  2.79s/it]loss_total_epoch 172.8381778281182
Training tokenizer:  35% 2787/8047 [1:43:07<4:04:41,  2.79s/it]loss_total_epoch 172.88586770556867
Training tokenizer:  35% 2788/8047 [1:43:10<4:04:21,  2.79s/it]loss_total_epoch 172.9368517678231
Training tokenizer:  35% 2789/8047 [1:43:13<4:04:14,  2.79s/it]loss_total_epoch 172.99711697734892
Training tokenizer:  35% 2790/8047 [1:43:16<4:03:47,  2.78s/it]loss_total_epoch 173.0531663838774
Training tokenizer:  35% 2791/8047 [1:43:18<4:03:40,  2.78s/it]loss_total_epoch 173.08628021366894
Training tokenizer:  35% 2792/8047 [1:43:21<4:03:17,  2.78s/it]loss_total_epoch 173.13118254952133
Training tokenizer:  35% 2793/8047 [1:43:24<4:03:49,  2.78s/it]loss_total_epoch 173.17559965513647
Training tokenizer:  35% 2794/8047 [1:43:27<4:04:30,  2.79s/it]loss_total_epoch 173.23047347925603
Training tokenizer:  35% 2795/8047 [1:43:30<4:05:25,  2.80s/it]loss_total_epoch 173.28675939701498
Training tokenizer:  35% 2796/8047 [1:43:32<4:03:31,  2.78s/it]loss_total_epoch 173.33582119084895
Training tokenizer:  35% 2797/8047 [1:43:35<4:02:55,  2.78s/it]loss_total_epoch 173.39218119345605
Training tokenizer:  35% 2798/8047 [1:43:38<4:02:39,  2.77s/it]loss_total_epoch 173.44632101245224
Training tokenizer:  35% 2799/8047 [1:43:41<4:03:26,  2.78s/it]loss_total_epoch 173.4956308733672
Training tokenizer:  35% 2800/8047 [1:43:43<4:02:56,  2.78s/it]loss_total_epoch 173.54111092351377
Training tokenizer:  35% 2801/8047 [1:43:46<4:03:10,  2.78s/it]loss_total_epoch 173.58983078040183
Training tokenizer:  35% 2802/8047 [1:43:49<4:03:31,  2.79s/it]loss_total_epoch 173.6334146540612
Training tokenizer:  35% 2803/8047 [1:43:52<4:03:26,  2.79s/it]loss_total_epoch 173.68543118424714
Training tokenizer:  35% 2804/8047 [1:43:55<4:04:04,  2.79s/it]loss_total_epoch 173.7383521515876
Training tokenizer:  35% 2805/8047 [1:43:57<4:05:11,  2.81s/it]loss_total_epoch 173.78373760543764
Training tokenizer:  35% 2806/8047 [1:44:00<4:05:15,  2.81s/it]loss_total_epoch 173.83215592242777
Training tokenizer:  35% 2807/8047 [1:44:03<4:05:08,  2.81s/it]loss_total_epoch 173.88347340933979
Training tokenizer:  35% 2808/8047 [1:44:06<4:05:44,  2.81s/it]loss_total_epoch 173.9320811573416
Training tokenizer:  35% 2809/8047 [1:44:09<4:04:51,  2.80s/it]loss_total_epoch 173.9772168379277
Training tokenizer:  35% 2810/8047 [1:44:11<4:03:27,  2.79s/it]loss_total_epoch 174.02527111582458
Training tokenizer:  35% 2811/8047 [1:44:14<4:04:27,  2.80s/it]loss_total_epoch 174.0771696921438
Training tokenizer:  35% 2812/8047 [1:44:17<4:04:33,  2.80s/it]loss_total_epoch 174.13236400298774
Training tokenizer:  35% 2813/8047 [1:44:20<4:03:16,  2.79s/it]loss_total_epoch 174.18804073520005
Training tokenizer:  35% 2814/8047 [1:44:23<4:04:23,  2.80s/it]loss_total_epoch 174.23999391682446
Training tokenizer:  35% 2815/8047 [1:44:25<4:03:03,  2.79s/it]loss_total_epoch 174.28606830351055
Training tokenizer:  35% 2816/8047 [1:44:28<4:04:25,  2.80s/it]loss_total_epoch 174.34605043940246
Training tokenizer:  35% 2817/8047 [1:44:31<4:04:34,  2.81s/it]loss_total_epoch 174.39407260529697
Training tokenizer:  35% 2818/8047 [1:44:34<4:05:15,  2.81s/it]loss_total_epoch 174.44100217707455
Training tokenizer:  35% 2819/8047 [1:44:37<4:05:10,  2.81s/it]loss_total_epoch 174.48632283695042
Training tokenizer:  35% 2820/8047 [1:44:40<4:03:59,  2.80s/it]loss_total_epoch 174.54881779663265
Training tokenizer:  35% 2821/8047 [1:44:42<4:04:18,  2.80s/it]loss_total_epoch 174.5937928762287
Training tokenizer:  35% 2822/8047 [1:44:45<4:05:59,  2.82s/it]loss_total_epoch 174.63282121531665
Training tokenizer:  35% 2823/8047 [1:44:48<4:05:48,  2.82s/it]loss_total_epoch 174.6869461145252
Training tokenizer:  35% 2824/8047 [1:44:51<4:05:07,  2.82s/it]loss_total_epoch 174.72851837612689
Training tokenizer:  35% 2825/8047 [1:44:54<4:05:31,  2.82s/it]loss_total_epoch 174.78870013915002
Training tokenizer:  35% 2826/8047 [1:44:56<4:06:10,  2.83s/it]loss_total_epoch 174.82942271791399
Training tokenizer:  35% 2827/8047 [1:44:59<4:03:31,  2.80s/it]loss_total_epoch 174.8791548591107
Training tokenizer:  35% 2828/8047 [1:45:02<4:03:08,  2.80s/it]loss_total_epoch 174.92553541995585
Training tokenizer:  35% 2829/8047 [1:45:05<4:03:58,  2.81s/it]loss_total_epoch 174.97322940640152
Training tokenizer:  35% 2830/8047 [1:45:08<4:03:50,  2.80s/it]loss_total_epoch 175.01881275884807
Training tokenizer:  35% 2831/8047 [1:45:10<4:04:17,  2.81s/it]loss_total_epoch 175.07097399793565
Training tokenizer:  35% 2832/8047 [1:45:13<4:05:08,  2.82s/it]loss_total_epoch 175.1372603084892
Training tokenizer:  35% 2833/8047 [1:45:16<4:04:50,  2.82s/it]loss_total_epoch 175.1895787473768
Training tokenizer:  35% 2834/8047 [1:45:19<4:04:57,  2.82s/it]loss_total_epoch 175.2504450250417
Training tokenizer:  35% 2835/8047 [1:45:22<4:04:28,  2.81s/it]loss_total_epoch 175.30455287732184
Training tokenizer:  35% 2836/8047 [1:45:25<4:03:40,  2.81s/it]loss_total_epoch 175.353922823444
Training tokenizer:  35% 2837/8047 [1:45:27<4:04:33,  2.82s/it]loss_total_epoch 175.40925494767725
Training tokenizer:  35% 2838/8047 [1:45:30<4:04:21,  2.81s/it]loss_total_epoch 175.46182493306696
Training tokenizer:  35% 2839/8047 [1:45:33<4:05:46,  2.83s/it]loss_total_epoch 175.50951769016683
Training tokenizer:  35% 2840/8047 [1:45:36<4:04:34,  2.82s/it]loss_total_epoch 175.56406162120402
Training tokenizer:  35% 2841/8047 [1:45:39<4:04:53,  2.82s/it]loss_total_epoch 175.6223396230489
Training tokenizer:  35% 2842/8047 [1:45:42<4:05:12,  2.83s/it]loss_total_epoch 175.67992584221065
Training tokenizer:  35% 2843/8047 [1:45:44<4:05:19,  2.83s/it]loss_total_epoch 175.72831859253347
Training tokenizer:  35% 2844/8047 [1:45:47<4:05:29,  2.83s/it]loss_total_epoch 175.7816101592034
Training tokenizer:  35% 2845/8047 [1:45:50<4:06:37,  2.84s/it]loss_total_epoch 175.8344447594136
Training tokenizer:  35% 2846/8047 [1:45:53<4:05:03,  2.83s/it]loss_total_epoch 175.87629995308816
Training tokenizer:  35% 2847/8047 [1:45:56<4:05:27,  2.83s/it]loss_total_epoch 175.92722214572132
Training tokenizer:  35% 2848/8047 [1:45:58<4:04:46,  2.82s/it]loss_total_epoch 175.99081808887422
Training tokenizer:  35% 2849/8047 [1:46:01<4:04:24,  2.82s/it]loss_total_epoch 176.04402981884778
Training tokenizer:  35% 2850/8047 [1:46:04<4:03:28,  2.81s/it]loss_total_epoch 176.08317347802222
Training tokenizer:  35% 2851/8047 [1:46:07<4:04:05,  2.82s/it]loss_total_epoch 176.1363837365061
Training tokenizer:  35% 2852/8047 [1:46:10<4:03:15,  2.81s/it]loss_total_epoch 176.1958812121302
Training tokenizer:  35% 2853/8047 [1:46:13<4:04:27,  2.82s/it]loss_total_epoch 176.25663506425917
Training tokenizer:  35% 2854/8047 [1:46:15<4:04:22,  2.82s/it]loss_total_epoch 176.30712342448533
Training tokenizer:  35% 2855/8047 [1:46:18<4:04:26,  2.82s/it]loss_total_epoch 176.3548288885504
Training tokenizer:  35% 2856/8047 [1:46:21<4:04:27,  2.83s/it]loss_total_epoch 176.40982383675873
Training tokenizer:  36% 2857/8047 [1:46:24<4:04:16,  2.82s/it]loss_total_epoch 176.46482302807271
Training tokenizer:  36% 2858/8047 [1:46:27<4:03:28,  2.82s/it]loss_total_epoch 176.50951837934554
Training tokenizer:  36% 2859/8047 [1:46:29<4:03:43,  2.82s/it]loss_total_epoch 176.55550306476653
Training tokenizer:  36% 2860/8047 [1:46:32<4:04:04,  2.82s/it]loss_total_epoch 176.61797886155546
Training tokenizer:  36% 2861/8047 [1:46:35<4:04:55,  2.83s/it]loss_total_epoch 176.66869685985148
Training tokenizer:  36% 2862/8047 [1:46:38<4:04:20,  2.83s/it]loss_total_epoch 176.71638114191592
Training tokenizer:  36% 2863/8047 [1:46:41<4:03:41,  2.82s/it]loss_total_epoch 176.75835549645126
Training tokenizer:  36% 2864/8047 [1:46:44<4:04:40,  2.83s/it]loss_total_epoch 176.80950357951224
Training tokenizer:  36% 2865/8047 [1:46:46<4:04:26,  2.83s/it]loss_total_epoch 176.85669668205082
Training tokenizer:  36% 2866/8047 [1:46:49<4:05:56,  2.85s/it]loss_total_epoch 176.9134366121143
Training tokenizer:  36% 2867/8047 [1:46:52<4:05:49,  2.85s/it]loss_total_epoch 176.97636067681015
Training tokenizer:  36% 2868/8047 [1:46:55<4:04:58,  2.84s/it]loss_total_epoch 177.0301399808377
Training tokenizer:  36% 2869/8047 [1:46:58<4:05:17,  2.84s/it]loss_total_epoch 177.0838029589504
Training tokenizer:  36% 2870/8047 [1:47:01<4:06:16,  2.85s/it]loss_total_epoch 177.14142235182226
Training tokenizer:  36% 2871/8047 [1:47:04<4:04:22,  2.83s/it]loss_total_epoch 177.20014972798526
Training tokenizer:  36% 2872/8047 [1:47:06<4:04:33,  2.84s/it]loss_total_epoch 177.2525028605014
Training tokenizer:  36% 2873/8047 [1:47:09<4:04:32,  2.84s/it]loss_total_epoch 177.2981783617288
Training tokenizer:  36% 2874/8047 [1:47:12<4:04:18,  2.83s/it]loss_total_epoch 177.33929467387497
Training tokenizer:  36% 2875/8047 [1:47:15<4:03:59,  2.83s/it]loss_total_epoch 177.38485528714955
Training tokenizer:  36% 2876/8047 [1:47:18<4:03:20,  2.82s/it]loss_total_epoch 177.43854252435267
Training tokenizer:  36% 2877/8047 [1:47:21<4:03:59,  2.83s/it]loss_total_epoch 177.4750119317323
Training tokenizer:  36% 2878/8047 [1:47:23<4:04:02,  2.83s/it]loss_total_epoch 177.5419510472566
Training tokenizer:  36% 2879/8047 [1:47:26<4:03:26,  2.83s/it]loss_total_epoch 177.59121211059391
Training tokenizer:  36% 2880/8047 [1:47:29<4:03:41,  2.83s/it]loss_total_epoch 177.63045681081712
Training tokenizer:  36% 2881/8047 [1:47:32<4:03:30,  2.83s/it]loss_total_epoch 177.67695388384163
Training tokenizer:  36% 2882/8047 [1:47:35<4:03:14,  2.83s/it]loss_total_epoch 177.73934574611485
Training tokenizer:  36% 2883/8047 [1:47:37<4:02:58,  2.82s/it]loss_total_epoch 177.78993862308562
Training tokenizer:  36% 2884/8047 [1:47:40<4:03:43,  2.83s/it]loss_total_epoch 177.83940711431205
Training tokenizer:  36% 2885/8047 [1:47:43<4:04:07,  2.84s/it]loss_total_epoch 177.88081138767302
Training tokenizer:  36% 2886/8047 [1:47:46<4:05:11,  2.85s/it]loss_total_epoch 177.93834722973406
Training tokenizer:  36% 2887/8047 [1:47:49<4:04:30,  2.84s/it]loss_total_epoch 177.9843925666064
Training tokenizer:  36% 2888/8047 [1:47:52<4:04:20,  2.84s/it]loss_total_epoch 178.05403517745435
Training tokenizer:  36% 2889/8047 [1:47:55<4:04:38,  2.85s/it]loss_total_epoch 178.09246905706823
Training tokenizer:  36% 2890/8047 [1:47:57<4:02:50,  2.83s/it]loss_total_epoch 178.147499775514
Training tokenizer:  36% 2891/8047 [1:48:00<4:02:40,  2.82s/it]loss_total_epoch 178.20625031925738
Training tokenizer:  36% 2892/8047 [1:48:03<4:03:17,  2.83s/it]loss_total_epoch 178.2578951548785
Training tokenizer:  36% 2893/8047 [1:48:06<4:03:18,  2.83s/it]loss_total_epoch 178.3098176252097
Training tokenizer:  36% 2894/8047 [1:48:09<4:02:49,  2.83s/it]loss_total_epoch 178.36752164177597
Training tokenizer:  36% 2895/8047 [1:48:12<4:03:14,  2.83s/it]loss_total_epoch 178.42735648714006
Training tokenizer:  36% 2896/8047 [1:48:14<4:04:14,  2.84s/it]loss_total_epoch 178.4810596536845
Training tokenizer:  36% 2897/8047 [1:48:17<4:03:25,  2.84s/it]loss_total_epoch 178.53579666651785
Training tokenizer:  36% 2898/8047 [1:48:20<4:03:30,  2.84s/it]loss_total_epoch 178.58794878609478
Training tokenizer:  36% 2899/8047 [1:48:23<4:04:18,  2.85s/it]loss_total_epoch 178.62904782779515
Training tokenizer:  36% 2900/8047 [1:48:26<4:04:15,  2.85s/it]loss_total_epoch 178.67436193488538
Training tokenizer:  36% 2901/8047 [1:48:29<4:03:55,  2.84s/it]loss_total_epoch 178.7291029971093
Training tokenizer:  36% 2902/8047 [1:48:31<4:03:06,  2.84s/it]loss_total_epoch 178.77704113535583
Training tokenizer:  36% 2903/8047 [1:48:34<4:04:07,  2.85s/it]loss_total_epoch 178.83052643202245
Training tokenizer:  36% 2904/8047 [1:48:37<4:03:28,  2.84s/it]loss_total_epoch 178.87501070834696
Training tokenizer:  36% 2905/8047 [1:48:40<4:03:01,  2.84s/it]loss_total_epoch 178.91800707019866
Training tokenizer:  36% 2906/8047 [1:48:43<4:02:35,  2.83s/it]loss_total_epoch 178.96632473357022
Training tokenizer:  36% 2907/8047 [1:48:46<4:03:01,  2.84s/it]loss_total_epoch 179.01204164512455
Training tokenizer:  36% 2908/8047 [1:48:49<4:04:01,  2.85s/it]loss_total_epoch 179.07155181653798
Training tokenizer:  36% 2909/8047 [1:48:51<4:04:36,  2.86s/it]loss_total_epoch 179.10964992456138
Training tokenizer:  36% 2910/8047 [1:48:54<4:05:02,  2.86s/it]loss_total_epoch 179.15358991362154
Training tokenizer:  36% 2911/8047 [1:48:57<4:04:56,  2.86s/it]loss_total_epoch 179.20781681872904
Training tokenizer:  36% 2912/8047 [1:49:00<4:05:16,  2.87s/it]loss_total_epoch 179.25103534571826
Training tokenizer:  36% 2913/8047 [1:49:03<4:05:27,  2.87s/it]loss_total_epoch 179.30423724092543
Training tokenizer:  36% 2914/8047 [1:49:06<4:05:34,  2.87s/it]loss_total_epoch 179.35769130848348
Training tokenizer:  36% 2915/8047 [1:49:09<4:05:32,  2.87s/it]loss_total_epoch 179.41181327588856
Training tokenizer:  36% 2916/8047 [1:49:11<4:05:46,  2.87s/it]loss_total_epoch 179.47097921557724
Training tokenizer:  36% 2917/8047 [1:49:14<4:04:59,  2.87s/it]loss_total_epoch 179.51507950760424
Training tokenizer:  36% 2918/8047 [1:49:17<4:03:53,  2.85s/it]loss_total_epoch 179.55954808555543
Training tokenizer:  36% 2919/8047 [1:49:20<4:03:04,  2.84s/it]loss_total_epoch 179.60502635873854
Training tokenizer:  36% 2920/8047 [1:49:23<4:03:18,  2.85s/it]loss_total_epoch 179.64428825117648
Training tokenizer:  36% 2921/8047 [1:49:26<4:04:04,  2.86s/it]loss_total_epoch 179.69642868079245
Training tokenizer:  36% 2922/8047 [1:49:29<4:04:12,  2.86s/it]loss_total_epoch 179.76054970361292
Training tokenizer:  36% 2923/8047 [1:49:31<4:04:10,  2.86s/it]loss_total_epoch 179.82557792402804
Training tokenizer:  36% 2924/8047 [1:49:34<4:03:49,  2.86s/it]loss_total_epoch 179.87517591379583
Training tokenizer:  36% 2925/8047 [1:49:37<4:03:26,  2.85s/it]loss_total_epoch 179.93385120294988
Training tokenizer:  36% 2926/8047 [1:49:40<4:04:12,  2.86s/it]loss_total_epoch 179.97890207357705
Training tokenizer:  36% 2927/8047 [1:49:43<4:04:09,  2.86s/it]loss_total_epoch 180.01870542950928
Training tokenizer:  36% 2928/8047 [1:49:46<4:04:13,  2.86s/it]loss_total_epoch 180.06326148100197
Training tokenizer:  36% 2929/8047 [1:49:49<4:03:30,  2.85s/it]loss_total_epoch 180.10516440682113
Training tokenizer:  36% 2930/8047 [1:49:51<4:02:50,  2.85s/it]loss_total_epoch 180.1585534159094
Training tokenizer:  36% 2931/8047 [1:49:54<4:02:48,  2.85s/it]loss_total_epoch 180.2127440366894
Training tokenizer:  36% 2932/8047 [1:49:57<4:02:39,  2.85s/it]loss_total_epoch 180.262869162485
Training tokenizer:  36% 2933/8047 [1:50:00<4:03:29,  2.86s/it]loss_total_epoch 180.30510235019028
Training tokenizer:  36% 2934/8047 [1:50:03<4:03:45,  2.86s/it]loss_total_epoch 180.35224004276097
Training tokenizer:  36% 2935/8047 [1:50:06<4:02:18,  2.84s/it]loss_total_epoch 180.40405779145658
Training tokenizer:  36% 2936/8047 [1:50:09<4:03:26,  2.86s/it]loss_total_epoch 180.44944700412452
Training tokenizer:  36% 2937/8047 [1:50:11<4:02:48,  2.85s/it]loss_total_epoch 180.4922774154693
Training tokenizer:  37% 2938/8047 [1:50:14<4:02:48,  2.85s/it]loss_total_epoch 180.5278304386884
Training tokenizer:  37% 2939/8047 [1:50:17<4:04:05,  2.87s/it]loss_total_epoch 180.58737365342677
Training tokenizer:  37% 2940/8047 [1:50:20<4:02:58,  2.85s/it]loss_total_epoch 180.65053580142558
Training tokenizer:  37% 2941/8047 [1:50:23<4:01:11,  2.83s/it]loss_total_epoch 180.68931873701513
Training tokenizer:  37% 2942/8047 [1:50:26<4:01:55,  2.84s/it]loss_total_epoch 180.73293084092438
Training tokenizer:  37% 2943/8047 [1:50:29<4:02:53,  2.86s/it]loss_total_epoch 180.78651836328208
Training tokenizer:  37% 2944/8047 [1:50:31<4:04:04,  2.87s/it]loss_total_epoch 180.8274770397693
Training tokenizer:  37% 2945/8047 [1:50:34<4:03:28,  2.86s/it]loss_total_epoch 180.8824257273227
Training tokenizer:  37% 2946/8047 [1:50:37<4:03:37,  2.87s/it]loss_total_epoch 180.93839685060084
Training tokenizer:  37% 2947/8047 [1:50:40<4:04:41,  2.88s/it]loss_total_epoch 180.98075306601822
Training tokenizer:  37% 2948/8047 [1:50:43<4:04:11,  2.87s/it]loss_total_epoch 181.01744470186532
Training tokenizer:  37% 2949/8047 [1:50:46<4:03:47,  2.87s/it]loss_total_epoch 181.0711591746658
Training tokenizer:  37% 2950/8047 [1:50:49<4:04:37,  2.88s/it]loss_total_epoch 181.11333465389907
Training tokenizer:  37% 2951/8047 [1:50:52<4:05:13,  2.89s/it]loss_total_epoch 181.16213450022042
Training tokenizer:  37% 2952/8047 [1:50:54<4:05:09,  2.89s/it]loss_total_epoch 181.2112492453307
Training tokenizer:  37% 2953/8047 [1:50:57<4:05:08,  2.89s/it]loss_total_epoch 181.2678919751197
Training tokenizer:  37% 2954/8047 [1:51:00<4:05:08,  2.89s/it]loss_total_epoch 181.31728665344417
Training tokenizer:  37% 2955/8047 [1:51:03<4:05:44,  2.90s/it]loss_total_epoch 181.36371359042823
Training tokenizer:  37% 2956/8047 [1:51:06<4:05:42,  2.90s/it]loss_total_epoch 181.41019202582538
Training tokenizer:  37% 2957/8047 [1:51:09<4:04:13,  2.88s/it]loss_total_epoch 181.47772571258247
Training tokenizer:  37% 2958/8047 [1:51:12<4:04:09,  2.88s/it]loss_total_epoch 181.53228839673102
Training tokenizer:  37% 2959/8047 [1:51:15<4:03:38,  2.87s/it]loss_total_epoch 181.59135078452528
Training tokenizer:  37% 2960/8047 [1:51:17<4:02:50,  2.86s/it]loss_total_epoch 181.63712120242417
Training tokenizer:  37% 2961/8047 [1:51:20<4:03:01,  2.87s/it]loss_total_epoch 181.67960037477314
Training tokenizer:  37% 2962/8047 [1:51:23<4:02:13,  2.86s/it]loss_total_epoch 181.72459384985268
Training tokenizer:  37% 2963/8047 [1:51:26<4:02:54,  2.87s/it]loss_total_epoch 181.7759826760739
Training tokenizer:  37% 2964/8047 [1:51:29<4:02:41,  2.86s/it]loss_total_epoch 181.82885900698602
Training tokenizer:  37% 2965/8047 [1:51:32<4:01:17,  2.85s/it]loss_total_epoch 181.87178148888052
Training tokenizer:  37% 2966/8047 [1:51:35<4:01:37,  2.85s/it]loss_total_epoch 181.91979266516864
Training tokenizer:  37% 2967/8047 [1:51:37<4:01:32,  2.85s/it]loss_total_epoch 181.95430397056043
Training tokenizer:  37% 2968/8047 [1:51:40<4:01:57,  2.86s/it]loss_total_epoch 182.00618949718773
Training tokenizer:  37% 2969/8047 [1:51:43<4:03:14,  2.87s/it]loss_total_epoch 182.0600244384259
Training tokenizer:  37% 2970/8047 [1:51:46<4:04:49,  2.89s/it]loss_total_epoch 182.12203226052225
Training tokenizer:  37% 2971/8047 [1:51:49<4:04:46,  2.89s/it]loss_total_epoch 182.17467358894646
Training tokenizer:  37% 2972/8047 [1:51:52<4:05:16,  2.90s/it]loss_total_epoch 182.23683891259134
Training tokenizer:  37% 2973/8047 [1:51:55<4:04:58,  2.90s/it]loss_total_epoch 182.29057479463518
Training tokenizer:  37% 2974/8047 [1:51:58<4:05:06,  2.90s/it]loss_total_epoch 182.33309665881097
Training tokenizer:  37% 2975/8047 [1:52:01<4:04:52,  2.90s/it]loss_total_epoch 182.39394696243107
Training tokenizer:  37% 2976/8047 [1:52:04<4:04:16,  2.89s/it]loss_total_epoch 182.44693416170776
Training tokenizer:  37% 2977/8047 [1:52:06<4:04:49,  2.90s/it]loss_total_epoch 182.50201237387955
Training tokenizer:  37% 2978/8047 [1:52:09<4:04:45,  2.90s/it]loss_total_epoch 182.54991167224944
Training tokenizer:  37% 2979/8047 [1:52:12<4:04:06,  2.89s/it]loss_total_epoch 182.59721883945167
Training tokenizer:  37% 2980/8047 [1:52:15<4:03:21,  2.88s/it]loss_total_epoch 182.6438070666045
Training tokenizer:  37% 2981/8047 [1:52:18<4:03:34,  2.88s/it]loss_total_epoch 182.69085288606584
Training tokenizer:  37% 2982/8047 [1:52:21<4:03:04,  2.88s/it]loss_total_epoch 182.73349609784782
Training tokenizer:  37% 2983/8047 [1:52:24<4:03:03,  2.88s/it]loss_total_epoch 182.78212832845747
Training tokenizer:  37% 2984/8047 [1:52:27<4:03:18,  2.88s/it]loss_total_epoch 182.83102286793292
Training tokenizer:  37% 2985/8047 [1:52:30<4:03:49,  2.89s/it]loss_total_epoch 182.88961473666131
Training tokenizer:  37% 2986/8047 [1:52:32<4:04:28,  2.90s/it]loss_total_epoch 182.93311718292534
Training tokenizer:  37% 2987/8047 [1:52:35<4:05:01,  2.91s/it]loss_total_epoch 182.9851646181196
Training tokenizer:  37% 2988/8047 [1:52:38<4:04:12,  2.90s/it]loss_total_epoch 183.04261798225343
Training tokenizer:  37% 2989/8047 [1:52:41<4:03:37,  2.89s/it]loss_total_epoch 183.08752964250743
Training tokenizer:  37% 2990/8047 [1:52:44<4:03:41,  2.89s/it]loss_total_epoch 183.13597313128412
Training tokenizer:  37% 2991/8047 [1:52:47<4:03:19,  2.89s/it]loss_total_epoch 183.1793514918536
Training tokenizer:  37% 2992/8047 [1:52:50<4:03:29,  2.89s/it]loss_total_epoch 183.23609519191086
Training tokenizer:  37% 2993/8047 [1:52:53<4:03:02,  2.89s/it]loss_total_epoch 183.2940307650715
Training tokenizer:  37% 2994/8047 [1:52:56<4:02:27,  2.88s/it]loss_total_epoch 183.3503204677254
Training tokenizer:  37% 2995/8047 [1:52:58<4:03:37,  2.89s/it]loss_total_epoch 183.3980842065066
Training tokenizer:  37% 2996/8047 [1:53:01<4:03:20,  2.89s/it]loss_total_epoch 183.4512837473303
Training tokenizer:  37% 2997/8047 [1:53:04<4:02:12,  2.88s/it]loss_total_epoch 183.49369273893535
Training tokenizer:  37% 2998/8047 [1:53:07<4:02:20,  2.88s/it]loss_total_epoch 183.54836488328874
Training tokenizer:  37% 2999/8047 [1:53:10<4:03:17,  2.89s/it]loss_total_epoch 183.5991701837629
Training tokenizer:  37% 3000/8047 [1:53:13<4:03:17,  2.89s/it]loss_total_epoch 183.641492607072
Training tokenizer:  37% 3001/8047 [1:53:16<4:02:56,  2.89s/it]loss_total_epoch 183.69429422728717
Training tokenizer:  37% 3002/8047 [1:53:19<4:02:03,  2.88s/it]loss_total_epoch 183.74598878063262
Training tokenizer:  37% 3003/8047 [1:53:21<4:01:55,  2.88s/it]loss_total_epoch 183.79817889072
Training tokenizer:  37% 3004/8047 [1:53:24<4:03:39,  2.90s/it]loss_total_epoch 183.84246387146413
Training tokenizer:  37% 3005/8047 [1:53:27<4:02:47,  2.89s/it]loss_total_epoch 183.88804273493588
Training tokenizer:  37% 3006/8047 [1:53:30<4:03:16,  2.90s/it]loss_total_epoch 183.93605402298272
Training tokenizer:  37% 3007/8047 [1:53:33<4:02:58,  2.89s/it]loss_total_epoch 183.9783628936857
Training tokenizer:  37% 3008/8047 [1:53:36<4:01:57,  2.88s/it]loss_total_epoch 184.02557253278792
Training tokenizer:  37% 3009/8047 [1:53:39<4:03:03,  2.89s/it]loss_total_epoch 184.07755358703434
Training tokenizer:  37% 3010/8047 [1:53:42<4:03:42,  2.90s/it]loss_total_epoch 184.1253754440695
Training tokenizer:  37% 3011/8047 [1:53:45<4:03:38,  2.90s/it]loss_total_epoch 184.17818733863533
Training tokenizer:  37% 3012/8047 [1:53:48<4:04:57,  2.92s/it]loss_total_epoch 184.244155889377
Training tokenizer:  37% 3013/8047 [1:53:51<4:04:32,  2.91s/it]loss_total_epoch 184.3019836563617
Training tokenizer:  37% 3014/8047 [1:53:53<4:03:32,  2.90s/it]loss_total_epoch 184.35161770693958
Training tokenizer:  37% 3015/8047 [1:53:56<4:03:47,  2.91s/it]loss_total_epoch 184.3904881272465
Training tokenizer:  37% 3016/8047 [1:53:59<4:03:57,  2.91s/it]loss_total_epoch 184.43950199894607
Training tokenizer:  37% 3017/8047 [1:54:02<4:04:50,  2.92s/it]loss_total_epoch 184.4869502875954
Training tokenizer:  38% 3018/8047 [1:54:05<4:04:11,  2.91s/it]loss_total_epoch 184.5433911923319
Training tokenizer:  38% 3019/8047 [1:54:08<4:04:32,  2.92s/it]loss_total_epoch 184.59743264876306
Training tokenizer:  38% 3020/8047 [1:54:11<4:04:38,  2.92s/it]loss_total_epoch 184.651964077726
Training tokenizer:  38% 3021/8047 [1:54:14<4:04:10,  2.91s/it]loss_total_epoch 184.70350818149745
Training tokenizer:  38% 3022/8047 [1:54:17<4:04:08,  2.92s/it]loss_total_epoch 184.74532108567655
Training tokenizer:  38% 3023/8047 [1:54:20<4:03:33,  2.91s/it]loss_total_epoch 184.79333983175457
Training tokenizer:  38% 3024/8047 [1:54:23<4:03:45,  2.91s/it]loss_total_epoch 184.84113237075508
Training tokenizer:  38% 3025/8047 [1:54:25<4:03:23,  2.91s/it]loss_total_epoch 184.8965046722442
Training tokenizer:  38% 3026/8047 [1:54:28<4:03:59,  2.92s/it]loss_total_epoch 184.94527694769204
Training tokenizer:  38% 3027/8047 [1:54:31<4:05:54,  2.94s/it]loss_total_epoch 185.00238080881536
Training tokenizer:  38% 3028/8047 [1:54:34<4:05:53,  2.94s/it]loss_total_epoch 185.05432059802115
Training tokenizer:  38% 3029/8047 [1:54:37<4:04:53,  2.93s/it]loss_total_epoch 185.1001749690622
Training tokenizer:  38% 3030/8047 [1:54:40<4:04:49,  2.93s/it]loss_total_epoch 185.14231014437973
Training tokenizer:  38% 3031/8047 [1:54:43<4:04:38,  2.93s/it]loss_total_epoch 185.2045707050711
Training tokenizer:  38% 3032/8047 [1:54:46<4:03:19,  2.91s/it]loss_total_epoch 185.24835479818285
Training tokenizer:  38% 3033/8047 [1:54:49<4:03:02,  2.91s/it]loss_total_epoch 185.2929612081498
Training tokenizer:  38% 3034/8047 [1:54:52<4:03:56,  2.92s/it]loss_total_epoch 185.3501250911504
Training tokenizer:  38% 3035/8047 [1:54:55<4:03:25,  2.91s/it]loss_total_epoch 185.3940105009824
Training tokenizer:  38% 3036/8047 [1:54:58<4:02:48,  2.91s/it]loss_total_epoch 185.44190506450832
Training tokenizer:  38% 3037/8047 [1:55:01<4:02:31,  2.90s/it]loss_total_epoch 185.48540836013854
Training tokenizer:  38% 3038/8047 [1:55:03<4:03:48,  2.92s/it]loss_total_epoch 185.53297604061663
Training tokenizer:  38% 3039/8047 [1:55:06<4:04:37,  2.93s/it]loss_total_epoch 185.58435800485313
Training tokenizer:  38% 3040/8047 [1:55:09<4:04:41,  2.93s/it]loss_total_epoch 185.6333340127021
Training tokenizer:  38% 3041/8047 [1:55:12<4:04:05,  2.93s/it]loss_total_epoch 185.68250245787203
Training tokenizer:  38% 3042/8047 [1:55:15<4:04:34,  2.93s/it]loss_total_epoch 185.72363319434226
Training tokenizer:  38% 3043/8047 [1:55:18<4:04:48,  2.94s/it]loss_total_epoch 185.7732401844114
Training tokenizer:  38% 3044/8047 [1:55:21<4:05:08,  2.94s/it]loss_total_epoch 185.8320315759629
Training tokenizer:  38% 3045/8047 [1:55:24<4:05:10,  2.94s/it]loss_total_epoch 185.88168123923242
Training tokenizer:  38% 3046/8047 [1:55:27<4:04:40,  2.94s/it]loss_total_epoch 185.9360698517412
Training tokenizer:  38% 3047/8047 [1:55:30<4:03:46,  2.93s/it]loss_total_epoch 185.97356772981584
Training tokenizer:  38% 3048/8047 [1:55:33<4:03:29,  2.92s/it]loss_total_epoch 186.0331673901528
Training tokenizer:  38% 3049/8047 [1:55:36<4:02:21,  2.91s/it]loss_total_epoch 186.08612776361406
Training tokenizer:  38% 3050/8047 [1:55:39<4:02:42,  2.91s/it]loss_total_epoch 186.1314946245402
Training tokenizer:  38% 3051/8047 [1:55:42<4:03:03,  2.92s/it]loss_total_epoch 186.1847132537514
Training tokenizer:  38% 3052/8047 [1:55:44<4:02:49,  2.92s/it]loss_total_epoch 186.2448519859463
Training tokenizer:  38% 3053/8047 [1:55:47<4:02:29,  2.91s/it]loss_total_epoch 186.3032131139189
Training tokenizer:  38% 3054/8047 [1:55:50<4:02:41,  2.92s/it]loss_total_epoch 186.37012790329754
Training tokenizer:  38% 3055/8047 [1:55:53<4:02:26,  2.91s/it]loss_total_epoch 186.41999315656722
Training tokenizer:  38% 3056/8047 [1:55:56<4:02:26,  2.91s/it]loss_total_epoch 186.4760033916682
Training tokenizer:  38% 3057/8047 [1:55:59<4:04:04,  2.93s/it]loss_total_epoch 186.5249860007316
Training tokenizer:  38% 3058/8047 [1:56:02<4:04:27,  2.94s/it]loss_total_epoch 186.57822655700147
Training tokenizer:  38% 3059/8047 [1:56:05<4:05:41,  2.96s/it]loss_total_epoch 186.6428266968578
Training tokenizer:  38% 3060/8047 [1:56:08<4:03:42,  2.93s/it]loss_total_epoch 186.6939924377948
Training tokenizer:  38% 3061/8047 [1:56:11<4:04:40,  2.94s/it]loss_total_epoch 186.73471550829709
Training tokenizer:  38% 3062/8047 [1:56:14<4:04:18,  2.94s/it]loss_total_epoch 186.77813321910799
Training tokenizer:  38% 3063/8047 [1:56:17<4:04:19,  2.94s/it]loss_total_epoch 186.83214268647134
Training tokenizer:  38% 3064/8047 [1:56:20<4:03:37,  2.93s/it]loss_total_epoch 186.87903952039778
Training tokenizer:  38% 3065/8047 [1:56:23<4:04:38,  2.95s/it]loss_total_epoch 186.93532242439687
Training tokenizer:  38% 3066/8047 [1:56:26<4:04:37,  2.95s/it]loss_total_epoch 186.98885531909764
Training tokenizer:  38% 3067/8047 [1:56:29<4:03:56,  2.94s/it]loss_total_epoch 187.04294789768755
Training tokenizer:  38% 3068/8047 [1:56:31<4:03:31,  2.93s/it]loss_total_epoch 187.09392486326396
Training tokenizer:  38% 3069/8047 [1:56:34<4:01:56,  2.92s/it]loss_total_epoch 187.1434125136584
Training tokenizer:  38% 3070/8047 [1:56:37<4:02:47,  2.93s/it]loss_total_epoch 187.19416621141136
Training tokenizer:  38% 3071/8047 [1:56:40<4:03:33,  2.94s/it]loss_total_epoch 187.24108209647238
Training tokenizer:  38% 3072/8047 [1:56:43<4:03:55,  2.94s/it]loss_total_epoch 187.28887776844203
Training tokenizer:  38% 3073/8047 [1:56:46<4:04:30,  2.95s/it]loss_total_epoch 187.34649449028075
Training tokenizer:  38% 3074/8047 [1:56:49<4:04:04,  2.94s/it]loss_total_epoch 187.4037804994732
Training tokenizer:  38% 3075/8047 [1:56:52<4:04:01,  2.94s/it]loss_total_epoch 187.46256879158318
Training tokenizer:  38% 3076/8047 [1:56:55<4:03:46,  2.94s/it]loss_total_epoch 187.51010978780687
Training tokenizer:  38% 3077/8047 [1:56:58<4:03:16,  2.94s/it]loss_total_epoch 187.56430216692388
Training tokenizer:  38% 3078/8047 [1:57:01<4:03:15,  2.94s/it]loss_total_epoch 187.6050277221948
Training tokenizer:  38% 3079/8047 [1:57:04<4:03:27,  2.94s/it]loss_total_epoch 187.66182080097497
Training tokenizer:  38% 3080/8047 [1:57:07<4:04:01,  2.95s/it]loss_total_epoch 187.70911623723805
Training tokenizer:  38% 3081/8047 [1:57:10<4:02:35,  2.93s/it]loss_total_epoch 187.7509676385671
Training tokenizer:  38% 3082/8047 [1:57:13<4:02:20,  2.93s/it]loss_total_epoch 187.79727934114635
Training tokenizer:  38% 3083/8047 [1:57:15<4:02:18,  2.93s/it]loss_total_epoch 187.84840455837548
Training tokenizer:  38% 3084/8047 [1:57:18<4:01:49,  2.92s/it]loss_total_epoch 187.90398588962853
Training tokenizer:  38% 3085/8047 [1:57:21<4:01:20,  2.92s/it]loss_total_epoch 187.94653777964413
Training tokenizer:  38% 3086/8047 [1:57:24<4:00:14,  2.91s/it]loss_total_epoch 188.00893385894597
Training tokenizer:  38% 3087/8047 [1:57:27<4:01:55,  2.93s/it]loss_total_epoch 188.04474822245538
Training tokenizer:  38% 3088/8047 [1:57:30<4:01:58,  2.93s/it]loss_total_epoch 188.10281671397388
Training tokenizer:  38% 3089/8047 [1:57:33<4:03:09,  2.94s/it]loss_total_epoch 188.16198360733688
Training tokenizer:  38% 3090/8047 [1:57:36<4:03:09,  2.94s/it]loss_total_epoch 188.20924092270434
Training tokenizer:  38% 3091/8047 [1:57:39<4:02:53,  2.94s/it]loss_total_epoch 188.26695503480732
Training tokenizer:  38% 3092/8047 [1:57:42<4:02:56,  2.94s/it]loss_total_epoch 188.31487857736647
Training tokenizer:  38% 3093/8047 [1:57:45<4:03:05,  2.94s/it]loss_total_epoch 188.3712807763368
Training tokenizer:  38% 3094/8047 [1:57:48<4:03:05,  2.94s/it]loss_total_epoch 188.4166025724262
Training tokenizer:  38% 3095/8047 [1:57:51<4:02:36,  2.94s/it]loss_total_epoch 188.47662555985153
Training tokenizer:  38% 3096/8047 [1:57:54<4:01:25,  2.93s/it]loss_total_epoch 188.53860685043037
Training tokenizer:  38% 3097/8047 [1:57:57<4:00:54,  2.92s/it]loss_total_epoch 188.5939742345363
Training tokenizer:  38% 3098/8047 [1:57:59<4:01:08,  2.92s/it]loss_total_epoch 188.64795583672822
Training tokenizer:  39% 3099/8047 [1:58:02<4:01:54,  2.93s/it]loss_total_epoch 188.70027452148497
Training tokenizer:  39% 3100/8047 [1:58:05<4:02:29,  2.94s/it]loss_total_epoch 188.7396383676678
Training tokenizer:  39% 3101/8047 [1:58:08<4:02:44,  2.94s/it]loss_total_epoch 188.79912977851927
Training tokenizer:  39% 3102/8047 [1:58:11<4:02:28,  2.94s/it]loss_total_epoch 188.8460434023291
Training tokenizer:  39% 3103/8047 [1:58:14<4:03:39,  2.96s/it]loss_total_epoch 188.8978274744004
Training tokenizer:  39% 3104/8047 [1:58:17<4:03:12,  2.95s/it]loss_total_epoch 188.95117791555822
Training tokenizer:  39% 3105/8047 [1:58:20<4:02:55,  2.95s/it]loss_total_epoch 189.00024599395692
Training tokenizer:  39% 3106/8047 [1:58:23<4:02:35,  2.95s/it]loss_total_epoch 189.04762504063547
Training tokenizer:  39% 3107/8047 [1:58:26<4:02:43,  2.95s/it]loss_total_epoch 189.10337620414793
Training tokenizer:  39% 3108/8047 [1:58:29<4:03:12,  2.95s/it]loss_total_epoch 189.15622972138226
Training tokenizer:  39% 3109/8047 [1:58:32<4:04:00,  2.96s/it]loss_total_epoch 189.20398329757154
Training tokenizer:  39% 3110/8047 [1:58:35<4:03:10,  2.96s/it]loss_total_epoch 189.26330241002142
Training tokenizer:  39% 3111/8047 [1:58:38<4:03:56,  2.97s/it]loss_total_epoch 189.3283423576504
Training tokenizer:  39% 3112/8047 [1:58:41<4:04:25,  2.97s/it]loss_total_epoch 189.37499316595495
Training tokenizer:  39% 3113/8047 [1:58:44<4:04:33,  2.97s/it]loss_total_epoch 189.41440467350185
Training tokenizer:  39% 3114/8047 [1:58:47<4:03:42,  2.96s/it]loss_total_epoch 189.46191843785346
Training tokenizer:  39% 3115/8047 [1:58:50<4:03:35,  2.96s/it]loss_total_epoch 189.51638571359217
Training tokenizer:  39% 3116/8047 [1:58:53<4:03:03,  2.96s/it]loss_total_epoch 189.5638371128589
Training tokenizer:  39% 3117/8047 [1:58:56<4:02:41,  2.95s/it]loss_total_epoch 189.6089678313583
Training tokenizer:  39% 3118/8047 [1:58:59<4:02:06,  2.95s/it]loss_total_epoch 189.65200329385698
Training tokenizer:  39% 3119/8047 [1:59:02<4:02:10,  2.95s/it]loss_total_epoch 189.7111133057624
Training tokenizer:  39% 3120/8047 [1:59:05<4:02:58,  2.96s/it]loss_total_epoch 189.76406015641987
Training tokenizer:  39% 3121/8047 [1:59:08<4:04:00,  2.97s/it]loss_total_epoch 189.81305943243206
Training tokenizer:  39% 3122/8047 [1:59:10<4:03:28,  2.97s/it]loss_total_epoch 189.87493906728923
Training tokenizer:  39% 3123/8047 [1:59:13<4:02:18,  2.95s/it]loss_total_epoch 189.93550010211766
Training tokenizer:  39% 3124/8047 [1:59:16<4:01:14,  2.94s/it]loss_total_epoch 189.9809906873852
Training tokenizer:  39% 3125/8047 [1:59:19<4:02:06,  2.95s/it]loss_total_epoch 190.03994499705732
Training tokenizer:  39% 3126/8047 [1:59:22<4:01:54,  2.95s/it]loss_total_epoch 190.07814223505557
Training tokenizer:  39% 3127/8047 [1:59:25<4:02:51,  2.96s/it]loss_total_epoch 190.1278905440122
Training tokenizer:  39% 3128/8047 [1:59:28<4:03:05,  2.97s/it]loss_total_epoch 190.17252280376852
Training tokenizer:  39% 3129/8047 [1:59:31<4:03:16,  2.97s/it]loss_total_epoch 190.22887310571969
Training tokenizer:  39% 3130/8047 [1:59:34<4:03:19,  2.97s/it]loss_total_epoch 190.27716105617583
Training tokenizer:  39% 3131/8047 [1:59:37<4:02:43,  2.96s/it]loss_total_epoch 190.32527103461325
Training tokenizer:  39% 3132/8047 [1:59:40<4:02:38,  2.96s/it]loss_total_epoch 190.37202008627355
Training tokenizer:  39% 3133/8047 [1:59:43<4:02:17,  2.96s/it]loss_total_epoch 190.42777455039322
Training tokenizer:  39% 3134/8047 [1:59:46<4:02:56,  2.97s/it]loss_total_epoch 190.47424964793026
Training tokenizer:  39% 3135/8047 [1:59:49<4:02:00,  2.96s/it]loss_total_epoch 190.51733952201903
Training tokenizer:  39% 3136/8047 [1:59:52<4:03:46,  2.98s/it]loss_total_epoch 190.55801670067012
Training tokenizer:  39% 3137/8047 [1:59:55<4:03:22,  2.97s/it]loss_total_epoch 190.62524368427694
Training tokenizer:  39% 3138/8047 [1:59:58<4:02:22,  2.96s/it]loss_total_epoch 190.67631273530424
Training tokenizer:  39% 3139/8047 [2:00:01<4:02:23,  2.96s/it]loss_total_epoch 190.72374051623046
Training tokenizer:  39% 3140/8047 [2:00:04<4:02:43,  2.97s/it]loss_total_epoch 190.77046976424754
Training tokenizer:  39% 3141/8047 [2:00:07<4:02:08,  2.96s/it]loss_total_epoch 190.8119127098471
Training tokenizer:  39% 3142/8047 [2:00:10<4:01:43,  2.96s/it]loss_total_epoch 190.8719947207719
Training tokenizer:  39% 3143/8047 [2:00:13<4:01:58,  2.96s/it]loss_total_epoch 190.92684063129127
Training tokenizer:  39% 3144/8047 [2:00:16<4:02:21,  2.97s/it]loss_total_epoch 190.98419737629592
Training tokenizer:  39% 3145/8047 [2:00:19<4:02:35,  2.97s/it]loss_total_epoch 191.03806726820767
Training tokenizer:  39% 3146/8047 [2:00:22<4:02:12,  2.97s/it]loss_total_epoch 191.09011474438012
Training tokenizer:  39% 3147/8047 [2:00:25<4:02:31,  2.97s/it]loss_total_epoch 191.14669077284634
Training tokenizer:  39% 3148/8047 [2:00:28<4:02:53,  2.97s/it]loss_total_epoch 191.20078737847507
Training tokenizer:  39% 3149/8047 [2:00:31<4:03:12,  2.98s/it]loss_total_epoch 191.23732927627861
Training tokenizer:  39% 3150/8047 [2:00:34<4:04:12,  2.99s/it]loss_total_epoch 191.2903073038906
Training tokenizer:  39% 3151/8047 [2:00:37<4:03:33,  2.98s/it]loss_total_epoch 191.34502101130784
Training tokenizer:  39% 3152/8047 [2:00:40<4:04:49,  3.00s/it]loss_total_epoch 191.40665844269097
Training tokenizer:  39% 3153/8047 [2:00:43<4:03:59,  2.99s/it]loss_total_epoch 191.44769291765988
Training tokenizer:  39% 3154/8047 [2:00:45<4:03:22,  2.98s/it]loss_total_epoch 191.49385586939752
Training tokenizer:  39% 3155/8047 [2:00:48<4:03:54,  2.99s/it]loss_total_epoch 191.54470772854984
Training tokenizer:  39% 3156/8047 [2:00:51<4:03:31,  2.99s/it]loss_total_epoch 191.58558579348028
Training tokenizer:  39% 3157/8047 [2:00:54<4:04:11,  3.00s/it]loss_total_epoch 191.6241181846708
Training tokenizer:  39% 3158/8047 [2:00:57<4:04:05,  3.00s/it]loss_total_epoch 191.679603965953
Training tokenizer:  39% 3159/8047 [2:01:00<4:03:09,  2.98s/it]loss_total_epoch 191.7327451799065
Training tokenizer:  39% 3160/8047 [2:01:03<4:03:53,  2.99s/it]loss_total_epoch 191.7874560188502
Training tokenizer:  39% 3161/8047 [2:01:06<4:03:36,  2.99s/it]loss_total_epoch 191.83020054362714
Training tokenizer:  39% 3162/8047 [2:01:09<4:02:42,  2.98s/it]loss_total_epoch 191.87790715135634
Training tokenizer:  39% 3163/8047 [2:01:12<4:02:52,  2.98s/it]loss_total_epoch 191.92847639136016
Training tokenizer:  39% 3164/8047 [2:01:15<4:02:31,  2.98s/it]loss_total_epoch 191.97502343915403
Training tokenizer:  39% 3165/8047 [2:01:18<4:02:09,  2.98s/it]loss_total_epoch 192.022312393412
Training tokenizer:  39% 3166/8047 [2:01:21<4:01:11,  2.96s/it]loss_total_epoch 192.07337741740048
Training tokenizer:  39% 3167/8047 [2:01:24<4:02:56,  2.99s/it]loss_total_epoch 192.1227329429239
Training tokenizer:  39% 3168/8047 [2:01:27<4:03:04,  2.99s/it]loss_total_epoch 192.1683106776327
Training tokenizer:  39% 3169/8047 [2:01:30<4:02:54,  2.99s/it]loss_total_epoch 192.21127302758396
Training tokenizer:  39% 3170/8047 [2:01:33<4:02:58,  2.99s/it]loss_total_epoch 192.26107867993414
Training tokenizer:  39% 3171/8047 [2:01:36<4:03:59,  3.00s/it]loss_total_epoch 192.31612149439752
Training tokenizer:  39% 3172/8047 [2:01:39<4:04:32,  3.01s/it]loss_total_epoch 192.37026607804
Training tokenizer:  39% 3173/8047 [2:01:42<4:04:37,  3.01s/it]loss_total_epoch 192.42142033018172
Training tokenizer:  39% 3174/8047 [2:01:45<4:04:15,  3.01s/it]loss_total_epoch 192.472619401291
Training tokenizer:  39% 3175/8047 [2:01:48<4:04:16,  3.01s/it]loss_total_epoch 192.52777208201587
Training tokenizer:  39% 3176/8047 [2:01:51<4:03:36,  3.00s/it]loss_total_epoch 192.5759599264711
Training tokenizer:  39% 3177/8047 [2:01:54<4:03:23,  3.00s/it]loss_total_epoch 192.63095209561288
Training tokenizer:  39% 3178/8047 [2:01:57<4:03:47,  3.00s/it]loss_total_epoch 192.68359812907875
Training tokenizer:  40% 3179/8047 [2:02:00<4:04:18,  3.01s/it]loss_total_epoch 192.73363236896694
Training tokenizer:  40% 3180/8047 [2:02:03<4:04:12,  3.01s/it]loss_total_epoch 192.7862951401621
Training tokenizer:  40% 3181/8047 [2:02:06<4:03:08,  3.00s/it]loss_total_epoch 192.83436445705593
Training tokenizer:  40% 3182/8047 [2:02:09<4:03:40,  3.01s/it]loss_total_epoch 192.88436730392277
Training tokenizer:  40% 3183/8047 [2:02:12<4:03:17,  3.00s/it]loss_total_epoch 192.93216414563358
Training tokenizer:  40% 3184/8047 [2:02:15<4:03:42,  3.01s/it]loss_total_epoch 192.98501054011285
Training tokenizer:  40% 3185/8047 [2:02:18<4:03:59,  3.01s/it]loss_total_epoch 193.03393816761672
Training tokenizer:  40% 3186/8047 [2:02:21<4:02:30,  2.99s/it]loss_total_epoch 193.07579736597836
Training tokenizer:  40% 3187/8047 [2:02:24<4:02:42,  3.00s/it]loss_total_epoch 193.12209097854793
Training tokenizer:  40% 3188/8047 [2:02:27<4:01:56,  2.99s/it]loss_total_epoch 193.18384986184537
Training tokenizer:  40% 3189/8047 [2:02:30<4:01:33,  2.98s/it]loss_total_epoch 193.23439781926572
Training tokenizer:  40% 3190/8047 [2:02:33<4:01:32,  2.98s/it]loss_total_epoch 193.28692968003452
Training tokenizer:  40% 3191/8047 [2:02:36<4:03:04,  3.00s/it]loss_total_epoch 193.33863809145987
Training tokenizer:  40% 3192/8047 [2:02:39<4:02:55,  3.00s/it]loss_total_epoch 193.38471274264157
Training tokenizer:  40% 3193/8047 [2:02:42<4:02:01,  2.99s/it]loss_total_epoch 193.4323900770396
Training tokenizer:  40% 3194/8047 [2:02:45<4:02:20,  3.00s/it]loss_total_epoch 193.4766221921891
Training tokenizer:  40% 3195/8047 [2:02:48<4:03:53,  3.02s/it]loss_total_epoch 193.52284412272274
Training tokenizer:  40% 3196/8047 [2:02:51<4:03:22,  3.01s/it]loss_total_epoch 193.56267041526735
Training tokenizer:  40% 3197/8047 [2:02:54<4:03:01,  3.01s/it]loss_total_epoch 193.59981335140765
Training tokenizer:  40% 3198/8047 [2:02:57<4:03:01,  3.01s/it]loss_total_epoch 193.64243890158832
Training tokenizer:  40% 3199/8047 [2:03:00<4:03:36,  3.02s/it]loss_total_epoch 193.69086143560708
Training tokenizer:  40% 3200/8047 [2:03:03<4:03:42,  3.02s/it]loss_total_epoch 193.75219095684588
Training tokenizer:  40% 3201/8047 [2:03:06<4:03:33,  3.02s/it]loss_total_epoch 193.8083088118583
Training tokenizer:  40% 3202/8047 [2:03:09<4:03:25,  3.01s/it]loss_total_epoch 193.85363187082112
Training tokenizer:  40% 3203/8047 [2:03:12<4:03:54,  3.02s/it]loss_total_epoch 193.90977506153286
Training tokenizer:  40% 3204/8047 [2:03:15<4:02:21,  3.00s/it]loss_total_epoch 193.95668442733586
Training tokenizer:  40% 3205/8047 [2:03:18<4:02:37,  3.01s/it]loss_total_epoch 194.00913818366826
Training tokenizer:  40% 3206/8047 [2:03:21<4:02:24,  3.00s/it]loss_total_epoch 194.06430386193097
Training tokenizer:  40% 3207/8047 [2:03:24<4:02:28,  3.01s/it]loss_total_epoch 194.09920640476048
Training tokenizer:  40% 3208/8047 [2:03:28<4:03:17,  3.02s/it]loss_total_epoch 194.15256620012224
Training tokenizer:  40% 3209/8047 [2:03:31<4:02:50,  3.01s/it]loss_total_epoch 194.20412636734545
Training tokenizer:  40% 3210/8047 [2:03:34<4:03:19,  3.02s/it]loss_total_epoch 194.25628414936364
Training tokenizer:  40% 3211/8047 [2:03:37<4:03:29,  3.02s/it]loss_total_epoch 194.3068646658212
Training tokenizer:  40% 3212/8047 [2:03:40<4:03:38,  3.02s/it]loss_total_epoch 194.3678526300937
Training tokenizer:  40% 3213/8047 [2:03:43<4:02:33,  3.01s/it]loss_total_epoch 194.4059760402888
Training tokenizer:  40% 3214/8047 [2:03:46<4:04:09,  3.03s/it]loss_total_epoch 194.451241357252
Training tokenizer:  40% 3215/8047 [2:03:49<4:04:33,  3.04s/it]loss_total_epoch 194.50099963881075
Training tokenizer:  40% 3216/8047 [2:03:52<4:04:28,  3.04s/it]loss_total_epoch 194.56104177422822
Training tokenizer:  40% 3217/8047 [2:03:55<4:04:03,  3.03s/it]loss_total_epoch 194.6058685835451
Training tokenizer:  40% 3218/8047 [2:03:58<4:04:23,  3.04s/it]loss_total_epoch 194.64515946619213
Training tokenizer:  40% 3219/8047 [2:04:01<4:03:19,  3.02s/it]loss_total_epoch 194.69784084893763
Training tokenizer:  40% 3220/8047 [2:04:04<4:04:15,  3.04s/it]loss_total_epoch 194.75027378462255
Training tokenizer:  40% 3221/8047 [2:04:07<4:03:31,  3.03s/it]loss_total_epoch 194.79490006156266
Training tokenizer:  40% 3222/8047 [2:04:10<4:03:58,  3.03s/it]loss_total_epoch 194.83864428289235
Training tokenizer:  40% 3223/8047 [2:04:13<4:04:36,  3.04s/it]loss_total_epoch 194.88543910346925
Training tokenizer:  40% 3224/8047 [2:04:16<4:04:11,  3.04s/it]loss_total_epoch 194.93746499158442
Training tokenizer:  40% 3225/8047 [2:04:19<4:03:56,  3.04s/it]loss_total_epoch 194.98364380188286
Training tokenizer:  40% 3226/8047 [2:04:22<4:03:00,  3.02s/it]loss_total_epoch 195.03202078677714
Training tokenizer:  40% 3227/8047 [2:04:25<4:01:17,  3.00s/it]loss_total_epoch 195.07659360952675
Training tokenizer:  40% 3228/8047 [2:04:28<4:01:31,  3.01s/it]loss_total_epoch 195.1234467048198
Training tokenizer:  40% 3229/8047 [2:04:31<3:59:43,  2.99s/it]loss_total_epoch 195.16120254807174
Training tokenizer:  40% 3230/8047 [2:04:34<4:00:53,  3.00s/it]loss_total_epoch 195.2110015656799
Training tokenizer:  40% 3231/8047 [2:04:37<4:01:17,  3.01s/it]loss_total_epoch 195.25730301626027
Training tokenizer:  40% 3232/8047 [2:04:40<4:02:40,  3.02s/it]loss_total_epoch 195.29156376607716
Training tokenizer:  40% 3233/8047 [2:04:43<4:02:50,  3.03s/it]loss_total_epoch 195.34934199787676
Training tokenizer:  40% 3234/8047 [2:04:46<4:03:40,  3.04s/it]loss_total_epoch 195.40087135322392
Training tokenizer:  40% 3235/8047 [2:04:49<4:03:44,  3.04s/it]loss_total_epoch 195.4502937514335
Training tokenizer:  40% 3236/8047 [2:04:52<4:03:43,  3.04s/it]loss_total_epoch 195.50932033918798
Training tokenizer:  40% 3237/8047 [2:04:55<4:04:16,  3.05s/it]loss_total_epoch 195.5591761711985
Training tokenizer:  40% 3238/8047 [2:04:58<4:04:18,  3.05s/it]loss_total_epoch 195.61221786029637
Training tokenizer:  40% 3239/8047 [2:05:01<4:03:42,  3.04s/it]loss_total_epoch 195.6637905780226
Training tokenizer:  40% 3240/8047 [2:05:04<4:03:34,  3.04s/it]loss_total_epoch 195.71099224500358
Training tokenizer:  40% 3241/8047 [2:05:07<4:03:35,  3.04s/it]loss_total_epoch 195.7564022038132
Training tokenizer:  40% 3242/8047 [2:05:10<4:02:29,  3.03s/it]loss_total_epoch 195.80118074454367
Training tokenizer:  40% 3243/8047 [2:05:14<4:02:53,  3.03s/it]loss_total_epoch 195.86605704762042
Training tokenizer:  40% 3244/8047 [2:05:17<4:02:16,  3.03s/it]loss_total_epoch 195.9213137049228
Training tokenizer:  40% 3245/8047 [2:05:20<4:03:13,  3.04s/it]loss_total_epoch 195.96366831101477
Training tokenizer:  40% 3246/8047 [2:05:23<4:02:36,  3.03s/it]loss_total_epoch 196.01315856166184
Training tokenizer:  40% 3247/8047 [2:05:26<4:03:01,  3.04s/it]loss_total_epoch 196.06649372912943
Training tokenizer:  40% 3248/8047 [2:05:29<4:03:11,  3.04s/it]loss_total_epoch 196.12180732004344
Training tokenizer:  40% 3249/8047 [2:05:32<4:02:13,  3.03s/it]loss_total_epoch 196.17046280391514
Training tokenizer:  40% 3250/8047 [2:05:35<4:00:50,  3.01s/it]loss_total_epoch 196.2258137036115
Training tokenizer:  40% 3251/8047 [2:05:38<4:00:21,  3.01s/it]loss_total_epoch 196.2712139915675
Training tokenizer:  40% 3252/8047 [2:05:41<4:01:27,  3.02s/it]loss_total_epoch 196.321511330083
Training tokenizer:  40% 3253/8047 [2:05:44<4:01:45,  3.03s/it]loss_total_epoch 196.37286594323814
Training tokenizer:  40% 3254/8047 [2:05:47<4:01:40,  3.03s/it]loss_total_epoch 196.41576798819005
Training tokenizer:  40% 3255/8047 [2:05:50<4:01:22,  3.02s/it]loss_total_epoch 196.47849646769464
Training tokenizer:  40% 3256/8047 [2:05:53<4:01:32,  3.02s/it]loss_total_epoch 196.53015466593206
Training tokenizer:  40% 3257/8047 [2:05:56<4:02:33,  3.04s/it]loss_total_epoch 196.58695360086858
Training tokenizer:  40% 3258/8047 [2:05:59<4:03:21,  3.05s/it]loss_total_epoch 196.6294285338372
Training tokenizer:  40% 3259/8047 [2:06:02<4:02:09,  3.03s/it]loss_total_epoch 196.68539824523032
Training tokenizer:  41% 3260/8047 [2:06:05<4:03:08,  3.05s/it]loss_total_epoch 196.72969938255847
Training tokenizer:  41% 3261/8047 [2:06:08<4:03:02,  3.05s/it]loss_total_epoch 196.7700338382274
Training tokenizer:  41% 3262/8047 [2:06:11<4:02:20,  3.04s/it]loss_total_epoch 196.81234995462
Training tokenizer:  41% 3263/8047 [2:06:14<4:02:18,  3.04s/it]loss_total_epoch 196.8729637172073
Training tokenizer:  41% 3264/8047 [2:06:17<4:01:20,  3.03s/it]loss_total_epoch 196.93261901848018
Training tokenizer:  41% 3265/8047 [2:06:20<4:01:46,  3.03s/it]loss_total_epoch 196.97577517293394
Training tokenizer:  41% 3266/8047 [2:06:23<4:01:40,  3.03s/it]loss_total_epoch 197.0348719637841
Training tokenizer:  41% 3267/8047 [2:06:26<4:00:59,  3.03s/it]loss_total_epoch 197.09684306941926
Training tokenizer:  41% 3268/8047 [2:06:29<4:02:44,  3.05s/it]loss_total_epoch 197.15187171660364
Training tokenizer:  41% 3269/8047 [2:06:32<4:03:47,  3.06s/it]loss_total_epoch 197.21869916282594
Training tokenizer:  41% 3270/8047 [2:06:35<4:02:58,  3.05s/it]loss_total_epoch 197.2658397462219
Training tokenizer:  41% 3271/8047 [2:06:39<4:02:32,  3.05s/it]loss_total_epoch 197.31741664372385
Training tokenizer:  41% 3272/8047 [2:06:42<4:02:09,  3.04s/it]loss_total_epoch 197.35917106084526
Training tokenizer:  41% 3273/8047 [2:06:45<4:02:28,  3.05s/it]loss_total_epoch 197.41592377983034
Training tokenizer:  41% 3274/8047 [2:06:48<4:03:00,  3.05s/it]loss_total_epoch 197.48932929895818
Training tokenizer:  41% 3275/8047 [2:06:51<4:03:25,  3.06s/it]loss_total_epoch 197.5366797503084
Training tokenizer:  41% 3276/8047 [2:06:54<4:03:17,  3.06s/it]loss_total_epoch 197.58099619857967
Training tokenizer:  41% 3277/8047 [2:06:57<4:03:11,  3.06s/it]loss_total_epoch 197.62925901822746
Training tokenizer:  41% 3278/8047 [2:07:00<4:03:19,  3.06s/it]loss_total_epoch 197.67767390049994
Training tokenizer:  41% 3279/8047 [2:07:03<4:01:36,  3.04s/it]loss_total_epoch 197.72392663545907
Training tokenizer:  41% 3280/8047 [2:07:06<4:01:42,  3.04s/it]loss_total_epoch 197.770282195881
Training tokenizer:  41% 3281/8047 [2:07:09<4:01:17,  3.04s/it]loss_total_epoch 197.81742289103568
Training tokenizer:  41% 3282/8047 [2:07:12<4:02:13,  3.05s/it]loss_total_epoch 197.86485096253455
Training tokenizer:  41% 3283/8047 [2:07:15<4:02:47,  3.06s/it]loss_total_epoch 197.91433270834386
Training tokenizer:  41% 3284/8047 [2:07:18<4:02:26,  3.05s/it]loss_total_epoch 197.96978673152626
Training tokenizer:  41% 3285/8047 [2:07:21<4:01:50,  3.05s/it]loss_total_epoch 198.00647244416177
Training tokenizer:  41% 3286/8047 [2:07:24<4:03:04,  3.06s/it]loss_total_epoch 198.05430493317544
Training tokenizer:  41% 3287/8047 [2:07:27<4:03:06,  3.06s/it]loss_total_epoch 198.10764166153967
Training tokenizer:  41% 3288/8047 [2:07:30<4:01:30,  3.04s/it]loss_total_epoch 198.153264882043
Training tokenizer:  41% 3289/8047 [2:07:33<4:01:20,  3.04s/it]loss_total_epoch 198.20893882401288
Training tokenizer:  41% 3290/8047 [2:07:37<4:01:34,  3.05s/it]loss_total_epoch 198.2530601825565
Training tokenizer:  41% 3291/8047 [2:07:40<4:03:31,  3.07s/it]loss_total_epoch 198.30634584091604
Training tokenizer:  41% 3292/8047 [2:07:43<4:02:32,  3.06s/it]loss_total_epoch 198.35573531128466
Training tokenizer:  41% 3293/8047 [2:07:46<4:02:52,  3.07s/it]loss_total_epoch 198.40500702895224
Training tokenizer:  41% 3294/8047 [2:07:49<4:02:05,  3.06s/it]loss_total_epoch 198.45089468546212
Training tokenizer:  41% 3295/8047 [2:07:52<4:03:18,  3.07s/it]loss_total_epoch 198.49924696050584
Training tokenizer:  41% 3296/8047 [2:07:55<4:02:56,  3.07s/it]loss_total_epoch 198.53427856229246
Training tokenizer:  41% 3297/8047 [2:07:58<4:02:55,  3.07s/it]loss_total_epoch 198.5891057383269
Training tokenizer:  41% 3298/8047 [2:08:01<4:02:47,  3.07s/it]loss_total_epoch 198.64533820562065
Training tokenizer:  41% 3299/8047 [2:08:04<4:02:32,  3.06s/it]loss_total_epoch 198.70215388573706
Training tokenizer:  41% 3300/8047 [2:08:07<4:02:58,  3.07s/it]loss_total_epoch 198.75137970782816
Training tokenizer:  41% 3301/8047 [2:08:10<4:02:34,  3.07s/it]loss_total_epoch 198.8162470702082
Training tokenizer:  41% 3302/8047 [2:08:13<4:02:27,  3.07s/it]loss_total_epoch 198.87017321027815
Training tokenizer:  41% 3303/8047 [2:08:16<4:02:11,  3.06s/it]loss_total_epoch 198.9212682042271
Training tokenizer:  41% 3304/8047 [2:08:19<4:02:34,  3.07s/it]loss_total_epoch 198.97104349918664
Training tokenizer:  41% 3305/8047 [2:08:23<4:02:46,  3.07s/it]loss_total_epoch 199.0141181331128
Training tokenizer:  41% 3306/8047 [2:08:26<4:02:22,  3.07s/it]loss_total_epoch 199.05313139222562
Training tokenizer:  41% 3307/8047 [2:08:29<4:02:33,  3.07s/it]loss_total_epoch 199.0953224208206
Training tokenizer:  41% 3308/8047 [2:08:32<4:03:11,  3.08s/it]loss_total_epoch 199.14941882155836
Training tokenizer:  41% 3309/8047 [2:08:35<4:02:25,  3.07s/it]loss_total_epoch 199.18019736744463
Training tokenizer:  41% 3310/8047 [2:08:38<4:01:26,  3.06s/it]loss_total_epoch 199.23201790638268
Training tokenizer:  41% 3311/8047 [2:08:41<4:02:53,  3.08s/it]loss_total_epoch 199.2959736455232
Training tokenizer:  41% 3312/8047 [2:08:44<4:02:56,  3.08s/it]loss_total_epoch 199.34178506024182
Training tokenizer:  41% 3313/8047 [2:08:47<4:02:15,  3.07s/it]loss_total_epoch 199.3841752205044
Training tokenizer:  41% 3314/8047 [2:08:50<4:02:17,  3.07s/it]loss_total_epoch 199.4386712592095
Training tokenizer:  41% 3315/8047 [2:08:53<4:02:22,  3.07s/it]loss_total_epoch 199.4943732265383
Training tokenizer:  41% 3316/8047 [2:08:56<4:03:01,  3.08s/it]loss_total_epoch 199.54659580253065
Training tokenizer:  41% 3317/8047 [2:08:59<4:03:18,  3.09s/it]loss_total_epoch 199.5873209964484
Training tokenizer:  41% 3318/8047 [2:09:03<4:03:11,  3.09s/it]loss_total_epoch 199.6308068688959
Training tokenizer:  41% 3319/8047 [2:09:06<4:04:26,  3.10s/it]loss_total_epoch 199.68287439458072
Training tokenizer:  41% 3320/8047 [2:09:09<4:04:35,  3.10s/it]loss_total_epoch 199.73849466629326
Training tokenizer:  41% 3321/8047 [2:09:12<4:03:36,  3.09s/it]loss_total_epoch 199.79152077250183
Training tokenizer:  41% 3322/8047 [2:09:15<4:02:46,  3.08s/it]loss_total_epoch 199.83428569324315
Training tokenizer:  41% 3323/8047 [2:09:18<4:02:33,  3.08s/it]loss_total_epoch 199.87593482248485
Training tokenizer:  41% 3324/8047 [2:09:21<4:02:07,  3.08s/it]loss_total_epoch 199.92783687449992
Training tokenizer:  41% 3325/8047 [2:09:24<4:01:53,  3.07s/it]loss_total_epoch 199.97202420048416
Training tokenizer:  41% 3326/8047 [2:09:27<4:01:38,  3.07s/it]loss_total_epoch 200.0092694554478
Training tokenizer:  41% 3327/8047 [2:09:30<4:02:37,  3.08s/it]loss_total_epoch 200.04906828142703
Training tokenizer:  41% 3328/8047 [2:09:33<4:01:59,  3.08s/it]loss_total_epoch 200.09730170480907
Training tokenizer:  41% 3329/8047 [2:09:36<4:02:18,  3.08s/it]loss_total_epoch 200.14831225387752
Training tokenizer:  41% 3330/8047 [2:09:40<4:02:23,  3.08s/it]loss_total_epoch 200.20709299854934
Training tokenizer:  41% 3331/8047 [2:09:43<4:09:58,  3.18s/it]loss_total_epoch 200.26270679198205
Training tokenizer:  41% 3332/8047 [2:09:46<4:07:19,  3.15s/it]loss_total_epoch 200.31538806669414
Training tokenizer:  41% 3333/8047 [2:09:49<4:05:40,  3.13s/it]loss_total_epoch 200.36930869333446
Training tokenizer:  41% 3334/8047 [2:09:52<4:05:04,  3.12s/it]loss_total_epoch 200.41438024304807
Training tokenizer:  41% 3335/8047 [2:09:55<4:04:14,  3.11s/it]loss_total_epoch 200.46127793379128
Training tokenizer:  41% 3336/8047 [2:09:58<4:02:47,  3.09s/it]loss_total_epoch 200.51241070218384
Training tokenizer:  41% 3337/8047 [2:10:01<4:02:22,  3.09s/it]loss_total_epoch 200.56416750885546
Training tokenizer:  41% 3338/8047 [2:10:05<4:02:13,  3.09s/it]loss_total_epoch 200.6236385460943
Training tokenizer:  41% 3339/8047 [2:10:08<4:02:19,  3.09s/it]loss_total_epoch 200.68375552259386
Training tokenizer:  42% 3340/8047 [2:10:11<4:02:35,  3.09s/it]loss_total_epoch 200.72653684578836
Training tokenizer:  42% 3341/8047 [2:10:14<4:03:44,  3.11s/it]loss_total_epoch 200.79008068703115
Training tokenizer:  42% 3342/8047 [2:10:17<4:01:42,  3.08s/it]loss_total_epoch 200.8542192634195
Training tokenizer:  42% 3343/8047 [2:10:20<4:00:32,  3.07s/it]loss_total_epoch 200.90471715666354
Training tokenizer:  42% 3344/8047 [2:10:23<4:00:41,  3.07s/it]loss_total_epoch 200.95354390330613
Training tokenizer:  42% 3345/8047 [2:10:26<4:00:39,  3.07s/it]loss_total_epoch 200.9984486643225
Training tokenizer:  42% 3346/8047 [2:10:29<4:01:02,  3.08s/it]loss_total_epoch 201.05448205210268
Training tokenizer:  42% 3347/8047 [2:10:32<4:00:05,  3.07s/it]loss_total_epoch 201.10644336231053
Training tokenizer:  42% 3348/8047 [2:10:35<4:00:51,  3.08s/it]loss_total_epoch 201.1529538128525
Training tokenizer:  42% 3349/8047 [2:10:38<4:01:00,  3.08s/it]loss_total_epoch 201.20384255982935
Training tokenizer:  42% 3350/8047 [2:10:41<4:01:28,  3.08s/it]loss_total_epoch 201.2661407981068
Training tokenizer:  42% 3351/8047 [2:10:45<4:01:35,  3.09s/it]loss_total_epoch 201.3175192642957
Training tokenizer:  42% 3352/8047 [2:10:48<4:00:42,  3.08s/it]loss_total_epoch 201.36484448798
Training tokenizer:  42% 3353/8047 [2:10:51<3:58:41,  3.05s/it]loss_total_epoch 201.41549365781248
Training tokenizer:  42% 3354/8047 [2:10:54<3:59:34,  3.06s/it]loss_total_epoch 201.47192302532494
Training tokenizer:  42% 3355/8047 [2:10:57<4:01:09,  3.08s/it]loss_total_epoch 201.51491175033152
Training tokenizer:  42% 3356/8047 [2:11:00<4:01:52,  3.09s/it]loss_total_epoch 201.56217071972787
Training tokenizer:  42% 3357/8047 [2:11:03<4:01:38,  3.09s/it]loss_total_epoch 201.6129428949207
Training tokenizer:  42% 3358/8047 [2:11:06<4:03:03,  3.11s/it]loss_total_epoch 201.67547108791769
Training tokenizer:  42% 3359/8047 [2:11:09<4:02:22,  3.10s/it]loss_total_epoch 201.72665807418525
Training tokenizer:  42% 3360/8047 [2:11:12<4:02:53,  3.11s/it]loss_total_epoch 201.76799940876663
Training tokenizer:  42% 3361/8047 [2:11:15<4:02:17,  3.10s/it]loss_total_epoch 201.80730931274593
Training tokenizer:  42% 3362/8047 [2:11:19<4:02:02,  3.10s/it]loss_total_epoch 201.84432866610587
Training tokenizer:  42% 3363/8047 [2:11:22<4:00:57,  3.09s/it]loss_total_epoch 201.90266701765358
Training tokenizer:  42% 3364/8047 [2:11:25<4:01:22,  3.09s/it]loss_total_epoch 201.95921758003533
Training tokenizer:  42% 3365/8047 [2:11:28<4:00:50,  3.09s/it]loss_total_epoch 202.01280632428825
Training tokenizer:  42% 3366/8047 [2:11:31<4:00:38,  3.08s/it]loss_total_epoch 202.0619957279414
Training tokenizer:  42% 3367/8047 [2:11:34<4:01:05,  3.09s/it]loss_total_epoch 202.1076618526131
Training tokenizer:  42% 3368/8047 [2:11:37<4:02:26,  3.11s/it]loss_total_epoch 202.1536989863962
Training tokenizer:  42% 3369/8047 [2:11:40<4:02:21,  3.11s/it]loss_total_epoch 202.20483153127134
Training tokenizer:  42% 3370/8047 [2:11:43<4:01:48,  3.10s/it]loss_total_epoch 202.24841150455177
Training tokenizer:  42% 3371/8047 [2:11:46<4:01:40,  3.10s/it]loss_total_epoch 202.30959534086287
Training tokenizer:  42% 3372/8047 [2:11:50<4:02:39,  3.11s/it]loss_total_epoch 202.3649200964719
Training tokenizer:  42% 3373/8047 [2:11:53<4:02:50,  3.12s/it]loss_total_epoch 202.4164263959974
Training tokenizer:  42% 3374/8047 [2:11:56<4:03:37,  3.13s/it]loss_total_epoch 202.47595283947885
Training tokenizer:  42% 3375/8047 [2:11:59<4:03:57,  3.13s/it]loss_total_epoch 202.53848974965513
Training tokenizer:  42% 3376/8047 [2:12:02<4:02:46,  3.12s/it]loss_total_epoch 202.58708807639778
Training tokenizer:  42% 3377/8047 [2:12:05<4:01:56,  3.11s/it]loss_total_epoch 202.63491117022932
Training tokenizer:  42% 3378/8047 [2:12:08<4:01:19,  3.10s/it]loss_total_epoch 202.6861651968211
Training tokenizer:  42% 3379/8047 [2:12:11<4:00:23,  3.09s/it]loss_total_epoch 202.73277982138097
Training tokenizer:  42% 3380/8047 [2:12:14<3:59:16,  3.08s/it]loss_total_epoch 202.79537677578628
Training tokenizer:  42% 3381/8047 [2:12:17<3:59:22,  3.08s/it]loss_total_epoch 202.85036041028798
Training tokenizer:  42% 3382/8047 [2:12:21<4:00:31,  3.09s/it]loss_total_epoch 202.90027280710638
Training tokenizer:  42% 3383/8047 [2:12:24<4:02:03,  3.11s/it]loss_total_epoch 202.95861264131963
Training tokenizer:  42% 3384/8047 [2:12:27<4:01:51,  3.11s/it]loss_total_epoch 203.01160404644907
Training tokenizer:  42% 3385/8047 [2:12:30<4:01:18,  3.11s/it]loss_total_epoch 203.05334573052824
Training tokenizer:  42% 3386/8047 [2:12:33<4:01:28,  3.11s/it]loss_total_epoch 203.0989390965551
Training tokenizer:  42% 3387/8047 [2:12:36<4:01:15,  3.11s/it]loss_total_epoch 203.15141456387937
Training tokenizer:  42% 3388/8047 [2:12:39<4:00:59,  3.10s/it]loss_total_epoch 203.20252297632396
Training tokenizer:  42% 3389/8047 [2:12:42<4:00:11,  3.09s/it]loss_total_epoch 203.24658601917326
Training tokenizer:  42% 3390/8047 [2:12:45<3:59:55,  3.09s/it]loss_total_epoch 203.2976755630225
Training tokenizer:  42% 3391/8047 [2:12:49<4:00:51,  3.10s/it]loss_total_epoch 203.3535234387964
Training tokenizer:  42% 3392/8047 [2:12:52<4:01:00,  3.11s/it]loss_total_epoch 203.40043653734028
Training tokenizer:  42% 3393/8047 [2:12:55<4:01:21,  3.11s/it]loss_total_epoch 203.46055292524397
Training tokenizer:  42% 3394/8047 [2:12:58<4:01:42,  3.12s/it]loss_total_epoch 203.52141799591482
Training tokenizer:  42% 3395/8047 [2:13:01<4:01:48,  3.12s/it]loss_total_epoch 203.57437760569155
Training tokenizer:  42% 3396/8047 [2:13:04<4:02:11,  3.12s/it]loss_total_epoch 203.61910359747708
Training tokenizer:  42% 3397/8047 [2:13:07<4:01:32,  3.12s/it]loss_total_epoch 203.66375623084605
Training tokenizer:  42% 3398/8047 [2:13:10<4:01:14,  3.11s/it]loss_total_epoch 203.73233174718916
Training tokenizer:  42% 3399/8047 [2:13:13<4:00:44,  3.11s/it]loss_total_epoch 203.786451311782
Training tokenizer:  42% 3400/8047 [2:13:17<3:59:55,  3.10s/it]loss_total_epoch 203.84906400181353
Training tokenizer:  42% 3401/8047 [2:13:20<4:00:03,  3.10s/it]loss_total_epoch 203.8947387058288
Training tokenizer:  42% 3402/8047 [2:13:23<3:59:59,  3.10s/it]loss_total_epoch 203.9522909987718
Training tokenizer:  42% 3403/8047 [2:13:26<4:00:07,  3.10s/it]loss_total_epoch 204.0138600450009
Training tokenizer:  42% 3404/8047 [2:13:29<3:59:52,  3.10s/it]loss_total_epoch 204.06562253646553
Training tokenizer:  42% 3405/8047 [2:13:32<4:00:31,  3.11s/it]loss_total_epoch 204.10875617526472
Training tokenizer:  42% 3406/8047 [2:13:35<4:00:56,  3.12s/it]loss_total_epoch 204.15966429002583
Training tokenizer:  42% 3407/8047 [2:13:38<4:00:20,  3.11s/it]loss_total_epoch 204.2123316358775
Training tokenizer:  42% 3408/8047 [2:13:41<4:00:22,  3.11s/it]loss_total_epoch 204.26871196366847
Training tokenizer:  42% 3409/8047 [2:13:45<4:01:06,  3.12s/it]loss_total_epoch 204.32489377073944
Training tokenizer:  42% 3410/8047 [2:13:48<4:01:19,  3.12s/it]loss_total_epoch 204.38545661605895
Training tokenizer:  42% 3411/8047 [2:13:51<4:01:53,  3.13s/it]loss_total_epoch 204.4316926766187
Training tokenizer:  42% 3412/8047 [2:13:54<4:02:09,  3.13s/it]loss_total_epoch 204.4964258018881
Training tokenizer:  42% 3413/8047 [2:13:57<4:02:01,  3.13s/it]loss_total_epoch 204.5375496391207
Training tokenizer:  42% 3414/8047 [2:14:00<4:02:21,  3.14s/it]loss_total_epoch 204.58160390891135
Training tokenizer:  42% 3415/8047 [2:14:03<4:01:54,  3.13s/it]loss_total_epoch 204.61819547601044
Training tokenizer:  42% 3416/8047 [2:14:06<4:01:02,  3.12s/it]loss_total_epoch 204.67305027134717
Training tokenizer:  42% 3417/8047 [2:14:10<4:00:27,  3.12s/it]loss_total_epoch 204.72160297445953
Training tokenizer:  42% 3418/8047 [2:14:13<3:58:34,  3.09s/it]loss_total_epoch 204.77247521840036
Training tokenizer:  42% 3419/8047 [2:14:16<3:59:06,  3.10s/it]loss_total_epoch 204.81657834909856
Training tokenizer:  43% 3420/8047 [2:14:19<4:00:16,  3.12s/it]loss_total_epoch 204.8667347934097
Training tokenizer:  43% 3421/8047 [2:14:22<4:00:35,  3.12s/it]loss_total_epoch 204.92047767527401
Training tokenizer:  43% 3422/8047 [2:14:25<4:00:45,  3.12s/it]loss_total_epoch 204.96970609016716
Training tokenizer:  43% 3423/8047 [2:14:28<4:00:56,  3.13s/it]loss_total_epoch 205.03208303265274
Training tokenizer:  43% 3424/8047 [2:14:31<4:00:21,  3.12s/it]loss_total_epoch 205.0723257381469
Training tokenizer:  43% 3425/8047 [2:14:35<4:00:47,  3.13s/it]loss_total_epoch 205.12700675614178
Training tokenizer:  43% 3426/8047 [2:14:38<4:00:24,  3.12s/it]loss_total_epoch 205.17818230949342
Training tokenizer:  43% 3427/8047 [2:14:41<4:00:45,  3.13s/it]loss_total_epoch 205.23534529469907
Training tokenizer:  43% 3428/8047 [2:14:44<4:01:02,  3.13s/it]loss_total_epoch 205.29015821777284
Training tokenizer:  43% 3429/8047 [2:14:47<3:59:44,  3.11s/it]loss_total_epoch 205.3411241080612
Training tokenizer:  43% 3430/8047 [2:14:50<4:00:06,  3.12s/it]loss_total_epoch 205.39028922654688
Training tokenizer:  43% 3431/8047 [2:14:53<3:59:52,  3.12s/it]loss_total_epoch 205.44381961412728
Training tokenizer:  43% 3432/8047 [2:14:56<4:00:01,  3.12s/it]loss_total_epoch 205.49936256371439
Training tokenizer:  43% 3433/8047 [2:14:59<3:59:29,  3.11s/it]loss_total_epoch 205.5575900543481
Training tokenizer:  43% 3434/8047 [2:15:03<4:00:40,  3.13s/it]loss_total_epoch 205.599457828328
Training tokenizer:  43% 3435/8047 [2:15:06<4:01:50,  3.15s/it]loss_total_epoch 205.6416246574372
Training tokenizer:  43% 3436/8047 [2:15:09<4:01:40,  3.14s/it]loss_total_epoch 205.68087426759303
Training tokenizer:  43% 3437/8047 [2:15:12<4:02:02,  3.15s/it]loss_total_epoch 205.73422496579587
Training tokenizer:  43% 3438/8047 [2:15:15<4:01:20,  3.14s/it]loss_total_epoch 205.7796679381281
Training tokenizer:  43% 3439/8047 [2:15:18<4:01:37,  3.15s/it]loss_total_epoch 205.83167698420584
Training tokenizer:  43% 3440/8047 [2:15:22<4:01:35,  3.15s/it]loss_total_epoch 205.88417158462107
Training tokenizer:  43% 3441/8047 [2:15:25<4:01:18,  3.14s/it]loss_total_epoch 205.93067743070424
Training tokenizer:  43% 3442/8047 [2:15:28<4:01:43,  3.15s/it]loss_total_epoch 205.98671020381153
Training tokenizer:  43% 3443/8047 [2:15:31<4:01:36,  3.15s/it]loss_total_epoch 206.02710101939738
Training tokenizer:  43% 3444/8047 [2:15:34<4:00:52,  3.14s/it]loss_total_epoch 206.08653237111866
Training tokenizer:  43% 3445/8047 [2:15:37<4:00:54,  3.14s/it]loss_total_epoch 206.13841301389039
Training tokenizer:  43% 3446/8047 [2:15:40<4:01:00,  3.14s/it]loss_total_epoch 206.18076320923865
Training tokenizer:  43% 3447/8047 [2:15:44<4:01:35,  3.15s/it]loss_total_epoch 206.22741092182696
Training tokenizer:  43% 3448/8047 [2:15:47<4:00:05,  3.13s/it]loss_total_epoch 206.26657865010202
Training tokenizer:  43% 3449/8047 [2:15:50<4:00:33,  3.14s/it]loss_total_epoch 206.31513425521553
Training tokenizer:  43% 3450/8047 [2:15:53<4:00:29,  3.14s/it]loss_total_epoch 206.3698167782277
Training tokenizer:  43% 3451/8047 [2:15:56<4:00:28,  3.14s/it]loss_total_epoch 206.41333069093525
Training tokenizer:  43% 3452/8047 [2:15:59<4:00:33,  3.14s/it]loss_total_epoch 206.46040633879602
Training tokenizer:  43% 3453/8047 [2:16:02<4:00:36,  3.14s/it]loss_total_epoch 206.5133177358657
Training tokenizer:  43% 3454/8047 [2:16:06<4:00:07,  3.14s/it]loss_total_epoch 206.56115076877177
Training tokenizer:  43% 3455/8047 [2:16:09<3:59:41,  3.13s/it]loss_total_epoch 206.61422658152878
Training tokenizer:  43% 3456/8047 [2:16:12<3:59:44,  3.13s/it]loss_total_epoch 206.657822297886
Training tokenizer:  43% 3457/8047 [2:16:15<4:00:02,  3.14s/it]loss_total_epoch 206.69850330613554
Training tokenizer:  43% 3458/8047 [2:16:18<4:00:21,  3.14s/it]loss_total_epoch 206.74464203231037
Training tokenizer:  43% 3459/8047 [2:16:21<3:59:38,  3.13s/it]loss_total_epoch 206.7981603052467
Training tokenizer:  43% 3460/8047 [2:16:24<4:00:34,  3.15s/it]loss_total_epoch 206.8497038502246
Training tokenizer:  43% 3461/8047 [2:16:27<3:59:42,  3.14s/it]loss_total_epoch 206.89932706020772
Training tokenizer:  43% 3462/8047 [2:16:31<4:00:39,  3.15s/it]loss_total_epoch 206.95404480211437
Training tokenizer:  43% 3463/8047 [2:16:34<4:01:03,  3.16s/it]loss_total_epoch 206.99712698720396
Training tokenizer:  43% 3464/8047 [2:16:37<4:01:27,  3.16s/it]loss_total_epoch 207.03815210424364
Training tokenizer:  43% 3465/8047 [2:16:40<4:00:55,  3.15s/it]loss_total_epoch 207.10077667795122
Training tokenizer:  43% 3466/8047 [2:16:43<4:00:22,  3.15s/it]loss_total_epoch 207.15691541694105
Training tokenizer:  43% 3467/8047 [2:16:46<4:00:05,  3.15s/it]loss_total_epoch 207.20890366844833
Training tokenizer:  43% 3468/8047 [2:16:50<4:00:34,  3.15s/it]loss_total_epoch 207.26984446309507
Training tokenizer:  43% 3469/8047 [2:16:53<4:01:01,  3.16s/it]loss_total_epoch 207.31984504126012
Training tokenizer:  43% 3470/8047 [2:16:56<4:00:36,  3.15s/it]loss_total_epoch 207.36811978183687
Training tokenizer:  43% 3471/8047 [2:16:59<4:01:07,  3.16s/it]loss_total_epoch 207.42520388774574
Training tokenizer:  43% 3472/8047 [2:17:02<4:00:45,  3.16s/it]loss_total_epoch 207.4733468350023
Training tokenizer:  43% 3473/8047 [2:17:05<4:00:59,  3.16s/it]loss_total_epoch 207.518418001011
Training tokenizer:  43% 3474/8047 [2:17:09<4:01:26,  3.17s/it]loss_total_epoch 207.5666415411979
Training tokenizer:  43% 3475/8047 [2:17:12<4:01:20,  3.17s/it]loss_total_epoch 207.61735113151371
Training tokenizer:  43% 3476/8047 [2:17:15<4:00:49,  3.16s/it]loss_total_epoch 207.66471374593675
Training tokenizer:  43% 3477/8047 [2:17:18<3:59:59,  3.15s/it]loss_total_epoch 207.71179631911218
Training tokenizer:  43% 3478/8047 [2:17:21<3:58:57,  3.14s/it]loss_total_epoch 207.76242890767753
Training tokenizer:  43% 3479/8047 [2:17:24<3:59:15,  3.14s/it]loss_total_epoch 207.81204443983734
Training tokenizer:  43% 3480/8047 [2:17:27<4:00:03,  3.15s/it]loss_total_epoch 207.85654349811375
Training tokenizer:  43% 3481/8047 [2:17:31<4:00:20,  3.16s/it]loss_total_epoch 207.90216900967062
Training tokenizer:  43% 3482/8047 [2:17:34<3:59:45,  3.15s/it]loss_total_epoch 207.95366757921875
Training tokenizer:  43% 3483/8047 [2:17:37<4:00:02,  3.16s/it]loss_total_epoch 208.0055041257292
Training tokenizer:  43% 3484/8047 [2:17:40<3:59:55,  3.15s/it]loss_total_epoch 208.06188560463488
Training tokenizer:  43% 3485/8047 [2:17:43<3:59:33,  3.15s/it]loss_total_epoch 208.09899531118572
Training tokenizer:  43% 3486/8047 [2:17:46<3:59:56,  3.16s/it]loss_total_epoch 208.15006623230875
Training tokenizer:  43% 3487/8047 [2:17:50<4:00:14,  3.16s/it]loss_total_epoch 208.20330091007054
Training tokenizer:  43% 3488/8047 [2:17:53<3:59:13,  3.15s/it]loss_total_epoch 208.24787791632116
Training tokenizer:  43% 3489/8047 [2:17:56<3:58:14,  3.14s/it]loss_total_epoch 208.28783663176
Training tokenizer:  43% 3490/8047 [2:17:59<3:58:59,  3.15s/it]loss_total_epoch 208.33021608181298
Training tokenizer:  43% 3491/8047 [2:18:02<3:59:30,  3.15s/it]loss_total_epoch 208.38373723439872
Training tokenizer:  43% 3492/8047 [2:18:05<3:59:09,  3.15s/it]loss_total_epoch 208.43539981730282
Training tokenizer:  43% 3493/8047 [2:18:08<4:00:25,  3.17s/it]loss_total_epoch 208.47183016501367
Training tokenizer:  43% 3494/8047 [2:18:12<4:00:52,  3.17s/it]loss_total_epoch 208.52351678349078
Training tokenizer:  43% 3495/8047 [2:18:15<3:59:50,  3.16s/it]loss_total_epoch 208.58262221701443
Training tokenizer:  43% 3496/8047 [2:18:18<3:59:23,  3.16s/it]loss_total_epoch 208.63375122658908
Training tokenizer:  43% 3497/8047 [2:18:21<3:59:21,  3.16s/it]loss_total_epoch 208.68929422087967
Training tokenizer:  43% 3498/8047 [2:18:24<3:59:03,  3.15s/it]loss_total_epoch 208.74269832856953
Training tokenizer:  43% 3499/8047 [2:18:27<3:59:07,  3.15s/it]loss_total_epoch 208.78072444163263
Training tokenizer:  43% 3500/8047 [2:18:31<3:59:30,  3.16s/it]loss_total_epoch 208.82641310431063
Training tokenizer:  44% 3501/8047 [2:18:34<3:58:33,  3.15s/it]loss_total_epoch 208.86813338659704
Training tokenizer:  44% 3502/8047 [2:18:37<3:59:17,  3.16s/it]loss_total_epoch 208.9161281194538
Training tokenizer:  44% 3503/8047 [2:18:40<4:00:05,  3.17s/it]loss_total_epoch 208.9684237856418
Training tokenizer:  44% 3504/8047 [2:18:43<3:59:23,  3.16s/it]loss_total_epoch 209.013678310439
Training tokenizer:  44% 3505/8047 [2:18:46<4:00:08,  3.17s/it]loss_total_epoch 209.05530092306435
Training tokenizer:  44% 3506/8047 [2:18:50<3:59:25,  3.16s/it]loss_total_epoch 209.0991123970598
Training tokenizer:  44% 3507/8047 [2:18:53<3:58:35,  3.15s/it]loss_total_epoch 209.1516086626798
Training tokenizer:  44% 3508/8047 [2:18:56<3:58:55,  3.16s/it]loss_total_epoch 209.20628787763417
Training tokenizer:  44% 3509/8047 [2:18:59<3:58:37,  3.15s/it]loss_total_epoch 209.25611687265337
Training tokenizer:  44% 3510/8047 [2:19:02<3:59:26,  3.17s/it]loss_total_epoch 209.30579143576324
Training tokenizer:  44% 3511/8047 [2:19:05<3:58:35,  3.16s/it]loss_total_epoch 209.36014164052904
Training tokenizer:  44% 3512/8047 [2:19:09<3:59:26,  3.17s/it]loss_total_epoch 209.4197389576584
Training tokenizer:  44% 3513/8047 [2:19:12<3:59:30,  3.17s/it]loss_total_epoch 209.4572392012924
Training tokenizer:  44% 3514/8047 [2:19:15<3:59:05,  3.16s/it]loss_total_epoch 209.51153601892292
Training tokenizer:  44% 3515/8047 [2:19:18<3:57:50,  3.15s/it]loss_total_epoch 209.54534456320107
Training tokenizer:  44% 3516/8047 [2:19:21<3:58:34,  3.16s/it]loss_total_epoch 209.58945103548467
Training tokenizer:  44% 3517/8047 [2:19:24<3:58:59,  3.17s/it]loss_total_epoch 209.64653413183987
Training tokenizer:  44% 3518/8047 [2:19:27<3:58:12,  3.16s/it]loss_total_epoch 209.69367728568614
Training tokenizer:  44% 3519/8047 [2:19:31<3:58:54,  3.17s/it]loss_total_epoch 209.74321195296943
Training tokenizer:  44% 3520/8047 [2:19:34<3:58:15,  3.16s/it]loss_total_epoch 209.7863831538707
Training tokenizer:  44% 3521/8047 [2:19:37<3:58:55,  3.17s/it]loss_total_epoch 209.83254813589156
Training tokenizer:  44% 3522/8047 [2:19:40<3:58:17,  3.16s/it]loss_total_epoch 209.88039993681014
Training tokenizer:  44% 3523/8047 [2:19:43<3:58:52,  3.17s/it]loss_total_epoch 209.92927779443562
Training tokenizer:  44% 3524/8047 [2:19:46<3:58:19,  3.16s/it]loss_total_epoch 209.9663313832134
Training tokenizer:  44% 3525/8047 [2:19:50<3:58:37,  3.17s/it]loss_total_epoch 210.02464940957725
Training tokenizer:  44% 3526/8047 [2:19:53<3:59:06,  3.17s/it]loss_total_epoch 210.07598234154284
Training tokenizer:  44% 3527/8047 [2:19:56<3:59:35,  3.18s/it]loss_total_epoch 210.11800755746663
Training tokenizer:  44% 3528/8047 [2:19:59<3:59:35,  3.18s/it]loss_total_epoch 210.15954248793423
Training tokenizer:  44% 3529/8047 [2:20:02<3:59:47,  3.18s/it]loss_total_epoch 210.21367963589728
Training tokenizer:  44% 3530/8047 [2:20:06<4:00:12,  3.19s/it]loss_total_epoch 210.26321895234287
Training tokenizer:  44% 3531/8047 [2:20:09<4:00:24,  3.19s/it]loss_total_epoch 210.3080978114158
Training tokenizer:  44% 3532/8047 [2:20:12<3:59:11,  3.18s/it]loss_total_epoch 210.35708669386804
Training tokenizer:  44% 3533/8047 [2:20:15<3:57:59,  3.16s/it]loss_total_epoch 210.39302593283355
Training tokenizer:  44% 3534/8047 [2:20:18<3:58:23,  3.17s/it]loss_total_epoch 210.43791946209967
Training tokenizer:  44% 3535/8047 [2:20:21<3:58:53,  3.18s/it]loss_total_epoch 210.48237782157958
Training tokenizer:  44% 3536/8047 [2:20:25<3:58:11,  3.17s/it]loss_total_epoch 210.5214643087238
Training tokenizer:  44% 3537/8047 [2:20:28<3:58:31,  3.17s/it]loss_total_epoch 210.57342285476625
Training tokenizer:  44% 3538/8047 [2:20:31<3:59:12,  3.18s/it]loss_total_epoch 210.6130786370486
Training tokenizer:  44% 3539/8047 [2:20:34<3:59:15,  3.18s/it]loss_total_epoch 210.6591557431966
Training tokenizer:  44% 3540/8047 [2:20:37<3:59:40,  3.19s/it]loss_total_epoch 210.69701719470322
Training tokenizer:  44% 3541/8047 [2:20:41<3:59:00,  3.18s/it]loss_total_epoch 210.75226424075663
Training tokenizer:  44% 3542/8047 [2:20:44<3:57:58,  3.17s/it]loss_total_epoch 210.7910293918103
Training tokenizer:  44% 3543/8047 [2:20:47<3:57:46,  3.17s/it]loss_total_epoch 210.829560527578
Training tokenizer:  44% 3544/8047 [2:20:50<3:58:01,  3.17s/it]loss_total_epoch 210.87476988323033
Training tokenizer:  44% 3545/8047 [2:20:53<3:58:27,  3.18s/it]loss_total_epoch 210.9310422781855
Training tokenizer:  44% 3546/8047 [2:20:56<3:58:56,  3.19s/it]loss_total_epoch 210.98835240863264
Training tokenizer:  44% 3547/8047 [2:21:00<3:58:26,  3.18s/it]loss_total_epoch 211.03653971664608
Training tokenizer:  44% 3548/8047 [2:21:03<3:58:36,  3.18s/it]loss_total_epoch 211.09294152073562
Training tokenizer:  44% 3549/8047 [2:21:06<3:58:12,  3.18s/it]loss_total_epoch 211.14009732194245
Training tokenizer:  44% 3550/8047 [2:21:09<3:59:06,  3.19s/it]loss_total_epoch 211.19572977162898
Training tokenizer:  44% 3551/8047 [2:21:12<3:58:50,  3.19s/it]loss_total_epoch 211.2485192026943
Training tokenizer:  44% 3552/8047 [2:21:16<3:59:17,  3.19s/it]loss_total_epoch 211.3076204303652
Training tokenizer:  44% 3553/8047 [2:21:19<3:58:57,  3.19s/it]loss_total_epoch 211.36309562064707
Training tokenizer:  44% 3554/8047 [2:21:22<3:58:41,  3.19s/it]loss_total_epoch 211.42061436735094
Training tokenizer:  44% 3555/8047 [2:21:25<3:59:18,  3.20s/it]loss_total_epoch 211.4657516758889
Training tokenizer:  44% 3556/8047 [2:21:28<3:58:25,  3.19s/it]loss_total_epoch 211.5176219958812
Training tokenizer:  44% 3557/8047 [2:21:31<3:57:21,  3.17s/it]loss_total_epoch 211.56943791918457
Training tokenizer:  44% 3558/8047 [2:21:35<3:57:34,  3.18s/it]loss_total_epoch 211.618287878111
Training tokenizer:  44% 3559/8047 [2:21:38<3:58:41,  3.19s/it]loss_total_epoch 211.67958282120526
Training tokenizer:  44% 3560/8047 [2:21:41<3:59:14,  3.20s/it]loss_total_epoch 211.72718679346144
Training tokenizer:  44% 3561/8047 [2:21:44<3:58:41,  3.19s/it]loss_total_epoch 211.7793638240546
Training tokenizer:  44% 3562/8047 [2:21:47<3:58:58,  3.20s/it]loss_total_epoch 211.82695297710598
Training tokenizer:  44% 3563/8047 [2:21:51<3:58:53,  3.20s/it]loss_total_epoch 211.88525783084333
Training tokenizer:  44% 3564/8047 [2:21:54<3:57:46,  3.18s/it]loss_total_epoch 211.93874740041792
Training tokenizer:  44% 3565/8047 [2:21:57<3:58:47,  3.20s/it]loss_total_epoch 211.98379822261631
Training tokenizer:  44% 3566/8047 [2:22:00<3:58:58,  3.20s/it]loss_total_epoch 212.04015602357686
Training tokenizer:  44% 3567/8047 [2:22:03<3:58:31,  3.19s/it]loss_total_epoch 212.09621329046786
Training tokenizer:  44% 3568/8047 [2:22:07<3:57:56,  3.19s/it]loss_total_epoch 212.13750726170838
Training tokenizer:  44% 3569/8047 [2:22:10<3:58:20,  3.19s/it]loss_total_epoch 212.182378673926
Training tokenizer:  44% 3570/8047 [2:22:13<3:58:05,  3.19s/it]loss_total_epoch 212.2384334821254
Training tokenizer:  44% 3571/8047 [2:22:16<3:57:43,  3.19s/it]loss_total_epoch 212.28554035909474
Training tokenizer:  44% 3572/8047 [2:22:19<3:57:58,  3.19s/it]loss_total_epoch 212.3341425601393
Training tokenizer:  44% 3573/8047 [2:22:23<3:58:06,  3.19s/it]loss_total_epoch 212.38398941792548
Training tokenizer:  44% 3574/8047 [2:22:26<3:57:40,  3.19s/it]loss_total_epoch 212.43116152472794
Training tokenizer:  44% 3575/8047 [2:22:29<3:57:40,  3.19s/it]loss_total_epoch 212.47637304849923
Training tokenizer:  44% 3576/8047 [2:22:32<3:57:37,  3.19s/it]loss_total_epoch 212.5274977926165
Training tokenizer:  44% 3577/8047 [2:22:35<3:57:28,  3.19s/it]loss_total_epoch 212.56920064426959
Training tokenizer:  44% 3578/8047 [2:22:38<3:57:27,  3.19s/it]loss_total_epoch 212.6236247587949
Training tokenizer:  44% 3579/8047 [2:22:42<3:54:25,  3.15s/it]loss_total_epoch 212.67195175029337
Training tokenizer:  44% 3580/8047 [2:22:45<3:55:53,  3.17s/it]loss_total_epoch 212.7368161994964
Training tokenizer:  45% 3581/8047 [2:22:48<3:56:45,  3.18s/it]loss_total_epoch 212.78686652891338
Training tokenizer:  45% 3582/8047 [2:22:51<3:57:22,  3.19s/it]loss_total_epoch 212.84065081365407
Training tokenizer:  45% 3583/8047 [2:22:54<3:57:53,  3.20s/it]loss_total_epoch 212.89510385878384
Training tokenizer:  45% 3584/8047 [2:22:58<3:58:03,  3.20s/it]loss_total_epoch 212.94347829557955
Training tokenizer:  45% 3585/8047 [2:23:01<3:57:58,  3.20s/it]loss_total_epoch 213.00054952315986
Training tokenizer:  45% 3586/8047 [2:23:04<3:57:29,  3.19s/it]loss_total_epoch 213.0668440219015
Training tokenizer:  45% 3587/8047 [2:23:07<3:57:41,  3.20s/it]loss_total_epoch 213.11773252300918
Training tokenizer:  45% 3588/8047 [2:23:10<3:58:21,  3.21s/it]loss_total_epoch 213.16399688832462
Training tokenizer:  45% 3589/8047 [2:23:14<3:58:41,  3.21s/it]loss_total_epoch 213.2198798339814
Training tokenizer:  45% 3590/8047 [2:23:17<3:59:01,  3.22s/it]loss_total_epoch 213.2627670560032
Training tokenizer:  45% 3591/8047 [2:23:20<3:58:50,  3.22s/it]loss_total_epoch 213.30353833921254
Training tokenizer:  45% 3592/8047 [2:23:23<3:58:34,  3.21s/it]loss_total_epoch 213.34418776072562
Training tokenizer:  45% 3593/8047 [2:23:27<3:58:30,  3.21s/it]loss_total_epoch 213.39190511219203
Training tokenizer:  45% 3594/8047 [2:23:30<3:58:44,  3.22s/it]loss_total_epoch 213.44312301836908
Training tokenizer:  45% 3595/8047 [2:23:33<3:58:25,  3.21s/it]loss_total_epoch 213.478405052796
Training tokenizer:  45% 3596/8047 [2:23:36<3:58:38,  3.22s/it]loss_total_epoch 213.52154393680394
Training tokenizer:  45% 3597/8047 [2:23:39<3:57:44,  3.21s/it]loss_total_epoch 213.57511221803725
Training tokenizer:  45% 3598/8047 [2:23:43<3:57:40,  3.21s/it]loss_total_epoch 213.63213556073606
Training tokenizer:  45% 3599/8047 [2:23:46<3:58:01,  3.21s/it]loss_total_epoch 213.68064297176898
Training tokenizer:  45% 3600/8047 [2:23:49<3:58:06,  3.21s/it]loss_total_epoch 213.73617291636765
Training tokenizer:  45% 3601/8047 [2:23:52<3:58:48,  3.22s/it]loss_total_epoch 213.78063607029617
Training tokenizer:  45% 3602/8047 [2:23:55<3:57:40,  3.21s/it]loss_total_epoch 213.82397714443505
Training tokenizer:  45% 3603/8047 [2:23:59<3:57:30,  3.21s/it]loss_total_epoch 213.86812467686832
Training tokenizer:  45% 3604/8047 [2:24:02<3:58:13,  3.22s/it]loss_total_epoch 213.9110217820853
Training tokenizer:  45% 3605/8047 [2:24:05<3:56:37,  3.20s/it]loss_total_epoch 213.9591774251312
Training tokenizer:  45% 3606/8047 [2:24:08<3:57:29,  3.21s/it]loss_total_epoch 213.99910811148584
Training tokenizer:  45% 3607/8047 [2:24:11<3:58:14,  3.22s/it]loss_total_epoch 214.0430789012462
Training tokenizer:  45% 3608/8047 [2:24:15<3:58:34,  3.22s/it]loss_total_epoch 214.08829918690026
Training tokenizer:  45% 3609/8047 [2:24:18<3:58:08,  3.22s/it]loss_total_epoch 214.14314525015652
Training tokenizer:  45% 3610/8047 [2:24:21<3:58:21,  3.22s/it]loss_total_epoch 214.19403879530728
Training tokenizer:  45% 3611/8047 [2:24:24<3:57:48,  3.22s/it]loss_total_epoch 214.25026291422546
Training tokenizer:  45% 3612/8047 [2:24:28<3:58:01,  3.22s/it]loss_total_epoch 214.2975430879742
Training tokenizer:  45% 3613/8047 [2:24:31<3:57:55,  3.22s/it]loss_total_epoch 214.3536160606891
Training tokenizer:  45% 3614/8047 [2:24:34<3:58:36,  3.23s/it]loss_total_epoch 214.40240727551281
Training tokenizer:  45% 3615/8047 [2:24:37<3:58:25,  3.23s/it]loss_total_epoch 214.45209597237408
Training tokenizer:  45% 3616/8047 [2:24:41<3:58:29,  3.23s/it]loss_total_epoch 214.50115738622844
Training tokenizer:  45% 3617/8047 [2:24:44<3:57:21,  3.21s/it]loss_total_epoch 214.55429056473076
Training tokenizer:  45% 3618/8047 [2:24:47<3:57:26,  3.22s/it]loss_total_epoch 214.6165248285979
Training tokenizer:  45% 3619/8047 [2:24:50<3:57:05,  3.21s/it]loss_total_epoch 214.66679609008133
Training tokenizer:  45% 3620/8047 [2:24:53<3:57:08,  3.21s/it]loss_total_epoch 214.71208658628166
Training tokenizer:  45% 3621/8047 [2:24:57<3:56:57,  3.21s/it]loss_total_epoch 214.76159971021116
Training tokenizer:  45% 3622/8047 [2:25:00<3:57:26,  3.22s/it]loss_total_epoch 214.8006850387901
Training tokenizer:  45% 3623/8047 [2:25:03<3:58:01,  3.23s/it]loss_total_epoch 214.8542299810797
Training tokenizer:  45% 3624/8047 [2:25:06<3:57:18,  3.22s/it]loss_total_epoch 214.89973777718842
Training tokenizer:  45% 3625/8047 [2:25:09<3:57:11,  3.22s/it]loss_total_epoch 214.946204373613
Training tokenizer:  45% 3626/8047 [2:25:13<4:00:49,  3.27s/it]loss_total_epoch 215.00520570762455
Training tokenizer:  45% 3627/8047 [2:25:16<3:59:12,  3.25s/it]loss_total_epoch 215.06426899321377
Training tokenizer:  45% 3628/8047 [2:25:19<3:58:00,  3.23s/it]loss_total_epoch 215.11777062900364
Training tokenizer:  45% 3629/8047 [2:25:22<3:58:17,  3.24s/it]loss_total_epoch 215.1747015658766
Training tokenizer:  45% 3630/8047 [2:25:26<3:58:45,  3.24s/it]loss_total_epoch 215.21311288140714
Training tokenizer:  45% 3631/8047 [2:25:29<3:57:48,  3.23s/it]loss_total_epoch 215.2626956757158
Training tokenizer:  45% 3632/8047 [2:25:32<3:57:39,  3.23s/it]loss_total_epoch 215.31377450190485
Training tokenizer:  45% 3633/8047 [2:25:35<3:57:51,  3.23s/it]loss_total_epoch 215.35913185216486
Training tokenizer:  45% 3634/8047 [2:25:39<3:58:24,  3.24s/it]loss_total_epoch 215.4018812198192
Training tokenizer:  45% 3635/8047 [2:25:42<3:58:53,  3.25s/it]loss_total_epoch 215.43449005670846
Training tokenizer:  45% 3636/8047 [2:25:45<3:58:24,  3.24s/it]loss_total_epoch 215.4733635429293
Training tokenizer:  45% 3637/8047 [2:25:48<3:58:26,  3.24s/it]loss_total_epoch 215.51619523204863
Training tokenizer:  45% 3638/8047 [2:25:52<3:57:37,  3.23s/it]loss_total_epoch 215.57071625255048
Training tokenizer:  45% 3639/8047 [2:25:55<3:58:17,  3.24s/it]loss_total_epoch 215.61177048645914
Training tokenizer:  45% 3640/8047 [2:25:58<3:57:27,  3.23s/it]loss_total_epoch 215.65757608227432
Training tokenizer:  45% 3641/8047 [2:26:01<3:55:52,  3.21s/it]loss_total_epoch 215.70788397453725
Training tokenizer:  45% 3642/8047 [2:26:04<3:55:32,  3.21s/it]loss_total_epoch 215.7669209036976
Training tokenizer:  45% 3643/8047 [2:26:08<3:55:25,  3.21s/it]loss_total_epoch 215.80404024384916
Training tokenizer:  45% 3644/8047 [2:26:11<3:55:26,  3.21s/it]loss_total_epoch 215.8555864263326
Training tokenizer:  45% 3645/8047 [2:26:14<3:55:21,  3.21s/it]loss_total_epoch 215.88692697696388
Training tokenizer:  45% 3646/8047 [2:26:17<3:56:09,  3.22s/it]loss_total_epoch 215.92818563245237
Training tokenizer:  45% 3647/8047 [2:26:21<3:55:44,  3.21s/it]loss_total_epoch 215.96392810158432
Training tokenizer:  45% 3648/8047 [2:26:24<3:55:16,  3.21s/it]loss_total_epoch 216.01016717217863
Training tokenizer:  45% 3649/8047 [2:26:27<3:55:05,  3.21s/it]loss_total_epoch 216.04524023272097
Training tokenizer:  45% 3650/8047 [2:26:30<3:55:25,  3.21s/it]loss_total_epoch 216.09895164705813
Training tokenizer:  45% 3651/8047 [2:26:33<3:56:10,  3.22s/it]loss_total_epoch 216.1517924349755
Training tokenizer:  45% 3652/8047 [2:26:37<3:56:16,  3.23s/it]loss_total_epoch 216.19343305937946
Training tokenizer:  45% 3653/8047 [2:26:40<3:55:56,  3.22s/it]loss_total_epoch 216.24451605789363
Training tokenizer:  45% 3654/8047 [2:26:43<3:57:09,  3.24s/it]loss_total_epoch 216.29365451820195
Training tokenizer:  45% 3655/8047 [2:26:46<3:57:11,  3.24s/it]loss_total_epoch 216.33448206447065
Training tokenizer:  45% 3656/8047 [2:26:50<3:56:12,  3.23s/it]loss_total_epoch 216.38605677895248
Training tokenizer:  45% 3657/8047 [2:26:53<3:57:00,  3.24s/it]loss_total_epoch 216.4293502997607
Training tokenizer:  45% 3658/8047 [2:26:56<3:56:35,  3.23s/it]loss_total_epoch 216.47556781210005
Training tokenizer:  45% 3659/8047 [2:26:59<3:57:27,  3.25s/it]loss_total_epoch 216.5334666017443
Training tokenizer:  45% 3660/8047 [2:27:03<3:56:03,  3.23s/it]loss_total_epoch 216.57362766750157
Training tokenizer:  45% 3661/8047 [2:27:06<3:56:42,  3.24s/it]loss_total_epoch 216.62666020728648
Training tokenizer:  46% 3662/8047 [2:27:09<3:56:29,  3.24s/it]loss_total_epoch 216.67950275726616
Training tokenizer:  46% 3663/8047 [2:27:12<3:57:02,  3.24s/it]loss_total_epoch 216.72654527239501
Training tokenizer:  46% 3664/8047 [2:27:15<3:56:24,  3.24s/it]loss_total_epoch 216.76782682351768
Training tokenizer:  46% 3665/8047 [2:27:19<3:56:00,  3.23s/it]loss_total_epoch 216.8151088114828
Training tokenizer:  46% 3666/8047 [2:27:22<3:55:47,  3.23s/it]loss_total_epoch 216.86784234456718
Training tokenizer:  46% 3667/8047 [2:27:25<3:56:24,  3.24s/it]loss_total_epoch 216.91549766622484
Training tokenizer:  46% 3668/8047 [2:27:28<3:55:35,  3.23s/it]loss_total_epoch 216.96079350449145
Training tokenizer:  46% 3669/8047 [2:27:32<3:56:09,  3.24s/it]loss_total_epoch 217.0063486304134
Training tokenizer:  46% 3670/8047 [2:27:35<3:56:01,  3.24s/it]loss_total_epoch 217.05434347875416
Training tokenizer:  46% 3671/8047 [2:27:38<3:55:42,  3.23s/it]loss_total_epoch 217.1011667791754
Training tokenizer:  46% 3672/8047 [2:27:41<3:55:58,  3.24s/it]loss_total_epoch 217.15598176978528
Training tokenizer:  46% 3673/8047 [2:27:45<3:56:45,  3.25s/it]loss_total_epoch 217.20804938487709
Training tokenizer:  46% 3674/8047 [2:27:48<3:56:42,  3.25s/it]loss_total_epoch 217.25461847893894
Training tokenizer:  46% 3675/8047 [2:27:51<3:56:23,  3.24s/it]loss_total_epoch 217.30895563028753
Training tokenizer:  46% 3676/8047 [2:27:54<3:55:03,  3.23s/it]loss_total_epoch 217.36862638406456
Training tokenizer:  46% 3677/8047 [2:27:58<3:55:40,  3.24s/it]loss_total_epoch 217.42902638949454
Training tokenizer:  46% 3678/8047 [2:28:01<3:55:45,  3.24s/it]loss_total_epoch 217.46483502723277
Training tokenizer:  46% 3679/8047 [2:28:04<3:56:46,  3.25s/it]loss_total_epoch 217.50333380140364
Training tokenizer:  46% 3680/8047 [2:28:07<3:56:12,  3.25s/it]loss_total_epoch 217.5481977481395
Training tokenizer:  46% 3681/8047 [2:28:11<3:57:25,  3.26s/it]loss_total_epoch 217.60822810046375
Training tokenizer:  46% 3682/8047 [2:28:14<3:56:28,  3.25s/it]loss_total_epoch 217.6581411305815
Training tokenizer:  46% 3683/8047 [2:28:17<3:56:21,  3.25s/it]loss_total_epoch 217.70712401904166
Training tokenizer:  46% 3684/8047 [2:28:20<3:56:22,  3.25s/it]loss_total_epoch 217.7558217998594
Training tokenizer:  46% 3685/8047 [2:28:24<3:55:49,  3.24s/it]loss_total_epoch 217.80928739719093
Training tokenizer:  46% 3686/8047 [2:28:27<3:56:13,  3.25s/it]loss_total_epoch 217.8591637481004
Training tokenizer:  46% 3687/8047 [2:28:30<3:55:22,  3.24s/it]loss_total_epoch 217.90751938335598
Training tokenizer:  46% 3688/8047 [2:28:33<3:55:24,  3.24s/it]loss_total_epoch 217.95319181866944
Training tokenizer:  46% 3689/8047 [2:28:37<3:56:06,  3.25s/it]loss_total_epoch 218.00081144459546
Training tokenizer:  46% 3690/8047 [2:28:40<3:56:16,  3.25s/it]loss_total_epoch 218.05157413892448
Training tokenizer:  46% 3691/8047 [2:28:43<3:56:22,  3.26s/it]loss_total_epoch 218.091262916103
Training tokenizer:  46% 3692/8047 [2:28:46<3:56:39,  3.26s/it]loss_total_epoch 218.14051641337574
Training tokenizer:  46% 3693/8047 [2:28:50<3:56:26,  3.26s/it]loss_total_epoch 218.18802117742598
Training tokenizer:  46% 3694/8047 [2:28:53<3:56:40,  3.26s/it]loss_total_epoch 218.23473440669477
Training tokenizer:  46% 3695/8047 [2:28:56<3:59:28,  3.30s/it]loss_total_epoch 218.2976062130183
Training tokenizer:  46% 3696/8047 [2:28:59<3:57:32,  3.28s/it]loss_total_epoch 218.3433246780187
Training tokenizer:  46% 3697/8047 [2:29:03<3:57:40,  3.28s/it]loss_total_epoch 218.38688541390002
Training tokenizer:  46% 3698/8047 [2:29:06<3:57:12,  3.27s/it]loss_total_epoch 218.43287135474384
Training tokenizer:  46% 3699/8047 [2:29:09<3:57:19,  3.27s/it]loss_total_epoch 218.48168074525893
Training tokenizer:  46% 3700/8047 [2:29:13<3:55:52,  3.26s/it]loss_total_epoch 218.53518312610686
Training tokenizer:  46% 3701/8047 [2:29:16<3:55:15,  3.25s/it]loss_total_epoch 218.58294595964253
Training tokenizer:  46% 3702/8047 [2:29:19<3:56:11,  3.26s/it]loss_total_epoch 218.6289591845125
Training tokenizer:  46% 3703/8047 [2:29:22<3:56:13,  3.26s/it]loss_total_epoch 218.6821437459439
Training tokenizer:  46% 3704/8047 [2:29:26<3:56:05,  3.26s/it]loss_total_epoch 218.72381009720266
Training tokenizer:  46% 3705/8047 [2:29:29<3:55:40,  3.26s/it]loss_total_epoch 218.78213953785598
Training tokenizer:  46% 3706/8047 [2:29:32<3:55:37,  3.26s/it]loss_total_epoch 218.8240283112973
Training tokenizer:  46% 3707/8047 [2:29:35<3:56:30,  3.27s/it]loss_total_epoch 218.86660839430988
Training tokenizer:  46% 3708/8047 [2:29:39<3:56:14,  3.27s/it]loss_total_epoch 218.91932178474963
Training tokenizer:  46% 3709/8047 [2:29:42<3:56:43,  3.27s/it]loss_total_epoch 218.96644383110106
Training tokenizer:  46% 3710/8047 [2:29:45<3:54:53,  3.25s/it]loss_total_epoch 219.0076749343425
Training tokenizer:  46% 3711/8047 [2:29:48<3:55:05,  3.25s/it]loss_total_epoch 219.05431426502764
Training tokenizer:  46% 3712/8047 [2:29:52<3:55:11,  3.26s/it]loss_total_epoch 219.0976449828595
Training tokenizer:  46% 3713/8047 [2:29:55<3:56:20,  3.27s/it]loss_total_epoch 219.15156114660203
Training tokenizer:  46% 3714/8047 [2:29:58<3:57:36,  3.29s/it]loss_total_epoch 219.20287942700088
Training tokenizer:  46% 3715/8047 [2:30:02<3:56:15,  3.27s/it]loss_total_epoch 219.25126920826733
Training tokenizer:  46% 3716/8047 [2:30:05<3:55:22,  3.26s/it]loss_total_epoch 219.2991603706032
Training tokenizer:  46% 3717/8047 [2:30:08<3:55:29,  3.26s/it]loss_total_epoch 219.34439436532557
Training tokenizer:  46% 3718/8047 [2:30:11<3:55:24,  3.26s/it]loss_total_epoch 219.3823485840112
Training tokenizer:  46% 3719/8047 [2:30:15<3:55:59,  3.27s/it]loss_total_epoch 219.43050805293024
Training tokenizer:  46% 3720/8047 [2:30:18<3:56:08,  3.27s/it]loss_total_epoch 219.47204984538257
Training tokenizer:  46% 3721/8047 [2:30:21<3:56:14,  3.28s/it]loss_total_epoch 219.5231481846422
Training tokenizer:  46% 3722/8047 [2:30:24<3:56:47,  3.28s/it]loss_total_epoch 219.5683150868863
Training tokenizer:  46% 3723/8047 [2:30:28<3:57:24,  3.29s/it]loss_total_epoch 219.61849855817854
Training tokenizer:  46% 3724/8047 [2:30:31<3:55:52,  3.27s/it]loss_total_epoch 219.6813794914633
Training tokenizer:  46% 3725/8047 [2:30:34<3:57:19,  3.29s/it]loss_total_epoch 219.7202564869076
Training tokenizer:  46% 3726/8047 [2:30:38<3:57:09,  3.29s/it]loss_total_epoch 219.78174647130072
Training tokenizer:  46% 3727/8047 [2:30:41<3:57:31,  3.30s/it]loss_total_epoch 219.83037463016808
Training tokenizer:  46% 3728/8047 [2:30:44<3:58:41,  3.32s/it]loss_total_epoch 219.88425618968904
Training tokenizer:  46% 3729/8047 [2:30:48<3:57:03,  3.29s/it]loss_total_epoch 219.93436830304563
Training tokenizer:  46% 3730/8047 [2:30:51<3:58:12,  3.31s/it]loss_total_epoch 219.9893853198737
Training tokenizer:  46% 3731/8047 [2:30:54<3:57:41,  3.30s/it]loss_total_epoch 220.03519837372005
Training tokenizer:  46% 3732/8047 [2:30:57<3:56:36,  3.29s/it]loss_total_epoch 220.08673554845154
Training tokenizer:  46% 3733/8047 [2:31:01<3:56:24,  3.29s/it]loss_total_epoch 220.14443556405604
Training tokenizer:  46% 3734/8047 [2:31:04<3:55:50,  3.28s/it]loss_total_epoch 220.19687094725668
Training tokenizer:  46% 3735/8047 [2:31:07<3:55:52,  3.28s/it]loss_total_epoch 220.24673584662378
Training tokenizer:  46% 3736/8047 [2:31:11<3:55:13,  3.27s/it]loss_total_epoch 220.30047404579818
Training tokenizer:  46% 3737/8047 [2:31:14<3:55:49,  3.28s/it]loss_total_epoch 220.35513493604958
Training tokenizer:  46% 3738/8047 [2:31:17<3:55:00,  3.27s/it]loss_total_epoch 220.39934993349016
Training tokenizer:  46% 3739/8047 [2:31:20<3:54:47,  3.27s/it]loss_total_epoch 220.4592122156173
Training tokenizer:  46% 3740/8047 [2:31:24<3:56:19,  3.29s/it]loss_total_epoch 220.49640761129558
Training tokenizer:  46% 3741/8047 [2:31:27<3:56:03,  3.29s/it]loss_total_epoch 220.532108547166
Training tokenizer:  47% 3742/8047 [2:31:30<3:55:02,  3.28s/it]loss_total_epoch 220.57761906273663
Training tokenizer:  47% 3743/8047 [2:31:33<3:54:07,  3.26s/it]loss_total_epoch 220.62425310723484
Training tokenizer:  47% 3744/8047 [2:31:37<3:53:19,  3.25s/it]loss_total_epoch 220.67540560476482
Training tokenizer:  47% 3745/8047 [2:31:40<3:53:12,  3.25s/it]loss_total_epoch 220.72176110558212
Training tokenizer:  47% 3746/8047 [2:31:43<3:54:53,  3.28s/it]loss_total_epoch 220.76281370036304
Training tokenizer:  47% 3747/8047 [2:31:47<3:55:01,  3.28s/it]loss_total_epoch 220.8032797705382
Training tokenizer:  47% 3748/8047 [2:31:50<3:55:27,  3.29s/it]loss_total_epoch 220.85329501144588
Training tokenizer:  47% 3749/8047 [2:31:53<3:56:09,  3.30s/it]loss_total_epoch 220.8987870682031
Training tokenizer:  47% 3750/8047 [2:31:56<3:55:50,  3.29s/it]loss_total_epoch 220.9378329347819
Training tokenizer:  47% 3751/8047 [2:32:00<3:55:39,  3.29s/it]loss_total_epoch 220.99095837958157
Training tokenizer:  47% 3752/8047 [2:32:03<3:54:57,  3.28s/it]loss_total_epoch 221.0311128627509
Training tokenizer:  47% 3753/8047 [2:32:06<3:55:14,  3.29s/it]loss_total_epoch 221.05952069722116
Training tokenizer:  47% 3754/8047 [2:32:10<3:55:29,  3.29s/it]loss_total_epoch 221.11055766232312
Training tokenizer:  47% 3755/8047 [2:32:13<3:54:21,  3.28s/it]loss_total_epoch 221.1597008239478
Training tokenizer:  47% 3756/8047 [2:32:16<3:54:31,  3.28s/it]loss_total_epoch 221.21054288186133
Training tokenizer:  47% 3757/8047 [2:32:19<3:54:32,  3.28s/it]loss_total_epoch 221.25212310440838
Training tokenizer:  47% 3758/8047 [2:32:23<3:53:52,  3.27s/it]loss_total_epoch 221.2978051174432
Training tokenizer:  47% 3759/8047 [2:32:26<3:54:28,  3.28s/it]loss_total_epoch 221.33922664262354
Training tokenizer:  47% 3760/8047 [2:32:29<3:54:02,  3.28s/it]loss_total_epoch 221.40319514460862
Training tokenizer:  47% 3761/8047 [2:32:32<3:53:27,  3.27s/it]loss_total_epoch 221.44044293276966
Training tokenizer:  47% 3762/8047 [2:32:36<3:53:54,  3.28s/it]loss_total_epoch 221.4942642506212
Training tokenizer:  47% 3763/8047 [2:32:39<3:54:34,  3.29s/it]loss_total_epoch 221.5402783807367
Training tokenizer:  47% 3764/8047 [2:32:42<3:55:24,  3.30s/it]loss_total_epoch 221.58098890818655
Training tokenizer:  47% 3765/8047 [2:32:46<3:55:20,  3.30s/it]loss_total_epoch 221.62441288121045
Training tokenizer:  47% 3766/8047 [2:32:49<3:55:14,  3.30s/it]loss_total_epoch 221.66847028024495
Training tokenizer:  47% 3767/8047 [2:32:52<3:53:58,  3.28s/it]loss_total_epoch 221.71104432456195
Training tokenizer:  47% 3768/8047 [2:32:55<3:53:29,  3.27s/it]loss_total_epoch 221.75692892633379
Training tokenizer:  47% 3769/8047 [2:32:59<3:53:03,  3.27s/it]loss_total_epoch 221.8118397090584
Training tokenizer:  47% 3770/8047 [2:33:02<3:54:02,  3.28s/it]loss_total_epoch 221.86527195386589
Training tokenizer:  47% 3771/8047 [2:33:05<3:54:57,  3.30s/it]loss_total_epoch 221.90980303101242
Training tokenizer:  47% 3772/8047 [2:33:09<3:53:31,  3.28s/it]loss_total_epoch 221.95660405792296
Training tokenizer:  47% 3773/8047 [2:33:12<3:53:32,  3.28s/it]loss_total_epoch 222.0018232036382
Training tokenizer:  47% 3774/8047 [2:33:15<3:53:40,  3.28s/it]loss_total_epoch 222.05183644406497
Training tokenizer:  47% 3775/8047 [2:33:18<3:53:46,  3.28s/it]loss_total_epoch 222.1004903856665
Training tokenizer:  47% 3776/8047 [2:33:22<3:53:44,  3.28s/it]loss_total_epoch 222.15293436683714
Training tokenizer:  47% 3777/8047 [2:33:25<3:53:53,  3.29s/it]loss_total_epoch 222.20416008122265
Training tokenizer:  47% 3778/8047 [2:33:28<3:54:27,  3.30s/it]loss_total_epoch 222.25587458349764
Training tokenizer:  47% 3779/8047 [2:33:32<3:54:10,  3.29s/it]loss_total_epoch 222.29507635347545
Training tokenizer:  47% 3780/8047 [2:33:35<3:53:45,  3.29s/it]loss_total_epoch 222.34259901754558
Training tokenizer:  47% 3781/8047 [2:33:38<3:53:55,  3.29s/it]loss_total_epoch 222.3962891306728
Training tokenizer:  47% 3782/8047 [2:33:42<3:54:29,  3.30s/it]loss_total_epoch 222.44328747875988
Training tokenizer:  47% 3783/8047 [2:33:45<3:54:38,  3.30s/it]loss_total_epoch 222.49336756579578
Training tokenizer:  47% 3784/8047 [2:33:48<3:54:47,  3.30s/it]loss_total_epoch 222.52602041698992
Training tokenizer:  47% 3785/8047 [2:33:51<3:55:13,  3.31s/it]loss_total_epoch 222.57338786683977
Training tokenizer:  47% 3786/8047 [2:33:55<3:54:32,  3.30s/it]loss_total_epoch 222.6158028114587
Training tokenizer:  47% 3787/8047 [2:33:58<3:54:45,  3.31s/it]loss_total_epoch 222.66535485349596
Training tokenizer:  47% 3788/8047 [2:34:01<3:54:32,  3.30s/it]loss_total_epoch 222.7067586760968
Training tokenizer:  47% 3789/8047 [2:34:05<3:53:20,  3.29s/it]loss_total_epoch 222.75434840284288
Training tokenizer:  47% 3790/8047 [2:34:08<3:54:01,  3.30s/it]loss_total_epoch 222.8095078524202
Training tokenizer:  47% 3791/8047 [2:34:11<3:53:34,  3.29s/it]loss_total_epoch 222.85780368931592
Training tokenizer:  47% 3792/8047 [2:34:15<3:53:17,  3.29s/it]loss_total_epoch 222.90752809680998
Training tokenizer:  47% 3793/8047 [2:34:18<3:54:19,  3.31s/it]loss_total_epoch 222.9687885697931
Training tokenizer:  47% 3794/8047 [2:34:21<3:53:27,  3.29s/it]loss_total_epoch 223.01884041912854
Training tokenizer:  47% 3795/8047 [2:34:24<3:54:54,  3.31s/it]loss_total_epoch 223.06659114547074
Training tokenizer:  47% 3796/8047 [2:34:28<3:54:53,  3.32s/it]loss_total_epoch 223.1246555801481
Training tokenizer:  47% 3797/8047 [2:34:31<3:55:05,  3.32s/it]loss_total_epoch 223.18272191099823
Training tokenizer:  47% 3798/8047 [2:34:34<3:55:00,  3.32s/it]loss_total_epoch 223.2402372788638
Training tokenizer:  47% 3799/8047 [2:34:38<3:54:00,  3.31s/it]loss_total_epoch 223.2931081932038
Training tokenizer:  47% 3800/8047 [2:34:41<3:54:17,  3.31s/it]loss_total_epoch 223.33190189115703
Training tokenizer:  47% 3801/8047 [2:34:44<3:54:39,  3.32s/it]loss_total_epoch 223.39509291760623
Training tokenizer:  47% 3802/8047 [2:34:48<3:54:05,  3.31s/it]loss_total_epoch 223.4473590683192
Training tokenizer:  47% 3803/8047 [2:34:51<3:54:23,  3.31s/it]loss_total_epoch 223.49353794567287
Training tokenizer:  47% 3804/8047 [2:34:54<3:53:56,  3.31s/it]loss_total_epoch 223.53971024788916
Training tokenizer:  47% 3805/8047 [2:34:58<3:53:45,  3.31s/it]loss_total_epoch 223.57971753366292
Training tokenizer:  47% 3806/8047 [2:35:01<3:52:59,  3.30s/it]loss_total_epoch 223.6315743792802
Training tokenizer:  47% 3807/8047 [2:35:04<3:53:24,  3.30s/it]loss_total_epoch 223.6769929137081
Training tokenizer:  47% 3808/8047 [2:35:08<3:53:54,  3.31s/it]loss_total_epoch 223.72900721244514
Training tokenizer:  47% 3809/8047 [2:35:11<3:53:41,  3.31s/it]loss_total_epoch 223.7868754696101
Training tokenizer:  47% 3810/8047 [2:35:14<3:52:44,  3.30s/it]loss_total_epoch 223.8397605922073
Training tokenizer:  47% 3811/8047 [2:35:17<3:53:57,  3.31s/it]loss_total_epoch 223.87986821494997
Training tokenizer:  47% 3812/8047 [2:35:21<3:53:38,  3.31s/it]loss_total_epoch 223.93049096874893
Training tokenizer:  47% 3813/8047 [2:35:24<3:53:04,  3.30s/it]loss_total_epoch 223.97628038562834
Training tokenizer:  47% 3814/8047 [2:35:27<3:53:25,  3.31s/it]loss_total_epoch 224.0177322793752
Training tokenizer:  47% 3815/8047 [2:35:31<3:53:52,  3.32s/it]loss_total_epoch 224.06713997013867
Training tokenizer:  47% 3816/8047 [2:35:34<3:52:52,  3.30s/it]loss_total_epoch 224.11681949533522
Training tokenizer:  47% 3817/8047 [2:35:37<3:53:45,  3.32s/it]loss_total_epoch 224.1612458731979
Training tokenizer:  47% 3818/8047 [2:35:41<3:57:03,  3.36s/it]loss_total_epoch 224.20624971203506
Training tokenizer:  47% 3819/8047 [2:35:44<3:55:01,  3.34s/it]loss_total_epoch 224.2526338044554
Training tokenizer:  47% 3820/8047 [2:35:47<3:54:24,  3.33s/it]loss_total_epoch 224.29680714942515
Training tokenizer:  47% 3821/8047 [2:35:51<3:53:54,  3.32s/it]loss_total_epoch 224.34398676268756
Training tokenizer:  47% 3822/8047 [2:35:54<3:53:04,  3.31s/it]loss_total_epoch 224.39067381806672
Training tokenizer:  48% 3823/8047 [2:35:57<3:52:41,  3.31s/it]loss_total_epoch 224.43373828195035
Training tokenizer:  48% 3824/8047 [2:36:01<3:53:37,  3.32s/it]loss_total_epoch 224.4807828720659
Training tokenizer:  48% 3825/8047 [2:36:04<3:53:08,  3.31s/it]loss_total_epoch 224.52846465446055
Training tokenizer:  48% 3826/8047 [2:36:07<3:53:14,  3.32s/it]loss_total_epoch 224.57790181227028
Training tokenizer:  48% 3827/8047 [2:36:11<3:53:00,  3.31s/it]loss_total_epoch 224.63091915659606
Training tokenizer:  48% 3828/8047 [2:36:14<3:53:49,  3.33s/it]loss_total_epoch 224.68450429104269
Training tokenizer:  48% 3829/8047 [2:36:17<3:53:44,  3.32s/it]loss_total_epoch 224.73057634197176
Training tokenizer:  48% 3830/8047 [2:36:20<3:52:26,  3.31s/it]loss_total_epoch 224.7737391386181
Training tokenizer:  48% 3831/8047 [2:36:24<3:52:45,  3.31s/it]loss_total_epoch 224.8254915755242
Training tokenizer:  48% 3832/8047 [2:36:27<3:53:03,  3.32s/it]loss_total_epoch 224.87494907714427
Training tokenizer:  48% 3833/8047 [2:36:30<3:53:35,  3.33s/it]loss_total_epoch 224.9271104875952
Training tokenizer:  48% 3834/8047 [2:36:34<3:54:22,  3.34s/it]loss_total_epoch 224.97042132355273
Training tokenizer:  48% 3835/8047 [2:36:37<3:53:02,  3.32s/it]loss_total_epoch 225.01374177075922
Training tokenizer:  48% 3836/8047 [2:36:40<3:53:10,  3.32s/it]loss_total_epoch 225.06645796634257
Training tokenizer:  48% 3837/8047 [2:36:44<3:52:59,  3.32s/it]loss_total_epoch 225.11441099084914
Training tokenizer:  48% 3838/8047 [2:36:47<3:53:52,  3.33s/it]loss_total_epoch 225.16214643605053
Training tokenizer:  48% 3839/8047 [2:36:50<3:53:18,  3.33s/it]loss_total_epoch 225.21975056268275
Training tokenizer:  48% 3840/8047 [2:36:54<3:53:40,  3.33s/it]loss_total_epoch 225.2582436222583
Training tokenizer:  48% 3841/8047 [2:36:57<3:53:15,  3.33s/it]loss_total_epoch 225.3013564478606
Training tokenizer:  48% 3842/8047 [2:37:00<3:51:18,  3.30s/it]loss_total_epoch 225.34669512324035
Training tokenizer:  48% 3843/8047 [2:37:04<3:52:16,  3.32s/it]loss_total_epoch 225.40073645301163
Training tokenizer:  48% 3844/8047 [2:37:07<3:51:51,  3.31s/it]loss_total_epoch 225.4488241802901
Training tokenizer:  48% 3845/8047 [2:37:10<3:50:50,  3.30s/it]loss_total_epoch 225.49406892992556
Training tokenizer:  48% 3846/8047 [2:37:14<3:52:11,  3.32s/it]loss_total_epoch 225.5441199671477
Training tokenizer:  48% 3847/8047 [2:37:17<3:52:25,  3.32s/it]loss_total_epoch 225.59249067492783
Training tokenizer:  48% 3848/8047 [2:37:20<3:52:55,  3.33s/it]loss_total_epoch 225.65207389183342
Training tokenizer:  48% 3849/8047 [2:37:24<3:52:21,  3.32s/it]loss_total_epoch 225.69199882633984
Training tokenizer:  48% 3850/8047 [2:37:27<3:52:10,  3.32s/it]loss_total_epoch 225.74086458794773
Training tokenizer:  48% 3851/8047 [2:37:30<3:52:39,  3.33s/it]loss_total_epoch 225.7888428810984
Training tokenizer:  48% 3852/8047 [2:37:34<3:53:10,  3.34s/it]loss_total_epoch 225.8359939623624
Training tokenizer:  48% 3853/8047 [2:37:37<3:53:04,  3.33s/it]loss_total_epoch 225.88465476967394
Training tokenizer:  48% 3854/8047 [2:37:40<3:52:19,  3.32s/it]loss_total_epoch 225.93202437274158
Training tokenizer:  48% 3855/8047 [2:37:44<3:52:26,  3.33s/it]loss_total_epoch 225.97824415005744
Training tokenizer:  48% 3856/8047 [2:37:47<3:52:32,  3.33s/it]loss_total_epoch 226.02824962697923
Training tokenizer:  48% 3857/8047 [2:37:50<3:53:05,  3.34s/it]loss_total_epoch 226.07478698901832
Training tokenizer:  48% 3858/8047 [2:37:54<3:52:44,  3.33s/it]loss_total_epoch 226.1247475426644
Training tokenizer:  48% 3859/8047 [2:37:57<3:52:08,  3.33s/it]loss_total_epoch 226.17330954782665
Training tokenizer:  48% 3860/8047 [2:38:00<3:51:01,  3.31s/it]loss_total_epoch 226.23042894341052
Training tokenizer:  48% 3861/8047 [2:38:04<3:51:48,  3.32s/it]loss_total_epoch 226.27874203585088
Training tokenizer:  48% 3862/8047 [2:38:07<3:52:13,  3.33s/it]loss_total_epoch 226.33005833812058
Training tokenizer:  48% 3863/8047 [2:38:10<3:51:50,  3.32s/it]loss_total_epoch 226.37743486650288
Training tokenizer:  48% 3864/8047 [2:38:13<3:51:39,  3.32s/it]loss_total_epoch 226.42160548083484
Training tokenizer:  48% 3865/8047 [2:38:17<3:52:05,  3.33s/it]loss_total_epoch 226.47713845036924
Training tokenizer:  48% 3866/8047 [2:38:20<3:51:26,  3.32s/it]loss_total_epoch 226.53409095294774
Training tokenizer:  48% 3867/8047 [2:38:23<3:52:10,  3.33s/it]loss_total_epoch 226.58578325994313
Training tokenizer:  48% 3868/8047 [2:38:27<3:50:36,  3.31s/it]loss_total_epoch 226.63835587911308
Training tokenizer:  48% 3869/8047 [2:38:30<3:52:20,  3.34s/it]loss_total_epoch 226.68522130139172
Training tokenizer:  48% 3870/8047 [2:38:34<3:52:35,  3.34s/it]loss_total_epoch 226.72582983784378
Training tokenizer:  48% 3871/8047 [2:38:37<3:51:42,  3.33s/it]loss_total_epoch 226.7784604933113
Training tokenizer:  48% 3872/8047 [2:38:40<3:51:14,  3.32s/it]loss_total_epoch 226.82233305834234
Training tokenizer:  48% 3873/8047 [2:38:43<3:51:30,  3.33s/it]loss_total_epoch 226.863276200369
Training tokenizer:  48% 3874/8047 [2:38:47<3:52:17,  3.34s/it]loss_total_epoch 226.91679946891963
Training tokenizer:  48% 3875/8047 [2:38:50<3:51:47,  3.33s/it]loss_total_epoch 226.96105472184718
Training tokenizer:  48% 3876/8047 [2:38:53<3:52:10,  3.34s/it]loss_total_epoch 227.0145146343857
Training tokenizer:  48% 3877/8047 [2:38:57<3:52:11,  3.34s/it]loss_total_epoch 227.0484483037144
Training tokenizer:  48% 3878/8047 [2:39:00<3:52:11,  3.34s/it]loss_total_epoch 227.09323483146727
Training tokenizer:  48% 3879/8047 [2:39:03<3:50:40,  3.32s/it]loss_total_epoch 227.14401264302433
Training tokenizer:  48% 3880/8047 [2:39:07<3:50:50,  3.32s/it]loss_total_epoch 227.20229726471007
Training tokenizer:  48% 3881/8047 [2:39:10<3:51:38,  3.34s/it]loss_total_epoch 227.24152047373354
Training tokenizer:  48% 3882/8047 [2:39:13<3:51:21,  3.33s/it]loss_total_epoch 227.28829021938145
Training tokenizer:  48% 3883/8047 [2:39:17<3:51:50,  3.34s/it]loss_total_epoch 227.3354180213064
Training tokenizer:  48% 3884/8047 [2:39:20<3:52:12,  3.35s/it]loss_total_epoch 227.37550481967628
Training tokenizer:  48% 3885/8047 [2:39:24<3:51:38,  3.34s/it]loss_total_epoch 227.418792007491
Training tokenizer:  48% 3886/8047 [2:39:27<3:51:49,  3.34s/it]loss_total_epoch 227.46497826091945
Training tokenizer:  48% 3887/8047 [2:39:30<3:52:35,  3.35s/it]loss_total_epoch 227.52464916743338
Training tokenizer:  48% 3888/8047 [2:39:34<3:52:08,  3.35s/it]loss_total_epoch 227.57977646775544
Training tokenizer:  48% 3889/8047 [2:39:37<3:51:19,  3.34s/it]loss_total_epoch 227.624309534207
Training tokenizer:  48% 3890/8047 [2:39:40<3:51:54,  3.35s/it]loss_total_epoch 227.67360058240592
Training tokenizer:  48% 3891/8047 [2:39:44<3:51:46,  3.35s/it]loss_total_epoch 227.71902870945632
Training tokenizer:  48% 3892/8047 [2:39:47<3:50:23,  3.33s/it]loss_total_epoch 227.75471592135727
Training tokenizer:  48% 3893/8047 [2:39:50<3:50:16,  3.33s/it]loss_total_epoch 227.80527743138373
Training tokenizer:  48% 3894/8047 [2:39:54<3:50:05,  3.32s/it]loss_total_epoch 227.85305434279144
Training tokenizer:  48% 3895/8047 [2:39:57<3:49:54,  3.32s/it]loss_total_epoch 227.90433738566935
Training tokenizer:  48% 3896/8047 [2:40:00<3:50:37,  3.33s/it]loss_total_epoch 227.9474425818771
Training tokenizer:  48% 3897/8047 [2:40:04<3:50:00,  3.33s/it]loss_total_epoch 228.00309120677412
Training tokenizer:  48% 3898/8047 [2:40:07<3:52:07,  3.36s/it]loss_total_epoch 228.05476363562047
Training tokenizer:  48% 3899/8047 [2:40:10<3:53:35,  3.38s/it]loss_total_epoch 228.09968929551542
Training tokenizer:  48% 3900/8047 [2:40:14<3:53:08,  3.37s/it]loss_total_epoch 228.13915488682687
Training tokenizer:  48% 3901/8047 [2:40:17<3:52:41,  3.37s/it]loss_total_epoch 228.18499376438558
Training tokenizer:  48% 3902/8047 [2:40:20<3:52:04,  3.36s/it]loss_total_epoch 228.2349304240197
Training tokenizer:  49% 3903/8047 [2:40:24<3:51:19,  3.35s/it]loss_total_epoch 228.28595149703324
Training tokenizer:  49% 3904/8047 [2:40:27<3:51:20,  3.35s/it]loss_total_epoch 228.32014192081988
Training tokenizer:  49% 3905/8047 [2:40:30<3:51:13,  3.35s/it]loss_total_epoch 228.37490672804415
Training tokenizer:  49% 3906/8047 [2:40:34<3:50:39,  3.34s/it]loss_total_epoch 228.42765317298472
Training tokenizer:  49% 3907/8047 [2:40:37<3:51:14,  3.35s/it]loss_total_epoch 228.47565185837448
Training tokenizer:  49% 3908/8047 [2:40:41<3:52:00,  3.36s/it]loss_total_epoch 228.5174105670303
Training tokenizer:  49% 3909/8047 [2:40:44<3:52:22,  3.37s/it]loss_total_epoch 228.56520566157997
Training tokenizer:  49% 3910/8047 [2:40:47<3:52:13,  3.37s/it]loss_total_epoch 228.61959888599813
Training tokenizer:  49% 3911/8047 [2:40:51<3:51:44,  3.36s/it]loss_total_epoch 228.67033319734037
Training tokenizer:  49% 3912/8047 [2:40:54<3:51:41,  3.36s/it]loss_total_epoch 228.71049307100475
Training tokenizer:  49% 3913/8047 [2:40:57<3:49:48,  3.34s/it]loss_total_epoch 228.7598769608885
Training tokenizer:  49% 3914/8047 [2:41:01<3:50:43,  3.35s/it]loss_total_epoch 228.80609841831028
Training tokenizer:  49% 3915/8047 [2:41:04<3:50:19,  3.34s/it]loss_total_epoch 228.84879639558494
Training tokenizer:  49% 3916/8047 [2:41:07<3:50:47,  3.35s/it]loss_total_epoch 228.90096553973854
Training tokenizer:  49% 3917/8047 [2:41:11<3:50:21,  3.35s/it]loss_total_epoch 228.94898191280663
Training tokenizer:  49% 3918/8047 [2:41:14<3:50:54,  3.36s/it]loss_total_epoch 228.9972931947559
Training tokenizer:  49% 3919/8047 [2:41:17<3:50:59,  3.36s/it]loss_total_epoch 229.0338834952563
Training tokenizer:  49% 3920/8047 [2:41:21<3:50:05,  3.35s/it]loss_total_epoch 229.0800578650087
Training tokenizer:  49% 3921/8047 [2:41:24<3:51:17,  3.36s/it]loss_total_epoch 229.1235250774771
Training tokenizer:  49% 3922/8047 [2:41:28<3:53:45,  3.40s/it]loss_total_epoch 229.16493496857584
Training tokenizer:  49% 3923/8047 [2:41:31<3:52:53,  3.39s/it]loss_total_epoch 229.21665388159454
Training tokenizer:  49% 3924/8047 [2:41:34<3:52:45,  3.39s/it]loss_total_epoch 229.26677718944848
Training tokenizer:  49% 3925/8047 [2:41:38<3:52:38,  3.39s/it]loss_total_epoch 229.3191018719226
Training tokenizer:  49% 3926/8047 [2:41:41<3:52:14,  3.38s/it]loss_total_epoch 229.37470677308738
Training tokenizer:  49% 3927/8047 [2:41:44<3:50:55,  3.36s/it]loss_total_epoch 229.42555515281856
Training tokenizer:  49% 3928/8047 [2:41:48<3:51:07,  3.37s/it]loss_total_epoch 229.47796702571213
Training tokenizer:  49% 3929/8047 [2:41:51<3:50:26,  3.36s/it]loss_total_epoch 229.5284851361066
Training tokenizer:  49% 3930/8047 [2:41:55<3:50:10,  3.35s/it]loss_total_epoch 229.5710766930133
Training tokenizer:  49% 3931/8047 [2:41:58<3:50:18,  3.36s/it]loss_total_epoch 229.62101590819657
Training tokenizer:  49% 3932/8047 [2:42:01<3:50:58,  3.37s/it]loss_total_epoch 229.67282423190773
Training tokenizer:  49% 3933/8047 [2:42:05<3:51:08,  3.37s/it]loss_total_epoch 229.7224975693971
Training tokenizer:  49% 3934/8047 [2:42:08<3:49:47,  3.35s/it]loss_total_epoch 229.7712682057172
Training tokenizer:  49% 3935/8047 [2:42:11<3:49:45,  3.35s/it]loss_total_epoch 229.81635152362287
Training tokenizer:  49% 3936/8047 [2:42:15<3:50:20,  3.36s/it]loss_total_epoch 229.8697559442371
Training tokenizer:  49% 3937/8047 [2:42:18<3:50:17,  3.36s/it]loss_total_epoch 229.92544664628804
Training tokenizer:  49% 3938/8047 [2:42:21<3:49:35,  3.35s/it]loss_total_epoch 229.97346469201148
Training tokenizer:  49% 3939/8047 [2:42:25<3:50:36,  3.37s/it]loss_total_epoch 230.01748294569552
Training tokenizer:  49% 3940/8047 [2:42:28<3:50:05,  3.36s/it]loss_total_epoch 230.05326350964606
Training tokenizer:  49% 3941/8047 [2:42:32<3:50:47,  3.37s/it]loss_total_epoch 230.10500069148839
Training tokenizer:  49% 3942/8047 [2:42:35<3:51:36,  3.39s/it]loss_total_epoch 230.14760059677064
Training tokenizer:  49% 3943/8047 [2:42:38<3:51:08,  3.38s/it]loss_total_epoch 230.18794566951692
Training tokenizer:  49% 3944/8047 [2:42:42<3:50:32,  3.37s/it]loss_total_epoch 230.2432024758309
Training tokenizer:  49% 3945/8047 [2:42:45<3:49:40,  3.36s/it]loss_total_epoch 230.30079339258373
Training tokenizer:  49% 3946/8047 [2:42:48<3:49:17,  3.35s/it]loss_total_epoch 230.3472184818238
Training tokenizer:  49% 3947/8047 [2:42:52<3:50:08,  3.37s/it]loss_total_epoch 230.40586064569652
Training tokenizer:  49% 3948/8047 [2:42:55<3:50:34,  3.38s/it]loss_total_epoch 230.45190319232643
Training tokenizer:  49% 3949/8047 [2:42:59<3:50:09,  3.37s/it]loss_total_epoch 230.5056884828955
Training tokenizer:  49% 3950/8047 [2:43:02<3:49:44,  3.36s/it]loss_total_epoch 230.54339002259076
Training tokenizer:  49% 3951/8047 [2:43:05<3:50:24,  3.38s/it]loss_total_epoch 230.59231749363244
Training tokenizer:  49% 3952/8047 [2:43:09<3:50:14,  3.37s/it]loss_total_epoch 230.64435487426817
Training tokenizer:  49% 3953/8047 [2:43:12<3:49:29,  3.36s/it]loss_total_epoch 230.69050374813378
Training tokenizer:  49% 3954/8047 [2:43:15<3:48:37,  3.35s/it]loss_total_epoch 230.73656296171248
Training tokenizer:  49% 3955/8047 [2:43:19<3:48:43,  3.35s/it]loss_total_epoch 230.78901942260563
Training tokenizer:  49% 3956/8047 [2:43:22<3:50:14,  3.38s/it]loss_total_epoch 230.82992848940194
Training tokenizer:  49% 3957/8047 [2:43:25<3:50:38,  3.38s/it]loss_total_epoch 230.87809053994715
Training tokenizer:  49% 3958/8047 [2:43:29<3:49:43,  3.37s/it]loss_total_epoch 230.93239154852927
Training tokenizer:  49% 3959/8047 [2:43:32<3:49:21,  3.37s/it]loss_total_epoch 230.97715753503144
Training tokenizer:  49% 3960/8047 [2:43:36<3:48:58,  3.36s/it]loss_total_epoch 231.01622521318495
Training tokenizer:  49% 3961/8047 [2:43:39<3:49:03,  3.36s/it]loss_total_epoch 231.0492445845157
Training tokenizer:  49% 3962/8047 [2:43:42<3:48:34,  3.36s/it]loss_total_epoch 231.10564418695867
Training tokenizer:  49% 3963/8047 [2:43:46<3:48:07,  3.35s/it]loss_total_epoch 231.1398142632097
Training tokenizer:  49% 3964/8047 [2:43:49<3:49:01,  3.37s/it]loss_total_epoch 231.19196022115648
Training tokenizer:  49% 3965/8047 [2:43:52<3:49:52,  3.38s/it]loss_total_epoch 231.2356939893216
Training tokenizer:  49% 3966/8047 [2:43:56<3:48:40,  3.36s/it]loss_total_epoch 231.2748539801687
Training tokenizer:  49% 3967/8047 [2:43:59<3:48:42,  3.36s/it]loss_total_epoch 231.32158814556897
Training tokenizer:  49% 3968/8047 [2:44:02<3:48:48,  3.37s/it]loss_total_epoch 231.37617474980652
Training tokenizer:  49% 3969/8047 [2:44:06<3:49:34,  3.38s/it]loss_total_epoch 231.42468778975308
Training tokenizer:  49% 3970/8047 [2:44:09<3:49:32,  3.38s/it]loss_total_epoch 231.47644222341478
Training tokenizer:  49% 3971/8047 [2:44:13<3:49:35,  3.38s/it]loss_total_epoch 231.51172254793346
Training tokenizer:  49% 3972/8047 [2:44:16<3:50:21,  3.39s/it]loss_total_epoch 231.54906536825
Training tokenizer:  49% 3973/8047 [2:44:19<3:49:02,  3.37s/it]loss_total_epoch 231.60251328907907
Training tokenizer:  49% 3974/8047 [2:44:23<3:48:45,  3.37s/it]loss_total_epoch 231.6559004392475
Training tokenizer:  49% 3975/8047 [2:44:26<3:49:04,  3.38s/it]loss_total_epoch 231.7073704507202
Training tokenizer:  49% 3976/8047 [2:44:30<3:49:22,  3.38s/it]loss_total_epoch 231.74736524559557
Training tokenizer:  49% 3977/8047 [2:44:33<3:49:20,  3.38s/it]loss_total_epoch 231.79330838657916
Training tokenizer:  49% 3978/8047 [2:44:36<3:49:17,  3.38s/it]loss_total_epoch 231.8405065406114
Training tokenizer:  49% 3979/8047 [2:44:40<3:48:53,  3.38s/it]loss_total_epoch 231.88094803877175
Training tokenizer:  49% 3980/8047 [2:44:43<3:49:09,  3.38s/it]loss_total_epoch 231.92585443519056
Training tokenizer:  49% 3981/8047 [2:44:46<3:49:16,  3.38s/it]loss_total_epoch 231.97222656570375
Training tokenizer:  49% 3982/8047 [2:44:50<3:49:35,  3.39s/it]loss_total_epoch 232.02155629359186
Training tokenizer:  49% 3983/8047 [2:44:53<3:49:24,  3.39s/it]loss_total_epoch 232.07108427025378
Training tokenizer:  50% 3984/8047 [2:44:57<3:49:09,  3.38s/it]loss_total_epoch 232.1301340330392
Training tokenizer:  50% 3985/8047 [2:45:00<3:48:51,  3.38s/it]loss_total_epoch 232.16804259084165
Training tokenizer:  50% 3986/8047 [2:45:03<3:48:06,  3.37s/it]loss_total_epoch 232.21256852336228
Training tokenizer:  50% 3987/8047 [2:45:07<3:48:17,  3.37s/it]loss_total_epoch 232.2624755781144
Training tokenizer:  50% 3988/8047 [2:45:10<3:48:32,  3.38s/it]loss_total_epoch 232.3159538488835
Training tokenizer:  50% 3989/8047 [2:45:13<3:48:55,  3.38s/it]loss_total_epoch 232.35689316131175
Training tokenizer:  50% 3990/8047 [2:45:17<3:48:54,  3.39s/it]loss_total_epoch 232.40199323184788
Training tokenizer:  50% 3991/8047 [2:45:20<3:48:33,  3.38s/it]loss_total_epoch 232.44191871769726
Training tokenizer:  50% 3992/8047 [2:45:24<3:49:19,  3.39s/it]loss_total_epoch 232.49812778271735
Training tokenizer:  50% 3993/8047 [2:45:27<3:48:40,  3.38s/it]loss_total_epoch 232.5562152620405
Training tokenizer:  50% 3994/8047 [2:45:30<3:48:32,  3.38s/it]loss_total_epoch 232.61549953185022
Training tokenizer:  50% 3995/8047 [2:45:34<3:48:49,  3.39s/it]loss_total_epoch 232.6639288570732
Training tokenizer:  50% 3996/8047 [2:45:37<3:48:49,  3.39s/it]loss_total_epoch 232.71480578370392
Training tokenizer:  50% 3997/8047 [2:45:41<3:48:41,  3.39s/it]loss_total_epoch 232.76156584359705
Training tokenizer:  50% 3998/8047 [2:45:44<3:48:06,  3.38s/it]loss_total_epoch 232.8083521667868
Training tokenizer:  50% 3999/8047 [2:45:47<3:49:11,  3.40s/it]loss_total_epoch 232.86139759980142
Training tokenizer:  50% 4000/8047 [2:45:51<3:48:44,  3.39s/it]loss_total_epoch 232.91049269028008
Training tokenizer:  50% 4001/8047 [2:45:54<3:48:16,  3.39s/it]loss_total_epoch 232.96772366203368
Training tokenizer:  50% 4002/8047 [2:45:57<3:47:30,  3.37s/it]loss_total_epoch 233.0098471660167
Training tokenizer:  50% 4003/8047 [2:46:01<3:47:28,  3.37s/it]loss_total_epoch 233.06901918165386
Training tokenizer:  50% 4004/8047 [2:46:04<3:47:51,  3.38s/it]loss_total_epoch 233.11907568015158
Training tokenizer:  50% 4005/8047 [2:46:08<3:48:51,  3.40s/it]loss_total_epoch 233.16475641913712
Training tokenizer:  50% 4006/8047 [2:46:11<3:48:49,  3.40s/it]loss_total_epoch 233.21711526997387
Training tokenizer:  50% 4007/8047 [2:46:14<3:48:06,  3.39s/it]loss_total_epoch 233.26056022383273
Training tokenizer:  50% 4008/8047 [2:46:18<3:48:22,  3.39s/it]loss_total_epoch 233.30711630545557
Training tokenizer:  50% 4009/8047 [2:46:21<3:48:14,  3.39s/it]loss_total_epoch 233.3537114020437
Training tokenizer:  50% 4010/8047 [2:46:25<3:48:24,  3.39s/it]loss_total_epoch 233.40870261006057
Training tokenizer:  50% 4011/8047 [2:46:28<3:48:28,  3.40s/it]loss_total_epoch 233.45093894563615
Training tokenizer:  50% 4012/8047 [2:46:31<3:48:43,  3.40s/it]loss_total_epoch 233.48609896562994
Training tokenizer:  50% 4013/8047 [2:46:35<3:48:01,  3.39s/it]loss_total_epoch 233.53376932628453
Training tokenizer:  50% 4014/8047 [2:46:38<3:48:32,  3.40s/it]loss_total_epoch 233.58055274002254
Training tokenizer:  50% 4015/8047 [2:46:42<3:48:37,  3.40s/it]loss_total_epoch 233.63168944604695
Training tokenizer:  50% 4016/8047 [2:46:45<3:49:14,  3.41s/it]loss_total_epoch 233.68220195360482
Training tokenizer:  50% 4017/8047 [2:46:49<3:53:22,  3.47s/it]loss_total_epoch 233.73432747460902
Training tokenizer:  50% 4018/8047 [2:46:52<3:50:05,  3.43s/it]loss_total_epoch 233.78087778575718
Training tokenizer:  50% 4019/8047 [2:46:55<3:49:15,  3.41s/it]loss_total_epoch 233.82649597339332
Training tokenizer:  50% 4020/8047 [2:46:59<3:48:56,  3.41s/it]loss_total_epoch 233.86951243691146
Training tokenizer:  50% 4021/8047 [2:47:02<3:48:44,  3.41s/it]loss_total_epoch 233.91200981102884
Training tokenizer:  50% 4022/8047 [2:47:06<3:47:36,  3.39s/it]loss_total_epoch 233.95116882957518
Training tokenizer:  50% 4023/8047 [2:47:09<3:46:32,  3.38s/it]loss_total_epoch 233.99601024575531
Training tokenizer:  50% 4024/8047 [2:47:12<3:47:07,  3.39s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-ti47_a1r'
loss_total_epoch 234.05190112628043
Training tokenizer:  50% 4025/8047 [2:47:16<3:46:29,  3.38s/it]loss_total_epoch 234.10790262930095
Training tokenizer:  50% 4026/8047 [2:47:19<3:46:29,  3.38s/it]loss_total_epoch 234.15482996217906
Training tokenizer:  50% 4027/8047 [2:47:22<3:47:09,  3.39s/it]loss_total_epoch 234.19349457509816
Training tokenizer:  50% 4028/8047 [2:47:26<3:47:47,  3.40s/it]loss_total_epoch 234.239571897313
Training tokenizer:  50% 4029/8047 [2:47:29<3:48:00,  3.40s/it]loss_total_epoch 234.28749093972147
Training tokenizer:  50% 4030/8047 [2:47:33<3:48:50,  3.42s/it]loss_total_epoch 234.3545906599611
Training tokenizer:  50% 4031/8047 [2:47:36<3:48:43,  3.42s/it]loss_total_epoch 234.40011652745306
Training tokenizer:  50% 4032/8047 [2:47:40<3:49:24,  3.43s/it]loss_total_epoch 234.45734055899084
Training tokenizer:  50% 4033/8047 [2:47:43<3:48:48,  3.42s/it]loss_total_epoch 234.50254946760833
Training tokenizer:  50% 4034/8047 [2:47:46<3:48:56,  3.42s/it]loss_total_epoch 234.54709300212562
Training tokenizer:  50% 4035/8047 [2:47:50<3:47:46,  3.41s/it]loss_total_epoch 234.60147173516452
Training tokenizer:  50% 4036/8047 [2:47:53<3:47:22,  3.40s/it]loss_total_epoch 234.6487137954682
Training tokenizer:  50% 4037/8047 [2:47:57<3:47:23,  3.40s/it]loss_total_epoch 234.69871600531042
Training tokenizer:  50% 4038/8047 [2:48:00<3:48:27,  3.42s/it]loss_total_epoch 234.7342832479626
Training tokenizer:  50% 4039/8047 [2:48:03<3:48:08,  3.42s/it]loss_total_epoch 234.77464923076332
Training tokenizer:  50% 4040/8047 [2:48:07<3:47:45,  3.41s/it]loss_total_epoch 234.82269378565252
Training tokenizer:  50% 4041/8047 [2:48:10<3:48:07,  3.42s/it]loss_total_epoch 234.86838327161968
Training tokenizer:  50% 4042/8047 [2:48:14<3:46:57,  3.40s/it]loss_total_epoch 234.92187961004674
Training tokenizer:  50% 4043/8047 [2:48:17<3:47:43,  3.41s/it]loss_total_epoch 234.9743782710284
Training tokenizer:  50% 4044/8047 [2:48:21<3:47:39,  3.41s/it]loss_total_epoch 235.03222802840173
Training tokenizer:  50% 4045/8047 [2:48:24<3:46:40,  3.40s/it]loss_total_epoch 235.08315829001367
Training tokenizer:  50% 4046/8047 [2:48:27<3:47:00,  3.40s/it]loss_total_epoch 235.12794524617493
Training tokenizer:  50% 4047/8047 [2:48:31<3:46:29,  3.40s/it]loss_total_epoch 235.1626587342471
Training tokenizer:  50% 4048/8047 [2:48:34<3:47:14,  3.41s/it]loss_total_epoch 235.21285421960056
Training tokenizer:  50% 4049/8047 [2:48:38<3:46:33,  3.40s/it]loss_total_epoch 235.26014467142522
Training tokenizer:  50% 4050/8047 [2:48:41<3:46:17,  3.40s/it]loss_total_epoch 235.31541360355914
Training tokenizer:  50% 4051/8047 [2:48:44<3:46:18,  3.40s/it]loss_total_epoch 235.3581623788923
Training tokenizer:  50% 4052/8047 [2:48:48<3:47:21,  3.41s/it]loss_total_epoch 235.40356611274183
Training tokenizer:  50% 4053/8047 [2:48:51<3:47:32,  3.42s/it]loss_total_epoch 235.4573927950114
Training tokenizer:  50% 4054/8047 [2:48:55<3:46:48,  3.41s/it]loss_total_epoch 235.49290113337338
Training tokenizer:  50% 4055/8047 [2:48:58<3:46:44,  3.41s/it]loss_total_epoch 235.53657047264278
Training tokenizer:  50% 4056/8047 [2:49:01<3:47:48,  3.42s/it]loss_total_epoch 235.5842210110277
Training tokenizer:  50% 4057/8047 [2:49:05<3:48:12,  3.43s/it]loss_total_epoch 235.63656269945204
Training tokenizer:  50% 4058/8047 [2:49:08<3:47:19,  3.42s/it]loss_total_epoch 235.68071456067264
Training tokenizer:  50% 4059/8047 [2:49:12<3:47:34,  3.42s/it]loss_total_epoch 235.7330010700971
Training tokenizer:  50% 4060/8047 [2:49:15<3:47:11,  3.42s/it]loss_total_epoch 235.77810243703425
Training tokenizer:  50% 4061/8047 [2:49:19<3:47:05,  3.42s/it]loss_total_epoch 235.82236466743052
Training tokenizer:  50% 4062/8047 [2:49:22<3:46:33,  3.41s/it]loss_total_epoch 235.87069147266448
Training tokenizer:  50% 4063/8047 [2:49:25<3:46:30,  3.41s/it]loss_total_epoch 235.9269846137613
Training tokenizer:  51% 4064/8047 [2:49:29<3:46:18,  3.41s/it]loss_total_epoch 235.97466829232872
Training tokenizer:  51% 4065/8047 [2:49:32<3:46:16,  3.41s/it]loss_total_epoch 236.02558158896863
Training tokenizer:  51% 4066/8047 [2:49:36<3:46:32,  3.41s/it]loss_total_epoch 236.0651924032718
Training tokenizer:  51% 4067/8047 [2:49:39<3:45:55,  3.41s/it]loss_total_epoch 236.1128168385476
Training tokenizer:  51% 4068/8047 [2:49:42<3:46:09,  3.41s/it]loss_total_epoch 236.1597921270877
Training tokenizer:  51% 4069/8047 [2:49:46<3:46:10,  3.41s/it]loss_total_epoch 236.2161616254598
Training tokenizer:  51% 4070/8047 [2:49:49<3:46:21,  3.42s/it]loss_total_epoch 236.27038908191025
Training tokenizer:  51% 4071/8047 [2:49:53<3:46:32,  3.42s/it]loss_total_epoch 236.31831163354218
Training tokenizer:  51% 4072/8047 [2:49:56<3:46:58,  3.43s/it]loss_total_epoch 236.36140460707247
Training tokenizer:  51% 4073/8047 [2:50:00<3:47:32,  3.44s/it]loss_total_epoch 236.41307195834816
Training tokenizer:  51% 4074/8047 [2:50:03<3:46:37,  3.42s/it]loss_total_epoch 236.47172873280942
Training tokenizer:  51% 4075/8047 [2:50:06<3:46:35,  3.42s/it]loss_total_epoch 236.51047396846116
Training tokenizer:  51% 4076/8047 [2:50:10<3:47:14,  3.43s/it]loss_total_epoch 236.55991468764842
Training tokenizer:  51% 4077/8047 [2:50:13<3:46:33,  3.42s/it]loss_total_epoch 236.58945425972342
Training tokenizer:  51% 4078/8047 [2:50:17<3:46:50,  3.43s/it]loss_total_epoch 236.63285034522414
Training tokenizer:  51% 4079/8047 [2:50:20<3:46:49,  3.43s/it]loss_total_epoch 236.68071452900767
Training tokenizer:  51% 4080/8047 [2:50:23<3:45:32,  3.41s/it]loss_total_epoch 236.73039440065622
Training tokenizer:  51% 4081/8047 [2:50:27<3:45:33,  3.41s/it]loss_total_epoch 236.7817450016737
Training tokenizer:  51% 4082/8047 [2:50:30<3:45:38,  3.41s/it]loss_total_epoch 236.80893278121948
Training tokenizer:  51% 4083/8047 [2:50:34<3:45:07,  3.41s/it]loss_total_epoch 236.86224166676402
Training tokenizer:  51% 4084/8047 [2:50:37<3:45:40,  3.42s/it]loss_total_epoch 236.90634835883975
Training tokenizer:  51% 4085/8047 [2:50:40<3:44:39,  3.40s/it]loss_total_epoch 236.96142481267452
Training tokenizer:  51% 4086/8047 [2:50:44<3:45:11,  3.41s/it]loss_total_epoch 237.0118630565703
Training tokenizer:  51% 4087/8047 [2:50:47<3:45:01,  3.41s/it]loss_total_epoch 237.05868046358228
Training tokenizer:  51% 4088/8047 [2:50:51<3:45:47,  3.42s/it]loss_total_epoch 237.10962384566665
Training tokenizer:  51% 4089/8047 [2:50:54<3:46:05,  3.43s/it]loss_total_epoch 237.15570049360394
Training tokenizer:  51% 4090/8047 [2:50:58<3:45:20,  3.42s/it]loss_total_epoch 237.1957139186561
Training tokenizer:  51% 4091/8047 [2:51:01<3:45:50,  3.43s/it]loss_total_epoch 237.24423491954803
Training tokenizer:  51% 4092/8047 [2:51:05<3:46:14,  3.43s/it]loss_total_epoch 237.2822775542736
Training tokenizer:  51% 4093/8047 [2:51:08<3:46:15,  3.43s/it]loss_total_epoch 237.3354014158249
Training tokenizer:  51% 4094/8047 [2:51:11<3:46:24,  3.44s/it]loss_total_epoch 237.38359919190407
Training tokenizer:  51% 4095/8047 [2:51:15<3:46:10,  3.43s/it]loss_total_epoch 237.4305535927415
Training tokenizer:  51% 4096/8047 [2:51:18<3:45:34,  3.43s/it]loss_total_epoch 237.4832153469324
Training tokenizer:  51% 4097/8047 [2:51:22<3:45:28,  3.42s/it]loss_total_epoch 237.5212135873735
Training tokenizer:  51% 4098/8047 [2:51:25<3:46:11,  3.44s/it]loss_total_epoch 237.57459846511483
Training tokenizer:  51% 4099/8047 [2:51:29<3:46:43,  3.45s/it]loss_total_epoch 237.60958372056484
Training tokenizer:  51% 4100/8047 [2:51:32<3:46:18,  3.44s/it]loss_total_epoch 237.63995095528662
Training tokenizer:  51% 4101/8047 [2:51:35<3:45:51,  3.43s/it]loss_total_epoch 237.68215712346137
Training tokenizer:  51% 4102/8047 [2:51:39<3:45:40,  3.43s/it]loss_total_epoch 237.72040618397295
Training tokenizer:  51% 4103/8047 [2:51:42<3:44:28,  3.41s/it]loss_total_epoch 237.77161472849548
Training tokenizer:  51% 4104/8047 [2:51:46<3:45:01,  3.42s/it]loss_total_epoch 237.80810877121985
Training tokenizer:  51% 4105/8047 [2:51:49<3:45:42,  3.44s/it]loss_total_epoch 237.85313442908227
Training tokenizer:  51% 4106/8047 [2:51:53<3:45:26,  3.43s/it]loss_total_epoch 237.89835022203624
Training tokenizer:  51% 4107/8047 [2:51:56<3:44:32,  3.42s/it]loss_total_epoch 237.9515842553228
Training tokenizer:  51% 4108/8047 [2:51:59<3:44:24,  3.42s/it]loss_total_epoch 237.99884311668575
Training tokenizer:  51% 4109/8047 [2:52:03<3:44:51,  3.43s/it]loss_total_epoch 238.0563121829182
Training tokenizer:  51% 4110/8047 [2:52:06<3:45:04,  3.43s/it]loss_total_epoch 238.11023602448404
Training tokenizer:  51% 4111/8047 [2:52:10<3:45:42,  3.44s/it]loss_total_epoch 238.15508736856282
Training tokenizer:  51% 4112/8047 [2:52:13<3:45:07,  3.43s/it]loss_total_epoch 238.20938922651112
Training tokenizer:  51% 4113/8047 [2:52:17<3:44:25,  3.42s/it]loss_total_epoch 238.2607093024999
Training tokenizer:  51% 4114/8047 [2:52:20<3:45:08,  3.43s/it]loss_total_epoch 238.3080565650016
Training tokenizer:  51% 4115/8047 [2:52:23<3:44:39,  3.43s/it]loss_total_epoch 238.35900560207665
Training tokenizer:  51% 4116/8047 [2:52:27<3:45:24,  3.44s/it]loss_total_epoch 238.41298265196383
Training tokenizer:  51% 4117/8047 [2:52:30<3:46:06,  3.45s/it]loss_total_epoch 238.4558315668255
Training tokenizer:  51% 4118/8047 [2:52:34<3:44:50,  3.43s/it]loss_total_epoch 238.49476679973304
Training tokenizer:  51% 4119/8047 [2:52:37<3:46:03,  3.45s/it]loss_total_epoch 238.54098862968385
Training tokenizer:  51% 4120/8047 [2:52:41<3:45:55,  3.45s/it]loss_total_epoch 238.58910833112895
Training tokenizer:  51% 4121/8047 [2:52:44<3:44:34,  3.43s/it]loss_total_epoch 238.64340244047344
Training tokenizer:  51% 4122/8047 [2:52:48<3:45:06,  3.44s/it]loss_total_epoch 238.6876031178981
Training tokenizer:  51% 4123/8047 [2:52:51<3:43:38,  3.42s/it]loss_total_epoch 238.74416985921562
Training tokenizer:  51% 4124/8047 [2:52:54<3:43:58,  3.43s/it]loss_total_epoch 238.8072170075029
Training tokenizer:  51% 4125/8047 [2:52:58<3:44:31,  3.43s/it]loss_total_epoch 238.851446473971
Training tokenizer:  51% 4126/8047 [2:53:01<3:44:17,  3.43s/it]loss_total_epoch 238.89024459011853
Training tokenizer:  51% 4127/8047 [2:53:05<3:44:23,  3.43s/it]loss_total_epoch 238.9382003825158
Training tokenizer:  51% 4128/8047 [2:53:08<3:44:38,  3.44s/it]loss_total_epoch 238.98930269293487
Training tokenizer:  51% 4129/8047 [2:53:12<3:45:29,  3.45s/it]loss_total_epoch 239.04555630497634
Training tokenizer:  51% 4130/8047 [2:53:15<3:44:55,  3.45s/it]loss_total_epoch 239.10398319549859
Training tokenizer:  51% 4131/8047 [2:53:18<3:45:19,  3.45s/it]loss_total_epoch 239.1452943366021
Training tokenizer:  51% 4132/8047 [2:53:22<3:45:37,  3.46s/it]loss_total_epoch 239.19312372617424
Training tokenizer:  51% 4133/8047 [2:53:25<3:45:58,  3.46s/it]loss_total_epoch 239.23202905617654
Training tokenizer:  51% 4134/8047 [2:53:29<3:45:36,  3.46s/it]loss_total_epoch 239.27305120788515
Training tokenizer:  51% 4135/8047 [2:53:32<3:44:52,  3.45s/it]loss_total_epoch 239.32784811593592
Training tokenizer:  51% 4136/8047 [2:53:36<3:44:47,  3.45s/it]loss_total_epoch 239.37963901273906
Training tokenizer:  51% 4137/8047 [2:53:39<3:45:08,  3.45s/it]loss_total_epoch 239.41829621978104
Training tokenizer:  51% 4138/8047 [2:53:43<3:44:56,  3.45s/it]loss_total_epoch 239.46530911885202
Training tokenizer:  51% 4139/8047 [2:53:46<3:44:38,  3.45s/it]loss_total_epoch 239.50807423330843
Training tokenizer:  51% 4140/8047 [2:53:50<3:45:14,  3.46s/it]loss_total_epoch 239.5527187604457
Training tokenizer:  51% 4141/8047 [2:53:53<3:45:33,  3.46s/it]loss_total_epoch 239.60991518385708
Training tokenizer:  51% 4142/8047 [2:53:57<3:45:16,  3.46s/it]loss_total_epoch 239.66367329843342
Training tokenizer:  51% 4143/8047 [2:54:00<3:44:51,  3.46s/it]loss_total_epoch 239.70432450436056
Training tokenizer:  51% 4144/8047 [2:54:03<3:45:25,  3.47s/it]loss_total_epoch 239.75034913606942
Training tokenizer:  52% 4145/8047 [2:54:07<3:44:55,  3.46s/it]loss_total_epoch 239.80664332397282
Training tokenizer:  52% 4146/8047 [2:54:10<3:44:54,  3.46s/it]loss_total_epoch 239.85360633023083
Training tokenizer:  52% 4147/8047 [2:54:14<3:44:44,  3.46s/it]loss_total_epoch 239.9105922561139
Training tokenizer:  52% 4148/8047 [2:54:17<3:44:36,  3.46s/it]loss_total_epoch 239.9482995774597
Training tokenizer:  52% 4149/8047 [2:54:21<3:44:19,  3.45s/it]loss_total_epoch 239.99010841362178
Training tokenizer:  52% 4150/8047 [2:54:24<3:43:31,  3.44s/it]loss_total_epoch 240.03244761936367
Training tokenizer:  52% 4151/8047 [2:54:28<3:43:29,  3.44s/it]loss_total_epoch 240.08432696945965
Training tokenizer:  52% 4152/8047 [2:54:31<3:43:08,  3.44s/it]loss_total_epoch 240.12861077301204
Training tokenizer:  52% 4153/8047 [2:54:34<3:43:05,  3.44s/it]loss_total_epoch 240.17368496768177
Training tokenizer:  52% 4154/8047 [2:54:38<3:42:44,  3.43s/it]loss_total_epoch 240.2255280483514
Training tokenizer:  52% 4155/8047 [2:54:41<3:43:41,  3.45s/it]loss_total_epoch 240.2634250614792
Training tokenizer:  52% 4156/8047 [2:54:45<3:44:11,  3.46s/it]loss_total_epoch 240.31693335808814
Training tokenizer:  52% 4157/8047 [2:54:48<3:44:25,  3.46s/it]loss_total_epoch 240.35969643853605
Training tokenizer:  52% 4158/8047 [2:54:52<3:44:40,  3.47s/it]loss_total_epoch 240.40105826966465
Training tokenizer:  52% 4159/8047 [2:54:55<3:44:36,  3.47s/it]loss_total_epoch 240.45037318579853
Training tokenizer:  52% 4160/8047 [2:54:59<3:44:28,  3.47s/it]loss_total_epoch 240.5031458567828
Training tokenizer:  52% 4161/8047 [2:55:02<3:44:27,  3.47s/it]loss_total_epoch 240.5537348780781
Training tokenizer:  52% 4162/8047 [2:55:06<3:44:10,  3.46s/it]loss_total_epoch 240.59877219237387
Training tokenizer:  52% 4163/8047 [2:55:09<3:43:55,  3.46s/it]loss_total_epoch 240.6528370063752
Training tokenizer:  52% 4164/8047 [2:55:13<3:44:18,  3.47s/it]loss_total_epoch 240.69955989904702
Training tokenizer:  52% 4165/8047 [2:55:16<3:43:51,  3.46s/it]loss_total_epoch 240.75525551475585
Training tokenizer:  52% 4166/8047 [2:55:19<3:44:01,  3.46s/it]loss_total_epoch 240.80045035667717
Training tokenizer:  52% 4167/8047 [2:55:23<3:44:04,  3.47s/it]loss_total_epoch 240.84433173574507
Training tokenizer:  52% 4168/8047 [2:55:26<3:44:07,  3.47s/it]loss_total_epoch 240.89111664704978
Training tokenizer:  52% 4169/8047 [2:55:30<3:43:21,  3.46s/it]loss_total_epoch 240.92654496617615
Training tokenizer:  52% 4170/8047 [2:55:33<3:43:47,  3.46s/it]loss_total_epoch 240.9809768293053
Training tokenizer:  52% 4171/8047 [2:55:37<3:44:08,  3.47s/it]loss_total_epoch 241.01898973621428
Training tokenizer:  52% 4172/8047 [2:55:40<3:43:16,  3.46s/it]loss_total_epoch 241.07904070056975
Training tokenizer:  52% 4173/8047 [2:55:44<3:43:29,  3.46s/it]loss_total_epoch 241.11766734533012
Training tokenizer:  52% 4174/8047 [2:55:47<3:43:32,  3.46s/it]loss_total_epoch 241.16343146376312
Training tokenizer:  52% 4175/8047 [2:55:51<3:43:20,  3.46s/it]loss_total_epoch 241.22091673500836
Training tokenizer:  52% 4176/8047 [2:55:54<3:43:07,  3.46s/it]loss_total_epoch 241.2789685409516
Training tokenizer:  52% 4177/8047 [2:55:58<3:43:14,  3.46s/it]loss_total_epoch 241.32719490118325
Training tokenizer:  52% 4178/8047 [2:56:01<3:43:02,  3.46s/it]loss_total_epoch 241.3818401452154
Training tokenizer:  52% 4179/8047 [2:56:05<3:44:01,  3.48s/it]loss_total_epoch 241.42707203142345
Training tokenizer:  52% 4180/8047 [2:56:08<3:44:05,  3.48s/it]loss_total_epoch 241.47558719851077
Training tokenizer:  52% 4181/8047 [2:56:11<3:44:02,  3.48s/it]loss_total_epoch 241.53335216455162
Training tokenizer:  52% 4182/8047 [2:56:15<3:43:52,  3.48s/it]loss_total_epoch 241.5840547848493
Training tokenizer:  52% 4183/8047 [2:56:18<3:44:14,  3.48s/it]loss_total_epoch 241.63104244880378
Training tokenizer:  52% 4184/8047 [2:56:22<3:44:33,  3.49s/it]loss_total_epoch 241.6861259844154
Training tokenizer:  52% 4185/8047 [2:56:25<3:44:42,  3.49s/it]loss_total_epoch 241.73173752985895
Training tokenizer:  52% 4186/8047 [2:56:29<3:44:59,  3.50s/it]loss_total_epoch 241.78317189775407
Training tokenizer:  52% 4187/8047 [2:56:32<3:44:24,  3.49s/it]loss_total_epoch 241.82173052243888
Training tokenizer:  52% 4188/8047 [2:56:36<3:43:35,  3.48s/it]loss_total_epoch 241.87891880981624
Training tokenizer:  52% 4189/8047 [2:56:39<3:43:09,  3.47s/it]loss_total_epoch 241.92727724649012
Training tokenizer:  52% 4190/8047 [2:56:43<3:41:32,  3.45s/it]loss_total_epoch 241.97518703900278
Training tokenizer:  52% 4191/8047 [2:56:46<3:41:51,  3.45s/it]loss_total_epoch 242.01592549867928
Training tokenizer:  52% 4192/8047 [2:56:50<3:43:17,  3.48s/it]loss_total_epoch 242.04632868245244
Training tokenizer:  52% 4193/8047 [2:56:53<3:43:24,  3.48s/it]loss_total_epoch 242.09307501092553
Training tokenizer:  52% 4194/8047 [2:56:57<3:42:15,  3.46s/it]loss_total_epoch 242.12862393260002
Training tokenizer:  52% 4195/8047 [2:57:00<3:42:09,  3.46s/it]loss_total_epoch 242.18243614956737
Training tokenizer:  52% 4196/8047 [2:57:04<3:42:30,  3.47s/it]loss_total_epoch 242.22361207008362
Training tokenizer:  52% 4197/8047 [2:57:07<3:42:50,  3.47s/it]loss_total_epoch 242.27365770190954
Training tokenizer:  52% 4198/8047 [2:57:11<3:42:14,  3.46s/it]loss_total_epoch 242.3097585104406
Training tokenizer:  52% 4199/8047 [2:57:14<3:42:40,  3.47s/it]loss_total_epoch 242.35904978588223
Training tokenizer:  52% 4200/8047 [2:57:17<3:42:31,  3.47s/it]loss_total_epoch 242.40071445330977
Training tokenizer:  52% 4201/8047 [2:57:21<3:41:48,  3.46s/it]loss_total_epoch 242.44940746203065
Training tokenizer:  52% 4202/8047 [2:57:24<3:42:57,  3.48s/it]loss_total_epoch 242.4941485337913
Training tokenizer:  52% 4203/8047 [2:57:28<3:42:31,  3.47s/it]loss_total_epoch 242.5414239615202
Training tokenizer:  52% 4204/8047 [2:57:31<3:42:58,  3.48s/it]loss_total_epoch 242.59657691791654
Training tokenizer:  52% 4205/8047 [2:57:35<3:42:13,  3.47s/it]loss_total_epoch 242.65334179997444
Training tokenizer:  52% 4206/8047 [2:57:38<3:41:58,  3.47s/it]loss_total_epoch 242.7027468457818
Training tokenizer:  52% 4207/8047 [2:57:42<3:41:29,  3.46s/it]loss_total_epoch 242.75240276753902
Training tokenizer:  52% 4208/8047 [2:57:45<3:42:45,  3.48s/it]loss_total_epoch 242.80140890553594
Training tokenizer:  52% 4209/8047 [2:57:49<3:41:59,  3.47s/it]loss_total_epoch 242.8397801667452
Training tokenizer:  52% 4210/8047 [2:57:52<3:41:44,  3.47s/it]loss_total_epoch 242.8822369799018
Training tokenizer:  52% 4211/8047 [2:57:56<3:42:38,  3.48s/it]loss_total_epoch 242.92200937867165
Training tokenizer:  52% 4212/8047 [2:57:59<3:42:35,  3.48s/it]loss_total_epoch 242.97858252748847
Training tokenizer:  52% 4213/8047 [2:58:03<3:42:16,  3.48s/it]loss_total_epoch 243.02215550467372
Training tokenizer:  52% 4214/8047 [2:58:06<3:40:23,  3.45s/it]loss_total_epoch 243.07340935617685
Training tokenizer:  52% 4215/8047 [2:58:10<3:41:04,  3.46s/it]loss_total_epoch 243.10944225639105
Training tokenizer:  52% 4216/8047 [2:58:13<3:39:22,  3.44s/it]loss_total_epoch 243.15460244566202
Training tokenizer:  52% 4217/8047 [2:58:16<3:40:00,  3.45s/it]loss_total_epoch 243.2014806754887
Training tokenizer:  52% 4218/8047 [2:58:20<3:39:37,  3.44s/it]loss_total_epoch 243.2516941614449
Training tokenizer:  52% 4219/8047 [2:58:23<3:39:56,  3.45s/it]loss_total_epoch 243.29458930715919
Training tokenizer:  52% 4220/8047 [2:58:27<3:40:31,  3.46s/it]loss_total_epoch 243.34449316561222
Training tokenizer:  52% 4221/8047 [2:58:30<3:41:51,  3.48s/it]loss_total_epoch 243.3894875496626
Training tokenizer:  52% 4222/8047 [2:58:34<3:42:13,  3.49s/it]loss_total_epoch 243.44263185560703
Training tokenizer:  52% 4223/8047 [2:58:37<3:42:30,  3.49s/it]loss_total_epoch 243.4819741435349
Training tokenizer:  52% 4224/8047 [2:58:41<3:43:24,  3.51s/it]loss_total_epoch 243.52387917414308
Training tokenizer:  53% 4225/8047 [2:58:44<3:42:05,  3.49s/it]loss_total_epoch 243.58118425682187
Training tokenizer:  53% 4226/8047 [2:58:48<3:41:05,  3.47s/it]loss_total_epoch 243.63098941370845
Training tokenizer:  53% 4227/8047 [2:58:51<3:41:53,  3.49s/it]loss_total_epoch 243.6843393072486
Training tokenizer:  53% 4228/8047 [2:58:55<3:42:53,  3.50s/it]loss_total_epoch 243.73799517005682
Training tokenizer:  53% 4229/8047 [2:58:58<3:43:15,  3.51s/it]loss_total_epoch 243.77744533866644
Training tokenizer:  53% 4230/8047 [2:59:02<3:46:06,  3.55s/it]loss_total_epoch 243.83321529626846
Training tokenizer:  53% 4231/8047 [2:59:05<3:45:47,  3.55s/it]loss_total_epoch 243.8976712524891
Training tokenizer:  53% 4232/8047 [2:59:09<3:43:58,  3.52s/it]loss_total_epoch 243.9481297545135
Training tokenizer:  53% 4233/8047 [2:59:12<3:43:31,  3.52s/it]loss_total_epoch 244.00127022713423
Training tokenizer:  53% 4234/8047 [2:59:16<3:43:52,  3.52s/it]loss_total_epoch 244.0571619346738
Training tokenizer:  53% 4235/8047 [2:59:19<3:43:33,  3.52s/it]loss_total_epoch 244.11082054302096
Training tokenizer:  53% 4236/8047 [2:59:23<3:42:53,  3.51s/it]loss_total_epoch 244.15759448334575
Training tokenizer:  53% 4237/8047 [2:59:26<3:43:02,  3.51s/it]loss_total_epoch 244.2094309143722
Training tokenizer:  53% 4238/8047 [2:59:30<3:42:04,  3.50s/it]loss_total_epoch 244.26132967323065
Training tokenizer:  53% 4239/8047 [2:59:33<3:42:44,  3.51s/it]loss_total_epoch 244.307194840163
Training tokenizer:  53% 4240/8047 [2:59:37<3:41:50,  3.50s/it]loss_total_epoch 244.36058535054326
Training tokenizer:  53% 4241/8047 [2:59:40<3:41:34,  3.49s/it]loss_total_epoch 244.41290859878063
Training tokenizer:  53% 4242/8047 [2:59:44<3:40:45,  3.48s/it]loss_total_epoch 244.46748873218894
Training tokenizer:  53% 4243/8047 [2:59:47<3:40:55,  3.48s/it]loss_total_epoch 244.51331198588014
Training tokenizer:  53% 4244/8047 [2:59:51<3:41:04,  3.49s/it]loss_total_epoch 244.55744533240795
Training tokenizer:  53% 4245/8047 [2:59:54<3:40:58,  3.49s/it]loss_total_epoch 244.60072067379951
Training tokenizer:  53% 4246/8047 [2:59:58<3:39:57,  3.47s/it]loss_total_epoch 244.65899689495564
Training tokenizer:  53% 4247/8047 [3:00:01<3:40:22,  3.48s/it]loss_total_epoch 244.71322503313422
Training tokenizer:  53% 4248/8047 [3:00:05<3:41:00,  3.49s/it]loss_total_epoch 244.76857475936413
Training tokenizer:  53% 4249/8047 [3:00:08<3:40:14,  3.48s/it]loss_total_epoch 244.804917935282
Training tokenizer:  53% 4250/8047 [3:00:12<3:39:54,  3.47s/it]loss_total_epoch 244.85477565973997
Training tokenizer:  53% 4251/8047 [3:00:15<3:41:02,  3.49s/it]loss_total_epoch 244.91402024030685
Training tokenizer:  53% 4252/8047 [3:00:19<3:44:18,  3.55s/it]loss_total_epoch 244.95406246185303
Training tokenizer:  53% 4253/8047 [3:00:22<3:43:48,  3.54s/it]loss_total_epoch 244.99666199088097
Training tokenizer:  53% 4254/8047 [3:00:26<3:43:40,  3.54s/it]loss_total_epoch 245.0473273806274
Training tokenizer:  53% 4255/8047 [3:00:30<3:45:45,  3.57s/it]loss_total_epoch 245.1084059998393
Training tokenizer:  53% 4256/8047 [3:00:33<3:43:16,  3.53s/it]loss_total_epoch 245.1560933291912
Training tokenizer:  53% 4257/8047 [3:00:37<3:42:23,  3.52s/it]loss_total_epoch 245.2086331769824
Training tokenizer:  53% 4258/8047 [3:00:40<3:42:17,  3.52s/it]loss_total_epoch 245.2621182911098
Training tokenizer:  53% 4259/8047 [3:00:44<3:42:09,  3.52s/it]loss_total_epoch 245.31402895227075
Training tokenizer:  53% 4260/8047 [3:00:47<3:42:27,  3.52s/it]loss_total_epoch 245.3598738797009
Training tokenizer:  53% 4261/8047 [3:00:51<3:41:59,  3.52s/it]loss_total_epoch 245.40441617369652
Training tokenizer:  53% 4262/8047 [3:00:54<3:41:56,  3.52s/it]loss_total_epoch 245.45772222802043
Training tokenizer:  53% 4263/8047 [3:00:58<3:41:33,  3.51s/it]loss_total_epoch 245.51126907393336
Training tokenizer:  53% 4264/8047 [3:01:01<3:41:05,  3.51s/it]loss_total_epoch 245.55356952920556
Training tokenizer:  53% 4265/8047 [3:01:05<3:41:00,  3.51s/it]loss_total_epoch 245.59420703724027
Training tokenizer:  53% 4266/8047 [3:01:08<3:40:42,  3.50s/it]loss_total_epoch 245.6424871943891
Training tokenizer:  53% 4267/8047 [3:01:12<3:40:17,  3.50s/it]loss_total_epoch 245.6928030177951
Training tokenizer:  53% 4268/8047 [3:01:15<3:41:10,  3.51s/it]loss_total_epoch 245.73938568681479
Training tokenizer:  53% 4269/8047 [3:01:19<3:41:27,  3.52s/it]loss_total_epoch 245.78628348186612
Training tokenizer:  53% 4270/8047 [3:01:22<3:42:07,  3.53s/it]loss_total_epoch 245.84353509545326
Training tokenizer:  53% 4271/8047 [3:01:26<3:42:18,  3.53s/it]loss_total_epoch 245.89285996556282
Training tokenizer:  53% 4272/8047 [3:01:29<3:42:05,  3.53s/it]loss_total_epoch 245.9543683603406
Training tokenizer:  53% 4273/8047 [3:01:33<3:40:49,  3.51s/it]loss_total_epoch 245.99948439374566
Training tokenizer:  53% 4274/8047 [3:01:36<3:40:41,  3.51s/it]loss_total_epoch 246.05899799987674
Training tokenizer:  53% 4275/8047 [3:01:40<3:40:56,  3.51s/it]loss_total_epoch 246.09520192071795
Training tokenizer:  53% 4276/8047 [3:01:43<3:40:48,  3.51s/it]loss_total_epoch 246.12805600836873
Training tokenizer:  53% 4277/8047 [3:01:47<3:40:44,  3.51s/it]loss_total_epoch 246.17460587993264
Training tokenizer:  53% 4278/8047 [3:01:50<3:39:07,  3.49s/it]loss_total_epoch 246.21456268802285
Training tokenizer:  53% 4279/8047 [3:01:54<3:39:26,  3.49s/it]loss_total_epoch 246.27596800401807
Training tokenizer:  53% 4280/8047 [3:01:57<3:39:45,  3.50s/it]loss_total_epoch 246.32131531089544
Training tokenizer:  53% 4281/8047 [3:02:01<3:39:35,  3.50s/it]loss_total_epoch 246.37196969613433
Training tokenizer:  53% 4282/8047 [3:02:04<3:39:36,  3.50s/it]loss_total_epoch 246.41703973710537
Training tokenizer:  53% 4283/8047 [3:02:08<3:40:17,  3.51s/it]loss_total_epoch 246.46042785048485
Training tokenizer:  53% 4284/8047 [3:02:11<3:42:42,  3.55s/it]loss_total_epoch 246.49340967461467
Training tokenizer:  53% 4285/8047 [3:02:15<3:41:43,  3.54s/it]loss_total_epoch 246.54840321093798
Training tokenizer:  53% 4286/8047 [3:02:19<3:41:42,  3.54s/it]loss_total_epoch 246.59614669159055
Training tokenizer:  53% 4287/8047 [3:02:22<3:39:52,  3.51s/it]loss_total_epoch 246.65368166938424
Training tokenizer:  53% 4288/8047 [3:02:25<3:38:59,  3.50s/it]loss_total_epoch 246.70071949064732
Training tokenizer:  53% 4289/8047 [3:02:29<3:39:27,  3.50s/it]loss_total_epoch 246.7400025576353
Training tokenizer:  53% 4290/8047 [3:02:32<3:39:05,  3.50s/it]loss_total_epoch 246.78352572023869
Training tokenizer:  53% 4291/8047 [3:02:36<3:39:11,  3.50s/it]loss_total_epoch 246.82442811504006
Training tokenizer:  53% 4292/8047 [3:02:40<3:39:57,  3.51s/it]loss_total_epoch 246.8695013821125
Training tokenizer:  53% 4293/8047 [3:02:43<3:40:03,  3.52s/it]loss_total_epoch 246.90826561301947
Training tokenizer:  53% 4294/8047 [3:02:47<3:39:43,  3.51s/it]loss_total_epoch 246.95958748832345
Training tokenizer:  53% 4295/8047 [3:02:50<3:40:07,  3.52s/it]loss_total_epoch 247.00927962735295
Training tokenizer:  53% 4296/8047 [3:02:54<3:40:55,  3.53s/it]loss_total_epoch 247.05019345879555
Training tokenizer:  53% 4297/8047 [3:02:57<3:40:03,  3.52s/it]loss_total_epoch 247.10271360352635
Training tokenizer:  53% 4298/8047 [3:03:01<3:40:14,  3.52s/it]loss_total_epoch 247.1591635942459
Training tokenizer:  53% 4299/8047 [3:03:04<3:40:00,  3.52s/it]loss_total_epoch 247.2004894465208
Training tokenizer:  53% 4300/8047 [3:03:08<3:38:58,  3.51s/it]loss_total_epoch 247.24583452939987
Training tokenizer:  53% 4301/8047 [3:03:11<3:40:03,  3.52s/it]loss_total_epoch 247.3041632398963
Training tokenizer:  53% 4302/8047 [3:03:15<3:39:51,  3.52s/it]loss_total_epoch 247.34649800136685
Training tokenizer:  53% 4303/8047 [3:03:18<3:40:09,  3.53s/it]loss_total_epoch 247.39993510767817
Training tokenizer:  53% 4304/8047 [3:03:22<3:40:27,  3.53s/it]loss_total_epoch 247.43893054500222
Training tokenizer:  53% 4305/8047 [3:03:25<3:39:56,  3.53s/it]loss_total_epoch 247.49375296011567
Training tokenizer:  54% 4306/8047 [3:03:29<3:40:43,  3.54s/it]loss_total_epoch 247.54426361620426
Training tokenizer:  54% 4307/8047 [3:03:32<3:40:09,  3.53s/it]loss_total_epoch 247.59614603966475
Training tokenizer:  54% 4308/8047 [3:03:36<3:40:55,  3.55s/it]loss_total_epoch 247.65163660049438
Training tokenizer:  54% 4309/8047 [3:03:40<3:40:54,  3.55s/it]loss_total_epoch 247.70166785642505
Training tokenizer:  54% 4310/8047 [3:03:43<3:40:27,  3.54s/it]loss_total_epoch 247.74145901575685
Training tokenizer:  54% 4311/8047 [3:03:47<3:40:00,  3.53s/it]loss_total_epoch 247.7923928759992
Training tokenizer:  54% 4312/8047 [3:03:50<3:40:23,  3.54s/it]loss_total_epoch 247.8481017909944
Training tokenizer:  54% 4313/8047 [3:03:54<3:40:09,  3.54s/it]loss_total_epoch 247.8977791890502
Training tokenizer:  54% 4314/8047 [3:03:57<3:39:56,  3.54s/it]loss_total_epoch 247.94067430868745
Training tokenizer:  54% 4315/8047 [3:04:01<3:39:42,  3.53s/it]loss_total_epoch 247.9816658720374
Training tokenizer:  54% 4316/8047 [3:04:04<3:40:35,  3.55s/it]loss_total_epoch 248.0346381291747
Training tokenizer:  54% 4317/8047 [3:04:08<3:39:56,  3.54s/it]loss_total_epoch 248.09024269133806
Training tokenizer:  54% 4318/8047 [3:04:11<3:39:58,  3.54s/it]loss_total_epoch 248.1505470648408
Training tokenizer:  54% 4319/8047 [3:04:15<3:39:15,  3.53s/it]loss_total_epoch 248.21225949376822
Training tokenizer:  54% 4320/8047 [3:04:18<3:39:55,  3.54s/it]loss_total_epoch 248.26404252275825
Training tokenizer:  54% 4321/8047 [3:04:22<3:39:03,  3.53s/it]loss_total_epoch 248.31521552801132
Training tokenizer:  54% 4322/8047 [3:04:25<3:38:32,  3.52s/it]loss_total_epoch 248.36409337818623
Training tokenizer:  54% 4323/8047 [3:04:29<3:38:55,  3.53s/it]loss_total_epoch 248.40582513436675
Training tokenizer:  54% 4324/8047 [3:04:33<3:39:06,  3.53s/it]loss_total_epoch 248.44165552034974
Training tokenizer:  54% 4325/8047 [3:04:36<3:39:58,  3.55s/it]loss_total_epoch 248.4948076196015
Training tokenizer:  54% 4326/8047 [3:04:40<3:40:46,  3.56s/it]loss_total_epoch 248.53912748768926
Training tokenizer:  54% 4327/8047 [3:04:43<3:40:36,  3.56s/it]loss_total_epoch 248.58710994198918
Training tokenizer:  54% 4328/8047 [3:04:47<3:39:39,  3.54s/it]loss_total_epoch 248.63532172888517
Training tokenizer:  54% 4329/8047 [3:04:50<3:40:11,  3.55s/it]loss_total_epoch 248.682454071939
Training tokenizer:  54% 4330/8047 [3:04:54<3:39:57,  3.55s/it]loss_total_epoch 248.7267748489976
Training tokenizer:  54% 4331/8047 [3:04:57<3:39:30,  3.54s/it]loss_total_epoch 248.78047285974026
Training tokenizer:  54% 4332/8047 [3:05:01<3:39:37,  3.55s/it]loss_total_epoch 248.82568932697177
Training tokenizer:  54% 4333/8047 [3:05:05<3:39:43,  3.55s/it]loss_total_epoch 248.8695192821324
Training tokenizer:  54% 4334/8047 [3:05:08<3:40:08,  3.56s/it]loss_total_epoch 248.92208064347506
Training tokenizer:  54% 4335/8047 [3:05:12<3:39:03,  3.54s/it]loss_total_epoch 248.9648017808795
Training tokenizer:  54% 4336/8047 [3:05:15<3:38:38,  3.54s/it]loss_total_epoch 249.02794006466866
Training tokenizer:  54% 4337/8047 [3:05:19<3:38:45,  3.54s/it]loss_total_epoch 249.0870813652873
Training tokenizer:  54% 4338/8047 [3:05:22<3:38:43,  3.54s/it]loss_total_epoch 249.13956390693784
Training tokenizer:  54% 4339/8047 [3:05:26<3:38:35,  3.54s/it]loss_total_epoch 249.1917351000011
Training tokenizer:  54% 4340/8047 [3:05:29<3:38:57,  3.54s/it]loss_total_epoch 249.23479169607162
Training tokenizer:  54% 4341/8047 [3:05:33<3:38:14,  3.53s/it]loss_total_epoch 249.28476816788316
Training tokenizer:  54% 4342/8047 [3:05:36<3:39:06,  3.55s/it]loss_total_epoch 249.3471054919064
Training tokenizer:  54% 4343/8047 [3:05:40<3:38:24,  3.54s/it]loss_total_epoch 249.39045113697648
Training tokenizer:  54% 4344/8047 [3:05:43<3:37:27,  3.52s/it]loss_total_epoch 249.4365858696401
Training tokenizer:  54% 4345/8047 [3:05:47<3:37:09,  3.52s/it]loss_total_epoch 249.4698872640729
Training tokenizer:  54% 4346/8047 [3:05:50<3:37:48,  3.53s/it]loss_total_epoch 249.5097881257534
Training tokenizer:  54% 4347/8047 [3:05:54<3:38:09,  3.54s/it]loss_total_epoch 249.56136726215482
Training tokenizer:  54% 4348/8047 [3:05:58<3:39:08,  3.55s/it]loss_total_epoch 249.6140966936946
Training tokenizer:  54% 4349/8047 [3:06:01<3:39:23,  3.56s/it]loss_total_epoch 249.65094408020377
Training tokenizer:  54% 4350/8047 [3:06:05<3:37:55,  3.54s/it]loss_total_epoch 249.69996489211917
Training tokenizer:  54% 4351/8047 [3:06:08<3:37:41,  3.53s/it]loss_total_epoch 249.7364495061338
Training tokenizer:  54% 4352/8047 [3:06:12<3:38:21,  3.55s/it]loss_total_epoch 249.77759373560548
Training tokenizer:  54% 4353/8047 [3:06:15<3:38:43,  3.55s/it]loss_total_epoch 249.83497617393732
Training tokenizer:  54% 4354/8047 [3:06:19<3:39:10,  3.56s/it]loss_total_epoch 249.88859999924898
Training tokenizer:  54% 4355/8047 [3:06:22<3:38:59,  3.56s/it]loss_total_epoch 249.93007906153798
Training tokenizer:  54% 4356/8047 [3:06:26<3:38:44,  3.56s/it]loss_total_epoch 249.9809745065868
Training tokenizer:  54% 4357/8047 [3:06:30<3:38:45,  3.56s/it]loss_total_epoch 250.02830866724253
Training tokenizer:  54% 4358/8047 [3:06:33<3:38:47,  3.56s/it]loss_total_epoch 250.0761798247695
Training tokenizer:  54% 4359/8047 [3:06:37<3:38:38,  3.56s/it]loss_total_epoch 250.13586106151342
Training tokenizer:  54% 4360/8047 [3:06:40<3:37:31,  3.54s/it]loss_total_epoch 250.1792987883091
Training tokenizer:  54% 4361/8047 [3:06:44<3:38:12,  3.55s/it]loss_total_epoch 250.21784779056907
Training tokenizer:  54% 4362/8047 [3:06:47<3:36:41,  3.53s/it]loss_total_epoch 250.26394390687346
Training tokenizer:  54% 4363/8047 [3:06:51<3:37:28,  3.54s/it]loss_total_epoch 250.31763622164726
Training tokenizer:  54% 4364/8047 [3:06:54<3:36:25,  3.53s/it]loss_total_epoch 250.3686827905476
Training tokenizer:  54% 4365/8047 [3:06:58<3:37:08,  3.54s/it]loss_total_epoch 250.41543252393603
Training tokenizer:  54% 4366/8047 [3:07:01<3:36:49,  3.53s/it]loss_total_epoch 250.46113235875964
Training tokenizer:  54% 4367/8047 [3:07:05<3:37:17,  3.54s/it]loss_total_epoch 250.51168970763683
Training tokenizer:  54% 4368/8047 [3:07:08<3:37:07,  3.54s/it]loss_total_epoch 250.5561849065125
Training tokenizer:  54% 4369/8047 [3:07:12<3:37:51,  3.55s/it]loss_total_epoch 250.59502063691616
Training tokenizer:  54% 4370/8047 [3:07:16<3:37:49,  3.55s/it]loss_total_epoch 250.6420215666294
Training tokenizer:  54% 4371/8047 [3:07:19<3:38:24,  3.56s/it]loss_total_epoch 250.692328941077
Training tokenizer:  54% 4372/8047 [3:07:23<3:37:23,  3.55s/it]loss_total_epoch 250.73230880871415
Training tokenizer:  54% 4373/8047 [3:07:26<3:37:41,  3.55s/it]loss_total_epoch 250.77505092695355
Training tokenizer:  54% 4374/8047 [3:07:30<3:37:50,  3.56s/it]loss_total_epoch 250.82752254605293
Training tokenizer:  54% 4375/8047 [3:07:33<3:37:35,  3.56s/it]loss_total_epoch 250.8724247701466
Training tokenizer:  54% 4376/8047 [3:07:37<3:36:23,  3.54s/it]loss_total_epoch 250.9176856391132
Training tokenizer:  54% 4377/8047 [3:07:40<3:36:17,  3.54s/it]loss_total_epoch 250.96483850106597
Training tokenizer:  54% 4378/8047 [3:07:44<3:36:43,  3.54s/it]loss_total_epoch 251.02175767347217
Training tokenizer:  54% 4379/8047 [3:07:48<3:36:31,  3.54s/it]loss_total_epoch 251.07051794975996
Training tokenizer:  54% 4380/8047 [3:07:51<3:36:52,  3.55s/it]loss_total_epoch 251.11752992868423
Training tokenizer:  54% 4381/8047 [3:07:55<3:37:47,  3.56s/it]loss_total_epoch 251.17724779993296
Training tokenizer:  54% 4382/8047 [3:07:58<3:37:48,  3.57s/it]loss_total_epoch 251.20876128971577
Training tokenizer:  54% 4383/8047 [3:08:02<3:37:48,  3.57s/it]loss_total_epoch 251.26105422899127
Training tokenizer:  54% 4384/8047 [3:08:05<3:37:08,  3.56s/it]loss_total_epoch 251.30049104616046
Training tokenizer:  54% 4385/8047 [3:08:09<3:36:47,  3.55s/it]loss_total_epoch 251.35034370049834
Training tokenizer:  55% 4386/8047 [3:08:13<3:38:14,  3.58s/it]loss_total_epoch 251.40180933848023
Training tokenizer:  55% 4387/8047 [3:08:16<3:36:47,  3.55s/it]loss_total_epoch 251.44720550626516
Training tokenizer:  55% 4388/8047 [3:08:20<3:37:57,  3.57s/it]loss_total_epoch 251.4994293861091
Training tokenizer:  55% 4389/8047 [3:08:23<3:38:23,  3.58s/it]loss_total_epoch 251.54901319369674
Training tokenizer:  55% 4390/8047 [3:08:27<3:37:33,  3.57s/it]loss_total_epoch 251.59444099664688
Training tokenizer:  55% 4391/8047 [3:08:30<3:38:17,  3.58s/it]loss_total_epoch 251.64457120001316
Training tokenizer:  55% 4392/8047 [3:08:34<3:36:57,  3.56s/it]loss_total_epoch 251.68834841623902
Training tokenizer:  55% 4393/8047 [3:08:38<3:37:04,  3.56s/it]loss_total_epoch 251.7230033800006
Training tokenizer:  55% 4394/8047 [3:08:41<3:36:52,  3.56s/it]loss_total_epoch 251.77144415676594
Training tokenizer:  55% 4395/8047 [3:08:45<3:37:13,  3.57s/it]loss_total_epoch 251.8369309604168
Training tokenizer:  55% 4396/8047 [3:08:48<3:37:07,  3.57s/it]loss_total_epoch 251.8934711329639
Training tokenizer:  55% 4397/8047 [3:08:52<3:37:04,  3.57s/it]loss_total_epoch 251.94164375960827
Training tokenizer:  55% 4398/8047 [3:08:55<3:37:34,  3.58s/it]loss_total_epoch 251.99142214283347
Training tokenizer:  55% 4399/8047 [3:08:59<3:37:40,  3.58s/it]loss_total_epoch 252.02819760888815
Training tokenizer:  55% 4400/8047 [3:09:03<3:38:11,  3.59s/it]loss_total_epoch 252.06647468358278
Training tokenizer:  55% 4401/8047 [3:09:06<3:36:59,  3.57s/it]loss_total_epoch 252.11954471468925
Training tokenizer:  55% 4402/8047 [3:09:10<3:36:24,  3.56s/it]loss_total_epoch 252.173873052001
Training tokenizer:  55% 4403/8047 [3:09:13<3:36:16,  3.56s/it]loss_total_epoch 252.2218593135476
Training tokenizer:  55% 4404/8047 [3:09:17<3:36:27,  3.57s/it]loss_total_epoch 252.26937912032008
Training tokenizer:  55% 4405/8047 [3:09:20<3:36:49,  3.57s/it]loss_total_epoch 252.31272209063172
Training tokenizer:  55% 4406/8047 [3:09:24<3:36:26,  3.57s/it]loss_total_epoch 252.34817430004478
Training tokenizer:  55% 4407/8047 [3:09:28<3:36:57,  3.58s/it]loss_total_epoch 252.3976820781827
Training tokenizer:  55% 4408/8047 [3:09:31<3:37:45,  3.59s/it]loss_total_epoch 252.44237435236573
Training tokenizer:  55% 4409/8047 [3:09:35<3:37:22,  3.59s/it]loss_total_epoch 252.4887335896492
Training tokenizer:  55% 4410/8047 [3:09:38<3:37:50,  3.59s/it]loss_total_epoch 252.5436170399189
Training tokenizer:  55% 4411/8047 [3:09:42<3:37:58,  3.60s/it]loss_total_epoch 252.5916544944048
Training tokenizer:  55% 4412/8047 [3:09:46<3:37:08,  3.58s/it]loss_total_epoch 252.63785020262003
Training tokenizer:  55% 4413/8047 [3:09:49<3:37:45,  3.60s/it]loss_total_epoch 252.68007338419557
Training tokenizer:  55% 4414/8047 [3:09:53<3:38:04,  3.60s/it]loss_total_epoch 252.72500796616077
Training tokenizer:  55% 4415/8047 [3:09:56<3:37:28,  3.59s/it]loss_total_epoch 252.7743850722909
Training tokenizer:  55% 4416/8047 [3:10:00<3:36:26,  3.58s/it]loss_total_epoch 252.81815826892853
Training tokenizer:  55% 4417/8047 [3:10:03<3:36:42,  3.58s/it]loss_total_epoch 252.85218476876616
Training tokenizer:  55% 4418/8047 [3:10:07<3:36:04,  3.57s/it]loss_total_epoch 252.90730216354132
Training tokenizer:  55% 4419/8047 [3:10:11<3:35:54,  3.57s/it]loss_total_epoch 252.95355515927076
Training tokenizer:  55% 4420/8047 [3:10:14<3:36:13,  3.58s/it]loss_total_epoch 253.0062891356647
Training tokenizer:  55% 4421/8047 [3:10:18<3:36:35,  3.58s/it]loss_total_epoch 253.05639050900936
Training tokenizer:  55% 4422/8047 [3:10:21<3:34:38,  3.55s/it]loss_total_epoch 253.11142655089498
Training tokenizer:  55% 4423/8047 [3:10:25<3:33:51,  3.54s/it]loss_total_epoch 253.1605136282742
Training tokenizer:  55% 4424/8047 [3:10:28<3:34:58,  3.56s/it]loss_total_epoch 253.1922046765685
Training tokenizer:  55% 4425/8047 [3:10:32<3:35:40,  3.57s/it]loss_total_epoch 253.2410283833742
Training tokenizer:  55% 4426/8047 [3:10:36<3:36:07,  3.58s/it]loss_total_epoch 253.30061660334468
Training tokenizer:  55% 4427/8047 [3:10:39<3:37:04,  3.60s/it]loss_total_epoch 253.34424871206284
Training tokenizer:  55% 4428/8047 [3:10:43<3:34:42,  3.56s/it]loss_total_epoch 253.39094480499625
Training tokenizer:  55% 4429/8047 [3:10:46<3:35:10,  3.57s/it]loss_total_epoch 253.43484419584274
Training tokenizer:  55% 4430/8047 [3:10:50<3:35:41,  3.58s/it]loss_total_epoch 253.4680798947811
Training tokenizer:  55% 4431/8047 [3:10:53<3:35:43,  3.58s/it]loss_total_epoch 253.50580571964383
Training tokenizer:  55% 4432/8047 [3:10:57<3:35:47,  3.58s/it]loss_total_epoch 253.55244754999876
Training tokenizer:  55% 4433/8047 [3:11:01<3:36:33,  3.60s/it]loss_total_epoch 253.604681879282
Training tokenizer:  55% 4434/8047 [3:11:04<3:35:16,  3.58s/it]loss_total_epoch 253.65119530633092
Training tokenizer:  55% 4435/8047 [3:11:08<3:35:18,  3.58s/it]loss_total_epoch 253.69300993159413
Training tokenizer:  55% 4436/8047 [3:11:11<3:35:42,  3.58s/it]loss_total_epoch 253.73514173552394
Training tokenizer:  55% 4437/8047 [3:11:15<3:34:09,  3.56s/it]loss_total_epoch 253.78242718800902
Training tokenizer:  55% 4438/8047 [3:11:18<3:35:14,  3.58s/it]loss_total_epoch 253.8498780094087
Training tokenizer:  55% 4439/8047 [3:11:22<3:35:37,  3.59s/it]loss_total_epoch 253.8896589167416
Training tokenizer:  55% 4440/8047 [3:11:26<3:35:15,  3.58s/it]loss_total_epoch 253.9353362135589
Training tokenizer:  55% 4441/8047 [3:11:29<3:35:19,  3.58s/it]loss_total_epoch 253.97760709375143
Training tokenizer:  55% 4442/8047 [3:11:33<3:36:12,  3.60s/it]loss_total_epoch 254.01345147192478
Training tokenizer:  55% 4443/8047 [3:11:36<3:36:04,  3.60s/it]loss_total_epoch 254.06062187999487
Training tokenizer:  55% 4444/8047 [3:11:40<3:35:42,  3.59s/it]loss_total_epoch 254.10579292848706
Training tokenizer:  55% 4445/8047 [3:11:44<3:35:50,  3.60s/it]loss_total_epoch 254.15483866631985
Training tokenizer:  55% 4446/8047 [3:11:47<3:36:31,  3.61s/it]loss_total_epoch 254.21719530969858
Training tokenizer:  55% 4447/8047 [3:11:51<3:36:19,  3.61s/it]loss_total_epoch 254.2593377418816
Training tokenizer:  55% 4448/8047 [3:11:54<3:35:56,  3.60s/it]loss_total_epoch 254.30645084381104
Training tokenizer:  55% 4449/8047 [3:11:58<3:35:47,  3.60s/it]loss_total_epoch 254.3464186936617
Training tokenizer:  55% 4450/8047 [3:12:02<3:36:18,  3.61s/it]loss_total_epoch 254.3917667903006
Training tokenizer:  55% 4451/8047 [3:12:05<3:36:20,  3.61s/it]loss_total_epoch 254.44712400063872
Training tokenizer:  55% 4452/8047 [3:12:09<3:36:38,  3.62s/it]loss_total_epoch 254.48995396122336
Training tokenizer:  55% 4453/8047 [3:12:13<3:35:36,  3.60s/it]loss_total_epoch 254.5271714515984
Training tokenizer:  55% 4454/8047 [3:12:16<3:35:18,  3.60s/it]loss_total_epoch 254.5726792924106
Training tokenizer:  55% 4455/8047 [3:12:20<3:35:34,  3.60s/it]loss_total_epoch 254.62308060377836
Training tokenizer:  55% 4456/8047 [3:12:23<3:35:02,  3.59s/it]loss_total_epoch 254.67120265960693
Training tokenizer:  55% 4457/8047 [3:12:27<3:34:59,  3.59s/it]loss_total_epoch 254.7227024026215
Training tokenizer:  55% 4458/8047 [3:12:31<3:35:42,  3.61s/it]loss_total_epoch 254.77811216562986
Training tokenizer:  55% 4459/8047 [3:12:34<3:35:43,  3.61s/it]loss_total_epoch 254.82270785421133
Training tokenizer:  55% 4460/8047 [3:12:38<3:35:09,  3.60s/it]loss_total_epoch 254.8626443222165
Training tokenizer:  55% 4461/8047 [3:12:41<3:35:40,  3.61s/it]loss_total_epoch 254.91031666100025
Training tokenizer:  55% 4462/8047 [3:12:45<3:35:19,  3.60s/it]loss_total_epoch 254.94553173705935
Training tokenizer:  55% 4463/8047 [3:12:49<3:35:06,  3.60s/it]loss_total_epoch 254.98601362481713
Training tokenizer:  55% 4464/8047 [3:12:52<3:34:43,  3.60s/it]loss_total_epoch 255.0367990732193
Training tokenizer:  55% 4465/8047 [3:12:56<3:35:54,  3.62s/it]loss_total_epoch 255.0753216855228
Training tokenizer:  55% 4466/8047 [3:12:59<3:36:19,  3.62s/it]loss_total_epoch 255.1307032071054
Training tokenizer:  56% 4467/8047 [3:13:03<3:36:08,  3.62s/it]loss_total_epoch 255.18808821216226
Training tokenizer:  56% 4468/8047 [3:13:07<3:36:14,  3.63s/it]loss_total_epoch 255.23522774875164
Training tokenizer:  56% 4469/8047 [3:13:10<3:35:52,  3.62s/it]loss_total_epoch 255.2911148071289
Training tokenizer:  56% 4470/8047 [3:13:14<3:35:45,  3.62s/it]loss_total_epoch 255.33120808005333
Training tokenizer:  56% 4471/8047 [3:13:17<3:35:20,  3.61s/it]loss_total_epoch 255.37384149432182
Training tokenizer:  56% 4472/8047 [3:13:21<3:34:37,  3.60s/it]loss_total_epoch 255.42494393512607
Training tokenizer:  56% 4473/8047 [3:13:25<3:34:15,  3.60s/it]loss_total_epoch 255.4733904339373
Training tokenizer:  56% 4474/8047 [3:13:28<3:34:13,  3.60s/it]loss_total_epoch 255.52689771354198
Training tokenizer:  56% 4475/8047 [3:13:32<3:34:26,  3.60s/it]loss_total_epoch 255.5777976512909
Training tokenizer:  56% 4476/8047 [3:13:35<3:34:12,  3.60s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-im6_dpv9'
loss_total_epoch 255.6216594465077
Training tokenizer:  56% 4477/8047 [3:13:39<3:34:23,  3.60s/it]loss_total_epoch 255.6719104833901
Training tokenizer:  56% 4478/8047 [3:13:43<3:34:31,  3.61s/it]loss_total_epoch 255.72351993992925
Training tokenizer:  56% 4479/8047 [3:13:46<3:34:10,  3.60s/it]loss_total_epoch 255.7681720852852
Training tokenizer:  56% 4480/8047 [3:13:50<3:33:41,  3.59s/it]loss_total_epoch 255.8174853026867
Training tokenizer:  56% 4481/8047 [3:13:53<3:32:53,  3.58s/it]loss_total_epoch 255.86281461641192
Training tokenizer:  56% 4482/8047 [3:13:57<3:33:58,  3.60s/it]loss_total_epoch 255.91246004402637
Training tokenizer:  56% 4483/8047 [3:14:01<3:34:25,  3.61s/it]loss_total_epoch 255.94993413984776
Training tokenizer:  56% 4484/8047 [3:14:04<3:35:05,  3.62s/it]loss_total_epoch 255.99601504579186
Training tokenizer:  56% 4485/8047 [3:14:08<3:35:21,  3.63s/it]loss_total_epoch 256.0483129583299
Training tokenizer:  56% 4486/8047 [3:14:12<3:35:07,  3.62s/it]loss_total_epoch 256.0898236632347
Training tokenizer:  56% 4487/8047 [3:14:15<3:34:26,  3.61s/it]loss_total_epoch 256.12618792802095
Training tokenizer:  56% 4488/8047 [3:14:19<3:33:21,  3.60s/it]loss_total_epoch 256.1702754497528
Training tokenizer:  56% 4489/8047 [3:14:22<3:31:53,  3.57s/it]loss_total_epoch 256.2179318778217
Training tokenizer:  56% 4490/8047 [3:14:26<3:31:47,  3.57s/it]loss_total_epoch 256.2653685249388
Training tokenizer:  56% 4491/8047 [3:14:29<3:32:38,  3.59s/it]loss_total_epoch 256.31256944313645
Training tokenizer:  56% 4492/8047 [3:14:33<3:33:54,  3.61s/it]loss_total_epoch 256.3584119938314
Training tokenizer:  56% 4493/8047 [3:14:37<3:34:40,  3.62s/it]loss_total_epoch 256.40923074632883
Training tokenizer:  56% 4494/8047 [3:14:40<3:33:29,  3.61s/it]loss_total_epoch 256.4560683593154
Training tokenizer:  56% 4495/8047 [3:14:44<3:34:15,  3.62s/it]loss_total_epoch 256.5080302134156
Training tokenizer:  56% 4496/8047 [3:14:48<3:34:36,  3.63s/it]loss_total_epoch 256.561143912375
Training tokenizer:  56% 4497/8047 [3:14:51<3:34:22,  3.62s/it]loss_total_epoch 256.6074657589197
Training tokenizer:  56% 4498/8047 [3:14:55<3:34:46,  3.63s/it]loss_total_epoch 256.65490574762225
Training tokenizer:  56% 4499/8047 [3:14:58<3:34:04,  3.62s/it]loss_total_epoch 256.6943383254111
Training tokenizer:  56% 4500/8047 [3:15:02<3:34:18,  3.63s/it]loss_total_epoch 256.73628306016326
Training tokenizer:  56% 4501/8047 [3:15:06<3:32:42,  3.60s/it]loss_total_epoch 256.7754983641207
Training tokenizer:  56% 4502/8047 [3:15:09<3:32:53,  3.60s/it]loss_total_epoch 256.81137178465724
Training tokenizer:  56% 4503/8047 [3:15:13<3:32:28,  3.60s/it]loss_total_epoch 256.8740249834955
Training tokenizer:  56% 4504/8047 [3:15:16<3:33:12,  3.61s/it]loss_total_epoch 256.9209557212889
Training tokenizer:  56% 4505/8047 [3:15:20<3:32:33,  3.60s/it]loss_total_epoch 256.9661344587803
Training tokenizer:  56% 4506/8047 [3:15:24<3:32:55,  3.61s/it]loss_total_epoch 257.0111777037382
Training tokenizer:  56% 4507/8047 [3:15:27<3:32:45,  3.61s/it]loss_total_epoch 257.05883438512683
Training tokenizer:  56% 4508/8047 [3:15:31<3:30:24,  3.57s/it]loss_total_epoch 257.1190369948745
Training tokenizer:  56% 4509/8047 [3:15:34<3:31:21,  3.58s/it]loss_total_epoch 257.15704856812954
Training tokenizer:  56% 4510/8047 [3:15:38<3:32:22,  3.60s/it]loss_total_epoch 257.2039305269718
Training tokenizer:  56% 4511/8047 [3:15:42<3:32:45,  3.61s/it]loss_total_epoch 257.24382400140166
Training tokenizer:  56% 4512/8047 [3:15:45<3:31:55,  3.60s/it]loss_total_epoch 257.2939357198775
Training tokenizer:  56% 4513/8047 [3:15:49<3:32:15,  3.60s/it]loss_total_epoch 257.3431474417448
Training tokenizer:  56% 4514/8047 [3:15:52<3:32:31,  3.61s/it]loss_total_epoch 257.3809045813978
Training tokenizer:  56% 4515/8047 [3:15:56<3:33:18,  3.62s/it]loss_total_epoch 257.428557485342
Training tokenizer:  56% 4516/8047 [3:16:00<3:32:25,  3.61s/it]loss_total_epoch 257.46617867052555
Training tokenizer:  56% 4517/8047 [3:16:03<3:32:41,  3.62s/it]loss_total_epoch 257.51375533640385
Training tokenizer:  56% 4518/8047 [3:16:07<3:32:30,  3.61s/it]loss_total_epoch 257.56631822511554
Training tokenizer:  56% 4519/8047 [3:16:11<3:33:16,  3.63s/it]loss_total_epoch 257.60696018114686
Training tokenizer:  56% 4520/8047 [3:16:14<3:32:28,  3.61s/it]loss_total_epoch 257.65943706035614
Training tokenizer:  56% 4521/8047 [3:16:18<3:33:03,  3.63s/it]loss_total_epoch 257.7091712281108
Training tokenizer:  56% 4522/8047 [3:16:21<3:32:20,  3.61s/it]loss_total_epoch 257.7570380754769
Training tokenizer:  56% 4523/8047 [3:16:25<3:32:09,  3.61s/it]loss_total_epoch 257.8016047440469
Training tokenizer:  56% 4524/8047 [3:16:29<3:32:37,  3.62s/it]loss_total_epoch 257.848629783839
Training tokenizer:  56% 4525/8047 [3:16:32<3:32:57,  3.63s/it]loss_total_epoch 257.8961754851043
Training tokenizer:  56% 4526/8047 [3:16:36<3:32:36,  3.62s/it]loss_total_epoch 257.94199642166495
Training tokenizer:  56% 4527/8047 [3:16:40<3:33:03,  3.63s/it]loss_total_epoch 257.987803902477
Training tokenizer:  56% 4528/8047 [3:16:43<3:32:10,  3.62s/it]loss_total_epoch 258.03338323906064
Training tokenizer:  56% 4529/8047 [3:16:47<3:32:05,  3.62s/it]loss_total_epoch 258.0772583819926
Training tokenizer:  56% 4530/8047 [3:16:50<3:32:55,  3.63s/it]loss_total_epoch 258.12433529645205
Training tokenizer:  56% 4531/8047 [3:16:54<3:33:16,  3.64s/it]loss_total_epoch 258.1801841109991
Training tokenizer:  56% 4532/8047 [3:16:58<3:33:03,  3.64s/it]loss_total_epoch 258.2254678197205
Training tokenizer:  56% 4533/8047 [3:17:01<3:33:25,  3.64s/it]loss_total_epoch 258.27772522345185
Training tokenizer:  56% 4534/8047 [3:17:05<3:33:20,  3.64s/it]loss_total_epoch 258.32378239929676
Training tokenizer:  56% 4535/8047 [3:17:09<3:31:49,  3.62s/it]loss_total_epoch 258.3645816370845
Training tokenizer:  56% 4536/8047 [3:17:12<3:31:46,  3.62s/it]loss_total_epoch 258.40543457865715
Training tokenizer:  56% 4537/8047 [3:17:16<3:32:37,  3.63s/it]loss_total_epoch 258.45003429427743
Training tokenizer:  56% 4538/8047 [3:17:20<3:31:57,  3.62s/it]loss_total_epoch 258.49661399796605
Training tokenizer:  56% 4539/8047 [3:17:23<3:32:20,  3.63s/it]loss_total_epoch 258.5546535924077
Training tokenizer:  56% 4540/8047 [3:17:27<3:31:03,  3.61s/it]loss_total_epoch 258.60591289401054
Training tokenizer:  56% 4541/8047 [3:17:30<3:31:00,  3.61s/it]loss_total_epoch 258.648573230952
Training tokenizer:  56% 4542/8047 [3:17:34<3:31:14,  3.62s/it]loss_total_epoch 258.7033302374184
Training tokenizer:  56% 4543/8047 [3:17:38<3:32:00,  3.63s/it]loss_total_epoch 258.7491428926587
Training tokenizer:  56% 4544/8047 [3:17:41<3:30:56,  3.61s/it]loss_total_epoch 258.8043742105365
Training tokenizer:  56% 4545/8047 [3:17:45<3:31:04,  3.62s/it]loss_total_epoch 258.84384713694453
Training tokenizer:  56% 4546/8047 [3:17:48<3:31:51,  3.63s/it]loss_total_epoch 258.89361265301704
Training tokenizer:  57% 4547/8047 [3:17:52<3:31:56,  3.63s/it]loss_total_epoch 258.93387189880013
Training tokenizer:  57% 4548/8047 [3:17:56<3:32:34,  3.65s/it]loss_total_epoch 258.98576498031616
Training tokenizer:  57% 4549/8047 [3:17:59<3:32:24,  3.64s/it]loss_total_epoch 259.03785157203674
Training tokenizer:  57% 4550/8047 [3:18:03<3:32:31,  3.65s/it]loss_total_epoch 259.08895026519895
Training tokenizer:  57% 4551/8047 [3:18:07<3:32:25,  3.65s/it]loss_total_epoch 259.1391809768975
Training tokenizer:  57% 4552/8047 [3:18:10<3:32:07,  3.64s/it]loss_total_epoch 259.18270602449775
Training tokenizer:  57% 4553/8047 [3:18:14<3:32:41,  3.65s/it]loss_total_epoch 259.2169611565769
Training tokenizer:  57% 4554/8047 [3:18:18<3:32:48,  3.66s/it]loss_total_epoch 259.26714799553156
Training tokenizer:  57% 4555/8047 [3:18:21<3:32:54,  3.66s/it]loss_total_epoch 259.32285164669156
Training tokenizer:  57% 4556/8047 [3:18:25<3:32:36,  3.65s/it]loss_total_epoch 259.36861910670996
Training tokenizer:  57% 4557/8047 [3:18:29<3:32:27,  3.65s/it]loss_total_epoch 259.4119590520859
Training tokenizer:  57% 4558/8047 [3:18:32<3:32:37,  3.66s/it]loss_total_epoch 259.4585193172097
Training tokenizer:  57% 4559/8047 [3:18:36<3:31:40,  3.64s/it]loss_total_epoch 259.5034482628107
Training tokenizer:  57% 4560/8047 [3:18:40<3:31:05,  3.63s/it]loss_total_epoch 259.54578655585647
Training tokenizer:  57% 4561/8047 [3:18:43<3:32:03,  3.65s/it]loss_total_epoch 259.5890883691609
Training tokenizer:  57% 4562/8047 [3:18:47<3:32:07,  3.65s/it]loss_total_epoch 259.6369310282171
Training tokenizer:  57% 4563/8047 [3:18:51<3:32:28,  3.66s/it]loss_total_epoch 259.69225776940584
Training tokenizer:  57% 4564/8047 [3:18:54<3:31:26,  3.64s/it]loss_total_epoch 259.7383145131171
Training tokenizer:  57% 4565/8047 [3:18:58<3:31:50,  3.65s/it]loss_total_epoch 259.7841001674533
Training tokenizer:  57% 4566/8047 [3:19:01<3:30:15,  3.62s/it]loss_total_epoch 259.8327458500862
Training tokenizer:  57% 4567/8047 [3:19:05<3:30:34,  3.63s/it]loss_total_epoch 259.87841346114874
Training tokenizer:  57% 4568/8047 [3:19:09<3:31:51,  3.65s/it]loss_total_epoch 259.9243118688464
Training tokenizer:  57% 4569/8047 [3:19:12<3:31:16,  3.64s/it]loss_total_epoch 259.97010800242424
Training tokenizer:  57% 4570/8047 [3:19:16<3:31:28,  3.65s/it]loss_total_epoch 260.00882932543755
Training tokenizer:  57% 4571/8047 [3:19:20<3:31:45,  3.66s/it]loss_total_epoch 260.0402901470661
Training tokenizer:  57% 4572/8047 [3:19:23<3:31:48,  3.66s/it]loss_total_epoch 260.0752828530967
Training tokenizer:  57% 4573/8047 [3:19:27<3:31:34,  3.65s/it]loss_total_epoch 260.1159826628864
Training tokenizer:  57% 4574/8047 [3:19:31<3:31:51,  3.66s/it]loss_total_epoch 260.1656767204404
Training tokenizer:  57% 4575/8047 [3:19:34<3:32:00,  3.66s/it]loss_total_epoch 260.2241019308567
Training tokenizer:  57% 4576/8047 [3:19:38<3:31:01,  3.65s/it]loss_total_epoch 260.275577288121
Training tokenizer:  57% 4577/8047 [3:19:42<3:30:58,  3.65s/it]loss_total_epoch 260.3259731642902
Training tokenizer:  57% 4578/8047 [3:19:45<3:29:43,  3.63s/it]loss_total_epoch 260.3609335757792
Training tokenizer:  57% 4579/8047 [3:19:49<3:30:00,  3.63s/it]loss_total_epoch 260.405483353883
Training tokenizer:  57% 4580/8047 [3:19:53<3:30:19,  3.64s/it]loss_total_epoch 260.4518003575504
Training tokenizer:  57% 4581/8047 [3:19:56<3:31:15,  3.66s/it]loss_total_epoch 260.5021400153637
Training tokenizer:  57% 4582/8047 [3:20:00<3:30:42,  3.65s/it]loss_total_epoch 260.5506949983537
Training tokenizer:  57% 4583/8047 [3:20:04<3:31:19,  3.66s/it]loss_total_epoch 260.59091478586197
Training tokenizer:  57% 4584/8047 [3:20:07<3:31:51,  3.67s/it]loss_total_epoch 260.6310343146324
Training tokenizer:  57% 4585/8047 [3:20:11<3:31:46,  3.67s/it]loss_total_epoch 260.6799499504268
Training tokenizer:  57% 4586/8047 [3:20:15<3:31:09,  3.66s/it]loss_total_epoch 260.7226711399853
Training tokenizer:  57% 4587/8047 [3:20:18<3:31:33,  3.67s/it]loss_total_epoch 260.7859241627157
Training tokenizer:  57% 4588/8047 [3:20:22<3:31:10,  3.66s/it]loss_total_epoch 260.83802830055356
Training tokenizer:  57% 4589/8047 [3:20:26<3:30:57,  3.66s/it]loss_total_epoch 260.8866782076657
Training tokenizer:  57% 4590/8047 [3:20:29<3:31:22,  3.67s/it]loss_total_epoch 260.92520866915584
Training tokenizer:  57% 4591/8047 [3:20:33<3:31:19,  3.67s/it]loss_total_epoch 260.9667546674609
Training tokenizer:  57% 4592/8047 [3:20:37<3:31:02,  3.66s/it]loss_total_epoch 261.0192106850445
Training tokenizer:  57% 4593/8047 [3:20:40<3:30:45,  3.66s/it]loss_total_epoch 261.0686579607427
Training tokenizer:  57% 4594/8047 [3:20:44<3:30:29,  3.66s/it]loss_total_epoch 261.1313927732408
Training tokenizer:  57% 4595/8047 [3:20:48<3:30:52,  3.67s/it]loss_total_epoch 261.1821315549314
Training tokenizer:  57% 4596/8047 [3:20:51<3:31:13,  3.67s/it]loss_total_epoch 261.2314789481461
Training tokenizer:  57% 4597/8047 [3:20:55<3:29:50,  3.65s/it]loss_total_epoch 261.27605713158846
Training tokenizer:  57% 4598/8047 [3:20:58<3:30:39,  3.66s/it]loss_total_epoch 261.33589068055153
Training tokenizer:  57% 4599/8047 [3:21:02<3:29:59,  3.65s/it]loss_total_epoch 261.38047524914145
Training tokenizer:  57% 4600/8047 [3:21:06<3:29:56,  3.65s/it]loss_total_epoch 261.42493380606174
Training tokenizer:  57% 4601/8047 [3:21:09<3:29:51,  3.65s/it]loss_total_epoch 261.4786526784301
Training tokenizer:  57% 4602/8047 [3:21:13<3:30:26,  3.67s/it]loss_total_epoch 261.52029337361455
Training tokenizer:  57% 4603/8047 [3:21:17<3:30:33,  3.67s/it]loss_total_epoch 261.56084398180246
Training tokenizer:  57% 4604/8047 [3:21:20<3:29:56,  3.66s/it]loss_total_epoch 261.6139481291175
Training tokenizer:  57% 4605/8047 [3:21:24<3:30:09,  3.66s/it]loss_total_epoch 261.6566576473415
Training tokenizer:  57% 4606/8047 [3:21:28<3:30:14,  3.67s/it]loss_total_epoch 261.69579458609223
Training tokenizer:  57% 4607/8047 [3:21:31<3:30:10,  3.67s/it]loss_total_epoch 261.74732735008
Training tokenizer:  57% 4608/8047 [3:21:35<3:30:48,  3.68s/it]loss_total_epoch 261.79205814749
Training tokenizer:  57% 4609/8047 [3:21:39<3:32:50,  3.71s/it]loss_total_epoch 261.85096734017134
Training tokenizer:  57% 4610/8047 [3:21:43<3:31:52,  3.70s/it]loss_total_epoch 261.9148182198405
Training tokenizer:  57% 4611/8047 [3:21:46<3:31:23,  3.69s/it]loss_total_epoch 261.96604653820395
Training tokenizer:  57% 4612/8047 [3:21:50<3:30:42,  3.68s/it]loss_total_epoch 262.0150166377425
Training tokenizer:  57% 4613/8047 [3:21:54<3:30:19,  3.67s/it]loss_total_epoch 262.07381988316774
Training tokenizer:  57% 4614/8047 [3:21:57<3:30:44,  3.68s/it]loss_total_epoch 262.1104484423995
Training tokenizer:  57% 4615/8047 [3:22:01<3:30:48,  3.69s/it]loss_total_epoch 262.14814725890756
Training tokenizer:  57% 4616/8047 [3:22:05<3:30:29,  3.68s/it]loss_total_epoch 262.20718209818006
Training tokenizer:  57% 4617/8047 [3:22:08<3:29:56,  3.67s/it]loss_total_epoch 262.25059079006314
Training tokenizer:  57% 4618/8047 [3:22:12<3:30:20,  3.68s/it]loss_total_epoch 262.28920448198915
Training tokenizer:  57% 4619/8047 [3:22:16<3:30:17,  3.68s/it]loss_total_epoch 262.3433888480067
Training tokenizer:  57% 4620/8047 [3:22:19<3:30:50,  3.69s/it]loss_total_epoch 262.39330545812845
Training tokenizer:  57% 4621/8047 [3:22:23<3:30:53,  3.69s/it]loss_total_epoch 262.4258303232491
Training tokenizer:  57% 4622/8047 [3:22:27<3:31:24,  3.70s/it]loss_total_epoch 262.47847735881805
Training tokenizer:  57% 4623/8047 [3:22:31<3:30:39,  3.69s/it]loss_total_epoch 262.531596403569
Training tokenizer:  57% 4624/8047 [3:22:34<3:30:10,  3.68s/it]loss_total_epoch 262.57559534162283
Training tokenizer:  57% 4625/8047 [3:22:38<3:29:44,  3.68s/it]loss_total_epoch 262.62180104106665
Training tokenizer:  57% 4626/8047 [3:22:42<3:29:43,  3.68s/it]loss_total_epoch 262.6649054437876
Training tokenizer:  57% 4627/8047 [3:22:45<3:29:50,  3.68s/it]loss_total_epoch 262.71755185723305
Training tokenizer:  58% 4628/8047 [3:22:49<3:29:42,  3.68s/it]loss_total_epoch 262.75287963077426
Training tokenizer:  58% 4629/8047 [3:22:53<3:30:09,  3.69s/it]loss_total_epoch 262.79342075809836
Training tokenizer:  58% 4630/8047 [3:22:56<3:30:23,  3.69s/it]loss_total_epoch 262.85153528675437
Training tokenizer:  58% 4631/8047 [3:23:00<3:29:54,  3.69s/it]loss_total_epoch 262.9025552235544
Training tokenizer:  58% 4632/8047 [3:23:04<3:29:30,  3.68s/it]loss_total_epoch 262.9479858353734
Training tokenizer:  58% 4633/8047 [3:23:07<3:30:25,  3.70s/it]loss_total_epoch 262.9819274023175
Training tokenizer:  58% 4634/8047 [3:23:11<3:29:45,  3.69s/it]loss_total_epoch 263.03282952308655
Training tokenizer:  58% 4635/8047 [3:23:15<3:28:56,  3.67s/it]loss_total_epoch 263.08005072176456
Training tokenizer:  58% 4636/8047 [3:23:18<3:29:08,  3.68s/it]loss_total_epoch 263.12954952940345
Training tokenizer:  58% 4637/8047 [3:23:22<3:29:16,  3.68s/it]loss_total_epoch 263.18033884838223
Training tokenizer:  58% 4638/8047 [3:23:26<3:29:17,  3.68s/it]loss_total_epoch 263.22419126331806
Training tokenizer:  58% 4639/8047 [3:23:29<3:29:24,  3.69s/it]loss_total_epoch 263.26816637068987
Training tokenizer:  58% 4640/8047 [3:23:33<3:29:13,  3.68s/it]loss_total_epoch 263.30971140787005
Training tokenizer:  58% 4641/8047 [3:23:37<3:29:37,  3.69s/it]loss_total_epoch 263.3495712056756
Training tokenizer:  58% 4642/8047 [3:23:41<3:29:48,  3.70s/it]loss_total_epoch 263.3931448198855
Training tokenizer:  58% 4643/8047 [3:23:44<3:30:07,  3.70s/it]loss_total_epoch 263.43380407243967
Training tokenizer:  58% 4644/8047 [3:23:48<3:30:08,  3.70s/it]loss_total_epoch 263.4782170280814
Training tokenizer:  58% 4645/8047 [3:23:52<3:29:50,  3.70s/it]loss_total_epoch 263.5254734158516
Training tokenizer:  58% 4646/8047 [3:23:55<3:29:02,  3.69s/it]loss_total_epoch 263.57395915314555
Training tokenizer:  58% 4647/8047 [3:23:59<3:29:26,  3.70s/it]loss_total_epoch 263.6226692311466
Training tokenizer:  58% 4648/8047 [3:24:03<3:29:44,  3.70s/it]loss_total_epoch 263.6562221646309
Training tokenizer:  58% 4649/8047 [3:24:06<3:28:12,  3.68s/it]loss_total_epoch 263.70332987233996
Training tokenizer:  58% 4650/8047 [3:24:10<3:28:19,  3.68s/it]loss_total_epoch 263.7530232705176
Training tokenizer:  58% 4651/8047 [3:24:14<3:27:54,  3.67s/it]loss_total_epoch 263.807964630425
Training tokenizer:  58% 4652/8047 [3:24:17<3:27:56,  3.68s/it]loss_total_epoch 263.85187796130776
Training tokenizer:  58% 4653/8047 [3:24:21<3:29:15,  3.70s/it]loss_total_epoch 263.8926914483309
Training tokenizer:  58% 4654/8047 [3:24:25<3:29:13,  3.70s/it]loss_total_epoch 263.93978948891163
Training tokenizer:  58% 4655/8047 [3:24:29<3:30:07,  3.72s/it]loss_total_epoch 263.9858178719878
Training tokenizer:  58% 4656/8047 [3:24:32<3:29:48,  3.71s/it]loss_total_epoch 264.0255776196718
Training tokenizer:  58% 4657/8047 [3:24:36<3:29:44,  3.71s/it]loss_total_epoch 264.06950541958213
Training tokenizer:  58% 4658/8047 [3:24:40<3:28:35,  3.69s/it]loss_total_epoch 264.12735050171614
Training tokenizer:  58% 4659/8047 [3:24:43<3:28:18,  3.69s/it]loss_total_epoch 264.1744875460863
Training tokenizer:  58% 4660/8047 [3:24:47<3:31:10,  3.74s/it]loss_total_epoch 264.22013153880835
Training tokenizer:  58% 4661/8047 [3:24:51<3:30:08,  3.72s/it]loss_total_epoch 264.2654512114823
Training tokenizer:  58% 4662/8047 [3:24:55<3:28:57,  3.70s/it]loss_total_epoch 264.30810628831387
Training tokenizer:  58% 4663/8047 [3:24:58<3:29:52,  3.72s/it]loss_total_epoch 264.3494798839092
Training tokenizer:  58% 4664/8047 [3:25:02<3:29:47,  3.72s/it]loss_total_epoch 264.3920891582966
Training tokenizer:  58% 4665/8047 [3:25:06<3:29:07,  3.71s/it]loss_total_epoch 264.4461598880589
Training tokenizer:  58% 4666/8047 [3:25:09<3:28:44,  3.70s/it]loss_total_epoch 264.4973071292043
Training tokenizer:  58% 4667/8047 [3:25:13<3:29:52,  3.73s/it]loss_total_epoch 264.53203140571713
Training tokenizer:  58% 4668/8047 [3:25:17<3:27:54,  3.69s/it]loss_total_epoch 264.5753659531474
Training tokenizer:  58% 4669/8047 [3:25:20<3:28:02,  3.70s/it]loss_total_epoch 264.62912787124515
Training tokenizer:  58% 4670/8047 [3:25:24<3:28:12,  3.70s/it]loss_total_epoch 264.67658734694123
Training tokenizer:  58% 4671/8047 [3:25:28<3:28:26,  3.70s/it]loss_total_epoch 264.7230749838054
Training tokenizer:  58% 4672/8047 [3:25:32<3:27:54,  3.70s/it]loss_total_epoch 264.7778740413487
Training tokenizer:  58% 4673/8047 [3:25:35<3:28:13,  3.70s/it]loss_total_epoch 264.8239412009716
Training tokenizer:  58% 4674/8047 [3:25:39<3:28:11,  3.70s/it]loss_total_epoch 264.8630054369569
Training tokenizer:  58% 4675/8047 [3:25:43<3:27:51,  3.70s/it]loss_total_epoch 264.9018437191844
Training tokenizer:  58% 4676/8047 [3:25:46<3:28:11,  3.71s/it]loss_total_epoch 264.9539123810828
Training tokenizer:  58% 4677/8047 [3:25:50<3:27:56,  3.70s/it]loss_total_epoch 265.00621127709746
Training tokenizer:  58% 4678/8047 [3:25:54<3:28:22,  3.71s/it]loss_total_epoch 265.0529139004648
Training tokenizer:  58% 4679/8047 [3:25:58<3:28:38,  3.72s/it]loss_total_epoch 265.09124482423067
Training tokenizer:  58% 4680/8047 [3:26:01<3:27:58,  3.71s/it]loss_total_epoch 265.1435555033386
Training tokenizer:  58% 4681/8047 [3:26:05<3:28:29,  3.72s/it]loss_total_epoch 265.18553702905774
Training tokenizer:  58% 4682/8047 [3:26:09<3:29:06,  3.73s/it]loss_total_epoch 265.23308599740267
Training tokenizer:  58% 4683/8047 [3:26:12<3:28:41,  3.72s/it]loss_total_epoch 265.2811952829361
Training tokenizer:  58% 4684/8047 [3:26:16<3:28:43,  3.72s/it]loss_total_epoch 265.335263799876
Training tokenizer:  58% 4685/8047 [3:26:20<3:27:35,  3.70s/it]loss_total_epoch 265.3837622962892
Training tokenizer:  58% 4686/8047 [3:26:24<3:27:50,  3.71s/it]loss_total_epoch 265.4343375787139
Training tokenizer:  58% 4687/8047 [3:26:27<3:27:49,  3.71s/it]loss_total_epoch 265.47975984960794
Training tokenizer:  58% 4688/8047 [3:26:31<3:27:47,  3.71s/it]loss_total_epoch 265.5212335214019
Training tokenizer:  58% 4689/8047 [3:26:35<3:27:36,  3.71s/it]loss_total_epoch 265.56956845521927
Training tokenizer:  58% 4690/8047 [3:26:38<3:26:07,  3.68s/it]loss_total_epoch 265.6260696351528
Training tokenizer:  58% 4691/8047 [3:26:42<3:26:43,  3.70s/it]loss_total_epoch 265.684401884675
Training tokenizer:  58% 4692/8047 [3:26:46<3:27:34,  3.71s/it]loss_total_epoch 265.73999799788
Training tokenizer:  58% 4693/8047 [3:26:50<3:27:58,  3.72s/it]loss_total_epoch 265.7830453924835
Training tokenizer:  58% 4694/8047 [3:26:53<3:27:30,  3.71s/it]loss_total_epoch 265.83361485973
Training tokenizer:  58% 4695/8047 [3:26:57<3:27:41,  3.72s/it]loss_total_epoch 265.88846968486905
Training tokenizer:  58% 4696/8047 [3:27:01<3:27:56,  3.72s/it]loss_total_epoch 265.9372885003686
Training tokenizer:  58% 4697/8047 [3:27:04<3:27:32,  3.72s/it]loss_total_epoch 265.9779172614217
Training tokenizer:  58% 4698/8047 [3:27:08<3:27:22,  3.72s/it]loss_total_epoch 266.0285234153271
Training tokenizer:  58% 4699/8047 [3:27:12<3:27:37,  3.72s/it]loss_total_epoch 266.08927788585424
Training tokenizer:  58% 4700/8047 [3:27:16<3:28:11,  3.73s/it]loss_total_epoch 266.1271902397275
Training tokenizer:  58% 4701/8047 [3:27:19<3:28:14,  3.73s/it]loss_total_epoch 266.1662361435592
Training tokenizer:  58% 4702/8047 [3:27:23<3:33:46,  3.83s/it]loss_total_epoch 266.22254007682204
Training tokenizer:  58% 4703/8047 [3:27:27<3:30:58,  3.79s/it]loss_total_epoch 266.2698472775519
Training tokenizer:  58% 4704/8047 [3:27:31<3:29:19,  3.76s/it]loss_total_epoch 266.3247958533466
Training tokenizer:  58% 4705/8047 [3:27:35<3:28:39,  3.75s/it]loss_total_epoch 266.3629184551537
Training tokenizer:  58% 4706/8047 [3:27:38<3:28:34,  3.75s/it]loss_total_epoch 266.42400519549847
Training tokenizer:  58% 4707/8047 [3:27:42<3:28:20,  3.74s/it]loss_total_epoch 266.4652953594923
Training tokenizer:  59% 4708/8047 [3:27:46<3:28:09,  3.74s/it]loss_total_epoch 266.5198345184326
Training tokenizer:  59% 4709/8047 [3:27:49<3:27:36,  3.73s/it]loss_total_epoch 266.5762354172766
Training tokenizer:  59% 4710/8047 [3:27:53<3:27:55,  3.74s/it]loss_total_epoch 266.62404519692063
Training tokenizer:  59% 4711/8047 [3:27:57<3:27:59,  3.74s/it]loss_total_epoch 266.67101111263037
Training tokenizer:  59% 4712/8047 [3:28:01<3:27:26,  3.73s/it]loss_total_epoch 266.7202573157847
Training tokenizer:  59% 4713/8047 [3:28:04<3:26:51,  3.72s/it]loss_total_epoch 266.7722499333322
Training tokenizer:  59% 4714/8047 [3:28:08<3:26:31,  3.72s/it]loss_total_epoch 266.82923966646194
Training tokenizer:  59% 4715/8047 [3:28:12<3:26:23,  3.72s/it]loss_total_epoch 266.88446087390184
Training tokenizer:  59% 4716/8047 [3:28:16<3:27:02,  3.73s/it]loss_total_epoch 266.93068100884557
Training tokenizer:  59% 4717/8047 [3:28:19<3:26:52,  3.73s/it]loss_total_epoch 266.9733880497515
Training tokenizer:  59% 4718/8047 [3:28:23<3:26:42,  3.73s/it]loss_total_epoch 267.0305024199188
Training tokenizer:  59% 4719/8047 [3:28:27<3:26:41,  3.73s/it]loss_total_epoch 267.079143833369
Training tokenizer:  59% 4720/8047 [3:28:30<3:27:04,  3.73s/it]loss_total_epoch 267.12360901013017
Training tokenizer:  59% 4721/8047 [3:28:34<3:27:04,  3.74s/it]loss_total_epoch 267.16916615888476
Training tokenizer:  59% 4722/8047 [3:28:38<3:26:35,  3.73s/it]loss_total_epoch 267.21630733832717
Training tokenizer:  59% 4723/8047 [3:28:42<3:26:50,  3.73s/it]loss_total_epoch 267.26790227741003
Training tokenizer:  59% 4724/8047 [3:28:45<3:27:02,  3.74s/it]loss_total_epoch 267.3228325508535
Training tokenizer:  59% 4725/8047 [3:28:49<3:27:06,  3.74s/it]loss_total_epoch 267.37488731369376
Training tokenizer:  59% 4726/8047 [3:28:53<3:25:47,  3.72s/it]loss_total_epoch 267.4241684153676
Training tokenizer:  59% 4727/8047 [3:28:57<3:25:19,  3.71s/it]loss_total_epoch 267.4713863171637
Training tokenizer:  59% 4728/8047 [3:29:00<3:25:20,  3.71s/it]loss_total_epoch 267.51343001797795
Training tokenizer:  59% 4729/8047 [3:29:04<3:25:52,  3.72s/it]loss_total_epoch 267.5605352073908
Training tokenizer:  59% 4730/8047 [3:29:08<3:26:27,  3.73s/it]loss_total_epoch 267.59972032904625
Training tokenizer:  59% 4731/8047 [3:29:11<3:26:20,  3.73s/it]loss_total_epoch 267.6537491865456
Training tokenizer:  59% 4732/8047 [3:29:15<3:26:34,  3.74s/it]loss_total_epoch 267.694391887635
Training tokenizer:  59% 4733/8047 [3:29:19<3:26:06,  3.73s/it]loss_total_epoch 267.7419589497149
Training tokenizer:  59% 4734/8047 [3:29:23<3:25:30,  3.72s/it]loss_total_epoch 267.79103564843535
Training tokenizer:  59% 4735/8047 [3:29:26<3:25:11,  3.72s/it]loss_total_epoch 267.82800897583365
Training tokenizer:  59% 4736/8047 [3:29:30<3:24:50,  3.71s/it]loss_total_epoch 267.8712698556483
Training tokenizer:  59% 4737/8047 [3:29:34<3:25:18,  3.72s/it]loss_total_epoch 267.92059345170856
Training tokenizer:  59% 4738/8047 [3:29:37<3:25:00,  3.72s/it]loss_total_epoch 267.9790330566466
Training tokenizer:  59% 4739/8047 [3:29:41<3:24:33,  3.71s/it]loss_total_epoch 268.02852269634604
Training tokenizer:  59% 4740/8047 [3:29:45<3:24:03,  3.70s/it]loss_total_epoch 268.070738311857
Training tokenizer:  59% 4741/8047 [3:29:49<3:23:33,  3.69s/it]loss_total_epoch 268.12167562544346
Training tokenizer:  59% 4742/8047 [3:29:52<3:24:13,  3.71s/it]loss_total_epoch 268.17715318500996
Training tokenizer:  59% 4743/8047 [3:29:56<3:23:52,  3.70s/it]loss_total_epoch 268.22497390955687
Training tokenizer:  59% 4744/8047 [3:30:00<3:25:01,  3.72s/it]loss_total_epoch 268.2702413573861
Training tokenizer:  59% 4745/8047 [3:30:03<3:25:15,  3.73s/it]loss_total_epoch 268.32036765292287
Training tokenizer:  59% 4746/8047 [3:30:07<3:24:49,  3.72s/it]loss_total_epoch 268.36623064801097
Training tokenizer:  59% 4747/8047 [3:30:11<3:25:09,  3.73s/it]loss_total_epoch 268.41050984710455
Training tokenizer:  59% 4748/8047 [3:30:15<3:25:03,  3.73s/it]loss_total_epoch 268.44804237782955
Training tokenizer:  59% 4749/8047 [3:30:18<3:25:00,  3.73s/it]loss_total_epoch 268.4993185773492
Training tokenizer:  59% 4750/8047 [3:30:22<3:23:49,  3.71s/it]loss_total_epoch 268.5476025752723
Training tokenizer:  59% 4751/8047 [3:30:26<3:25:10,  3.74s/it]loss_total_epoch 268.59656158462167
Training tokenizer:  59% 4752/8047 [3:30:30<3:25:42,  3.75s/it]loss_total_epoch 268.65346758440137
Training tokenizer:  59% 4753/8047 [3:30:33<3:24:56,  3.73s/it]loss_total_epoch 268.68628844991326
Training tokenizer:  59% 4754/8047 [3:30:37<3:24:26,  3.73s/it]loss_total_epoch 268.73138703778386
Training tokenizer:  59% 4755/8047 [3:30:41<3:24:58,  3.74s/it]loss_total_epoch 268.7728168256581
Training tokenizer:  59% 4756/8047 [3:30:45<3:24:28,  3.73s/it]loss_total_epoch 268.824165917933
Training tokenizer:  59% 4757/8047 [3:30:48<3:24:57,  3.74s/it]loss_total_epoch 268.8645002171397
Training tokenizer:  59% 4758/8047 [3:30:52<3:24:50,  3.74s/it]loss_total_epoch 268.91756561398506
Training tokenizer:  59% 4759/8047 [3:30:56<3:25:06,  3.74s/it]loss_total_epoch 268.96568613499403
Training tokenizer:  59% 4760/8047 [3:30:59<3:24:50,  3.74s/it]loss_total_epoch 269.0112149938941
Training tokenizer:  59% 4761/8047 [3:31:03<3:24:51,  3.74s/it]loss_total_epoch 269.0595578290522
Training tokenizer:  59% 4762/8047 [3:31:07<3:24:37,  3.74s/it]loss_total_epoch 269.0955403447151
Training tokenizer:  59% 4763/8047 [3:31:11<3:25:27,  3.75s/it]loss_total_epoch 269.1447193324566
Training tokenizer:  59% 4764/8047 [3:31:14<3:24:33,  3.74s/it]loss_total_epoch 269.19037702307105
Training tokenizer:  59% 4765/8047 [3:31:18<3:25:17,  3.75s/it]loss_total_epoch 269.2401501983404
Training tokenizer:  59% 4766/8047 [3:31:22<3:25:41,  3.76s/it]loss_total_epoch 269.2881482280791
Training tokenizer:  59% 4767/8047 [3:31:26<3:25:38,  3.76s/it]loss_total_epoch 269.33551425114274
Training tokenizer:  59% 4768/8047 [3:31:30<3:25:16,  3.76s/it]loss_total_epoch 269.3831049464643
Training tokenizer:  59% 4769/8047 [3:31:33<3:24:53,  3.75s/it]loss_total_epoch 269.43310518190265
Training tokenizer:  59% 4770/8047 [3:31:37<3:24:20,  3.74s/it]loss_total_epoch 269.49245293438435
Training tokenizer:  59% 4771/8047 [3:31:41<3:23:20,  3.72s/it]loss_total_epoch 269.53942009061575
Training tokenizer:  59% 4772/8047 [3:31:44<3:23:12,  3.72s/it]loss_total_epoch 269.58786898106337
Training tokenizer:  59% 4773/8047 [3:31:48<3:23:10,  3.72s/it]loss_total_epoch 269.6315681710839
Training tokenizer:  59% 4774/8047 [3:31:52<3:24:12,  3.74s/it]loss_total_epoch 269.6680293418467
Training tokenizer:  59% 4775/8047 [3:31:56<3:24:54,  3.76s/it]loss_total_epoch 269.71737408638
Training tokenizer:  59% 4776/8047 [3:31:59<3:25:25,  3.77s/it]loss_total_epoch 269.7696396484971
Training tokenizer:  59% 4777/8047 [3:32:03<3:24:45,  3.76s/it]loss_total_epoch 269.8158440440893
Training tokenizer:  59% 4778/8047 [3:32:07<3:24:44,  3.76s/it]loss_total_epoch 269.8581595867872
Training tokenizer:  59% 4779/8047 [3:32:11<3:24:31,  3.76s/it]loss_total_epoch 269.8923497721553
Training tokenizer:  59% 4780/8047 [3:32:14<3:23:52,  3.74s/it]loss_total_epoch 269.9366500750184
Training tokenizer:  59% 4781/8047 [3:32:18<3:24:42,  3.76s/it]loss_total_epoch 269.9889327213168
Training tokenizer:  59% 4782/8047 [3:32:22<3:23:57,  3.75s/it]loss_total_epoch 270.0346913859248
Training tokenizer:  59% 4783/8047 [3:32:26<3:23:41,  3.74s/it]loss_total_epoch 270.09113951027393
Training tokenizer:  59% 4784/8047 [3:32:29<3:22:54,  3.73s/it]loss_total_epoch 270.1268371194601
Training tokenizer:  59% 4785/8047 [3:32:33<3:23:50,  3.75s/it]loss_total_epoch 270.1772945858538
Training tokenizer:  59% 4786/8047 [3:32:37<3:23:53,  3.75s/it]loss_total_epoch 270.22996350750327
Training tokenizer:  59% 4787/8047 [3:32:41<3:23:53,  3.75s/it]loss_total_epoch 270.28691612556577
Training tokenizer:  60% 4788/8047 [3:32:44<3:24:01,  3.76s/it]loss_total_epoch 270.3322991915047
Training tokenizer:  60% 4789/8047 [3:32:48<3:24:23,  3.76s/it]loss_total_epoch 270.37838999554515
Training tokenizer:  60% 4790/8047 [3:32:52<3:24:31,  3.77s/it]loss_total_epoch 270.42910170182586
Training tokenizer:  60% 4791/8047 [3:32:56<3:24:54,  3.78s/it]loss_total_epoch 270.47308158129454
Training tokenizer:  60% 4792/8047 [3:33:00<3:24:29,  3.77s/it]loss_total_epoch 270.517712906003
Training tokenizer:  60% 4793/8047 [3:33:03<3:24:18,  3.77s/it]loss_total_epoch 270.5733255557716
Training tokenizer:  60% 4794/8047 [3:33:07<3:24:05,  3.76s/it]loss_total_epoch 270.59491717256606
Training tokenizer:  60% 4795/8047 [3:33:11<3:24:45,  3.78s/it]loss_total_epoch 270.6397287826985
Training tokenizer:  60% 4796/8047 [3:33:15<3:24:05,  3.77s/it]loss_total_epoch 270.6959768962115
Training tokenizer:  60% 4797/8047 [3:33:18<3:22:06,  3.73s/it]loss_total_epoch 270.749036712572
Training tokenizer:  60% 4798/8047 [3:33:22<3:22:05,  3.73s/it]loss_total_epoch 270.7966394405812
Training tokenizer:  60% 4799/8047 [3:33:26<3:22:19,  3.74s/it]loss_total_epoch 270.8417210858315
Training tokenizer:  60% 4800/8047 [3:33:29<3:21:47,  3.73s/it]loss_total_epoch 270.89008373580873
Training tokenizer:  60% 4801/8047 [3:33:33<3:22:20,  3.74s/it]loss_total_epoch 270.9350395295769
Training tokenizer:  60% 4802/8047 [3:33:37<3:22:37,  3.75s/it]loss_total_epoch 270.9874425698072
Training tokenizer:  60% 4803/8047 [3:33:41<3:22:22,  3.74s/it]loss_total_epoch 271.03150469250977
Training tokenizer:  60% 4804/8047 [3:33:45<3:22:46,  3.75s/it]loss_total_epoch 271.08063371293247
Training tokenizer:  60% 4805/8047 [3:33:48<3:22:50,  3.75s/it]loss_total_epoch 271.1171409729868
Training tokenizer:  60% 4806/8047 [3:33:52<3:23:30,  3.77s/it]loss_total_epoch 271.17218065448105
Training tokenizer:  60% 4807/8047 [3:33:56<3:23:36,  3.77s/it]loss_total_epoch 271.2217354197055
Training tokenizer:  60% 4808/8047 [3:34:00<3:23:21,  3.77s/it]loss_total_epoch 271.27750279568136
Training tokenizer:  60% 4809/8047 [3:34:03<3:22:24,  3.75s/it]loss_total_epoch 271.32223368249834
Training tokenizer:  60% 4810/8047 [3:34:07<3:23:49,  3.78s/it]loss_total_epoch 271.36121845804155
Training tokenizer:  60% 4811/8047 [3:34:11<3:23:56,  3.78s/it]loss_total_epoch 271.40571936778724
Training tokenizer:  60% 4812/8047 [3:34:15<3:23:43,  3.78s/it]loss_total_epoch 271.458644265309
Training tokenizer:  60% 4813/8047 [3:34:18<3:23:27,  3.77s/it]loss_total_epoch 271.5037755910307
Training tokenizer:  60% 4814/8047 [3:34:22<3:23:07,  3.77s/it]loss_total_epoch 271.5576701890677
Training tokenizer:  60% 4815/8047 [3:34:26<3:23:11,  3.77s/it]loss_total_epoch 271.6038272622973
Training tokenizer:  60% 4816/8047 [3:34:30<3:23:21,  3.78s/it]loss_total_epoch 271.6495602261275
Training tokenizer:  60% 4817/8047 [3:34:34<3:23:14,  3.78s/it]loss_total_epoch 271.6932191643864
Training tokenizer:  60% 4818/8047 [3:34:37<3:22:28,  3.76s/it]loss_total_epoch 271.74280940182507
Training tokenizer:  60% 4819/8047 [3:34:41<3:22:32,  3.76s/it]loss_total_epoch 271.784098951146
Training tokenizer:  60% 4820/8047 [3:34:45<3:23:07,  3.78s/it]loss_total_epoch 271.8292699176818
Training tokenizer:  60% 4821/8047 [3:34:49<3:23:11,  3.78s/it]loss_total_epoch 271.8656138461083
Training tokenizer:  60% 4822/8047 [3:34:52<3:23:09,  3.78s/it]loss_total_epoch 271.91670055128634
Training tokenizer:  60% 4823/8047 [3:34:56<3:23:09,  3.78s/it]loss_total_epoch 271.9521500971168
Training tokenizer:  60% 4824/8047 [3:35:00<3:22:49,  3.78s/it]loss_total_epoch 271.99113748408854
Training tokenizer:  60% 4825/8047 [3:35:04<3:23:15,  3.78s/it]loss_total_epoch 272.0494544673711
Training tokenizer:  60% 4826/8047 [3:35:08<3:22:52,  3.78s/it]loss_total_epoch 272.09391971863806
Training tokenizer:  60% 4827/8047 [3:35:11<3:22:59,  3.78s/it]loss_total_epoch 272.1533003617078
Training tokenizer:  60% 4828/8047 [3:35:15<3:23:02,  3.78s/it]loss_total_epoch 272.1943048145622
Training tokenizer:  60% 4829/8047 [3:35:19<3:22:43,  3.78s/it]loss_total_epoch 272.24166964925826
Training tokenizer:  60% 4830/8047 [3:35:23<3:23:15,  3.79s/it]loss_total_epoch 272.2842896897346
Training tokenizer:  60% 4831/8047 [3:35:27<3:22:44,  3.78s/it]loss_total_epoch 272.3298669178039
Training tokenizer:  60% 4832/8047 [3:35:30<3:22:18,  3.78s/it]loss_total_epoch 272.3689464945346
Training tokenizer:  60% 4833/8047 [3:35:34<3:22:15,  3.78s/it]loss_total_epoch 272.4127510059625
Training tokenizer:  60% 4834/8047 [3:35:38<3:22:48,  3.79s/it]loss_total_epoch 272.46711466647685
Training tokenizer:  60% 4835/8047 [3:35:42<3:23:03,  3.79s/it]loss_total_epoch 272.50548225827515
Training tokenizer:  60% 4836/8047 [3:35:45<3:22:12,  3.78s/it]loss_total_epoch 272.5559040699154
Training tokenizer:  60% 4837/8047 [3:35:49<3:22:27,  3.78s/it]loss_total_epoch 272.61038639582694
Training tokenizer:  60% 4838/8047 [3:35:53<3:21:51,  3.77s/it]loss_total_epoch 272.65759046934545
Training tokenizer:  60% 4839/8047 [3:35:57<3:22:19,  3.78s/it]loss_total_epoch 272.70525280945003
Training tokenizer:  60% 4840/8047 [3:36:01<3:22:08,  3.78s/it]loss_total_epoch 272.7482825871557
Training tokenizer:  60% 4841/8047 [3:36:04<3:22:43,  3.79s/it]loss_total_epoch 272.7996801342815
Training tokenizer:  60% 4842/8047 [3:36:08<3:22:24,  3.79s/it]loss_total_epoch 272.8524567577988
Training tokenizer:  60% 4843/8047 [3:36:12<3:21:27,  3.77s/it]loss_total_epoch 272.8930349815637
Training tokenizer:  60% 4844/8047 [3:36:16<3:21:52,  3.78s/it]loss_total_epoch 272.93983557634056
Training tokenizer:  60% 4845/8047 [3:36:19<3:22:04,  3.79s/it]loss_total_epoch 272.9885269869119
Training tokenizer:  60% 4846/8047 [3:36:23<3:21:25,  3.78s/it]loss_total_epoch 273.0168161019683
Training tokenizer:  60% 4847/8047 [3:36:27<3:21:33,  3.78s/it]loss_total_epoch 273.0548368059099
Training tokenizer:  60% 4848/8047 [3:36:31<3:21:34,  3.78s/it]loss_total_epoch 273.09662303328514
Training tokenizer:  60% 4849/8047 [3:36:35<3:21:34,  3.78s/it]loss_total_epoch 273.14492063969374
Training tokenizer:  60% 4850/8047 [3:36:38<3:21:34,  3.78s/it]loss_total_epoch 273.19315909966826
Training tokenizer:  60% 4851/8047 [3:36:42<3:21:56,  3.79s/it]loss_total_epoch 273.23798145726323
Training tokenizer:  60% 4852/8047 [3:36:46<3:21:44,  3.79s/it]loss_total_epoch 273.2890962101519
Training tokenizer:  60% 4853/8047 [3:36:50<3:21:26,  3.78s/it]loss_total_epoch 273.3353890068829
Training tokenizer:  60% 4854/8047 [3:36:54<3:22:19,  3.80s/it]loss_total_epoch 273.3850599527359
Training tokenizer:  60% 4855/8047 [3:36:57<3:21:15,  3.78s/it]loss_total_epoch 273.42982703074813
Training tokenizer:  60% 4856/8047 [3:37:01<3:21:32,  3.79s/it]loss_total_epoch 273.4749277308583
Training tokenizer:  60% 4857/8047 [3:37:05<3:21:01,  3.78s/it]loss_total_epoch 273.52272422611713
Training tokenizer:  60% 4858/8047 [3:37:09<3:21:28,  3.79s/it]loss_total_epoch 273.5579641573131
Training tokenizer:  60% 4859/8047 [3:37:13<3:22:28,  3.81s/it]loss_total_epoch 273.60536289587617
Training tokenizer:  60% 4860/8047 [3:37:16<3:22:29,  3.81s/it]loss_total_epoch 273.6480760537088
Training tokenizer:  60% 4861/8047 [3:37:20<3:22:04,  3.81s/it]loss_total_epoch 273.6846771426499
Training tokenizer:  60% 4862/8047 [3:37:24<3:22:37,  3.82s/it]loss_total_epoch 273.7323101311922
Training tokenizer:  60% 4863/8047 [3:37:28<3:21:41,  3.80s/it]loss_total_epoch 273.7838199585676
Training tokenizer:  60% 4864/8047 [3:37:31<3:20:09,  3.77s/it]loss_total_epoch 273.83128184452653
Training tokenizer:  60% 4865/8047 [3:37:35<3:20:38,  3.78s/it]loss_total_epoch 273.8749846033752
Training tokenizer:  60% 4866/8047 [3:37:39<3:20:42,  3.79s/it]loss_total_epoch 273.918017141521
Training tokenizer:  60% 4867/8047 [3:37:43<3:21:05,  3.79s/it]loss_total_epoch 273.96141285821795
Training tokenizer:  60% 4868/8047 [3:37:47<3:20:49,  3.79s/it]loss_total_epoch 274.01118332892656
Training tokenizer:  61% 4869/8047 [3:37:51<3:21:34,  3.81s/it]loss_total_epoch 274.0548984259367
Training tokenizer:  61% 4870/8047 [3:37:54<3:21:05,  3.80s/it]loss_total_epoch 274.1073085181415
Training tokenizer:  61% 4871/8047 [3:37:58<3:20:56,  3.80s/it]loss_total_epoch 274.15439972281456
Training tokenizer:  61% 4872/8047 [3:38:02<3:20:35,  3.79s/it]loss_total_epoch 274.1884775236249
Training tokenizer:  61% 4873/8047 [3:38:06<3:20:03,  3.78s/it]loss_total_epoch 274.23932007327676
Training tokenizer:  61% 4874/8047 [3:38:09<3:20:44,  3.80s/it]loss_total_epoch 274.286151509732
Training tokenizer:  61% 4875/8047 [3:38:13<3:20:28,  3.79s/it]loss_total_epoch 274.32607705518603
Training tokenizer:  61% 4876/8047 [3:38:17<3:19:40,  3.78s/it]loss_total_epoch 274.3722864314914
Training tokenizer:  61% 4877/8047 [3:38:21<3:20:13,  3.79s/it]loss_total_epoch 274.4260120987892
Training tokenizer:  61% 4878/8047 [3:38:25<3:20:04,  3.79s/it]loss_total_epoch 274.4641422480345
Training tokenizer:  61% 4879/8047 [3:38:28<3:20:19,  3.79s/it]loss_total_epoch 274.50989481434226
Training tokenizer:  61% 4880/8047 [3:38:32<3:20:20,  3.80s/it]loss_total_epoch 274.5515976175666
Training tokenizer:  61% 4881/8047 [3:38:36<3:20:01,  3.79s/it]loss_total_epoch 274.5968456119299
Training tokenizer:  61% 4882/8047 [3:38:40<3:19:56,  3.79s/it]loss_total_epoch 274.649760812521
Training tokenizer:  61% 4883/8047 [3:38:43<3:18:49,  3.77s/it]loss_total_epoch 274.7020568884909
Training tokenizer:  61% 4884/8047 [3:38:47<3:19:07,  3.78s/it]loss_total_epoch 274.7512377239764
Training tokenizer:  61% 4885/8047 [3:38:51<3:19:21,  3.78s/it]loss_total_epoch 274.80419870093465
Training tokenizer:  61% 4886/8047 [3:38:55<3:19:53,  3.79s/it]loss_total_epoch 274.8456491455436
Training tokenizer:  61% 4887/8047 [3:38:59<3:20:49,  3.81s/it]loss_total_epoch 274.8872054591775
Training tokenizer:  61% 4888/8047 [3:39:03<3:19:56,  3.80s/it]loss_total_epoch 274.93434465676546
Training tokenizer:  61% 4889/8047 [3:39:06<3:19:36,  3.79s/it]loss_total_epoch 275.0001414716244
Training tokenizer:  61% 4890/8047 [3:39:10<3:19:38,  3.79s/it]loss_total_epoch 275.0421103388071
Training tokenizer:  61% 4891/8047 [3:39:14<3:19:49,  3.80s/it]loss_total_epoch 275.09192596003413
Training tokenizer:  61% 4892/8047 [3:39:18<3:19:35,  3.80s/it]loss_total_epoch 275.13284565880895
Training tokenizer:  61% 4893/8047 [3:39:21<3:19:18,  3.79s/it]loss_total_epoch 275.17111365124583
Training tokenizer:  61% 4894/8047 [3:39:25<3:19:37,  3.80s/it]loss_total_epoch 275.21377171576023
Training tokenizer:  61% 4895/8047 [3:39:29<3:19:42,  3.80s/it]loss_total_epoch 275.2561438269913
Training tokenizer:  61% 4896/8047 [3:39:33<3:19:43,  3.80s/it]loss_total_epoch 275.2973299585283
Training tokenizer:  61% 4897/8047 [3:39:37<3:19:18,  3.80s/it]loss_total_epoch 275.3537080697715
Training tokenizer:  61% 4898/8047 [3:39:41<3:19:49,  3.81s/it]loss_total_epoch 275.4051295183599
Training tokenizer:  61% 4899/8047 [3:39:44<3:20:00,  3.81s/it]loss_total_epoch 275.453021902591
Training tokenizer:  61% 4900/8047 [3:39:48<3:18:00,  3.78s/it]loss_total_epoch 275.4923974312842
Training tokenizer:  61% 4901/8047 [3:39:52<3:18:29,  3.79s/it]loss_total_epoch 275.5323947779834
Training tokenizer:  61% 4902/8047 [3:39:56<3:18:42,  3.79s/it]loss_total_epoch 275.57067278400064
Training tokenizer:  61% 4903/8047 [3:39:59<3:19:19,  3.80s/it]loss_total_epoch 275.61625514552
Training tokenizer:  61% 4904/8047 [3:40:03<3:19:39,  3.81s/it]loss_total_epoch 275.65243212506175
Training tokenizer:  61% 4905/8047 [3:40:07<3:19:47,  3.82s/it]loss_total_epoch 275.7031814754009
Training tokenizer:  61% 4906/8047 [3:40:11<3:19:37,  3.81s/it]loss_total_epoch 275.7473571896553
Training tokenizer:  61% 4907/8047 [3:40:15<3:19:01,  3.80s/it]loss_total_epoch 275.79870299622416
Training tokenizer:  61% 4908/8047 [3:40:19<3:18:54,  3.80s/it]loss_total_epoch 275.8466152586043
Training tokenizer:  61% 4909/8047 [3:40:22<3:18:44,  3.80s/it]loss_total_epoch 275.8871163390577
Training tokenizer:  61% 4910/8047 [3:40:26<3:19:07,  3.81s/it]loss_total_epoch 275.92297783121467
Training tokenizer:  61% 4911/8047 [3:40:30<3:17:23,  3.78s/it]loss_total_epoch 275.9870033301413
Training tokenizer:  61% 4912/8047 [3:40:34<3:17:47,  3.79s/it]loss_total_epoch 276.03406627476215
Training tokenizer:  61% 4913/8047 [3:40:37<3:18:32,  3.80s/it]loss_total_epoch 276.07512007281184
Training tokenizer:  61% 4914/8047 [3:40:41<3:18:38,  3.80s/it]loss_total_epoch 276.1012881323695
Training tokenizer:  61% 4915/8047 [3:40:45<3:18:46,  3.81s/it]loss_total_epoch 276.14558996260166
Training tokenizer:  61% 4916/8047 [3:40:49<3:19:22,  3.82s/it]loss_total_epoch 276.19150245189667
Training tokenizer:  61% 4917/8047 [3:40:53<3:19:02,  3.82s/it]loss_total_epoch 276.22203658521175
Training tokenizer:  61% 4918/8047 [3:40:57<3:18:18,  3.80s/it]loss_total_epoch 276.27263378724456
Training tokenizer:  61% 4919/8047 [3:41:00<3:18:21,  3.80s/it]loss_total_epoch 276.3217774294317
Training tokenizer:  61% 4920/8047 [3:41:04<3:17:43,  3.79s/it]loss_total_epoch 276.34649224206805
Training tokenizer:  61% 4921/8047 [3:41:08<3:17:10,  3.78s/it]loss_total_epoch 276.39828330650926
Training tokenizer:  61% 4922/8047 [3:41:12<3:17:53,  3.80s/it]loss_total_epoch 276.4467508532107
Training tokenizer:  61% 4923/8047 [3:41:15<3:17:33,  3.79s/it]loss_total_epoch 276.4929394572973
Training tokenizer:  61% 4924/8047 [3:41:19<3:17:17,  3.79s/it]loss_total_epoch 276.5308157503605
Training tokenizer:  61% 4925/8047 [3:41:23<3:18:09,  3.81s/it]loss_total_epoch 276.57565354555845
Training tokenizer:  61% 4926/8047 [3:41:27<3:18:35,  3.82s/it]loss_total_epoch 276.6235246323049
Training tokenizer:  61% 4927/8047 [3:41:31<3:19:10,  3.83s/it]loss_total_epoch 276.6727145127952
Training tokenizer:  61% 4928/8047 [3:41:35<3:20:59,  3.87s/it]loss_total_epoch 276.71543957293034
Training tokenizer:  61% 4929/8047 [3:41:39<3:20:45,  3.86s/it]loss_total_epoch 276.76044584810734
Training tokenizer:  61% 4930/8047 [3:41:42<3:20:37,  3.86s/it]loss_total_epoch 276.8079294115305
Training tokenizer:  61% 4931/8047 [3:41:46<3:20:02,  3.85s/it]loss_total_epoch 276.84005627036095
Training tokenizer:  61% 4932/8047 [3:41:50<3:20:07,  3.85s/it]loss_total_epoch 276.87740886956453
Training tokenizer:  61% 4933/8047 [3:41:54<3:19:55,  3.85s/it]loss_total_epoch 276.92800453677773
Training tokenizer:  61% 4934/8047 [3:41:58<3:20:23,  3.86s/it]loss_total_epoch 276.96899247169495
Training tokenizer:  61% 4935/8047 [3:42:02<3:20:01,  3.86s/it]loss_total_epoch 277.0154954381287
Training tokenizer:  61% 4936/8047 [3:42:06<3:19:27,  3.85s/it]loss_total_epoch 277.072767239064
Training tokenizer:  61% 4937/8047 [3:42:09<3:19:11,  3.84s/it]loss_total_epoch 277.12207482382655
Training tokenizer:  61% 4938/8047 [3:42:13<3:19:02,  3.84s/it]loss_total_epoch 277.1718470863998
Training tokenizer:  61% 4939/8047 [3:42:17<3:19:47,  3.86s/it]loss_total_epoch 277.2191732004285
Training tokenizer:  61% 4940/8047 [3:42:21<3:19:08,  3.85s/it]loss_total_epoch 277.26610716432333
Training tokenizer:  61% 4941/8047 [3:42:25<3:18:19,  3.83s/it]loss_total_epoch 277.31780184060335
Training tokenizer:  61% 4942/8047 [3:42:29<3:17:55,  3.82s/it]loss_total_epoch 277.36254259571433
Training tokenizer:  61% 4943/8047 [3:42:32<3:17:42,  3.82s/it]loss_total_epoch 277.39992333576083
Training tokenizer:  61% 4944/8047 [3:42:36<3:17:16,  3.81s/it]loss_total_epoch 277.43806671351194
Training tokenizer:  61% 4945/8047 [3:42:40<3:16:49,  3.81s/it]loss_total_epoch 277.47437312453985
Training tokenizer:  61% 4946/8047 [3:42:44<3:16:38,  3.80s/it]loss_total_epoch 277.53011287003756
Training tokenizer:  61% 4947/8047 [3:42:48<3:17:06,  3.82s/it]loss_total_epoch 277.5812826268375
Training tokenizer:  61% 4948/8047 [3:42:51<3:17:49,  3.83s/it]loss_total_epoch 277.6122295279056
Training tokenizer:  62% 4949/8047 [3:42:55<3:18:22,  3.84s/it]loss_total_epoch 277.65807579644024
Training tokenizer:  62% 4950/8047 [3:42:59<3:17:23,  3.82s/it]loss_total_epoch 277.7070573735982
Training tokenizer:  62% 4951/8047 [3:43:03<3:17:39,  3.83s/it]loss_total_epoch 277.76271221973
Training tokenizer:  62% 4952/8047 [3:43:07<3:16:41,  3.81s/it]loss_total_epoch 277.80855082161725
Training tokenizer:  62% 4953/8047 [3:43:11<3:17:24,  3.83s/it]loss_total_epoch 277.8515397813171
Training tokenizer:  62% 4954/8047 [3:43:14<3:16:49,  3.82s/it]loss_total_epoch 277.91062076948583
Training tokenizer:  62% 4955/8047 [3:43:18<3:16:28,  3.81s/it]loss_total_epoch 277.96549745835364
Training tokenizer:  62% 4956/8047 [3:43:22<3:16:53,  3.82s/it]loss_total_epoch 278.0086054224521
Training tokenizer:  62% 4957/8047 [3:43:26<3:16:38,  3.82s/it]loss_total_epoch 278.06467149965465
Training tokenizer:  62% 4958/8047 [3:43:30<3:16:46,  3.82s/it]loss_total_epoch 278.105657575652
Training tokenizer:  62% 4959/8047 [3:43:34<3:16:47,  3.82s/it]loss_total_epoch 278.1436239425093
Training tokenizer:  62% 4960/8047 [3:43:37<3:16:45,  3.82s/it]loss_total_epoch 278.1913443673402
Training tokenizer:  62% 4961/8047 [3:43:41<3:16:44,  3.83s/it]loss_total_epoch 278.2393626328558
Training tokenizer:  62% 4962/8047 [3:43:45<3:17:15,  3.84s/it]loss_total_epoch 278.28533596359193
Training tokenizer:  62% 4963/8047 [3:43:49<3:16:51,  3.83s/it]loss_total_epoch 278.32579897902906
Training tokenizer:  62% 4964/8047 [3:43:53<3:16:29,  3.82s/it]loss_total_epoch 278.36883754841983
Training tokenizer:  62% 4965/8047 [3:43:56<3:15:49,  3.81s/it]loss_total_epoch 278.41050145588815
Training tokenizer:  62% 4966/8047 [3:44:00<3:16:07,  3.82s/it]loss_total_epoch 278.45712964423
Training tokenizer:  62% 4967/8047 [3:44:04<3:16:19,  3.82s/it]loss_total_epoch 278.5030450243503
Training tokenizer:  62% 4968/8047 [3:44:08<3:16:39,  3.83s/it]loss_total_epoch 278.5561608429998
Training tokenizer:  62% 4969/8047 [3:44:12<3:17:00,  3.84s/it]loss_total_epoch 278.5952000040561
Training tokenizer:  62% 4970/8047 [3:44:16<3:17:36,  3.85s/it]loss_total_epoch 278.6410346645862
Training tokenizer:  62% 4971/8047 [3:44:20<3:17:14,  3.85s/it]loss_total_epoch 278.6901884097606
Training tokenizer:  62% 4972/8047 [3:44:23<3:16:49,  3.84s/it]loss_total_epoch 278.741153890267
Training tokenizer:  62% 4973/8047 [3:44:27<3:16:03,  3.83s/it]loss_total_epoch 278.7734194304794
Training tokenizer:  62% 4974/8047 [3:44:31<3:16:23,  3.83s/it]loss_total_epoch 278.8137348908931
Training tokenizer:  62% 4975/8047 [3:44:35<3:16:26,  3.84s/it]loss_total_epoch 278.86581543274224
Training tokenizer:  62% 4976/8047 [3:44:39<3:16:57,  3.85s/it]loss_total_epoch 278.9107975605875
Training tokenizer:  62% 4977/8047 [3:44:43<3:16:19,  3.84s/it]loss_total_epoch 278.9460102189332
Training tokenizer:  62% 4978/8047 [3:44:46<3:16:00,  3.83s/it]loss_total_epoch 278.97737893275917
Training tokenizer:  62% 4979/8047 [3:44:50<3:16:31,  3.84s/it]loss_total_epoch 279.01380362920463
Training tokenizer:  62% 4980/8047 [3:44:54<3:16:25,  3.84s/it]loss_total_epoch 279.05214741267264
Training tokenizer:  62% 4981/8047 [3:44:58<3:16:52,  3.85s/it]loss_total_epoch 279.09231822006404
Training tokenizer:  62% 4982/8047 [3:45:02<3:16:56,  3.86s/it]loss_total_epoch 279.14051601849496
Training tokenizer:  62% 4983/8047 [3:45:06<3:16:23,  3.85s/it]loss_total_epoch 279.1890352051705
Training tokenizer:  62% 4984/8047 [3:45:09<3:16:36,  3.85s/it]loss_total_epoch 279.2383578810841
Training tokenizer:  62% 4985/8047 [3:45:13<3:16:14,  3.85s/it]loss_total_epoch 279.2715181503445
Training tokenizer:  62% 4986/8047 [3:45:17<3:16:06,  3.84s/it]loss_total_epoch 279.3198282327503
Training tokenizer:  62% 4987/8047 [3:45:21<3:15:19,  3.83s/it]loss_total_epoch 279.36729974485934
Training tokenizer:  62% 4988/8047 [3:45:25<3:16:00,  3.84s/it]loss_total_epoch 279.4143642280251
Training tokenizer:  62% 4989/8047 [3:45:29<3:16:26,  3.85s/it]loss_total_epoch 279.4604408610612
Training tokenizer:  62% 4990/8047 [3:45:33<3:16:29,  3.86s/it]loss_total_epoch 279.50459842942655
Training tokenizer:  62% 4991/8047 [3:45:36<3:16:09,  3.85s/it]loss_total_epoch 279.549867188558
Training tokenizer:  62% 4992/8047 [3:45:40<3:16:27,  3.86s/it]loss_total_epoch 279.5958018582314
Training tokenizer:  62% 4993/8047 [3:45:44<3:17:13,  3.87s/it]loss_total_epoch 279.6535836998373
Training tokenizer:  62% 4994/8047 [3:45:48<3:17:44,  3.89s/it]loss_total_epoch 279.69349008612335
Training tokenizer:  62% 4995/8047 [3:45:52<3:17:37,  3.89s/it]loss_total_epoch 279.734638242051
Training tokenizer:  62% 4996/8047 [3:45:56<3:17:24,  3.88s/it]loss_total_epoch 279.78755491040647
Training tokenizer:  62% 4997/8047 [3:46:00<3:16:54,  3.87s/it]loss_total_epoch 279.83498696796596
Training tokenizer:  62% 4998/8047 [3:46:04<3:17:14,  3.88s/it]loss_total_epoch 279.8780698198825
Training tokenizer:  62% 4999/8047 [3:46:07<3:16:39,  3.87s/it]loss_total_epoch 279.931763401255
Training tokenizer:  62% 5000/8047 [3:46:11<3:16:43,  3.87s/it]loss_total_epoch 279.98304862342775
Training tokenizer:  62% 5001/8047 [3:46:15<3:16:33,  3.87s/it]loss_total_epoch 280.0234213080257
Training tokenizer:  62% 5002/8047 [3:46:19<3:15:59,  3.86s/it]loss_total_epoch 280.0714179556817
Training tokenizer:  62% 5003/8047 [3:46:23<3:15:51,  3.86s/it]loss_total_epoch 280.1277599837631
Training tokenizer:  62% 5004/8047 [3:46:27<3:15:16,  3.85s/it]loss_total_epoch 280.1766847874969
Training tokenizer:  62% 5005/8047 [3:46:31<3:15:17,  3.85s/it]loss_total_epoch 280.2299601417035
Training tokenizer:  62% 5006/8047 [3:46:34<3:15:07,  3.85s/it]loss_total_epoch 280.27047548629344
Training tokenizer:  62% 5007/8047 [3:46:38<3:15:09,  3.85s/it]loss_total_epoch 280.30198485963047
Training tokenizer:  62% 5008/8047 [3:46:42<3:15:28,  3.86s/it]loss_total_epoch 280.3505494799465
Training tokenizer:  62% 5009/8047 [3:46:46<3:15:42,  3.87s/it]loss_total_epoch 280.3963523376733
Training tokenizer:  62% 5010/8047 [3:46:50<3:15:29,  3.86s/it]loss_total_epoch 280.45649562589824
Training tokenizer:  62% 5011/8047 [3:46:54<3:15:29,  3.86s/it]loss_total_epoch 280.49328275583684
Training tokenizer:  62% 5012/8047 [3:46:58<3:13:47,  3.83s/it]loss_total_epoch 280.5356895048171
Training tokenizer:  62% 5013/8047 [3:47:01<3:14:05,  3.84s/it]loss_total_epoch 280.57797156460583
Training tokenizer:  62% 5014/8047 [3:47:05<3:13:52,  3.84s/it]loss_total_epoch 280.619370078668
Training tokenizer:  62% 5015/8047 [3:47:09<3:14:29,  3.85s/it]loss_total_epoch 280.66005376540124
Training tokenizer:  62% 5016/8047 [3:47:13<3:14:39,  3.85s/it]loss_total_epoch 280.69950241036713
Training tokenizer:  62% 5017/8047 [3:47:17<3:15:07,  3.86s/it]loss_total_epoch 280.75258770026267
Training tokenizer:  62% 5018/8047 [3:47:21<3:14:30,  3.85s/it]loss_total_epoch 280.798554899171
Training tokenizer:  62% 5019/8047 [3:47:25<3:15:23,  3.87s/it]loss_total_epoch 280.83432504720986
Training tokenizer:  62% 5020/8047 [3:47:29<3:16:32,  3.90s/it]loss_total_epoch 280.87132586725056
Training tokenizer:  62% 5021/8047 [3:47:33<3:17:39,  3.92s/it]loss_total_epoch 280.9198369178921
Training tokenizer:  62% 5022/8047 [3:47:36<3:15:12,  3.87s/it]loss_total_epoch 280.97172000817955
Training tokenizer:  62% 5023/8047 [3:47:40<3:15:24,  3.88s/it]loss_total_epoch 281.0286357123405
Training tokenizer:  62% 5024/8047 [3:47:44<3:15:03,  3.87s/it]loss_total_epoch 281.0746308695525
Training tokenizer:  62% 5025/8047 [3:47:48<3:14:30,  3.86s/it]loss_total_epoch 281.1256124023348
Training tokenizer:  62% 5026/8047 [3:47:52<3:15:16,  3.88s/it]loss_total_epoch 281.1709091383964
Training tokenizer:  62% 5027/8047 [3:47:56<3:15:11,  3.88s/it]loss_total_epoch 281.2246924992651
Training tokenizer:  62% 5028/8047 [3:48:00<3:14:34,  3.87s/it]loss_total_epoch 281.26838134787977
Training tokenizer:  62% 5029/8047 [3:48:03<3:13:47,  3.85s/it]loss_total_epoch 281.31798130087554
Training tokenizer:  63% 5030/8047 [3:48:07<3:14:00,  3.86s/it]loss_total_epoch 281.37369208596647
Training tokenizer:  63% 5031/8047 [3:48:11<3:15:10,  3.88s/it]loss_total_epoch 281.42825701646507
Training tokenizer:  63% 5032/8047 [3:48:15<3:14:24,  3.87s/it]loss_total_epoch 281.48031626082957
Training tokenizer:  63% 5033/8047 [3:48:19<3:14:37,  3.87s/it]loss_total_epoch 281.533777249977
Training tokenizer:  63% 5034/8047 [3:48:23<3:14:57,  3.88s/it]loss_total_epoch 281.5784933809191
Training tokenizer:  63% 5035/8047 [3:48:27<3:15:40,  3.90s/it]loss_total_epoch 281.6146423686296
Training tokenizer:  63% 5036/8047 [3:48:31<3:16:09,  3.91s/it]loss_total_epoch 281.6616109330207
Training tokenizer:  63% 5037/8047 [3:48:35<3:15:30,  3.90s/it]loss_total_epoch 281.7112438250333
Training tokenizer:  63% 5038/8047 [3:48:38<3:15:07,  3.89s/it]loss_total_epoch 281.76652180962265
Training tokenizer:  63% 5039/8047 [3:48:42<3:15:29,  3.90s/it]loss_total_epoch 281.81973578222096
Training tokenizer:  63% 5040/8047 [3:48:46<3:15:39,  3.90s/it]loss_total_epoch 281.8751582298428
Training tokenizer:  63% 5041/8047 [3:48:50<3:15:17,  3.90s/it]loss_total_epoch 281.9295256752521
Training tokenizer:  63% 5042/8047 [3:48:54<3:14:37,  3.89s/it]loss_total_epoch 281.9681549165398
Training tokenizer:  63% 5043/8047 [3:48:58<3:14:21,  3.88s/it]loss_total_epoch 282.02242069877684
Training tokenizer:  63% 5044/8047 [3:49:02<3:13:23,  3.86s/it]loss_total_epoch 282.070019679144
Training tokenizer:  63% 5045/8047 [3:49:06<3:13:54,  3.88s/it]loss_total_epoch 282.12247330881655
Training tokenizer:  63% 5046/8047 [3:49:09<3:13:32,  3.87s/it]loss_total_epoch 282.16678760014474
Training tokenizer:  63% 5047/8047 [3:49:13<3:13:28,  3.87s/it]loss_total_epoch 282.209226032719
Training tokenizer:  63% 5048/8047 [3:49:17<3:13:12,  3.87s/it]loss_total_epoch 282.25529299117625
Training tokenizer:  63% 5049/8047 [3:49:21<3:13:20,  3.87s/it]loss_total_epoch 282.2964693699032
Training tokenizer:  63% 5050/8047 [3:49:25<3:13:11,  3.87s/it]loss_total_epoch 282.339719844982
Training tokenizer:  63% 5051/8047 [3:49:29<3:13:39,  3.88s/it]loss_total_epoch 282.3801852259785
Training tokenizer:  63% 5052/8047 [3:49:33<3:13:19,  3.87s/it]loss_total_epoch 282.4238982331008
Training tokenizer:  63% 5053/8047 [3:49:37<3:14:08,  3.89s/it]loss_total_epoch 282.4648798201233
Training tokenizer:  63% 5054/8047 [3:49:40<3:14:16,  3.89s/it]loss_total_epoch 282.50783515535295
Training tokenizer:  63% 5055/8047 [3:49:44<3:13:52,  3.89s/it]loss_total_epoch 282.55585640855134
Training tokenizer:  63% 5056/8047 [3:49:48<3:12:50,  3.87s/it]loss_total_epoch 282.60537817142904
Training tokenizer:  63% 5057/8047 [3:49:52<3:13:14,  3.88s/it]loss_total_epoch 282.65443054772913
Training tokenizer:  63% 5058/8047 [3:49:56<3:13:33,  3.89s/it]loss_total_epoch 282.7025462370366
Training tokenizer:  63% 5059/8047 [3:50:00<3:13:36,  3.89s/it]loss_total_epoch 282.7489669825882
Training tokenizer:  63% 5060/8047 [3:50:04<3:13:39,  3.89s/it]loss_total_epoch 282.7883827406913
Training tokenizer:  63% 5061/8047 [3:50:08<3:13:23,  3.89s/it]loss_total_epoch 282.81881608068943
Training tokenizer:  63% 5062/8047 [3:50:12<3:14:03,  3.90s/it]loss_total_epoch 282.8608015961945
Training tokenizer:  63% 5063/8047 [3:50:15<3:13:32,  3.89s/it]loss_total_epoch 282.9048320837319
Training tokenizer:  63% 5064/8047 [3:50:19<3:13:35,  3.89s/it]loss_total_epoch 282.9512804299593
Training tokenizer:  63% 5065/8047 [3:50:23<3:13:38,  3.90s/it]loss_total_epoch 283.0065860003233
Training tokenizer:  63% 5066/8047 [3:50:27<3:12:37,  3.88s/it]loss_total_epoch 283.04541590064764
Training tokenizer:  63% 5067/8047 [3:50:31<3:12:09,  3.87s/it]loss_total_epoch 283.0908063277602
Training tokenizer:  63% 5068/8047 [3:50:35<3:11:46,  3.86s/it]loss_total_epoch 283.13481936231256
Training tokenizer:  63% 5069/8047 [3:50:39<3:12:25,  3.88s/it]loss_total_epoch 283.181882340461
Training tokenizer:  63% 5070/8047 [3:50:43<3:13:18,  3.90s/it]loss_total_epoch 283.2299566715956
Training tokenizer:  63% 5071/8047 [3:50:47<3:12:55,  3.89s/it]loss_total_epoch 283.27714049071074
Training tokenizer:  63% 5072/8047 [3:50:50<3:12:39,  3.89s/it]loss_total_epoch 283.32055327296257
Training tokenizer:  63% 5073/8047 [3:50:54<3:13:07,  3.90s/it]loss_total_epoch 283.37251441925764
Training tokenizer:  63% 5074/8047 [3:50:58<3:13:19,  3.90s/it]loss_total_epoch 283.43235371634364
Training tokenizer:  63% 5075/8047 [3:51:02<3:12:58,  3.90s/it]loss_total_epoch 283.4866451136768
Training tokenizer:  63% 5076/8047 [3:51:06<3:12:09,  3.88s/it]loss_total_epoch 283.5303008519113
Training tokenizer:  63% 5077/8047 [3:51:10<3:12:09,  3.88s/it]loss_total_epoch 283.56762581318617
Training tokenizer:  63% 5078/8047 [3:51:14<3:11:53,  3.88s/it]loss_total_epoch 283.6068497672677
Training tokenizer:  63% 5079/8047 [3:51:18<3:12:14,  3.89s/it]loss_total_epoch 283.65077532827854
Training tokenizer:  63% 5080/8047 [3:51:21<3:12:17,  3.89s/it]loss_total_epoch 283.69378082081676
Training tokenizer:  63% 5081/8047 [3:51:25<3:12:38,  3.90s/it]loss_total_epoch 283.7533996514976
Training tokenizer:  63% 5082/8047 [3:51:29<3:12:18,  3.89s/it]loss_total_epoch 283.7991812340915
Training tokenizer:  63% 5083/8047 [3:51:33<3:12:27,  3.90s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-q84z8aeo'
loss_total_epoch 283.84848775342107
Training tokenizer:  63% 5084/8047 [3:51:37<3:12:06,  3.89s/it]loss_total_epoch 283.8877478428185
Training tokenizer:  63% 5085/8047 [3:51:41<3:15:30,  3.96s/it]loss_total_epoch 283.9313658103347
Training tokenizer:  63% 5086/8047 [3:51:45<3:13:36,  3.92s/it]loss_total_epoch 283.97761149704456
Training tokenizer:  63% 5087/8047 [3:51:49<3:13:27,  3.92s/it]loss_total_epoch 284.02357253432274
Training tokenizer:  63% 5088/8047 [3:51:53<3:13:06,  3.92s/it]loss_total_epoch 284.0605925694108
Training tokenizer:  63% 5089/8047 [3:51:57<3:13:12,  3.92s/it]loss_total_epoch 284.1015849933028
Training tokenizer:  63% 5090/8047 [3:52:01<3:12:30,  3.91s/it]loss_total_epoch 284.14033269509673
Training tokenizer:  63% 5091/8047 [3:52:05<3:11:33,  3.89s/it]loss_total_epoch 284.18071134388447
Training tokenizer:  63% 5092/8047 [3:52:08<3:11:36,  3.89s/it]loss_total_epoch 284.218534052372
Training tokenizer:  63% 5093/8047 [3:52:12<3:11:15,  3.88s/it]loss_total_epoch 284.26840119063854
Training tokenizer:  63% 5094/8047 [3:52:16<3:11:38,  3.89s/it]loss_total_epoch 284.31134947389364
Training tokenizer:  63% 5095/8047 [3:52:20<3:12:25,  3.91s/it]loss_total_epoch 284.3515173345804
Training tokenizer:  63% 5096/8047 [3:52:24<3:11:05,  3.89s/it]loss_total_epoch 284.39619790017605
Training tokenizer:  63% 5097/8047 [3:52:28<3:10:57,  3.88s/it]loss_total_epoch 284.4520868472755
Training tokenizer:  63% 5098/8047 [3:52:32<3:10:44,  3.88s/it]loss_total_epoch 284.50134900584817
Training tokenizer:  63% 5099/8047 [3:52:36<3:10:34,  3.88s/it]loss_total_epoch 284.5465117357671
Training tokenizer:  63% 5100/8047 [3:52:39<3:10:05,  3.87s/it]loss_total_epoch 284.58994844928384
Training tokenizer:  63% 5101/8047 [3:52:43<3:09:27,  3.86s/it]loss_total_epoch 284.64037431031466
Training tokenizer:  63% 5102/8047 [3:52:47<3:11:02,  3.89s/it]loss_total_epoch 284.6810337640345
Training tokenizer:  63% 5103/8047 [3:52:51<3:10:09,  3.88s/it]loss_total_epoch 284.72521756216884
Training tokenizer:  63% 5104/8047 [3:52:55<3:09:45,  3.87s/it]loss_total_epoch 284.771012917161
Training tokenizer:  63% 5105/8047 [3:52:59<3:10:01,  3.88s/it]loss_total_epoch 284.82233476638794
Training tokenizer:  63% 5106/8047 [3:53:03<3:09:58,  3.88s/it]loss_total_epoch 284.87181912362576
Training tokenizer:  63% 5107/8047 [3:53:07<3:10:18,  3.88s/it]loss_total_epoch 284.9132530093193
Training tokenizer:  63% 5108/8047 [3:53:11<3:10:32,  3.89s/it]loss_total_epoch 284.9683633260429
Training tokenizer:  63% 5109/8047 [3:53:14<3:11:27,  3.91s/it]loss_total_epoch 285.01418648287654
Training tokenizer:  64% 5110/8047 [3:53:18<3:11:28,  3.91s/it]loss_total_epoch 285.06591134518385
Training tokenizer:  64% 5111/8047 [3:53:22<3:10:59,  3.90s/it]loss_total_epoch 285.11832089722157
Training tokenizer:  64% 5112/8047 [3:53:26<3:11:07,  3.91s/it]loss_total_epoch 285.166178509593
Training tokenizer:  64% 5113/8047 [3:53:30<3:10:15,  3.89s/it]loss_total_epoch 285.2053467705846
Training tokenizer:  64% 5114/8047 [3:53:34<3:09:46,  3.88s/it]loss_total_epoch 285.25672621279955
Training tokenizer:  64% 5115/8047 [3:53:38<3:09:52,  3.89s/it]loss_total_epoch 285.3060061894357
Training tokenizer:  64% 5116/8047 [3:53:42<3:10:05,  3.89s/it]loss_total_epoch 285.3569564707577
Training tokenizer:  64% 5117/8047 [3:53:46<3:10:14,  3.90s/it]loss_total_epoch 285.4024136401713
Training tokenizer:  64% 5118/8047 [3:53:49<3:10:06,  3.89s/it]loss_total_epoch 285.44967095181346
Training tokenizer:  64% 5119/8047 [3:53:53<3:09:43,  3.89s/it]loss_total_epoch 285.4925941899419
Training tokenizer:  64% 5120/8047 [3:53:57<3:10:28,  3.90s/it]loss_total_epoch 285.5429434925318
Training tokenizer:  64% 5121/8047 [3:54:01<3:10:56,  3.92s/it]loss_total_epoch 285.5844088345766
Training tokenizer:  64% 5122/8047 [3:54:05<3:11:36,  3.93s/it]loss_total_epoch 285.6279257014394
Training tokenizer:  64% 5123/8047 [3:54:09<3:10:43,  3.91s/it]loss_total_epoch 285.6706493124366
Training tokenizer:  64% 5124/8047 [3:54:13<3:10:31,  3.91s/it]loss_total_epoch 285.71481592953205
Training tokenizer:  64% 5125/8047 [3:54:17<3:10:05,  3.90s/it]loss_total_epoch 285.7554844804108
Training tokenizer:  64% 5126/8047 [3:54:21<3:09:28,  3.89s/it]loss_total_epoch 285.80506571009755
Training tokenizer:  64% 5127/8047 [3:54:25<3:09:24,  3.89s/it]loss_total_epoch 285.8575037419796
Training tokenizer:  64% 5128/8047 [3:54:29<3:09:58,  3.90s/it]loss_total_epoch 285.9053884334862
Training tokenizer:  64% 5129/8047 [3:54:32<3:10:01,  3.91s/it]loss_total_epoch 285.95031797140837
Training tokenizer:  64% 5130/8047 [3:54:36<3:10:28,  3.92s/it]loss_total_epoch 285.9948726147413
Training tokenizer:  64% 5131/8047 [3:54:40<3:10:27,  3.92s/it]loss_total_epoch 286.04531379044056
Training tokenizer:  64% 5132/8047 [3:54:44<3:09:43,  3.91s/it]loss_total_epoch 286.08621720969677
Training tokenizer:  64% 5133/8047 [3:54:48<3:09:54,  3.91s/it]loss_total_epoch 286.1303420327604
Training tokenizer:  64% 5134/8047 [3:54:52<3:09:29,  3.90s/it]loss_total_epoch 286.1702912040055
Training tokenizer:  64% 5135/8047 [3:54:56<3:09:53,  3.91s/it]loss_total_epoch 286.20840691030025
Training tokenizer:  64% 5136/8047 [3:55:00<3:10:04,  3.92s/it]loss_total_epoch 286.2568046115339
Training tokenizer:  64% 5137/8047 [3:55:04<3:10:51,  3.94s/it]loss_total_epoch 286.3056634403765
Training tokenizer:  64% 5138/8047 [3:55:08<3:10:45,  3.93s/it]loss_total_epoch 286.34400161355734
Training tokenizer:  64% 5139/8047 [3:55:12<3:10:16,  3.93s/it]loss_total_epoch 286.3793585412204
Training tokenizer:  64% 5140/8047 [3:55:16<3:09:07,  3.90s/it]loss_total_epoch 286.42783573269844
Training tokenizer:  64% 5141/8047 [3:55:19<3:09:00,  3.90s/it]loss_total_epoch 286.4713355153799
Training tokenizer:  64% 5142/8047 [3:55:23<3:08:31,  3.89s/it]loss_total_epoch 286.5221713259816
Training tokenizer:  64% 5143/8047 [3:55:27<3:09:13,  3.91s/it]loss_total_epoch 286.5666639171541
Training tokenizer:  64% 5144/8047 [3:55:31<3:09:29,  3.92s/it]loss_total_epoch 286.61061100661755
Training tokenizer:  64% 5145/8047 [3:55:35<3:09:02,  3.91s/it]loss_total_epoch 286.64592479914427
Training tokenizer:  64% 5146/8047 [3:55:39<3:09:22,  3.92s/it]loss_total_epoch 286.69953951239586
Training tokenizer:  64% 5147/8047 [3:55:43<3:09:00,  3.91s/it]loss_total_epoch 286.7499566413462
Training tokenizer:  64% 5148/8047 [3:55:47<3:09:10,  3.92s/it]loss_total_epoch 286.8047209046781
Training tokenizer:  64% 5149/8047 [3:55:51<3:09:45,  3.93s/it]loss_total_epoch 286.85257879272103
Training tokenizer:  64% 5150/8047 [3:55:55<3:10:14,  3.94s/it]loss_total_epoch 286.89955637604
Training tokenizer:  64% 5151/8047 [3:55:59<3:09:42,  3.93s/it]loss_total_epoch 286.9424476996064
Training tokenizer:  64% 5152/8047 [3:56:03<3:09:06,  3.92s/it]loss_total_epoch 286.9963393583894
Training tokenizer:  64% 5153/8047 [3:56:06<3:07:35,  3.89s/it]loss_total_epoch 287.0352888852358
Training tokenizer:  64% 5154/8047 [3:56:10<3:08:18,  3.91s/it]loss_total_epoch 287.09081276506186
Training tokenizer:  64% 5155/8047 [3:56:14<3:08:35,  3.91s/it]loss_total_epoch 287.1274596937001
Training tokenizer:  64% 5156/8047 [3:56:18<3:08:11,  3.91s/it]loss_total_epoch 287.17224663868546
Training tokenizer:  64% 5157/8047 [3:56:22<3:07:59,  3.90s/it]loss_total_epoch 287.2271563336253
Training tokenizer:  64% 5158/8047 [3:56:26<3:08:40,  3.92s/it]loss_total_epoch 287.27331820875406
Training tokenizer:  64% 5159/8047 [3:56:30<3:08:40,  3.92s/it]loss_total_epoch 287.31980696320534
Training tokenizer:  64% 5160/8047 [3:56:34<3:09:01,  3.93s/it]loss_total_epoch 287.36895786598325
Training tokenizer:  64% 5161/8047 [3:56:38<3:09:04,  3.93s/it]loss_total_epoch 287.4122002236545
Training tokenizer:  64% 5162/8047 [3:56:42<3:09:52,  3.95s/it]loss_total_epoch 287.4549333937466
Training tokenizer:  64% 5163/8047 [3:56:46<3:09:21,  3.94s/it]loss_total_epoch 287.4977458305657
Training tokenizer:  64% 5164/8047 [3:56:50<3:09:54,  3.95s/it]loss_total_epoch 287.53750378638506
Training tokenizer:  64% 5165/8047 [3:56:54<3:09:23,  3.94s/it]loss_total_epoch 287.5813174024224
Training tokenizer:  64% 5166/8047 [3:56:58<3:09:23,  3.94s/it]loss_total_epoch 287.6340537108481
Training tokenizer:  64% 5167/8047 [3:57:02<3:09:18,  3.94s/it]loss_total_epoch 287.676010992378
Training tokenizer:  64% 5168/8047 [3:57:05<3:08:59,  3.94s/it]loss_total_epoch 287.7253911308944
Training tokenizer:  64% 5169/8047 [3:57:09<3:08:44,  3.93s/it]loss_total_epoch 287.76475608348846
Training tokenizer:  64% 5170/8047 [3:57:13<3:08:39,  3.93s/it]loss_total_epoch 287.8118038661778
Training tokenizer:  64% 5171/8047 [3:57:17<3:09:07,  3.95s/it]loss_total_epoch 287.85640393942595
Training tokenizer:  64% 5172/8047 [3:57:21<3:08:55,  3.94s/it]loss_total_epoch 287.90427261963487
Training tokenizer:  64% 5173/8047 [3:57:25<3:09:03,  3.95s/it]loss_total_epoch 287.9495598860085
Training tokenizer:  64% 5174/8047 [3:57:29<3:08:27,  3.94s/it]loss_total_epoch 287.986865144223
Training tokenizer:  64% 5175/8047 [3:57:33<3:08:38,  3.94s/it]loss_total_epoch 288.02566554397345
Training tokenizer:  64% 5176/8047 [3:57:37<3:08:50,  3.95s/it]loss_total_epoch 288.06734416633844
Training tokenizer:  64% 5177/8047 [3:57:41<3:07:47,  3.93s/it]loss_total_epoch 288.11398976668715
Training tokenizer:  64% 5178/8047 [3:57:45<3:07:52,  3.93s/it]loss_total_epoch 288.1661793664098
Training tokenizer:  64% 5179/8047 [3:57:49<3:07:45,  3.93s/it]loss_total_epoch 288.20067085698247
Training tokenizer:  64% 5180/8047 [3:57:53<3:08:05,  3.94s/it]loss_total_epoch 288.2412236966193
Training tokenizer:  64% 5181/8047 [3:57:57<3:07:00,  3.92s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-irofvdct'
loss_total_epoch 288.2863429710269
Training tokenizer:  64% 5182/8047 [3:58:00<3:06:44,  3.91s/it]loss_total_epoch 288.32355999946594
Training tokenizer:  64% 5183/8047 [3:58:04<3:07:04,  3.92s/it]loss_total_epoch 288.37658785283566
Training tokenizer:  64% 5184/8047 [3:58:08<3:06:28,  3.91s/it]loss_total_epoch 288.4168288782239
Training tokenizer:  64% 5185/8047 [3:58:12<3:06:51,  3.92s/it]loss_total_epoch 288.4593988880515
Training tokenizer:  64% 5186/8047 [3:58:16<3:06:43,  3.92s/it]loss_total_epoch 288.5051889717579
Training tokenizer:  64% 5187/8047 [3:58:20<3:06:16,  3.91s/it]loss_total_epoch 288.548319876194
Training tokenizer:  64% 5188/8047 [3:58:24<3:06:11,  3.91s/it]loss_total_epoch 288.5940451696515
Training tokenizer:  64% 5189/8047 [3:58:28<3:06:14,  3.91s/it]loss_total_epoch 288.642715562135
Training tokenizer:  64% 5190/8047 [3:58:32<3:06:53,  3.93s/it]loss_total_epoch 288.69286769255996
Training tokenizer:  65% 5191/8047 [3:58:36<3:06:30,  3.92s/it]loss_total_epoch 288.7461078427732
Training tokenizer:  65% 5192/8047 [3:58:40<3:06:09,  3.91s/it]loss_total_epoch 288.78793128207326
Training tokenizer:  65% 5193/8047 [3:58:44<3:06:19,  3.92s/it]loss_total_epoch 288.8248929195106
Training tokenizer:  65% 5194/8047 [3:58:47<3:06:37,  3.92s/it]loss_total_epoch 288.8755563721061
Training tokenizer:  65% 5195/8047 [3:58:51<3:07:03,  3.94s/it]loss_total_epoch 288.909545481205
Training tokenizer:  65% 5196/8047 [3:58:55<3:06:56,  3.93s/it]loss_total_epoch 288.9507310986519
Training tokenizer:  65% 5197/8047 [3:58:59<3:06:20,  3.92s/it]loss_total_epoch 288.99023509025574
Training tokenizer:  65% 5198/8047 [3:59:03<3:07:16,  3.94s/it]loss_total_epoch 289.0463067740202
Training tokenizer:  65% 5199/8047 [3:59:07<3:06:59,  3.94s/it]loss_total_epoch 289.0836759097874
Training tokenizer:  65% 5200/8047 [3:59:11<3:06:47,  3.94s/it]loss_total_epoch 289.1280091702938
Training tokenizer:  65% 5201/8047 [3:59:15<3:07:05,  3.94s/it]loss_total_epoch 289.1694552786648
Training tokenizer:  65% 5202/8047 [3:59:19<3:06:46,  3.94s/it]loss_total_epoch 289.2105009369552
Training tokenizer:  65% 5203/8047 [3:59:23<3:06:09,  3.93s/it]loss_total_epoch 289.25771410018206
Training tokenizer:  65% 5204/8047 [3:59:27<3:06:37,  3.94s/it]loss_total_epoch 289.3004577793181
Training tokenizer:  65% 5205/8047 [3:59:31<3:06:59,  3.95s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-4vkz42_v'
loss_total_epoch 289.3447306640446
Training tokenizer:  65% 5206/8047 [3:59:35<3:06:43,  3.94s/it]loss_total_epoch 289.3841773979366
Training tokenizer:  65% 5207/8047 [3:59:39<3:07:02,  3.95s/it]loss_total_epoch 289.42724300548434
Training tokenizer:  65% 5208/8047 [3:59:43<3:07:23,  3.96s/it]loss_total_epoch 289.46354603394866
Training tokenizer:  65% 5209/8047 [3:59:47<3:06:43,  3.95s/it]loss_total_epoch 289.511094853282
Training tokenizer:  65% 5210/8047 [3:59:51<3:07:08,  3.96s/it]loss_total_epoch 289.5529302954674
Training tokenizer:  65% 5211/8047 [3:59:55<3:07:13,  3.96s/it]loss_total_epoch 289.60083646327257
Training tokenizer:  65% 5212/8047 [3:59:59<3:06:36,  3.95s/it]loss_total_epoch 289.6510895192623
Training tokenizer:  65% 5213/8047 [4:00:02<3:06:10,  3.94s/it]loss_total_epoch 289.69286646693945
Training tokenizer:  65% 5214/8047 [4:00:06<3:06:38,  3.95s/it]loss_total_epoch 289.7312674075365
Training tokenizer:  65% 5215/8047 [4:00:10<3:07:33,  3.97s/it]loss_total_epoch 289.7760927155614
Training tokenizer:  65% 5216/8047 [4:00:14<3:07:15,  3.97s/it]loss_total_epoch 289.81957963109016
Training tokenizer:  65% 5217/8047 [4:00:18<3:06:41,  3.96s/it]loss_total_epoch 289.8669772185385
Training tokenizer:  65% 5218/8047 [4:00:22<3:06:20,  3.95s/it]loss_total_epoch 289.91285609826446
Training tokenizer:  65% 5219/8047 [4:00:26<3:07:18,  3.97s/it]loss_total_epoch 289.94925459846854
Training tokenizer:  65% 5220/8047 [4:00:30<3:07:26,  3.98s/it]loss_total_epoch 289.9890369065106
Training tokenizer:  65% 5221/8047 [4:00:34<3:07:13,  3.97s/it]loss_total_epoch 290.0390164144337
Training tokenizer:  65% 5222/8047 [4:00:38<3:08:04,  3.99s/it]loss_total_epoch 290.0838065184653
Training tokenizer:  65% 5223/8047 [4:00:42<3:06:50,  3.97s/it]loss_total_epoch 290.13380233570933
Training tokenizer:  65% 5224/8047 [4:00:46<3:07:17,  3.98s/it]loss_total_epoch 290.17879021540284
Training tokenizer:  65% 5225/8047 [4:00:50<3:05:55,  3.95s/it]loss_total_epoch 290.2274618856609
Training tokenizer:  65% 5226/8047 [4:00:54<3:06:15,  3.96s/it]loss_total_epoch 290.27486161515117
Training tokenizer:  65% 5227/8047 [4:00:58<3:05:53,  3.96s/it]loss_total_epoch 290.3173998147249
Training tokenizer:  65% 5228/8047 [4:01:02<3:06:16,  3.96s/it]loss_total_epoch 290.37250508740544
Training tokenizer:  65% 5229/8047 [4:01:06<3:06:36,  3.97s/it]loss_total_epoch 290.4181483052671
Training tokenizer:  65% 5230/8047 [4:01:10<3:10:02,  4.05s/it]loss_total_epoch 290.4633431471884
Training tokenizer:  65% 5231/8047 [4:01:14<3:09:53,  4.05s/it]loss_total_epoch 290.5176225565374
Training tokenizer:  65% 5232/8047 [4:01:18<3:09:10,  4.03s/it]loss_total_epoch 290.55410412326455
Training tokenizer:  65% 5233/8047 [4:01:22<3:08:16,  4.01s/it]loss_total_epoch 290.5929095931351
Training tokenizer:  65% 5234/8047 [4:01:26<3:06:55,  3.99s/it]loss_total_epoch 290.6461606733501
Training tokenizer:  65% 5235/8047 [4:01:30<3:06:15,  3.97s/it]loss_total_epoch 290.68126629292965
Training tokenizer:  65% 5236/8047 [4:01:34<3:06:28,  3.98s/it]loss_total_epoch 290.7257372960448
Training tokenizer:  65% 5237/8047 [4:01:38<3:06:18,  3.98s/it]loss_total_epoch 290.76082717999816
Training tokenizer:  65% 5238/8047 [4:01:42<3:06:01,  3.97s/it]loss_total_epoch 290.8031832911074
Training tokenizer:  65% 5239/8047 [4:01:46<3:05:39,  3.97s/it]loss_total_epoch 290.84519501775503
Training tokenizer:  65% 5240/8047 [4:01:50<3:05:37,  3.97s/it]loss_total_epoch 290.8916726782918
Training tokenizer:  65% 5241/8047 [4:01:54<3:04:33,  3.95s/it]loss_total_epoch 290.94760068133473
Training tokenizer:  65% 5242/8047 [4:01:58<3:03:25,  3.92s/it]loss_total_epoch 290.9960727132857
Training tokenizer:  65% 5243/8047 [4:02:02<3:03:56,  3.94s/it]loss_total_epoch 291.04994059354067
Training tokenizer:  65% 5244/8047 [4:02:06<3:04:39,  3.95s/it]loss_total_epoch 291.08794032782316
Training tokenizer:  65% 5245/8047 [4:02:10<3:04:59,  3.96s/it]loss_total_epoch 291.13101981952786
Training tokenizer:  65% 5246/8047 [4:02:14<3:05:34,  3.98s/it]loss_total_epoch 291.176577154547
Training tokenizer:  65% 5247/8047 [4:02:18<3:04:56,  3.96s/it]loss_total_epoch 291.22429921478033
Training tokenizer:  65% 5248/8047 [4:02:22<3:04:26,  3.95s/it]loss_total_epoch 291.26240756735206
Training tokenizer:  65% 5249/8047 [4:02:25<3:04:21,  3.95s/it]loss_total_epoch 291.292008055374
Training tokenizer:  65% 5250/8047 [4:02:29<3:05:00,  3.97s/it]loss_total_epoch 291.34122488088906
Training tokenizer:  65% 5251/8047 [4:02:33<3:04:54,  3.97s/it]loss_total_epoch 291.39511057920754
Training tokenizer:  65% 5252/8047 [4:02:37<3:04:57,  3.97s/it]loss_total_epoch 291.44018843211234
Training tokenizer:  65% 5253/8047 [4:02:41<3:04:21,  3.96s/it]loss_total_epoch 291.4677233323455
Training tokenizer:  65% 5254/8047 [4:02:45<3:04:18,  3.96s/it]loss_total_epoch 291.5045217163861
Training tokenizer:  65% 5255/8047 [4:02:49<3:04:24,  3.96s/it]loss_total_epoch 291.56119753047824
Training tokenizer:  65% 5256/8047 [4:02:53<3:04:16,  3.96s/it]loss_total_epoch 291.60404225066304
Training tokenizer:  65% 5257/8047 [4:02:57<3:04:24,  3.97s/it]loss_total_epoch 291.6529305279255
Training tokenizer:  65% 5258/8047 [4:03:01<3:04:24,  3.97s/it]loss_total_epoch 291.68874288350344
Training tokenizer:  65% 5259/8047 [4:03:05<3:04:14,  3.97s/it]loss_total_epoch 291.73171324282885
Training tokenizer:  65% 5260/8047 [4:03:09<3:03:34,  3.95s/it]loss_total_epoch 291.7919692210853
Training tokenizer:  65% 5261/8047 [4:03:13<3:03:19,  3.95s/it]loss_total_epoch 291.8257482238114
Training tokenizer:  65% 5262/8047 [4:03:17<3:03:14,  3.95s/it]loss_total_epoch 291.8585835546255
Training tokenizer:  65% 5263/8047 [4:03:21<3:03:29,  3.95s/it]loss_total_epoch 291.90487149730325
Training tokenizer:  65% 5264/8047 [4:03:25<3:03:59,  3.97s/it]loss_total_epoch 291.94561146199703
Training tokenizer:  65% 5265/8047 [4:03:29<3:04:05,  3.97s/it]loss_total_epoch 291.98875464498997
Training tokenizer:  65% 5266/8047 [4:03:33<3:04:40,  3.98s/it]loss_total_epoch 292.0323063470423
Training tokenizer:  65% 5267/8047 [4:03:37<3:04:50,  3.99s/it]loss_total_epoch 292.0827693641186
Training tokenizer:  65% 5268/8047 [4:03:41<3:05:08,  4.00s/it]loss_total_epoch 292.1271239519119
Training tokenizer:  65% 5269/8047 [4:03:45<3:04:38,  3.99s/it]loss_total_epoch 292.17075914517045
Training tokenizer:  65% 5270/8047 [4:03:49<3:04:27,  3.99s/it]loss_total_epoch 292.21860690042377
Training tokenizer:  66% 5271/8047 [4:03:53<3:04:32,  3.99s/it]loss_total_epoch 292.26478895172477
Training tokenizer:  66% 5272/8047 [4:03:57<3:04:48,  4.00s/it]loss_total_epoch 292.30124439671636
Training tokenizer:  66% 5273/8047 [4:04:01<3:03:50,  3.98s/it]loss_total_epoch 292.3464855365455
Training tokenizer:  66% 5274/8047 [4:04:05<3:03:42,  3.97s/it]loss_total_epoch 292.38897754251957
Training tokenizer:  66% 5275/8047 [4:04:09<3:03:24,  3.97s/it]loss_total_epoch 292.4283679872751
Training tokenizer:  66% 5276/8047 [4:04:13<3:03:13,  3.97s/it]loss_total_epoch 292.47031385824084
Training tokenizer:  66% 5277/8047 [4:04:17<3:03:08,  3.97s/it]loss_total_epoch 292.5118728727102
Training tokenizer:  66% 5278/8047 [4:04:21<3:02:53,  3.96s/it]loss_total_epoch 292.5565053783357
Training tokenizer:  66% 5279/8047 [4:04:25<3:03:18,  3.97s/it]loss_total_epoch 292.59880520775914
Training tokenizer:  66% 5280/8047 [4:04:29<3:02:53,  3.97s/it]loss_total_epoch 292.64935963228345
Training tokenizer:  66% 5281/8047 [4:04:33<3:02:59,  3.97s/it]loss_total_epoch 292.6994974426925
Training tokenizer:  66% 5282/8047 [4:04:37<3:03:02,  3.97s/it]loss_total_epoch 292.7355104163289
Training tokenizer:  66% 5283/8047 [4:04:41<3:03:08,  3.98s/it]loss_total_epoch 292.7828053943813
Training tokenizer:  66% 5284/8047 [4:04:45<3:03:21,  3.98s/it]loss_total_epoch 292.82657601311803
Training tokenizer:  66% 5285/8047 [4:04:48<3:03:07,  3.98s/it]loss_total_epoch 292.87226239964366
Training tokenizer:  66% 5286/8047 [4:04:53<3:03:56,  4.00s/it]loss_total_epoch 292.9251127690077
Training tokenizer:  66% 5287/8047 [4:04:57<3:03:37,  3.99s/it]loss_total_epoch 292.9768708907068
Training tokenizer:  66% 5288/8047 [4:05:01<3:05:43,  4.04s/it]loss_total_epoch 293.0191709436476
Training tokenizer:  66% 5289/8047 [4:05:05<3:04:45,  4.02s/it]loss_total_epoch 293.0580976828933
Training tokenizer:  66% 5290/8047 [4:05:09<3:04:49,  4.02s/it]loss_total_epoch 293.09986179694533
Training tokenizer:  66% 5291/8047 [4:05:13<3:03:53,  4.00s/it]loss_total_epoch 293.14875607565045
Training tokenizer:  66% 5292/8047 [4:05:17<3:03:24,  3.99s/it]loss_total_epoch 293.18556324392557
Training tokenizer:  66% 5293/8047 [4:05:21<3:04:40,  4.02s/it]loss_total_epoch 293.22035163640976
Training tokenizer:  66% 5294/8047 [4:05:25<3:04:00,  4.01s/it]loss_total_epoch 293.2552720531821
Training tokenizer:  66% 5295/8047 [4:05:29<3:05:18,  4.04s/it]loss_total_epoch 293.2876953072846
Training tokenizer:  66% 5296/8047 [4:05:33<3:04:17,  4.02s/it]loss_total_epoch 293.32236133143306
Training tokenizer:  66% 5297/8047 [4:05:37<3:04:05,  4.02s/it]loss_total_epoch 293.37079217657447
Training tokenizer:  66% 5298/8047 [4:05:41<3:03:28,  4.00s/it]loss_total_epoch 293.4219564013183
Training tokenizer:  66% 5299/8047 [4:05:45<3:02:27,  3.98s/it]loss_total_epoch 293.47179076448083
Training tokenizer:  66% 5300/8047 [4:05:49<3:02:02,  3.98s/it]loss_total_epoch 293.51234524697065
Training tokenizer:  66% 5301/8047 [4:05:53<3:01:53,  3.97s/it]loss_total_epoch 293.5495658516884
Training tokenizer:  66% 5302/8047 [4:05:57<3:01:06,  3.96s/it]loss_total_epoch 293.60830381140113
Training tokenizer:  66% 5303/8047 [4:06:00<3:01:08,  3.96s/it]loss_total_epoch 293.642225574702
Training tokenizer:  66% 5304/8047 [4:06:04<3:01:17,  3.97s/it]loss_total_epoch 293.6801847331226
Training tokenizer:  66% 5305/8047 [4:06:08<3:01:55,  3.98s/it]loss_total_epoch 293.7300192601979
Training tokenizer:  66% 5306/8047 [4:06:12<3:01:56,  3.98s/it]loss_total_epoch 293.78277941420674
Training tokenizer:  66% 5307/8047 [4:06:16<3:01:49,  3.98s/it]loss_total_epoch 293.8189489804208
Training tokenizer:  66% 5308/8047 [4:06:20<3:02:11,  3.99s/it]loss_total_epoch 293.8661633506417
Training tokenizer:  66% 5309/8047 [4:06:24<3:01:38,  3.98s/it]loss_total_epoch 293.9148339033127
Training tokenizer:  66% 5310/8047 [4:06:28<3:01:58,  3.99s/it]loss_total_epoch 293.9538470879197
Training tokenizer:  66% 5311/8047 [4:06:32<3:01:18,  3.98s/it]loss_total_epoch 293.99718480184674
Training tokenizer:  66% 5312/8047 [4:06:36<3:01:52,  3.99s/it]loss_total_epoch 294.04486486688256
Training tokenizer:  66% 5313/8047 [4:06:40<3:01:08,  3.98s/it]loss_total_epoch 294.09064765647054
Training tokenizer:  66% 5314/8047 [4:06:44<3:00:51,  3.97s/it]loss_total_epoch 294.1417517438531
Training tokenizer:  66% 5315/8047 [4:06:48<3:01:28,  3.99s/it]loss_total_epoch 294.17609495669603
Training tokenizer:  66% 5316/8047 [4:06:52<3:01:25,  3.99s/it]loss_total_epoch 294.22313959151506
Training tokenizer:  66% 5317/8047 [4:06:56<3:00:48,  3.97s/it]loss_total_epoch 294.26779325306416
Training tokenizer:  66% 5318/8047 [4:07:00<3:01:29,  3.99s/it]loss_total_epoch 294.3096573986113
Training tokenizer:  66% 5319/8047 [4:07:04<3:01:43,  4.00s/it]loss_total_epoch 294.3582678884268
Training tokenizer:  66% 5320/8047 [4:07:08<3:01:22,  3.99s/it]loss_total_epoch 294.4087145216763
Training tokenizer:  66% 5321/8047 [4:07:12<3:01:07,  3.99s/it]loss_total_epoch 294.4626583196223
Training tokenizer:  66% 5322/8047 [4:07:16<3:00:48,  3.98s/it]loss_total_epoch 294.5098655410111
Training tokenizer:  66% 5323/8047 [4:07:20<3:01:32,  4.00s/it]loss_total_epoch 294.54674354195595
Training tokenizer:  66% 5324/8047 [4:07:24<3:01:17,  3.99s/it]loss_total_epoch 294.5940022394061
Training tokenizer:  66% 5325/8047 [4:07:28<3:01:18,  4.00s/it]loss_total_epoch 294.6393543407321
Training tokenizer:  66% 5326/8047 [4:07:32<3:01:42,  4.01s/it]loss_total_epoch 294.69246505200863
Training tokenizer:  66% 5327/8047 [4:07:36<3:02:02,  4.02s/it]loss_total_epoch 294.7362480945885
Training tokenizer:  66% 5328/8047 [4:07:40<3:01:52,  4.01s/it]loss_total_epoch 294.7823969088495
Training tokenizer:  66% 5329/8047 [4:07:44<3:01:30,  4.01s/it]loss_total_epoch 294.8245074376464
Training tokenizer:  66% 5330/8047 [4:07:48<3:01:33,  4.01s/it]loss_total_epoch 294.8624025285244
Training tokenizer:  66% 5331/8047 [4:07:52<3:01:19,  4.01s/it]loss_total_epoch 294.9100866690278
Training tokenizer:  66% 5332/8047 [4:07:56<3:00:50,  4.00s/it]loss_total_epoch 294.9455200061202
Training tokenizer:  66% 5333/8047 [4:08:00<3:00:31,  3.99s/it]loss_total_epoch 294.98797495663166
Training tokenizer:  66% 5334/8047 [4:08:04<3:00:29,  3.99s/it]loss_total_epoch 295.0313181094825
Training tokenizer:  66% 5335/8047 [4:08:08<3:00:38,  4.00s/it]loss_total_epoch 295.06828498095274
Training tokenizer:  66% 5336/8047 [4:08:12<3:00:35,  4.00s/it]loss_total_epoch 295.1069826632738
Training tokenizer:  66% 5337/8047 [4:08:16<3:00:17,  3.99s/it]loss_total_epoch 295.15159252658486
Training tokenizer:  66% 5338/8047 [4:08:20<2:59:31,  3.98s/it]loss_total_epoch 295.2012505196035
Training tokenizer:  66% 5339/8047 [4:08:24<3:00:14,  3.99s/it]loss_total_epoch 295.2389292009175
Training tokenizer:  66% 5340/8047 [4:08:28<2:59:40,  3.98s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-2tro9c4e'
loss_total_epoch 295.2942541502416
Training tokenizer:  66% 5341/8047 [4:08:32<3:00:05,  3.99s/it]loss_total_epoch 295.33871921896935
Training tokenizer:  66% 5342/8047 [4:08:36<3:00:14,  4.00s/it]loss_total_epoch 295.37485345825553
Training tokenizer:  66% 5343/8047 [4:08:40<2:59:41,  3.99s/it]loss_total_epoch 295.4152803570032
Training tokenizer:  66% 5344/8047 [4:08:44<2:59:33,  3.99s/it]loss_total_epoch 295.45323257148266
Training tokenizer:  66% 5345/8047 [4:08:48<2:59:31,  3.99s/it]loss_total_epoch 295.4895869791508
Training tokenizer:  66% 5346/8047 [4:08:52<2:59:25,  3.99s/it]loss_total_epoch 295.53130431473255
Training tokenizer:  66% 5347/8047 [4:08:56<3:00:10,  4.00s/it]loss_total_epoch 295.58179201185703
Training tokenizer:  66% 5348/8047 [4:09:00<3:00:10,  4.01s/it]loss_total_epoch 295.63254791870713
Training tokenizer:  66% 5349/8047 [4:09:04<2:58:49,  3.98s/it]loss_total_epoch 295.6735997311771
Training tokenizer:  66% 5350/8047 [4:09:08<2:59:18,  3.99s/it]loss_total_epoch 295.7238748036325
Training tokenizer:  66% 5351/8047 [4:09:12<2:59:44,  4.00s/it]loss_total_epoch 295.76923083886504
Training tokenizer:  67% 5352/8047 [4:09:16<3:00:20,  4.01s/it]loss_total_epoch 295.82221280410886
Training tokenizer:  67% 5353/8047 [4:09:20<3:00:08,  4.01s/it]loss_total_epoch 295.8648844882846
Training tokenizer:  67% 5354/8047 [4:09:24<3:00:14,  4.02s/it]loss_total_epoch 295.9062452353537
Training tokenizer:  67% 5355/8047 [4:09:28<3:00:34,  4.02s/it]loss_total_epoch 295.95164129137993
Training tokenizer:  67% 5356/8047 [4:09:32<3:00:47,  4.03s/it]loss_total_epoch 295.98631197586656
Training tokenizer:  67% 5357/8047 [4:09:36<3:00:06,  4.02s/it]loss_total_epoch 296.0365219414234
Training tokenizer:  67% 5358/8047 [4:09:40<3:00:31,  4.03s/it]loss_total_epoch 296.0861017033458
Training tokenizer:  67% 5359/8047 [4:09:44<3:00:16,  4.02s/it]loss_total_epoch 296.12651639431715
Training tokenizer:  67% 5360/8047 [4:09:48<2:59:59,  4.02s/it]loss_total_epoch 296.167428098619
Training tokenizer:  67% 5361/8047 [4:09:52<3:00:43,  4.04s/it]loss_total_epoch 296.2138800993562
Training tokenizer:  67% 5362/8047 [4:09:57<3:01:11,  4.05s/it]loss_total_epoch 296.26297449320555
Training tokenizer:  67% 5363/8047 [4:10:01<3:01:27,  4.06s/it]loss_total_epoch 296.30715239048004
Training tokenizer:  67% 5364/8047 [4:10:05<3:00:47,  4.04s/it]loss_total_epoch 296.3559512384236
Training tokenizer:  67% 5365/8047 [4:10:09<2:59:51,  4.02s/it]loss_total_epoch 296.4077804647386
Training tokenizer:  67% 5366/8047 [4:10:13<3:00:10,  4.03s/it]loss_total_epoch 296.4590560756624
Training tokenizer:  67% 5367/8047 [4:10:17<3:00:08,  4.03s/it]loss_total_epoch 296.498875759542
Training tokenizer:  67% 5368/8047 [4:10:21<3:00:10,  4.04s/it]loss_total_epoch 296.5477099120617
Training tokenizer:  67% 5369/8047 [4:10:25<2:59:54,  4.03s/it]loss_total_epoch 296.5875945389271
Training tokenizer:  67% 5370/8047 [4:10:29<3:00:00,  4.03s/it]loss_total_epoch 296.6321864910424
Training tokenizer:  67% 5371/8047 [4:10:33<2:59:35,  4.03s/it]loss_total_epoch 296.6841815970838
Training tokenizer:  67% 5372/8047 [4:10:37<2:59:23,  4.02s/it]loss_total_epoch 296.7208936959505
Training tokenizer:  67% 5373/8047 [4:10:41<2:59:59,  4.04s/it]loss_total_epoch 296.76725533232093
Training tokenizer:  67% 5374/8047 [4:10:45<2:59:53,  4.04s/it]loss_total_epoch 296.81187940761447
Training tokenizer:  67% 5375/8047 [4:10:49<2:59:34,  4.03s/it]loss_total_epoch 296.8517613746226
Training tokenizer:  67% 5376/8047 [4:10:53<2:59:47,  4.04s/it]loss_total_epoch 296.900952257216
Training tokenizer:  67% 5377/8047 [4:10:57<2:59:24,  4.03s/it]loss_total_epoch 296.9439545571804
Training tokenizer:  67% 5378/8047 [4:11:01<2:58:57,  4.02s/it]loss_total_epoch 296.98965771496296
Training tokenizer:  67% 5379/8047 [4:11:05<2:58:26,  4.01s/it]loss_total_epoch 297.0290679037571
Training tokenizer:  67% 5380/8047 [4:11:09<2:58:08,  4.01s/it]loss_total_epoch 297.0837974399328
Training tokenizer:  67% 5381/8047 [4:11:13<2:58:47,  4.02s/it]loss_total_epoch 297.129570402205
Training tokenizer:  67% 5382/8047 [4:11:17<2:58:52,  4.03s/it]loss_total_epoch 297.17005831748247
Training tokenizer:  67% 5383/8047 [4:11:21<2:58:42,  4.03s/it]loss_total_epoch 297.21741277724504
Training tokenizer:  67% 5384/8047 [4:11:25<2:58:34,  4.02s/it]loss_total_epoch 297.26889165490866
Training tokenizer:  67% 5385/8047 [4:11:29<2:57:56,  4.01s/it]loss_total_epoch 297.3078091368079
Training tokenizer:  67% 5386/8047 [4:11:33<2:57:57,  4.01s/it]loss_total_epoch 297.3593887910247
Training tokenizer:  67% 5387/8047 [4:11:37<2:58:19,  4.02s/it]loss_total_epoch 297.4116273596883
Training tokenizer:  67% 5388/8047 [4:11:41<2:57:52,  4.01s/it]loss_total_epoch 297.4472182765603
Training tokenizer:  67% 5389/8047 [4:11:45<2:58:38,  4.03s/it]loss_total_epoch 297.49819741025567
Training tokenizer:  67% 5390/8047 [4:11:49<2:58:35,  4.03s/it]loss_total_epoch 297.53320564702153
Training tokenizer:  67% 5391/8047 [4:11:53<2:57:25,  4.01s/it]loss_total_epoch 297.58570148423314
Training tokenizer:  67% 5392/8047 [4:11:57<2:58:04,  4.02s/it]loss_total_epoch 297.62895930185914
Training tokenizer:  67% 5393/8047 [4:12:01<2:59:48,  4.06s/it]loss_total_epoch 297.67612450942397
Training tokenizer:  67% 5394/8047 [4:12:05<2:59:15,  4.05s/it]loss_total_epoch 297.72335893288255
Training tokenizer:  67% 5395/8047 [4:12:09<2:58:36,  4.04s/it]loss_total_epoch 297.7729567848146
Training tokenizer:  67% 5396/8047 [4:12:13<2:58:04,  4.03s/it]loss_total_epoch 297.81987734511495
Training tokenizer:  67% 5397/8047 [4:12:18<2:58:07,  4.03s/it]loss_total_epoch 297.8707791082561
Training tokenizer:  67% 5398/8047 [4:12:22<2:57:30,  4.02s/it]loss_total_epoch 297.9148199893534
Training tokenizer:  67% 5399/8047 [4:12:26<2:57:48,  4.03s/it]loss_total_epoch 297.9570756778121
Training tokenizer:  67% 5400/8047 [4:12:30<2:57:51,  4.03s/it]loss_total_epoch 298.0001931488514
Training tokenizer:  67% 5401/8047 [4:12:34<2:56:29,  4.00s/it]loss_total_epoch 298.0429983101785
Training tokenizer:  67% 5402/8047 [4:12:38<2:57:14,  4.02s/it]loss_total_epoch 298.08188669010997
Training tokenizer:  67% 5403/8047 [4:12:42<2:56:51,  4.01s/it]loss_total_epoch 298.11764473840594
Training tokenizer:  67% 5404/8047 [4:12:46<2:56:41,  4.01s/it]loss_total_epoch 298.15908597782254
Training tokenizer:  67% 5405/8047 [4:12:50<2:57:12,  4.02s/it]loss_total_epoch 298.2027526162565
Training tokenizer:  67% 5406/8047 [4:12:54<2:57:16,  4.03s/it]loss_total_epoch 298.24859019368887
Training tokenizer:  67% 5407/8047 [4:12:58<2:57:04,  4.02s/it]loss_total_epoch 298.2830092087388
Training tokenizer:  67% 5408/8047 [4:13:02<2:57:16,  4.03s/it]loss_total_epoch 298.3193021379411
Training tokenizer:  67% 5409/8047 [4:13:06<2:57:20,  4.03s/it]loss_total_epoch 298.3595083542168
Training tokenizer:  67% 5410/8047 [4:13:10<2:57:18,  4.03s/it]loss_total_epoch 298.39692933112383
Training tokenizer:  67% 5411/8047 [4:13:14<2:57:54,  4.05s/it]loss_total_epoch 298.4390458278358
Training tokenizer:  67% 5412/8047 [4:13:18<2:58:04,  4.05s/it]loss_total_epoch 298.4796654731035
Training tokenizer:  67% 5413/8047 [4:13:22<2:57:51,  4.05s/it]loss_total_epoch 298.5264727920294
Training tokenizer:  67% 5414/8047 [4:13:26<2:57:25,  4.04s/it]loss_total_epoch 298.56393740326166
Training tokenizer:  67% 5415/8047 [4:13:30<2:57:23,  4.04s/it]loss_total_epoch 298.60998321324587
Training tokenizer:  67% 5416/8047 [4:13:34<2:57:10,  4.04s/it]loss_total_epoch 298.6521403566003
Training tokenizer:  67% 5417/8047 [4:13:38<2:57:00,  4.04s/it]loss_total_epoch 298.7017742022872
Training tokenizer:  67% 5418/8047 [4:13:42<2:57:39,  4.05s/it]loss_total_epoch 298.7496523782611
Training tokenizer:  67% 5419/8047 [4:13:46<2:57:39,  4.06s/it]loss_total_epoch 298.78656516224146
Training tokenizer:  67% 5420/8047 [4:13:50<2:57:25,  4.05s/it]loss_total_epoch 298.8301679678261
Training tokenizer:  67% 5421/8047 [4:13:54<2:56:55,  4.04s/it]loss_total_epoch 298.8672754727304
Training tokenizer:  67% 5422/8047 [4:13:58<2:57:19,  4.05s/it]loss_total_epoch 298.9118496105075
Training tokenizer:  67% 5423/8047 [4:14:02<2:56:49,  4.04s/it]loss_total_epoch 298.95881870388985
Training tokenizer:  67% 5424/8047 [4:14:07<2:57:24,  4.06s/it]loss_total_epoch 299.00701570138335
Training tokenizer:  67% 5425/8047 [4:14:11<2:57:43,  4.07s/it]loss_total_epoch 299.04582592844963
Training tokenizer:  67% 5426/8047 [4:14:15<2:57:11,  4.06s/it]loss_total_epoch 299.0934478417039
Training tokenizer:  67% 5427/8047 [4:14:19<2:56:57,  4.05s/it]loss_total_epoch 299.14265931397676
Training tokenizer:  67% 5428/8047 [4:14:23<2:56:35,  4.05s/it]loss_total_epoch 299.1983863078058
Training tokenizer:  67% 5429/8047 [4:14:27<2:56:02,  4.03s/it]loss_total_epoch 299.2401616051793
Training tokenizer:  67% 5430/8047 [4:14:31<2:55:49,  4.03s/it]loss_total_epoch 299.2819219008088
Training tokenizer:  67% 5431/8047 [4:14:35<2:56:15,  4.04s/it]loss_total_epoch 299.32323099672794
Training tokenizer:  68% 5432/8047 [4:14:39<2:55:52,  4.04s/it]loss_total_epoch 299.3766208663583
Training tokenizer:  68% 5433/8047 [4:14:43<2:55:44,  4.03s/it]loss_total_epoch 299.4249742552638
Training tokenizer:  68% 5434/8047 [4:14:47<2:56:14,  4.05s/it]loss_total_epoch 299.4790557473898
Training tokenizer:  68% 5435/8047 [4:14:51<2:56:14,  4.05s/it]loss_total_epoch 299.5174259878695
Training tokenizer:  68% 5436/8047 [4:14:55<2:56:15,  4.05s/it]loss_total_epoch 299.55637292191386
Training tokenizer:  68% 5437/8047 [4:14:59<2:56:07,  4.05s/it]loss_total_epoch 299.60758827254176
Training tokenizer:  68% 5438/8047 [4:15:03<2:55:50,  4.04s/it]loss_total_epoch 299.65631871670485
Training tokenizer:  68% 5439/8047 [4:15:07<2:55:46,  4.04s/it]loss_total_epoch 299.6932403333485
Training tokenizer:  68% 5440/8047 [4:15:11<2:55:40,  4.04s/it]loss_total_epoch 299.7328625842929
Training tokenizer:  68% 5441/8047 [4:15:15<2:55:35,  4.04s/it]loss_total_epoch 299.77172553539276
Training tokenizer:  68% 5442/8047 [4:15:19<2:55:27,  4.04s/it]loss_total_epoch 299.8218425177038
Training tokenizer:  68% 5443/8047 [4:15:23<2:56:51,  4.07s/it]loss_total_epoch 299.86896700412035
Training tokenizer:  68% 5444/8047 [4:15:28<2:56:50,  4.08s/it]loss_total_epoch 299.91790337860584
Training tokenizer:  68% 5445/8047 [4:15:32<2:56:53,  4.08s/it]loss_total_epoch 299.9544454626739
Training tokenizer:  68% 5446/8047 [4:15:36<2:56:03,  4.06s/it]loss_total_epoch 300.00338984280825
Training tokenizer:  68% 5447/8047 [4:15:40<2:55:13,  4.04s/it]loss_total_epoch 300.05602956190705
Training tokenizer:  68% 5448/8047 [4:15:44<2:55:17,  4.05s/it]loss_total_epoch 300.1004219688475
Training tokenizer:  68% 5449/8047 [4:15:48<2:55:15,  4.05s/it]loss_total_epoch 300.1417048089206
Training tokenizer:  68% 5450/8047 [4:15:52<2:55:25,  4.05s/it]loss_total_epoch 300.1947738304734
Training tokenizer:  68% 5451/8047 [4:15:56<2:55:41,  4.06s/it]loss_total_epoch 300.241481333971
Training tokenizer:  68% 5452/8047 [4:16:00<2:56:26,  4.08s/it]loss_total_epoch 300.2893915399909
Training tokenizer:  68% 5453/8047 [4:16:04<2:55:57,  4.07s/it]loss_total_epoch 300.3445826023817
Training tokenizer:  68% 5454/8047 [4:16:08<2:55:56,  4.07s/it]loss_total_epoch 300.3775888197124
Training tokenizer:  68% 5455/8047 [4:16:12<2:56:21,  4.08s/it]loss_total_epoch 300.429041672498
Training tokenizer:  68% 5456/8047 [4:16:16<2:55:57,  4.07s/it]loss_total_epoch 300.47706665098667
Training tokenizer:  68% 5457/8047 [4:16:20<2:55:15,  4.06s/it]loss_total_epoch 300.51230596750975
Training tokenizer:  68% 5458/8047 [4:16:24<2:55:09,  4.06s/it]loss_total_epoch 300.56803791970015
Training tokenizer:  68% 5459/8047 [4:16:29<2:55:34,  4.07s/it]loss_total_epoch 300.61222736537457
Training tokenizer:  68% 5460/8047 [4:16:33<2:55:10,  4.06s/it]loss_total_epoch 300.66120806336403
Training tokenizer:  68% 5461/8047 [4:16:37<2:55:10,  4.06s/it]loss_total_epoch 300.6903871521354
Training tokenizer:  68% 5462/8047 [4:16:41<2:55:05,  4.06s/it]loss_total_epoch 300.7387612387538
Training tokenizer:  68% 5463/8047 [4:16:45<2:55:04,  4.07s/it]loss_total_epoch 300.7834912389517
Training tokenizer:  68% 5464/8047 [4:16:49<2:55:10,  4.07s/it]loss_total_epoch 300.8285538293421
Training tokenizer:  68% 5465/8047 [4:16:53<2:55:07,  4.07s/it]loss_total_epoch 300.86359936743975
Training tokenizer:  68% 5466/8047 [4:16:57<2:54:57,  4.07s/it]loss_total_epoch 300.9189826026559
Training tokenizer:  68% 5467/8047 [4:17:01<2:54:36,  4.06s/it]loss_total_epoch 300.9616532549262
Training tokenizer:  68% 5468/8047 [4:17:05<2:54:50,  4.07s/it]loss_total_epoch 301.0018514432013
Training tokenizer:  68% 5469/8047 [4:17:09<2:54:52,  4.07s/it]loss_total_epoch 301.04953376576304
Training tokenizer:  68% 5470/8047 [4:17:13<2:54:53,  4.07s/it]loss_total_epoch 301.0870006121695
Training tokenizer:  68% 5471/8047 [4:17:17<2:54:44,  4.07s/it]loss_total_epoch 301.1341866031289
Training tokenizer:  68% 5472/8047 [4:17:21<2:55:13,  4.08s/it]loss_total_epoch 301.1808443926275
Training tokenizer:  68% 5473/8047 [4:17:26<2:55:45,  4.10s/it]loss_total_epoch 301.22504495456815
Training tokenizer:  68% 5474/8047 [4:17:30<2:54:49,  4.08s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-p4c75mlf'
loss_total_epoch 301.26657017692924
Training tokenizer:  68% 5475/8047 [4:17:34<2:54:24,  4.07s/it]loss_total_epoch 301.308364007622
Training tokenizer:  68% 5476/8047 [4:17:38<2:54:57,  4.08s/it]loss_total_epoch 301.35867501422763
Training tokenizer:  68% 5477/8047 [4:17:42<2:55:08,  4.09s/it]loss_total_epoch 301.41677967086434
Training tokenizer:  68% 5478/8047 [4:17:46<2:55:20,  4.10s/it]loss_total_epoch 301.45526580512524
Training tokenizer:  68% 5479/8047 [4:17:50<2:55:01,  4.09s/it]loss_total_epoch 301.49993651360273
Training tokenizer:  68% 5480/8047 [4:17:54<2:55:13,  4.10s/it]loss_total_epoch 301.54615727066994
Training tokenizer:  68% 5481/8047 [4:17:58<2:55:54,  4.11s/it]loss_total_epoch 301.5912543460727
Training tokenizer:  68% 5482/8047 [4:18:02<2:54:52,  4.09s/it]loss_total_epoch 301.6389551348984
Training tokenizer:  68% 5483/8047 [4:18:06<2:54:29,  4.08s/it]loss_total_epoch 301.6824397556484
Training tokenizer:  68% 5484/8047 [4:18:11<2:54:28,  4.08s/it]loss_total_epoch 301.72562454268336
Training tokenizer:  68% 5485/8047 [4:18:15<2:54:19,  4.08s/it]loss_total_epoch 301.76627422869205
Training tokenizer:  68% 5486/8047 [4:18:19<2:53:42,  4.07s/it]loss_total_epoch 301.81277568265796
Training tokenizer:  68% 5487/8047 [4:18:23<2:54:04,  4.08s/it]loss_total_epoch 301.864698369056
Training tokenizer:  68% 5488/8047 [4:18:27<2:53:23,  4.07s/it]loss_total_epoch 301.9060107059777
Training tokenizer:  68% 5489/8047 [4:18:31<2:53:32,  4.07s/it]loss_total_epoch 301.9475486539304
Training tokenizer:  68% 5490/8047 [4:18:35<2:53:43,  4.08s/it]loss_total_epoch 301.9915652163327
Training tokenizer:  68% 5491/8047 [4:18:39<2:53:49,  4.08s/it]loss_total_epoch 302.03908900916576
Training tokenizer:  68% 5492/8047 [4:18:43<2:53:48,  4.08s/it]loss_total_epoch 302.08425464108586
Training tokenizer:  68% 5493/8047 [4:18:47<2:53:14,  4.07s/it]loss_total_epoch 302.1334859356284
Training tokenizer:  68% 5494/8047 [4:18:51<2:53:18,  4.07s/it]loss_total_epoch 302.1768639534712
Training tokenizer:  68% 5495/8047 [4:18:55<2:52:29,  4.06s/it]loss_total_epoch 302.22397754341364
Training tokenizer:  68% 5496/8047 [4:18:59<2:52:37,  4.06s/it]loss_total_epoch 302.26077546551824
Training tokenizer:  68% 5497/8047 [4:19:03<2:52:33,  4.06s/it]loss_total_epoch 302.3049656711519
Training tokenizer:  68% 5498/8047 [4:19:07<2:51:46,  4.04s/it]loss_total_epoch 302.35471127182245
Training tokenizer:  68% 5499/8047 [4:19:11<2:52:07,  4.05s/it]loss_total_epoch 302.41235821694136
Training tokenizer:  68% 5500/8047 [4:19:16<2:52:09,  4.06s/it]loss_total_epoch 302.4610259272158
Training tokenizer:  68% 5501/8047 [4:19:20<2:51:30,  4.04s/it]loss_total_epoch 302.5054270476103
Training tokenizer:  68% 5502/8047 [4:19:24<2:52:58,  4.08s/it]loss_total_epoch 302.545431420207
Training tokenizer:  68% 5503/8047 [4:19:28<2:52:47,  4.08s/it]loss_total_epoch 302.5812565162778
Training tokenizer:  68% 5504/8047 [4:19:32<2:51:41,  4.05s/it]loss_total_epoch 302.63498460128903
Training tokenizer:  68% 5505/8047 [4:19:36<2:52:12,  4.06s/it]loss_total_epoch 302.67785051837564
Training tokenizer:  68% 5506/8047 [4:19:40<2:52:29,  4.07s/it]loss_total_epoch 302.72069758176804
Training tokenizer:  68% 5507/8047 [4:19:44<2:52:56,  4.09s/it]loss_total_epoch 302.7577319815755
Training tokenizer:  68% 5508/8047 [4:19:48<2:51:12,  4.05s/it]loss_total_epoch 302.8053506016731
Training tokenizer:  68% 5509/8047 [4:19:52<2:51:51,  4.06s/it]loss_total_epoch 302.8555484190583
Training tokenizer:  68% 5510/8047 [4:19:56<2:51:57,  4.07s/it]loss_total_epoch 302.8913931809366
Training tokenizer:  68% 5511/8047 [4:20:00<2:51:55,  4.07s/it]loss_total_epoch 302.934144705534
Training tokenizer:  68% 5512/8047 [4:20:04<2:52:43,  4.09s/it]loss_total_epoch 302.97251222655177
Training tokenizer:  69% 5513/8047 [4:20:08<2:52:55,  4.09s/it]loss_total_epoch 303.01270469278097
Training tokenizer:  69% 5514/8047 [4:20:13<2:53:03,  4.10s/it]loss_total_epoch 303.0690344311297
Training tokenizer:  69% 5515/8047 [4:20:17<2:52:36,  4.09s/it]loss_total_epoch 303.1043962314725
Training tokenizer:  69% 5516/8047 [4:20:21<2:52:35,  4.09s/it]loss_total_epoch 303.1450419910252
Training tokenizer:  69% 5517/8047 [4:20:25<2:51:47,  4.07s/it]loss_total_epoch 303.1973647810519
Training tokenizer:  69% 5518/8047 [4:20:29<2:51:20,  4.06s/it]loss_total_epoch 303.2360027693212
Training tokenizer:  69% 5519/8047 [4:20:33<2:50:33,  4.05s/it]loss_total_epoch 303.280844502151
Training tokenizer:  69% 5520/8047 [4:20:37<2:51:41,  4.08s/it]loss_total_epoch 303.3198317475617
Training tokenizer:  69% 5521/8047 [4:20:41<2:51:14,  4.07s/it]loss_total_epoch 303.3599972911179
Training tokenizer:  69% 5522/8047 [4:20:45<2:51:47,  4.08s/it]loss_total_epoch 303.4064769335091
Training tokenizer:  69% 5523/8047 [4:20:49<2:52:02,  4.09s/it]loss_total_epoch 303.4640078060329
Training tokenizer:  69% 5524/8047 [4:20:53<2:51:49,  4.09s/it]loss_total_epoch 303.4942663926631
Training tokenizer:  69% 5525/8047 [4:20:57<2:51:52,  4.09s/it]loss_total_epoch 303.5347168762237
Training tokenizer:  69% 5526/8047 [4:21:02<2:52:10,  4.10s/it]loss_total_epoch 303.5810687150806
Training tokenizer:  69% 5527/8047 [4:21:06<2:51:40,  4.09s/it]loss_total_epoch 303.62302671931684
Training tokenizer:  69% 5528/8047 [4:21:10<2:51:31,  4.09s/it]loss_total_epoch 303.67448532767594
Training tokenizer:  69% 5529/8047 [4:21:14<2:52:16,  4.11s/it]loss_total_epoch 303.7213778588921
Training tokenizer:  69% 5530/8047 [4:21:18<2:51:54,  4.10s/it]loss_total_epoch 303.75483359210193
Training tokenizer:  69% 5531/8047 [4:21:22<2:52:03,  4.10s/it]loss_total_epoch 303.79914898239076
Training tokenizer:  69% 5532/8047 [4:21:26<2:51:41,  4.10s/it]loss_total_epoch 303.8471024688333
Training tokenizer:  69% 5533/8047 [4:21:30<2:52:02,  4.11s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-s33y5ua9'
loss_total_epoch 303.8962766584009
Training tokenizer:  69% 5534/8047 [4:21:34<2:52:02,  4.11s/it]loss_total_epoch 303.9479866679758
Training tokenizer:  69% 5535/8047 [4:21:39<2:52:12,  4.11s/it]loss_total_epoch 303.99328490532935
Training tokenizer:  69% 5536/8047 [4:21:43<2:51:37,  4.10s/it]loss_total_epoch 304.03980114124715
Training tokenizer:  69% 5537/8047 [4:21:47<2:51:49,  4.11s/it]loss_total_epoch 304.0842152554542
Training tokenizer:  69% 5538/8047 [4:21:51<2:51:26,  4.10s/it]loss_total_epoch 304.1363999377936
Training tokenizer:  69% 5539/8047 [4:21:55<2:51:44,  4.11s/it]loss_total_epoch 304.18132310546935
Training tokenizer:  69% 5540/8047 [4:21:59<2:52:24,  4.13s/it]loss_total_epoch 304.2269249241799
Training tokenizer:  69% 5541/8047 [4:22:03<2:52:17,  4.13s/it]loss_total_epoch 304.2651793267578
Training tokenizer:  69% 5542/8047 [4:22:07<2:52:16,  4.13s/it]loss_total_epoch 304.3086975943297
Training tokenizer:  69% 5543/8047 [4:22:11<2:51:09,  4.10s/it]loss_total_epoch 304.3499491047114
Training tokenizer:  69% 5544/8047 [4:22:16<2:51:37,  4.11s/it]loss_total_epoch 304.39428306929767
Training tokenizer:  69% 5545/8047 [4:22:20<2:50:48,  4.10s/it]loss_total_epoch 304.4243420287967
Training tokenizer:  69% 5546/8047 [4:22:24<2:51:17,  4.11s/it]loss_total_epoch 304.46560302376747
Training tokenizer:  69% 5547/8047 [4:22:28<2:51:13,  4.11s/it]loss_total_epoch 304.51475213468075
Training tokenizer:  69% 5548/8047 [4:22:32<2:50:58,  4.10s/it]loss_total_epoch 304.55502746626735
Training tokenizer:  69% 5549/8047 [4:22:36<2:50:48,  4.10s/it]loss_total_epoch 304.59743724018335
Training tokenizer:  69% 5550/8047 [4:22:40<2:50:19,  4.09s/it]loss_total_epoch 304.6368037648499
Training tokenizer:  69% 5551/8047 [4:22:44<2:50:31,  4.10s/it]loss_total_epoch 304.67374154925346
Training tokenizer:  69% 5552/8047 [4:22:48<2:50:35,  4.10s/it]loss_total_epoch 304.71565521880984
Training tokenizer:  69% 5553/8047 [4:22:52<2:51:34,  4.13s/it]loss_total_epoch 304.7620558999479
Training tokenizer:  69% 5554/8047 [4:22:57<2:51:12,  4.12s/it]loss_total_epoch 304.8117691166699
Training tokenizer:  69% 5555/8047 [4:23:01<2:51:10,  4.12s/it]loss_total_epoch 304.8608956709504
Training tokenizer:  69% 5556/8047 [4:23:05<2:51:05,  4.12s/it]loss_total_epoch 304.9018683992326
Training tokenizer:  69% 5557/8047 [4:23:09<2:50:49,  4.12s/it]loss_total_epoch 304.9440357685089
Training tokenizer:  69% 5558/8047 [4:23:13<2:50:37,  4.11s/it]loss_total_epoch 304.9878363646567
Training tokenizer:  69% 5559/8047 [4:23:17<2:49:18,  4.08s/it]loss_total_epoch 305.03093618527055
Training tokenizer:  69% 5560/8047 [4:23:21<2:49:59,  4.10s/it]loss_total_epoch 305.0698791705072
Training tokenizer:  69% 5561/8047 [4:23:25<2:51:03,  4.13s/it]loss_total_epoch 305.10893562808633
Training tokenizer:  69% 5562/8047 [4:23:29<2:50:29,  4.12s/it]loss_total_epoch 305.1608185581863
Training tokenizer:  69% 5563/8047 [4:23:34<2:49:49,  4.10s/it]loss_total_epoch 305.2067747451365
Training tokenizer:  69% 5564/8047 [4:23:38<2:50:01,  4.11s/it]loss_total_epoch 305.2517514824867
Training tokenizer:  69% 5565/8047 [4:23:42<2:49:24,  4.10s/it]loss_total_epoch 305.30167108029127
Training tokenizer:  69% 5566/8047 [4:23:46<2:50:03,  4.11s/it]loss_total_epoch 305.3494210243225
Training tokenizer:  69% 5567/8047 [4:23:50<2:50:30,  4.13s/it]loss_total_epoch 305.3859860226512
Training tokenizer:  69% 5568/8047 [4:23:54<2:49:55,  4.11s/it]loss_total_epoch 305.4278245419264
Training tokenizer:  69% 5569/8047 [4:23:58<2:49:55,  4.11s/it]loss_total_epoch 305.4736211076379
Training tokenizer:  69% 5570/8047 [4:24:02<2:49:04,  4.10s/it]loss_total_epoch 305.51642567291856
Training tokenizer:  69% 5571/8047 [4:24:06<2:49:08,  4.10s/it]loss_total_epoch 305.55644566193223
Training tokenizer:  69% 5572/8047 [4:24:11<2:49:31,  4.11s/it]loss_total_epoch 305.59915921092033
Training tokenizer:  69% 5573/8047 [4:24:15<2:49:05,  4.10s/it]loss_total_epoch 305.6395432278514
Training tokenizer:  69% 5574/8047 [4:24:19<2:49:12,  4.11s/it]loss_total_epoch 305.6885840445757
Training tokenizer:  69% 5575/8047 [4:24:23<2:48:59,  4.10s/it]loss_total_epoch 305.72414711490273
Training tokenizer:  69% 5576/8047 [4:24:27<2:49:17,  4.11s/it]loss_total_epoch 305.76001884415746
Training tokenizer:  69% 5577/8047 [4:24:31<2:49:42,  4.12s/it]loss_total_epoch 305.8015306405723
Training tokenizer:  69% 5578/8047 [4:24:35<2:49:35,  4.12s/it]loss_total_epoch 305.84546269476414
Training tokenizer:  69% 5579/8047 [4:24:39<2:49:32,  4.12s/it]loss_total_epoch 305.8932477235794
Training tokenizer:  69% 5580/8047 [4:24:43<2:48:44,  4.10s/it]loss_total_epoch 305.9464341290295
Training tokenizer:  69% 5581/8047 [4:24:48<2:48:51,  4.11s/it]loss_total_epoch 305.9850190281868
Training tokenizer:  69% 5582/8047 [4:24:52<2:49:10,  4.12s/it]loss_total_epoch 306.0327105820179
Training tokenizer:  69% 5583/8047 [4:24:56<2:48:59,  4.11s/it]loss_total_epoch 306.0787405818701
Training tokenizer:  69% 5584/8047 [4:25:00<2:49:28,  4.13s/it]loss_total_epoch 306.12523804977536
Training tokenizer:  69% 5585/8047 [4:25:04<2:49:28,  4.13s/it]loss_total_epoch 306.1706372201443
Training tokenizer:  69% 5586/8047 [4:25:08<2:48:56,  4.12s/it]loss_total_epoch 306.20808793231845
Training tokenizer:  69% 5587/8047 [4:25:12<2:48:52,  4.12s/it]loss_total_epoch 306.25612957403064
Training tokenizer:  69% 5588/8047 [4:25:16<2:48:37,  4.11s/it]loss_total_epoch 306.2942603863776
Training tokenizer:  69% 5589/8047 [4:25:21<2:48:29,  4.11s/it]loss_total_epoch 306.33425502479076
Training tokenizer:  69% 5590/8047 [4:25:25<2:49:03,  4.13s/it]loss_total_epoch 306.3775618970394
Training tokenizer:  69% 5591/8047 [4:25:29<2:48:32,  4.12s/it]loss_total_epoch 306.42611510679126
Training tokenizer:  69% 5592/8047 [4:25:33<2:48:27,  4.12s/it]loss_total_epoch 306.48039596155286
Training tokenizer:  70% 5593/8047 [4:25:37<2:48:26,  4.12s/it]loss_total_epoch 306.52587747201324
Training tokenizer:  70% 5594/8047 [4:25:41<2:48:35,  4.12s/it]loss_total_epoch 306.56864593550563
Training tokenizer:  70% 5595/8047 [4:25:45<2:48:09,  4.11s/it]loss_total_epoch 306.60589412227273
Training tokenizer:  70% 5596/8047 [4:25:49<2:48:32,  4.13s/it]loss_total_epoch 306.63959635421634
Training tokenizer:  70% 5597/8047 [4:25:53<2:48:18,  4.12s/it]loss_total_epoch 306.68373150750995
Training tokenizer:  70% 5598/8047 [4:25:58<2:47:36,  4.11s/it]loss_total_epoch 306.72267090529203
Training tokenizer:  70% 5599/8047 [4:26:02<2:46:48,  4.09s/it]loss_total_epoch 306.76823729276657
Training tokenizer:  70% 5600/8047 [4:26:06<2:47:25,  4.11s/it]loss_total_epoch 306.8103019744158
Training tokenizer:  70% 5601/8047 [4:26:10<2:47:20,  4.11s/it]loss_total_epoch 306.86181254684925
Training tokenizer:  70% 5602/8047 [4:26:14<2:47:33,  4.11s/it]loss_total_epoch 306.9183611795306
Training tokenizer:  70% 5603/8047 [4:26:18<2:48:00,  4.12s/it]loss_total_epoch 306.97316659241915
Training tokenizer:  70% 5604/8047 [4:26:22<2:47:51,  4.12s/it]loss_total_epoch 307.0199882313609
Training tokenizer:  70% 5605/8047 [4:26:26<2:47:31,  4.12s/it]loss_total_epoch 307.0669448263943
Training tokenizer:  70% 5606/8047 [4:26:30<2:47:25,  4.12s/it]loss_total_epoch 307.1098389439285
Training tokenizer:  70% 5607/8047 [4:26:35<2:47:27,  4.12s/it]loss_total_epoch 307.1528718024492
Training tokenizer:  70% 5608/8047 [4:26:39<2:47:39,  4.12s/it]loss_total_epoch 307.19095274060965
Training tokenizer:  70% 5609/8047 [4:26:43<2:47:37,  4.13s/it]loss_total_epoch 307.2453651688993
Training tokenizer:  70% 5610/8047 [4:26:47<2:48:01,  4.14s/it]loss_total_epoch 307.2847708798945
Training tokenizer:  70% 5611/8047 [4:26:51<2:48:19,  4.15s/it]loss_total_epoch 307.3286502622068
Training tokenizer:  70% 5612/8047 [4:26:55<2:48:01,  4.14s/it]loss_total_epoch 307.36938164010644
Training tokenizer:  70% 5613/8047 [4:26:59<2:48:10,  4.15s/it]loss_total_epoch 307.4271960966289
Training tokenizer:  70% 5614/8047 [4:27:04<2:48:08,  4.15s/it]loss_total_epoch 307.4749860614538
Training tokenizer:  70% 5615/8047 [4:27:08<2:49:01,  4.17s/it]loss_total_epoch 307.5193497762084
Training tokenizer:  70% 5616/8047 [4:27:12<2:48:35,  4.16s/it]loss_total_epoch 307.5684504136443
Training tokenizer:  70% 5617/8047 [4:27:16<2:48:22,  4.16s/it]loss_total_epoch 307.61175438016653
Training tokenizer:  70% 5618/8047 [4:27:20<2:47:59,  4.15s/it]loss_total_epoch 307.66524789482355
Training tokenizer:  70% 5619/8047 [4:27:24<2:47:46,  4.15s/it]loss_total_epoch 307.72202306240797
Training tokenizer:  70% 5620/8047 [4:27:29<2:47:40,  4.15s/it]loss_total_epoch 307.7595568969846
Training tokenizer:  70% 5621/8047 [4:27:33<2:47:23,  4.14s/it]loss_total_epoch 307.8013969808817
Training tokenizer:  70% 5622/8047 [4:27:37<2:47:14,  4.14s/it]loss_total_epoch 307.84811774641275
Training tokenizer:  70% 5623/8047 [4:27:41<2:47:08,  4.14s/it]loss_total_epoch 307.8946257866919
Training tokenizer:  70% 5624/8047 [4:27:45<2:47:20,  4.14s/it]loss_total_epoch 307.92840211465955
Training tokenizer:  70% 5625/8047 [4:27:49<2:46:58,  4.14s/it]loss_total_epoch 307.9690505042672
Training tokenizer:  70% 5626/8047 [4:27:53<2:46:43,  4.13s/it]loss_total_epoch 308.0125159472227
Training tokenizer:  70% 5627/8047 [4:27:57<2:46:48,  4.14s/it]loss_total_epoch 308.05257426947355
Training tokenizer:  70% 5628/8047 [4:28:02<2:46:40,  4.13s/it]loss_total_epoch 308.1036802493036
Training tokenizer:  70% 5629/8047 [4:28:06<2:46:26,  4.13s/it]loss_total_epoch 308.1455593034625
Training tokenizer:  70% 5630/8047 [4:28:10<2:46:03,  4.12s/it]loss_total_epoch 308.189520675689
Training tokenizer:  70% 5631/8047 [4:28:14<2:46:22,  4.13s/it]loss_total_epoch 308.23776319250464
Training tokenizer:  70% 5632/8047 [4:28:18<2:45:53,  4.12s/it]loss_total_epoch 308.27724888548255
Training tokenizer:  70% 5633/8047 [4:28:22<2:45:27,  4.11s/it]loss_total_epoch 308.3270471394062
Training tokenizer:  70% 5634/8047 [4:28:26<2:45:26,  4.11s/it]loss_total_epoch 308.38321010768414
Training tokenizer:  70% 5635/8047 [4:28:30<2:46:15,  4.14s/it]loss_total_epoch 308.42992582172155
Training tokenizer:  70% 5636/8047 [4:28:35<2:46:36,  4.15s/it]loss_total_epoch 308.48037177324295
Training tokenizer:  70% 5637/8047 [4:28:39<2:46:53,  4.15s/it]loss_total_epoch 308.53591226786375
Training tokenizer:  70% 5638/8047 [4:28:43<2:46:36,  4.15s/it]loss_total_epoch 308.57132034003735
Training tokenizer:  70% 5639/8047 [4:28:47<2:46:37,  4.15s/it]loss_total_epoch 308.6099058762193
Training tokenizer:  70% 5640/8047 [4:28:51<2:46:13,  4.14s/it]loss_total_epoch 308.65516255423427
Training tokenizer:  70% 5641/8047 [4:28:55<2:45:58,  4.14s/it]loss_total_epoch 308.694633949548
Training tokenizer:  70% 5642/8047 [4:29:00<2:45:51,  4.14s/it]loss_total_epoch 308.73752580210567
Training tokenizer:  70% 5643/8047 [4:29:04<2:45:36,  4.13s/it]loss_total_epoch 308.78339275345206
Training tokenizer:  70% 5644/8047 [4:29:08<2:45:11,  4.12s/it]loss_total_epoch 308.83107313886285
Training tokenizer:  70% 5645/8047 [4:29:12<2:44:56,  4.12s/it]loss_total_epoch 308.87195639684796
Training tokenizer:  70% 5646/8047 [4:29:16<2:45:37,  4.14s/it]loss_total_epoch 308.9155207797885
Training tokenizer:  70% 5647/8047 [4:29:20<2:45:17,  4.13s/it]loss_total_epoch 308.96089005097747
Training tokenizer:  70% 5648/8047 [4:29:24<2:44:37,  4.12s/it]loss_total_epoch 309.006909891963
Training tokenizer:  70% 5649/8047 [4:29:28<2:44:59,  4.13s/it]loss_total_epoch 309.04554172977805
Training tokenizer:  70% 5650/8047 [4:29:33<2:44:51,  4.13s/it]loss_total_epoch 309.0863215737045
Training tokenizer:  70% 5651/8047 [4:29:37<2:45:05,  4.13s/it]loss_total_epoch 309.1294439099729
Training tokenizer:  70% 5652/8047 [4:29:41<2:45:39,  4.15s/it]loss_total_epoch 309.17751689627767
Training tokenizer:  70% 5653/8047 [4:29:45<2:45:19,  4.14s/it]loss_total_epoch 309.21815990284085
Training tokenizer:  70% 5654/8047 [4:29:49<2:45:00,  4.14s/it]loss_total_epoch 309.26273795589805
Training tokenizer:  70% 5655/8047 [4:29:53<2:45:23,  4.15s/it]loss_total_epoch 309.3159517236054
Training tokenizer:  70% 5656/8047 [4:29:57<2:45:53,  4.16s/it]loss_total_epoch 309.3583956025541
Training tokenizer:  70% 5657/8047 [4:30:02<2:45:19,  4.15s/it]loss_total_epoch 309.4074983038008
Training tokenizer:  70% 5658/8047 [4:30:06<2:45:37,  4.16s/it]loss_total_epoch 309.4475118033588
Training tokenizer:  70% 5659/8047 [4:30:10<2:45:13,  4.15s/it]loss_total_epoch 309.49181081727147
Training tokenizer:  70% 5660/8047 [4:30:14<2:44:52,  4.14s/it]loss_total_epoch 309.5303859822452
Training tokenizer:  70% 5661/8047 [4:30:18<2:44:43,  4.14s/it]loss_total_epoch 309.586777664721
Training tokenizer:  70% 5662/8047 [4:30:22<2:44:30,  4.14s/it]loss_total_epoch 309.62531611323357
Training tokenizer:  70% 5663/8047 [4:30:26<2:44:30,  4.14s/it]loss_total_epoch 309.66898112371564
Training tokenizer:  70% 5664/8047 [4:30:31<2:44:24,  4.14s/it]loss_total_epoch 309.718954320997
Training tokenizer:  70% 5665/8047 [4:30:35<2:44:39,  4.15s/it]loss_total_epoch 309.76583329960704
Training tokenizer:  70% 5666/8047 [4:30:39<2:44:56,  4.16s/it]loss_total_epoch 309.81816340982914
Training tokenizer:  70% 5667/8047 [4:30:43<2:44:41,  4.15s/it]loss_total_epoch 309.8610341772437
Training tokenizer:  70% 5668/8047 [4:30:47<2:44:30,  4.15s/it]loss_total_epoch 309.90453612431884
Training tokenizer:  70% 5669/8047 [4:30:51<2:45:03,  4.16s/it]loss_total_epoch 309.94845720008016
Training tokenizer:  70% 5670/8047 [4:30:56<2:44:57,  4.16s/it]loss_total_epoch 309.9924696832895
Training tokenizer:  70% 5671/8047 [4:31:00<2:44:58,  4.17s/it]loss_total_epoch 310.02717788517475
Training tokenizer:  70% 5672/8047 [4:31:04<2:44:32,  4.16s/it]loss_total_epoch 310.07948376238346
Training tokenizer:  70% 5673/8047 [4:31:08<2:44:27,  4.16s/it]loss_total_epoch 310.1269300952554
Training tokenizer:  71% 5674/8047 [4:31:12<2:42:43,  4.11s/it]loss_total_epoch 310.17122028395534
Training tokenizer:  71% 5675/8047 [4:31:16<2:43:01,  4.12s/it]loss_total_epoch 310.2239906191826
Training tokenizer:  71% 5676/8047 [4:31:20<2:43:24,  4.14s/it]loss_total_epoch 310.27030328661203
Training tokenizer:  71% 5677/8047 [4:31:25<2:44:02,  4.15s/it]loss_total_epoch 310.31743209064007
Training tokenizer:  71% 5678/8047 [4:31:29<2:43:35,  4.14s/it]loss_total_epoch 310.36553829163313
Training tokenizer:  71% 5679/8047 [4:31:33<2:43:56,  4.15s/it]loss_total_epoch 310.4026143550873
Training tokenizer:  71% 5680/8047 [4:31:37<2:43:37,  4.15s/it]loss_total_epoch 310.44067918509245
Training tokenizer:  71% 5681/8047 [4:31:41<2:44:02,  4.16s/it]loss_total_epoch 310.4757927879691
Training tokenizer:  71% 5682/8047 [4:31:45<2:43:38,  4.15s/it]loss_total_epoch 310.52219105139375
Training tokenizer:  71% 5683/8047 [4:31:49<2:43:39,  4.15s/it]loss_total_epoch 310.56398940458894
Training tokenizer:  71% 5684/8047 [4:31:54<2:43:58,  4.16s/it]loss_total_epoch 310.6097198203206
Training tokenizer:  71% 5685/8047 [4:31:58<2:43:46,  4.16s/it]loss_total_epoch 310.65743244066834
Training tokenizer:  71% 5686/8047 [4:32:02<2:43:26,  4.15s/it]loss_total_epoch 310.7001112885773
Training tokenizer:  71% 5687/8047 [4:32:06<2:43:11,  4.15s/it]loss_total_epoch 310.7361376211047
Training tokenizer:  71% 5688/8047 [4:32:10<2:43:28,  4.16s/it]loss_total_epoch 310.78314926847816
Training tokenizer:  71% 5689/8047 [4:32:14<2:44:03,  4.17s/it]loss_total_epoch 310.82339707016945
Training tokenizer:  71% 5690/8047 [4:32:19<2:43:48,  4.17s/it]loss_total_epoch 310.8743203394115
Training tokenizer:  71% 5691/8047 [4:32:23<2:43:19,  4.16s/it]loss_total_epoch 310.9148401580751
Training tokenizer:  71% 5692/8047 [4:32:27<2:43:25,  4.16s/it]loss_total_epoch 310.9665614143014
Training tokenizer:  71% 5693/8047 [4:32:31<2:43:09,  4.16s/it]loss_total_epoch 311.0120842270553
Training tokenizer:  71% 5694/8047 [4:32:35<2:43:47,  4.18s/it]loss_total_epoch 311.0523846298456
Training tokenizer:  71% 5695/8047 [4:32:39<2:43:41,  4.18s/it]loss_total_epoch 311.0941763073206
Training tokenizer:  71% 5696/8047 [4:32:44<2:44:13,  4.19s/it]loss_total_epoch 311.1356247738004
Training tokenizer:  71% 5697/8047 [4:32:48<2:44:39,  4.20s/it]loss_total_epoch 311.18158342689276
Training tokenizer:  71% 5698/8047 [4:32:52<2:44:14,  4.20s/it]loss_total_epoch 311.21464440971613
Training tokenizer:  71% 5699/8047 [4:32:56<2:44:44,  4.21s/it]loss_total_epoch 311.2602975964546
Training tokenizer:  71% 5700/8047 [4:33:01<2:43:52,  4.19s/it]loss_total_epoch 311.3036004193127
Training tokenizer:  71% 5701/8047 [4:33:05<2:43:11,  4.17s/it]loss_total_epoch 311.3532908819616
Training tokenizer:  71% 5702/8047 [4:33:09<2:43:45,  4.19s/it]loss_total_epoch 311.39990301802754
Training tokenizer:  71% 5703/8047 [4:33:13<2:43:42,  4.19s/it]loss_total_epoch 311.45041409134865
Training tokenizer:  71% 5704/8047 [4:33:17<2:43:13,  4.18s/it]loss_total_epoch 311.4852549210191
Training tokenizer:  71% 5705/8047 [4:33:21<2:43:19,  4.18s/it]loss_total_epoch 311.5340595394373
Training tokenizer:  71% 5706/8047 [4:33:26<2:44:01,  4.20s/it]loss_total_epoch 311.58354956656694
Training tokenizer:  71% 5707/8047 [4:33:30<2:43:15,  4.19s/it]loss_total_epoch 311.63199074566364
Training tokenizer:  71% 5708/8047 [4:33:34<2:43:12,  4.19s/it]loss_total_epoch 311.67572977393866
Training tokenizer:  71% 5709/8047 [4:33:38<2:43:15,  4.19s/it]loss_total_epoch 311.70745654404163
Training tokenizer:  71% 5710/8047 [4:33:42<2:43:54,  4.21s/it]loss_total_epoch 311.7517824023962
Training tokenizer:  71% 5711/8047 [4:33:47<2:45:38,  4.25s/it]loss_total_epoch 311.79513085633516
Training tokenizer:  71% 5712/8047 [4:33:51<2:43:52,  4.21s/it]loss_total_epoch 311.83762903138995
Training tokenizer:  71% 5713/8047 [4:33:55<2:42:51,  4.19s/it]loss_total_epoch 311.88445103541017
Training tokenizer:  71% 5714/8047 [4:33:59<2:43:06,  4.19s/it]loss_total_epoch 311.91861141100526
Training tokenizer:  71% 5715/8047 [4:34:03<2:42:59,  4.19s/it]loss_total_epoch 311.96085212379694
Training tokenizer:  71% 5716/8047 [4:34:08<2:42:03,  4.17s/it]loss_total_epoch 312.0023106224835
Training tokenizer:  71% 5717/8047 [4:34:12<2:40:09,  4.12s/it]loss_total_epoch 312.04126359149814
Training tokenizer:  71% 5718/8047 [4:34:16<2:40:36,  4.14s/it]loss_total_epoch 312.0887904576957
Training tokenizer:  71% 5719/8047 [4:34:20<2:41:36,  4.17s/it]loss_total_epoch 312.1354128494859
Training tokenizer:  71% 5720/8047 [4:34:24<2:41:40,  4.17s/it]loss_total_epoch 312.1760153733194
Training tokenizer:  71% 5721/8047 [4:34:28<2:41:12,  4.16s/it]loss_total_epoch 312.21311735361814
Training tokenizer:  71% 5722/8047 [4:34:33<2:41:57,  4.18s/it]loss_total_epoch 312.26459116861224
Training tokenizer:  71% 5723/8047 [4:34:37<2:41:53,  4.18s/it]loss_total_epoch 312.305825073272
Training tokenizer:  71% 5724/8047 [4:34:41<2:41:53,  4.18s/it]loss_total_epoch 312.33954770863056
Training tokenizer:  71% 5725/8047 [4:34:45<2:41:26,  4.17s/it]loss_total_epoch 312.38712015748024
Training tokenizer:  71% 5726/8047 [4:34:49<2:41:55,  4.19s/it]loss_total_epoch 312.43155512213707
Training tokenizer:  71% 5727/8047 [4:34:53<2:42:14,  4.20s/it]loss_total_epoch 312.47342977300286
Training tokenizer:  71% 5728/8047 [4:34:58<2:41:43,  4.18s/it]loss_total_epoch 312.5244121737778
Training tokenizer:  71% 5729/8047 [4:35:02<2:41:46,  4.19s/it]loss_total_epoch 312.5690600797534
Training tokenizer:  71% 5730/8047 [4:35:06<2:41:37,  4.19s/it]loss_total_epoch 312.61893175169826
Training tokenizer:  71% 5731/8047 [4:35:10<2:41:48,  4.19s/it]loss_total_epoch 312.671510245651
Training tokenizer:  71% 5732/8047 [4:35:14<2:42:01,  4.20s/it]loss_total_epoch 312.71130883693695
Training tokenizer:  71% 5733/8047 [4:35:19<2:41:24,  4.19s/it]loss_total_epoch 312.7587371394038
Training tokenizer:  71% 5734/8047 [4:35:23<2:41:22,  4.19s/it]loss_total_epoch 312.79340270906687
Training tokenizer:  71% 5735/8047 [4:35:27<2:41:29,  4.19s/it]loss_total_epoch 312.832510009408
Training tokenizer:  71% 5736/8047 [4:35:31<2:41:03,  4.18s/it]loss_total_epoch 312.87836280837655
Training tokenizer:  71% 5737/8047 [4:35:35<2:40:26,  4.17s/it]loss_total_epoch 312.92876065149903
Training tokenizer:  71% 5738/8047 [4:35:39<2:40:19,  4.17s/it]loss_total_epoch 312.9629078246653
Training tokenizer:  71% 5739/8047 [4:35:44<2:39:37,  4.15s/it]loss_total_epoch 313.0092908889055
Training tokenizer:  71% 5740/8047 [4:35:48<2:39:32,  4.15s/it]loss_total_epoch 313.05284356698394
Training tokenizer:  71% 5741/8047 [4:35:52<2:40:10,  4.17s/it]loss_total_epoch 313.10407489165664
Training tokenizer:  71% 5742/8047 [4:35:56<2:39:50,  4.16s/it]loss_total_epoch 313.14774303138256
Training tokenizer:  71% 5743/8047 [4:36:00<2:39:58,  4.17s/it]loss_total_epoch 313.1915364935994
Training tokenizer:  71% 5744/8047 [4:36:04<2:40:08,  4.17s/it]loss_total_epoch 313.2406298369169
Training tokenizer:  71% 5745/8047 [4:36:09<2:40:18,  4.18s/it]loss_total_epoch 313.278946518898
Training tokenizer:  71% 5746/8047 [4:36:13<2:40:16,  4.18s/it]loss_total_epoch 313.3257624544203
Training tokenizer:  71% 5747/8047 [4:36:17<2:40:32,  4.19s/it]loss_total_epoch 313.36620266363025
Training tokenizer:  71% 5748/8047 [4:36:21<2:41:00,  4.20s/it]loss_total_epoch 313.4139818139374
Training tokenizer:  71% 5749/8047 [4:36:25<2:40:52,  4.20s/it]loss_total_epoch 313.4621575586498
Training tokenizer:  71% 5750/8047 [4:36:30<2:40:52,  4.20s/it]loss_total_epoch 313.50919407978654
Training tokenizer:  71% 5751/8047 [4:36:34<2:40:36,  4.20s/it]loss_total_epoch 313.55636800453067
Training tokenizer:  71% 5752/8047 [4:36:38<2:41:05,  4.21s/it]loss_total_epoch 313.59804144129157
Training tokenizer:  71% 5753/8047 [4:36:42<2:41:08,  4.21s/it]loss_total_epoch 313.6455157548189
Training tokenizer:  72% 5754/8047 [4:36:46<2:40:19,  4.20s/it]loss_total_epoch 313.6967573016882
Training tokenizer:  72% 5755/8047 [4:36:51<2:40:33,  4.20s/it]loss_total_epoch 313.7417703345418
Training tokenizer:  72% 5756/8047 [4:36:55<2:40:31,  4.20s/it]loss_total_epoch 313.794570222497
Training tokenizer:  72% 5757/8047 [4:36:59<2:40:46,  4.21s/it]loss_total_epoch 313.8398385681212
Training tokenizer:  72% 5758/8047 [4:37:03<2:40:56,  4.22s/it]loss_total_epoch 313.8815802410245
Training tokenizer:  72% 5759/8047 [4:37:07<2:40:15,  4.20s/it]loss_total_epoch 313.91825550049543
Training tokenizer:  72% 5760/8047 [4:37:12<2:40:39,  4.21s/it]loss_total_epoch 313.95408191904426
Training tokenizer:  72% 5761/8047 [4:37:16<2:41:09,  4.23s/it]loss_total_epoch 313.99747075885534
Training tokenizer:  72% 5762/8047 [4:37:20<2:40:39,  4.22s/it]loss_total_epoch 314.0476240180433
Training tokenizer:  72% 5763/8047 [4:37:24<2:40:10,  4.21s/it]loss_total_epoch 314.0952799767256
Training tokenizer:  72% 5764/8047 [4:37:29<2:39:46,  4.20s/it]loss_total_epoch 314.13677429407835
Training tokenizer:  72% 5765/8047 [4:37:33<2:39:06,  4.18s/it]loss_total_epoch 314.18528993055224
Training tokenizer:  72% 5766/8047 [4:37:37<2:39:17,  4.19s/it]loss_total_epoch 314.2208744995296
Training tokenizer:  72% 5767/8047 [4:37:41<2:38:53,  4.18s/it]loss_total_epoch 314.2644145041704
Training tokenizer:  72% 5768/8047 [4:37:45<2:39:40,  4.20s/it]loss_total_epoch 314.3082520328462
Training tokenizer:  72% 5769/8047 [4:37:50<2:39:14,  4.19s/it]loss_total_epoch 314.3537427075207
Training tokenizer:  72% 5770/8047 [4:37:54<2:37:46,  4.16s/it]loss_total_epoch 314.39929109066725
Training tokenizer:  72% 5771/8047 [4:37:58<2:37:53,  4.16s/it]loss_total_epoch 314.44558718428016
Training tokenizer:  72% 5772/8047 [4:38:02<2:38:08,  4.17s/it]loss_total_epoch 314.49171198159456
Training tokenizer:  72% 5773/8047 [4:38:06<2:38:34,  4.18s/it]loss_total_epoch 314.5315935499966
Training tokenizer:  72% 5774/8047 [4:38:10<2:38:41,  4.19s/it]loss_total_epoch 314.5675980709493
Training tokenizer:  72% 5775/8047 [4:38:15<2:38:59,  4.20s/it]loss_total_epoch 314.604206237942
Training tokenizer:  72% 5776/8047 [4:38:19<2:39:18,  4.21s/it]loss_total_epoch 314.6416300609708
Training tokenizer:  72% 5777/8047 [4:38:23<2:39:15,  4.21s/it]loss_total_epoch 314.68688417226076
Training tokenizer:  72% 5778/8047 [4:38:27<2:39:18,  4.21s/it]loss_total_epoch 314.7262849137187
Training tokenizer:  72% 5779/8047 [4:38:31<2:39:42,  4.23s/it]loss_total_epoch 314.76642848551273
Training tokenizer:  72% 5780/8047 [4:38:36<2:38:57,  4.21s/it]loss_total_epoch 314.8105033785105
Training tokenizer:  72% 5781/8047 [4:38:40<2:38:38,  4.20s/it]loss_total_epoch 314.84980139136314
Training tokenizer:  72% 5782/8047 [4:38:44<2:38:42,  4.20s/it]loss_total_epoch 314.8969450369477
Training tokenizer:  72% 5783/8047 [4:38:48<2:38:31,  4.20s/it]loss_total_epoch 314.93896370381117
Training tokenizer:  72% 5784/8047 [4:38:52<2:38:04,  4.19s/it]loss_total_epoch 314.9873667322099
Training tokenizer:  72% 5785/8047 [4:38:57<2:38:12,  4.20s/it]loss_total_epoch 315.0298494324088
Training tokenizer:  72% 5786/8047 [4:39:01<2:37:49,  4.19s/it]loss_total_epoch 315.07804710790515
Training tokenizer:  72% 5787/8047 [4:39:05<2:37:57,  4.19s/it]loss_total_epoch 315.11595928296447
Training tokenizer:  72% 5788/8047 [4:39:09<2:37:59,  4.20s/it]loss_total_epoch 315.1584249138832
Training tokenizer:  72% 5789/8047 [4:39:13<2:37:38,  4.19s/it]loss_total_epoch 315.20370022952557
Training tokenizer:  72% 5790/8047 [4:39:18<2:40:07,  4.26s/it]loss_total_epoch 315.24233881384134
Training tokenizer:  72% 5791/8047 [4:39:22<2:38:49,  4.22s/it]loss_total_epoch 315.29052413254976
Training tokenizer:  72% 5792/8047 [4:39:26<2:38:37,  4.22s/it]loss_total_epoch 315.33573243394494
Training tokenizer:  72% 5793/8047 [4:39:30<2:38:02,  4.21s/it]loss_total_epoch 315.38979360088706
Training tokenizer:  72% 5794/8047 [4:39:35<2:37:37,  4.20s/it]loss_total_epoch 315.4299839474261
Training tokenizer:  72% 5795/8047 [4:39:39<2:37:45,  4.20s/it]loss_total_epoch 315.4649107903242
Training tokenizer:  72% 5796/8047 [4:39:43<2:37:15,  4.19s/it]loss_total_epoch 315.51303632184863
Training tokenizer:  72% 5797/8047 [4:39:47<2:36:48,  4.18s/it]loss_total_epoch 315.561649158597
Training tokenizer:  72% 5798/8047 [4:39:51<2:37:17,  4.20s/it]loss_total_epoch 315.6010273247957
Training tokenizer:  72% 5799/8047 [4:39:55<2:37:16,  4.20s/it]loss_total_epoch 315.6423158161342
Training tokenizer:  72% 5800/8047 [4:40:00<2:37:02,  4.19s/it]loss_total_epoch 315.6926477961242
Training tokenizer:  72% 5801/8047 [4:40:04<2:36:38,  4.18s/it]loss_total_epoch 315.73145285621285
Training tokenizer:  72% 5802/8047 [4:40:08<2:37:22,  4.21s/it]loss_total_epoch 315.7805922999978
Training tokenizer:  72% 5803/8047 [4:40:12<2:38:40,  4.24s/it]loss_total_epoch 315.81303073838353
Training tokenizer:  72% 5804/8047 [4:40:17<2:38:19,  4.24s/it]loss_total_epoch 315.85567904636264
Training tokenizer:  72% 5805/8047 [4:40:21<2:38:11,  4.23s/it]loss_total_epoch 315.89237900823355
Training tokenizer:  72% 5806/8047 [4:40:25<2:37:24,  4.21s/it]loss_total_epoch 315.9309414550662
Training tokenizer:  72% 5807/8047 [4:40:29<2:37:29,  4.22s/it]loss_total_epoch 315.9652084261179
Training tokenizer:  72% 5808/8047 [4:40:33<2:37:19,  4.22s/it]loss_total_epoch 315.99666349962354
Training tokenizer:  72% 5809/8047 [4:40:38<2:37:40,  4.23s/it]loss_total_epoch 316.0317097902298
Training tokenizer:  72% 5810/8047 [4:40:42<2:37:30,  4.22s/it]loss_total_epoch 316.0710655339062
Training tokenizer:  72% 5811/8047 [4:40:46<2:37:33,  4.23s/it]loss_total_epoch 316.11450431495905
Training tokenizer:  72% 5812/8047 [4:40:50<2:37:46,  4.24s/it]loss_total_epoch 316.17399676144123
Training tokenizer:  72% 5813/8047 [4:40:55<2:37:43,  4.24s/it]loss_total_epoch 316.2251488119364
Training tokenizer:  72% 5814/8047 [4:40:59<2:37:29,  4.23s/it]loss_total_epoch 316.2761069573462
Training tokenizer:  72% 5815/8047 [4:41:03<2:37:18,  4.23s/it]loss_total_epoch 316.324436083436
Training tokenizer:  72% 5816/8047 [4:41:07<2:37:12,  4.23s/it]loss_total_epoch 316.36402497440577
Training tokenizer:  72% 5817/8047 [4:41:12<2:37:32,  4.24s/it]loss_total_epoch 316.4182092435658
Training tokenizer:  72% 5818/8047 [4:41:16<2:37:28,  4.24s/it]loss_total_epoch 316.46152832731605
Training tokenizer:  72% 5819/8047 [4:41:20<2:36:12,  4.21s/it]loss_total_epoch 316.5081924684346
Training tokenizer:  72% 5820/8047 [4:41:24<2:36:07,  4.21s/it]loss_total_epoch 316.5473231263459
Training tokenizer:  72% 5821/8047 [4:41:28<2:36:19,  4.21s/it]loss_total_epoch 316.59340877458453
Training tokenizer:  72% 5822/8047 [4:41:33<2:36:23,  4.22s/it]loss_total_epoch 316.6334160082042
Training tokenizer:  72% 5823/8047 [4:41:37<2:36:46,  4.23s/it]loss_total_epoch 316.67570815980434
Training tokenizer:  72% 5824/8047 [4:41:41<2:36:42,  4.23s/it]loss_total_epoch 316.7132612913847
Training tokenizer:  72% 5825/8047 [4:41:45<2:37:12,  4.25s/it]loss_total_epoch 316.760230474174
Training tokenizer:  72% 5826/8047 [4:41:50<2:37:19,  4.25s/it]loss_total_epoch 316.79865876957774
Training tokenizer:  72% 5827/8047 [4:41:54<2:37:09,  4.25s/it]loss_total_epoch 316.8306800276041
Training tokenizer:  72% 5828/8047 [4:41:58<2:36:56,  4.24s/it]loss_total_epoch 316.87980534136295
Training tokenizer:  72% 5829/8047 [4:42:02<2:37:01,  4.25s/it]loss_total_epoch 316.93389385193586
Training tokenizer:  72% 5830/8047 [4:42:07<2:36:18,  4.23s/it]loss_total_epoch 316.9874500259757
Training tokenizer:  72% 5831/8047 [4:42:11<2:36:40,  4.24s/it]loss_total_epoch 317.0241549052298
Training tokenizer:  72% 5832/8047 [4:42:15<2:37:00,  4.25s/it]loss_total_epoch 317.0676094070077
Training tokenizer:  72% 5833/8047 [4:42:19<2:36:56,  4.25s/it]loss_total_epoch 317.11156820878386
Training tokenizer:  72% 5834/8047 [4:42:24<2:36:57,  4.26s/it]loss_total_epoch 317.1536533795297
Training tokenizer:  73% 5835/8047 [4:42:28<2:38:58,  4.31s/it]loss_total_epoch 317.2123576849699
Training tokenizer:  73% 5836/8047 [4:42:32<2:37:57,  4.29s/it]loss_total_epoch 317.263565633446
Training tokenizer:  73% 5837/8047 [4:42:36<2:36:21,  4.24s/it]loss_total_epoch 317.3120417520404
Training tokenizer:  73% 5838/8047 [4:42:41<2:36:12,  4.24s/it]loss_total_epoch 317.3520812615752
Training tokenizer:  73% 5839/8047 [4:42:45<2:35:51,  4.24s/it]loss_total_epoch 317.40111573040485
Training tokenizer:  73% 5840/8047 [4:42:49<2:35:51,  4.24s/it]loss_total_epoch 317.452961768955
Training tokenizer:  73% 5841/8047 [4:42:53<2:35:36,  4.23s/it]loss_total_epoch 317.49709444865584
Training tokenizer:  73% 5842/8047 [4:42:58<2:36:06,  4.25s/it]loss_total_epoch 317.536020770669
Training tokenizer:  73% 5843/8047 [4:43:02<2:35:54,  4.24s/it]loss_total_epoch 317.58123652637005
Training tokenizer:  73% 5844/8047 [4:43:06<2:35:31,  4.24s/it]loss_total_epoch 317.6206897124648
Training tokenizer:  73% 5845/8047 [4:43:10<2:35:14,  4.23s/it]loss_total_epoch 317.66547503322363
Training tokenizer:  73% 5846/8047 [4:43:15<2:35:39,  4.24s/it]loss_total_epoch 317.71289632469416
Training tokenizer:  73% 5847/8047 [4:43:19<2:35:47,  4.25s/it]loss_total_epoch 317.7597787231207
Training tokenizer:  73% 5848/8047 [4:43:23<2:35:15,  4.24s/it]loss_total_epoch 317.79102719016373
Training tokenizer:  73% 5849/8047 [4:43:27<2:35:01,  4.23s/it]loss_total_epoch 317.83340669609606
Training tokenizer:  73% 5850/8047 [4:43:32<2:35:17,  4.24s/it]loss_total_epoch 317.87743556313217
Training tokenizer:  73% 5851/8047 [4:43:36<2:34:50,  4.23s/it]loss_total_epoch 317.92013433016837
Training tokenizer:  73% 5852/8047 [4:43:40<2:34:37,  4.23s/it]loss_total_epoch 317.9537427332252
Training tokenizer:  73% 5853/8047 [4:43:44<2:34:37,  4.23s/it]loss_total_epoch 317.9936799388379
Training tokenizer:  73% 5854/8047 [4:43:48<2:34:29,  4.23s/it]loss_total_epoch 318.0417376179248
Training tokenizer:  73% 5855/8047 [4:43:53<2:34:47,  4.24s/it]loss_total_epoch 318.0735506694764
Training tokenizer:  73% 5856/8047 [4:43:57<2:34:47,  4.24s/it]loss_total_epoch 318.1208813022822
Training tokenizer:  73% 5857/8047 [4:44:01<2:34:40,  4.24s/it]loss_total_epoch 318.1676478665322
Training tokenizer:  73% 5858/8047 [4:44:05<2:34:19,  4.23s/it]loss_total_epoch 318.2077794428915
Training tokenizer:  73% 5859/8047 [4:44:10<2:34:09,  4.23s/it]loss_total_epoch 318.25320869870484
Training tokenizer:  73% 5860/8047 [4:44:14<2:32:43,  4.19s/it]loss_total_epoch 318.2984595876187
Training tokenizer:  73% 5861/8047 [4:44:18<2:33:43,  4.22s/it]loss_total_epoch 318.3552298154682
Training tokenizer:  73% 5862/8047 [4:44:22<2:33:34,  4.22s/it]loss_total_epoch 318.39214793778956
Training tokenizer:  73% 5863/8047 [4:44:26<2:33:50,  4.23s/it]loss_total_epoch 318.44159985892475
Training tokenizer:  73% 5864/8047 [4:44:31<2:34:24,  4.24s/it]loss_total_epoch 318.4827826078981
Training tokenizer:  73% 5865/8047 [4:44:35<2:34:52,  4.26s/it]loss_total_epoch 318.5154169332236
Training tokenizer:  73% 5866/8047 [4:44:39<2:34:17,  4.24s/it]loss_total_epoch 318.5632098969072
Training tokenizer:  73% 5867/8047 [4:44:43<2:33:56,  4.24s/it]loss_total_epoch 318.6079293061048
Training tokenizer:  73% 5868/8047 [4:44:48<2:33:41,  4.23s/it]loss_total_epoch 318.6466505546123
Training tokenizer:  73% 5869/8047 [4:44:52<2:33:50,  4.24s/it]loss_total_epoch 318.6815768610686
Training tokenizer:  73% 5870/8047 [4:44:56<2:34:50,  4.27s/it]loss_total_epoch 318.72966015152633
Training tokenizer:  73% 5871/8047 [4:45:01<2:35:02,  4.27s/it]loss_total_epoch 318.77673182450235
Training tokenizer:  73% 5872/8047 [4:45:05<2:34:40,  4.27s/it]loss_total_epoch 318.8157640825957
Training tokenizer:  73% 5873/8047 [4:45:09<2:34:00,  4.25s/it]loss_total_epoch 318.86624488420784
Training tokenizer:  73% 5874/8047 [4:45:13<2:33:36,  4.24s/it]loss_total_epoch 318.9070923309773
Training tokenizer:  73% 5875/8047 [4:45:17<2:32:29,  4.21s/it]loss_total_epoch 318.95248976908624
Training tokenizer:  73% 5876/8047 [4:45:22<2:32:36,  4.22s/it]loss_total_epoch 318.99339195899665
Training tokenizer:  73% 5877/8047 [4:45:26<2:33:40,  4.25s/it]loss_total_epoch 319.0331900436431
Training tokenizer:  73% 5878/8047 [4:45:30<2:34:08,  4.26s/it]loss_total_epoch 319.08153149299324
Training tokenizer:  73% 5879/8047 [4:45:35<2:35:53,  4.31s/it]loss_total_epoch 319.11707007698715
Training tokenizer:  73% 5880/8047 [4:45:39<2:35:01,  4.29s/it]loss_total_epoch 319.162748305127
Training tokenizer:  73% 5881/8047 [4:45:43<2:34:21,  4.28s/it]loss_total_epoch 319.207282776013
Training tokenizer:  73% 5882/8047 [4:45:47<2:34:00,  4.27s/it]loss_total_epoch 319.25103958137333
Training tokenizer:  73% 5883/8047 [4:45:52<2:33:33,  4.26s/it]loss_total_epoch 319.2878504227847
Training tokenizer:  73% 5884/8047 [4:45:56<2:33:35,  4.26s/it]loss_total_epoch 319.3334170784801
Training tokenizer:  73% 5885/8047 [4:46:00<2:33:50,  4.27s/it]loss_total_epoch 319.37731187976897
Training tokenizer:  73% 5886/8047 [4:46:04<2:33:17,  4.26s/it]loss_total_epoch 319.41594486124814
Training tokenizer:  73% 5887/8047 [4:46:09<2:33:10,  4.25s/it]loss_total_epoch 319.4578577708453
Training tokenizer:  73% 5888/8047 [4:46:13<2:33:02,  4.25s/it]loss_total_epoch 319.4953239504248
Training tokenizer:  73% 5889/8047 [4:46:17<2:33:23,  4.26s/it]loss_total_epoch 319.54124053008854
Training tokenizer:  73% 5890/8047 [4:46:21<2:33:16,  4.26s/it]loss_total_epoch 319.5894421581179
Training tokenizer:  73% 5891/8047 [4:46:26<2:33:04,  4.26s/it]loss_total_epoch 319.6228490639478
Training tokenizer:  73% 5892/8047 [4:46:30<2:33:41,  4.28s/it]loss_total_epoch 319.66699422337115
Training tokenizer:  73% 5893/8047 [4:46:34<2:33:54,  4.29s/it]loss_total_epoch 319.7138995733112
Training tokenizer:  73% 5894/8047 [4:46:39<2:33:35,  4.28s/it]loss_total_epoch 319.7599554415792
Training tokenizer:  73% 5895/8047 [4:46:43<2:33:06,  4.27s/it]loss_total_epoch 319.80643949471414
Training tokenizer:  73% 5896/8047 [4:46:47<2:32:59,  4.27s/it]loss_total_epoch 319.84662846289575
Training tokenizer:  73% 5897/8047 [4:46:51<2:33:12,  4.28s/it]loss_total_epoch 319.8831155579537
Training tokenizer:  73% 5898/8047 [4:46:56<2:33:09,  4.28s/it]loss_total_epoch 319.92674906365573
Training tokenizer:  73% 5899/8047 [4:47:00<2:32:53,  4.27s/it]loss_total_epoch 319.96359094046056
Training tokenizer:  73% 5900/8047 [4:47:04<2:32:52,  4.27s/it]loss_total_epoch 320.0146893467754
Training tokenizer:  73% 5901/8047 [4:47:08<2:32:24,  4.26s/it]loss_total_epoch 320.05597152002156
Training tokenizer:  73% 5902/8047 [4:47:13<2:32:25,  4.26s/it]loss_total_epoch 320.0983752887696
Training tokenizer:  73% 5903/8047 [4:47:17<2:32:20,  4.26s/it]loss_total_epoch 320.1437334585935
Training tokenizer:  73% 5904/8047 [4:47:21<2:32:07,  4.26s/it]loss_total_epoch 320.1745102368295
Training tokenizer:  73% 5905/8047 [4:47:26<2:32:02,  4.26s/it]loss_total_epoch 320.20855213701725
Training tokenizer:  73% 5906/8047 [4:47:30<2:32:53,  4.28s/it]loss_total_epoch 320.2516912817955
Training tokenizer:  73% 5907/8047 [4:47:34<2:32:55,  4.29s/it]loss_total_epoch 320.2909700796008
Training tokenizer:  73% 5908/8047 [4:47:38<2:32:49,  4.29s/it]loss_total_epoch 320.3306211568415
Training tokenizer:  73% 5909/8047 [4:47:43<2:32:31,  4.28s/it]loss_total_epoch 320.3725130036473
Training tokenizer:  73% 5910/8047 [4:47:47<2:31:57,  4.27s/it]loss_total_epoch 320.421411793679
Training tokenizer:  73% 5911/8047 [4:47:51<2:31:56,  4.27s/it]loss_total_epoch 320.466004550457
Training tokenizer:  73% 5912/8047 [4:47:56<2:32:28,  4.28s/it]loss_total_epoch 320.51370344311
Training tokenizer:  73% 5913/8047 [4:48:00<2:32:07,  4.28s/it]loss_total_epoch 320.5601111315191
Training tokenizer:  73% 5914/8047 [4:48:04<2:32:06,  4.28s/it]loss_total_epoch 320.600128185004
Training tokenizer:  74% 5915/8047 [4:48:08<2:32:38,  4.30s/it]loss_total_epoch 320.644474580884
Training tokenizer:  74% 5916/8047 [4:48:13<2:32:16,  4.29s/it]loss_total_epoch 320.6790325567126
Training tokenizer:  74% 5917/8047 [4:48:17<2:31:56,  4.28s/it]loss_total_epoch 320.73289328068495
Training tokenizer:  74% 5918/8047 [4:48:21<2:32:14,  4.29s/it]loss_total_epoch 320.77548018842936
Training tokenizer:  74% 5919/8047 [4:48:26<2:32:09,  4.29s/it]loss_total_epoch 320.8090779967606
Training tokenizer:  74% 5920/8047 [4:48:30<2:31:34,  4.28s/it]loss_total_epoch 320.8507754355669
Training tokenizer:  74% 5921/8047 [4:48:34<2:31:42,  4.28s/it]loss_total_epoch 320.88848312199116
Training tokenizer:  74% 5922/8047 [4:48:38<2:31:27,  4.28s/it]loss_total_epoch 320.9346681907773
Training tokenizer:  74% 5923/8047 [4:48:43<2:32:13,  4.30s/it]loss_total_epoch 320.9795468226075
Training tokenizer:  74% 5924/8047 [4:48:47<2:31:42,  4.29s/it]loss_total_epoch 321.02038007602096
Training tokenizer:  74% 5925/8047 [4:48:51<2:31:24,  4.28s/it]loss_total_epoch 321.06646997481585
Training tokenizer:  74% 5926/8047 [4:48:55<2:31:15,  4.28s/it]loss_total_epoch 321.1013840995729
Training tokenizer:  74% 5927/8047 [4:49:00<2:31:26,  4.29s/it]loss_total_epoch 321.14314607158303
Training tokenizer:  74% 5928/8047 [4:49:04<2:31:11,  4.28s/it]loss_total_epoch 321.1854177862406
Training tokenizer:  74% 5929/8047 [4:49:08<2:31:36,  4.30s/it]loss_total_epoch 321.2292011268437
Training tokenizer:  74% 5930/8047 [4:49:13<2:31:46,  4.30s/it]loss_total_epoch 321.2836537323892
Training tokenizer:  74% 5931/8047 [4:49:17<2:31:30,  4.30s/it]loss_total_epoch 321.3256860934198
Training tokenizer:  74% 5932/8047 [4:49:21<2:31:47,  4.31s/it]loss_total_epoch 321.3671604581177
Training tokenizer:  74% 5933/8047 [4:49:26<2:31:01,  4.29s/it]loss_total_epoch 321.41375025361776
Training tokenizer:  74% 5934/8047 [4:49:30<2:31:34,  4.30s/it]loss_total_epoch 321.45430041477084
Training tokenizer:  74% 5935/8047 [4:49:34<2:30:42,  4.28s/it]loss_total_epoch 321.497852884233
Training tokenizer:  74% 5936/8047 [4:49:38<2:30:13,  4.27s/it]loss_total_epoch 321.53977742418647
Training tokenizer:  74% 5937/8047 [4:49:43<2:30:18,  4.27s/it]loss_total_epoch 321.5773054212332
Training tokenizer:  74% 5938/8047 [4:49:47<2:30:09,  4.27s/it]loss_total_epoch 321.62386014685035
Training tokenizer:  74% 5939/8047 [4:49:51<2:30:23,  4.28s/it]loss_total_epoch 321.6590476371348
Training tokenizer:  74% 5940/8047 [4:49:56<2:30:37,  4.29s/it]loss_total_epoch 321.7007671110332
Training tokenizer:  74% 5941/8047 [4:50:00<2:30:34,  4.29s/it]loss_total_epoch 321.7424535229802
Training tokenizer:  74% 5942/8047 [4:50:04<2:30:05,  4.28s/it]loss_total_epoch 321.7911887615919
Training tokenizer:  74% 5943/8047 [4:50:08<2:29:58,  4.28s/it]loss_total_epoch 321.84287136793137
Training tokenizer:  74% 5944/8047 [4:50:13<2:30:09,  4.28s/it]loss_total_epoch 321.8861723281443
Training tokenizer:  74% 5945/8047 [4:50:17<2:30:32,  4.30s/it]loss_total_epoch 321.92789827659726
Training tokenizer:  74% 5946/8047 [4:50:21<2:29:44,  4.28s/it]loss_total_epoch 321.9783018641174
Training tokenizer:  74% 5947/8047 [4:50:25<2:29:35,  4.27s/it]loss_total_epoch 322.02296882867813
Training tokenizer:  74% 5948/8047 [4:50:30<2:29:46,  4.28s/it]loss_total_epoch 322.07332902401686
Training tokenizer:  74% 5949/8047 [4:50:34<2:30:01,  4.29s/it]loss_total_epoch 322.11801912635565
Training tokenizer:  74% 5950/8047 [4:50:39<2:31:29,  4.33s/it]loss_total_epoch 322.15536142140627
Training tokenizer:  74% 5951/8047 [4:50:43<2:31:01,  4.32s/it]loss_total_epoch 322.20297046005726
Training tokenizer:  74% 5952/8047 [4:50:47<2:30:50,  4.32s/it]loss_total_epoch 322.2534355893731
Training tokenizer:  74% 5953/8047 [4:50:51<2:30:05,  4.30s/it]loss_total_epoch 322.3028835617006
Training tokenizer:  74% 5954/8047 [4:50:56<2:30:38,  4.32s/it]loss_total_epoch 322.34486470371485
Training tokenizer:  74% 5955/8047 [4:51:00<2:30:53,  4.33s/it]loss_total_epoch 322.39109555631876
Training tokenizer:  74% 5956/8047 [4:51:04<2:30:47,  4.33s/it]loss_total_epoch 322.45161839202046
Training tokenizer:  74% 5957/8047 [4:51:09<2:29:47,  4.30s/it]loss_total_epoch 322.48333705589175
Training tokenizer:  74% 5958/8047 [4:51:13<2:29:42,  4.30s/it]loss_total_epoch 322.5247388072312
Training tokenizer:  74% 5959/8047 [4:51:17<2:29:22,  4.29s/it]loss_total_epoch 322.5721394754946
Training tokenizer:  74% 5960/8047 [4:51:22<2:29:10,  4.29s/it]loss_total_epoch 322.6149843260646
Training tokenizer:  74% 5961/8047 [4:51:26<2:28:36,  4.27s/it]loss_total_epoch 322.6575807183981
Training tokenizer:  74% 5962/8047 [4:51:30<2:28:47,  4.28s/it]loss_total_epoch 322.70079267024994
Training tokenizer:  74% 5963/8047 [4:51:34<2:29:19,  4.30s/it]loss_total_epoch 322.73825888708234
Training tokenizer:  74% 5964/8047 [4:51:39<2:29:11,  4.30s/it]loss_total_epoch 322.7895285151899
Training tokenizer:  74% 5965/8047 [4:51:43<2:29:12,  4.30s/it]loss_total_epoch 322.82866817340255
Training tokenizer:  74% 5966/8047 [4:51:47<2:29:20,  4.31s/it]loss_total_epoch 322.8802440799773
Training tokenizer:  74% 5967/8047 [4:51:52<2:28:42,  4.29s/it]loss_total_epoch 322.9221806563437
Training tokenizer:  74% 5968/8047 [4:51:56<2:28:53,  4.30s/it]loss_total_epoch 322.9720651321113
Training tokenizer:  74% 5969/8047 [4:52:00<2:29:17,  4.31s/it]loss_total_epoch 323.00937731936574
Training tokenizer:  74% 5970/8047 [4:52:04<2:28:43,  4.30s/it]loss_total_epoch 323.0537404604256
Training tokenizer:  74% 5971/8047 [4:52:09<2:28:36,  4.30s/it]loss_total_epoch 323.09757366776466
Training tokenizer:  74% 5972/8047 [4:52:13<2:28:39,  4.30s/it]loss_total_epoch 323.1420697309077
Training tokenizer:  74% 5973/8047 [4:52:17<2:28:40,  4.30s/it]loss_total_epoch 323.1867931596935
Training tokenizer:  74% 5974/8047 [4:52:22<2:28:54,  4.31s/it]loss_total_epoch 323.2329893372953
Training tokenizer:  74% 5975/8047 [4:52:26<2:28:38,  4.30s/it]loss_total_epoch 323.2730837389827
Training tokenizer:  74% 5976/8047 [4:52:30<2:29:11,  4.32s/it]loss_total_epoch 323.3179119043052
Training tokenizer:  74% 5977/8047 [4:52:35<2:29:16,  4.33s/it]loss_total_epoch 323.36339747160673
Training tokenizer:  74% 5978/8047 [4:52:39<2:28:45,  4.31s/it]loss_total_epoch 323.40718388929963
Training tokenizer:  74% 5979/8047 [4:52:43<2:28:25,  4.31s/it]loss_total_epoch 323.4516884572804
Training tokenizer:  74% 5980/8047 [4:52:48<2:27:54,  4.29s/it]loss_total_epoch 323.50060183927417
Training tokenizer:  74% 5981/8047 [4:52:52<2:28:03,  4.30s/it]loss_total_epoch 323.538391392678
Training tokenizer:  74% 5982/8047 [4:52:56<2:28:07,  4.30s/it]loss_total_epoch 323.5801611542702
Training tokenizer:  74% 5983/8047 [4:53:00<2:27:29,  4.29s/it]loss_total_epoch 323.6212886199355
Training tokenizer:  74% 5984/8047 [4:53:05<2:27:49,  4.30s/it]loss_total_epoch 323.6701179891825
Training tokenizer:  74% 5985/8047 [4:53:09<2:28:12,  4.31s/it]loss_total_epoch 323.7068779952824
Training tokenizer:  74% 5986/8047 [4:53:13<2:27:46,  4.30s/it]loss_total_epoch 323.74698312580585
Training tokenizer:  74% 5987/8047 [4:53:18<2:27:30,  4.30s/it]loss_total_epoch 323.7908409945667
Training tokenizer:  74% 5988/8047 [4:53:22<2:27:28,  4.30s/it]loss_total_epoch 323.8272222876549
Training tokenizer:  74% 5989/8047 [4:53:26<2:27:11,  4.29s/it]loss_total_epoch 323.8728190474212
Training tokenizer:  74% 5990/8047 [4:53:31<2:27:19,  4.30s/it]loss_total_epoch 323.9166833497584
Training tokenizer:  74% 5991/8047 [4:53:35<2:27:15,  4.30s/it]loss_total_epoch 323.9688261002302
Training tokenizer:  74% 5992/8047 [4:53:39<2:27:18,  4.30s/it]loss_total_epoch 324.0061187259853
Training tokenizer:  74% 5993/8047 [4:53:43<2:27:14,  4.30s/it]loss_total_epoch 324.0522434860468
Training tokenizer:  74% 5994/8047 [4:53:48<2:29:00,  4.36s/it]loss_total_epoch 324.0948916710913
Training tokenizer:  74% 5995/8047 [4:53:52<2:28:29,  4.34s/it]loss_total_epoch 324.1308755017817
Training tokenizer:  75% 5996/8047 [4:53:57<2:27:40,  4.32s/it]loss_total_epoch 324.16340831667185
Training tokenizer:  75% 5997/8047 [4:54:01<2:27:19,  4.31s/it]loss_total_epoch 324.2160435952246
Training tokenizer:  75% 5998/8047 [4:54:05<2:26:51,  4.30s/it]loss_total_epoch 324.26259288564324
Training tokenizer:  75% 5999/8047 [4:54:09<2:26:14,  4.28s/it]loss_total_epoch 324.30217691883445
Training tokenizer:  75% 6000/8047 [4:54:14<2:26:38,  4.30s/it]loss_total_epoch 324.3579563945532
Training tokenizer:  75% 6001/8047 [4:54:18<2:27:02,  4.31s/it]loss_total_epoch 324.3962968811393
Training tokenizer:  75% 6002/8047 [4:54:22<2:26:55,  4.31s/it]loss_total_epoch 324.4369134940207
Training tokenizer:  75% 6003/8047 [4:54:27<2:26:55,  4.31s/it]loss_total_epoch 324.4786242842674
Training tokenizer:  75% 6004/8047 [4:54:31<2:27:15,  4.32s/it]loss_total_epoch 324.53003653883934
Training tokenizer:  75% 6005/8047 [4:54:35<2:27:02,  4.32s/it]loss_total_epoch 324.5719827488065
Training tokenizer:  75% 6006/8047 [4:54:40<2:27:09,  4.33s/it]loss_total_epoch 324.6018754579127
Training tokenizer:  75% 6007/8047 [4:54:44<2:26:38,  4.31s/it]loss_total_epoch 324.63208656571805
Training tokenizer:  75% 6008/8047 [4:54:48<2:27:07,  4.33s/it]loss_total_epoch 324.6651902105659
Training tokenizer:  75% 6009/8047 [4:54:53<2:28:26,  4.37s/it]loss_total_epoch 324.6946550235152
Training tokenizer:  75% 6010/8047 [4:54:57<2:27:40,  4.35s/it]loss_total_epoch 324.74785159155726
Training tokenizer:  75% 6011/8047 [4:55:01<2:27:35,  4.35s/it]loss_total_epoch 324.7926632836461
Training tokenizer:  75% 6012/8047 [4:55:06<2:27:18,  4.34s/it]loss_total_epoch 324.82665630057454
Training tokenizer:  75% 6013/8047 [4:55:10<2:26:10,  4.31s/it]loss_total_epoch 324.87010372057557
Training tokenizer:  75% 6014/8047 [4:55:14<2:26:05,  4.31s/it]loss_total_epoch 324.9130011871457
Training tokenizer:  75% 6015/8047 [4:55:19<2:26:04,  4.31s/it]loss_total_epoch 324.9410670939833
Training tokenizer:  75% 6016/8047 [4:55:23<2:26:01,  4.31s/it]loss_total_epoch 324.9788010921329
Training tokenizer:  75% 6017/8047 [4:55:27<2:25:28,  4.30s/it]loss_total_epoch 325.014576504007
Training tokenizer:  75% 6018/8047 [4:55:32<2:26:08,  4.32s/it]loss_total_epoch 325.0608911346644
Training tokenizer:  75% 6019/8047 [4:55:36<2:25:56,  4.32s/it]loss_total_epoch 325.0963458735496
Training tokenizer:  75% 6020/8047 [4:55:40<2:26:06,  4.32s/it]loss_total_epoch 325.14084344543517
Training tokenizer:  75% 6021/8047 [4:55:45<2:26:15,  4.33s/it]loss_total_epoch 325.1859657522291
Training tokenizer:  75% 6022/8047 [4:55:49<2:26:24,  4.34s/it]loss_total_epoch 325.2267510574311
Training tokenizer:  75% 6023/8047 [4:55:53<2:26:53,  4.35s/it]loss_total_epoch 325.26659922115505
Training tokenizer:  75% 6024/8047 [4:55:58<2:26:35,  4.35s/it]loss_total_epoch 325.3134598303586
Training tokenizer:  75% 6025/8047 [4:56:02<2:26:32,  4.35s/it]loss_total_epoch 325.36094863153994
Training tokenizer:  75% 6026/8047 [4:56:06<2:26:32,  4.35s/it]loss_total_epoch 325.40529025904834
Training tokenizer:  75% 6027/8047 [4:56:11<2:26:19,  4.35s/it]loss_total_epoch 325.4563683178276
Training tokenizer:  75% 6028/8047 [4:56:15<2:26:06,  4.34s/it]loss_total_epoch 325.4919462185353
Training tokenizer:  75% 6029/8047 [4:56:19<2:25:41,  4.33s/it]loss_total_epoch 325.5449348334223
Training tokenizer:  75% 6030/8047 [4:56:24<2:25:50,  4.34s/it]loss_total_epoch 325.588269142434
Training tokenizer:  75% 6031/8047 [4:56:28<2:25:32,  4.33s/it]loss_total_epoch 325.6296903472394
Training tokenizer:  75% 6032/8047 [4:56:32<2:25:25,  4.33s/it]loss_total_epoch 325.6775236222893
Training tokenizer:  75% 6033/8047 [4:56:37<2:25:07,  4.32s/it]loss_total_epoch 325.7192229423672
Training tokenizer:  75% 6034/8047 [4:56:41<2:25:09,  4.33s/it]loss_total_epoch 325.7653034944087
Training tokenizer:  75% 6035/8047 [4:56:45<2:25:34,  4.34s/it]loss_total_epoch 325.81118748895824
Training tokenizer:  75% 6036/8047 [4:56:50<2:24:37,  4.32s/it]loss_total_epoch 325.85512965358794
Training tokenizer:  75% 6037/8047 [4:56:54<2:24:17,  4.31s/it]loss_total_epoch 325.9011577088386
Training tokenizer:  75% 6038/8047 [4:56:58<2:24:19,  4.31s/it]loss_total_epoch 325.941616224125
Training tokenizer:  75% 6039/8047 [4:57:03<2:24:47,  4.33s/it]loss_total_epoch 325.9830636885017
Training tokenizer:  75% 6040/8047 [4:57:07<2:24:30,  4.32s/it]loss_total_epoch 326.02984279207885
Training tokenizer:  75% 6041/8047 [4:57:11<2:24:52,  4.33s/it]loss_total_epoch 326.0735183712095
Training tokenizer:  75% 6042/8047 [4:57:16<2:25:05,  4.34s/it]loss_total_epoch 326.1133136358112
Training tokenizer:  75% 6043/8047 [4:57:20<2:24:40,  4.33s/it]loss_total_epoch 326.1485808622092
Training tokenizer:  75% 6044/8047 [4:57:24<2:24:29,  4.33s/it]loss_total_epoch 326.1942697595805
Training tokenizer:  75% 6045/8047 [4:57:29<2:24:02,  4.32s/it]loss_total_epoch 326.23644831590354
Training tokenizer:  75% 6046/8047 [4:57:33<2:23:58,  4.32s/it]loss_total_epoch 326.2824347447604
Training tokenizer:  75% 6047/8047 [4:57:37<2:23:45,  4.31s/it]loss_total_epoch 326.3370388429612
Training tokenizer:  75% 6048/8047 [4:57:41<2:23:16,  4.30s/it]loss_total_epoch 326.39802973903716
Training tokenizer:  75% 6049/8047 [4:57:46<2:23:31,  4.31s/it]loss_total_epoch 326.4338609781116
Training tokenizer:  75% 6050/8047 [4:57:50<2:23:14,  4.30s/it]loss_total_epoch 326.48797514103353
Training tokenizer:  75% 6051/8047 [4:57:54<2:22:56,  4.30s/it]loss_total_epoch 326.5385073106736
Training tokenizer:  75% 6052/8047 [4:57:59<2:23:12,  4.31s/it]loss_total_epoch 326.57828316278756
Training tokenizer:  75% 6053/8047 [4:58:03<2:23:21,  4.31s/it]loss_total_epoch 326.62813067249954
Training tokenizer:  75% 6054/8047 [4:58:07<2:23:38,  4.32s/it]loss_total_epoch 326.65841744467616
Training tokenizer:  75% 6055/8047 [4:58:12<2:23:37,  4.33s/it]loss_total_epoch 326.70337860286236
Training tokenizer:  75% 6056/8047 [4:58:16<2:23:53,  4.34s/it]loss_total_epoch 326.7470577657223
Training tokenizer:  75% 6057/8047 [4:58:20<2:23:43,  4.33s/it]loss_total_epoch 326.7927500233054
Training tokenizer:  75% 6058/8047 [4:58:25<2:24:18,  4.35s/it]loss_total_epoch 326.8363046273589
Training tokenizer:  75% 6059/8047 [4:58:29<2:23:44,  4.34s/it]loss_total_epoch 326.8841559365392
Training tokenizer:  75% 6060/8047 [4:58:33<2:23:55,  4.35s/it]loss_total_epoch 326.92588359862566
Training tokenizer:  75% 6061/8047 [4:58:38<2:23:54,  4.35s/it]loss_total_epoch 326.96456445008516
Training tokenizer:  75% 6062/8047 [4:58:42<2:23:51,  4.35s/it]loss_total_epoch 327.00747130811214
Training tokenizer:  75% 6063/8047 [4:58:46<2:23:46,  4.35s/it]loss_total_epoch 327.04876239970326
Training tokenizer:  75% 6064/8047 [4:58:51<2:23:44,  4.35s/it]loss_total_epoch 327.09932381287217
Training tokenizer:  75% 6065/8047 [4:58:55<2:22:57,  4.33s/it]loss_total_epoch 327.14083456248045
Training tokenizer:  75% 6066/8047 [4:58:59<2:22:42,  4.32s/it]loss_total_epoch 327.18034448474646
Training tokenizer:  75% 6067/8047 [4:59:04<2:22:43,  4.33s/it]loss_total_epoch 327.21127510257065
Training tokenizer:  75% 6068/8047 [4:59:08<2:22:22,  4.32s/it]loss_total_epoch 327.2548164334148
Training tokenizer:  75% 6069/8047 [4:59:12<2:22:24,  4.32s/it]loss_total_epoch 327.2944243904203
Training tokenizer:  75% 6070/8047 [4:59:17<2:22:07,  4.31s/it]loss_total_epoch 327.3479626905173
Training tokenizer:  75% 6071/8047 [4:59:21<2:22:24,  4.32s/it]loss_total_epoch 327.39257708750665
Training tokenizer:  75% 6072/8047 [4:59:25<2:22:14,  4.32s/it]loss_total_epoch 327.4332567807287
Training tokenizer:  75% 6073/8047 [4:59:30<2:22:07,  4.32s/it]loss_total_epoch 327.47469050623477
Training tokenizer:  75% 6074/8047 [4:59:34<2:22:12,  4.32s/it]loss_total_epoch 327.5218489486724
Training tokenizer:  75% 6075/8047 [4:59:38<2:22:32,  4.34s/it]loss_total_epoch 327.5740617457777
Training tokenizer:  76% 6076/8047 [4:59:43<2:22:38,  4.34s/it]loss_total_epoch 327.60600741766393
Training tokenizer:  76% 6077/8047 [4:59:47<2:22:20,  4.34s/it]loss_total_epoch 327.6436432953924
Training tokenizer:  76% 6078/8047 [4:59:51<2:22:21,  4.34s/it]loss_total_epoch 327.69067951478064
Training tokenizer:  76% 6079/8047 [4:59:56<2:22:39,  4.35s/it]loss_total_epoch 327.7295849043876
Training tokenizer:  76% 6080/8047 [5:00:00<2:22:37,  4.35s/it]loss_total_epoch 327.7769436594099
Training tokenizer:  76% 6081/8047 [5:00:04<2:22:21,  4.34s/it]loss_total_epoch 327.8165409658104
Training tokenizer:  76% 6082/8047 [5:00:09<2:22:35,  4.35s/it]loss_total_epoch 327.85623534582555
Training tokenizer:  76% 6083/8047 [5:00:13<2:22:03,  4.34s/it]loss_total_epoch 327.89989013783634
Training tokenizer:  76% 6084/8047 [5:00:17<2:21:53,  4.34s/it]loss_total_epoch 327.93936368636787
Training tokenizer:  76% 6085/8047 [5:00:22<2:21:55,  4.34s/it]loss_total_epoch 327.9783816356212
Training tokenizer:  76% 6086/8047 [5:00:26<2:22:28,  4.36s/it]loss_total_epoch 328.01601167581975
Training tokenizer:  76% 6087/8047 [5:00:30<2:22:12,  4.35s/it]loss_total_epoch 328.0634121093899
Training tokenizer:  76% 6088/8047 [5:00:35<2:22:03,  4.35s/it]loss_total_epoch 328.1141225900501
Training tokenizer:  76% 6089/8047 [5:00:39<2:22:13,  4.36s/it]loss_total_epoch 328.15340110100806
Training tokenizer:  76% 6090/8047 [5:00:44<2:22:04,  4.36s/it]loss_total_epoch 328.19540516473353
Training tokenizer:  76% 6091/8047 [5:00:48<2:22:19,  4.37s/it]loss_total_epoch 328.23902749828994
Training tokenizer:  76% 6092/8047 [5:00:52<2:22:25,  4.37s/it]loss_total_epoch 328.27193352393806
Training tokenizer:  76% 6093/8047 [5:00:57<2:22:23,  4.37s/it]loss_total_epoch 328.3089454937726
Training tokenizer:  76% 6094/8047 [5:01:01<2:22:16,  4.37s/it]loss_total_epoch 328.3430797699839
Training tokenizer:  76% 6095/8047 [5:01:05<2:21:57,  4.36s/it]loss_total_epoch 328.3765469249338
Training tokenizer:  76% 6096/8047 [5:01:10<2:22:12,  4.37s/it]loss_total_epoch 328.42517128400505
Training tokenizer:  76% 6097/8047 [5:01:14<2:21:47,  4.36s/it]loss_total_epoch 328.46725842542946
Training tokenizer:  76% 6098/8047 [5:01:19<2:21:44,  4.36s/it]loss_total_epoch 328.51327171735466
Training tokenizer:  76% 6099/8047 [5:01:23<2:21:22,  4.35s/it]loss_total_epoch 328.5558929499239
Training tokenizer:  76% 6100/8047 [5:01:27<2:21:13,  4.35s/it]loss_total_epoch 328.6025336328894
Training tokenizer:  76% 6101/8047 [5:01:32<2:21:26,  4.36s/it]loss_total_epoch 328.6353830266744
Training tokenizer:  76% 6102/8047 [5:01:36<2:21:11,  4.36s/it]loss_total_epoch 328.67957318387926
Training tokenizer:  76% 6103/8047 [5:01:40<2:21:37,  4.37s/it]loss_total_epoch 328.72750280611217
Training tokenizer:  76% 6104/8047 [5:01:45<2:21:22,  4.37s/it]loss_total_epoch 328.7730100993067
Training tokenizer:  76% 6105/8047 [5:01:49<2:21:45,  4.38s/it]loss_total_epoch 328.8145332913846
Training tokenizer:  76% 6106/8047 [5:01:53<2:21:33,  4.38s/it]loss_total_epoch 328.8568738680333
Training tokenizer:  76% 6107/8047 [5:01:58<2:21:20,  4.37s/it]loss_total_epoch 328.8957708571106
Training tokenizer:  76% 6108/8047 [5:02:02<2:20:50,  4.36s/it]loss_total_epoch 328.937144337222
Training tokenizer:  76% 6109/8047 [5:02:07<2:20:56,  4.36s/it]loss_total_epoch 328.9818638358265
Training tokenizer:  76% 6110/8047 [5:02:11<2:21:11,  4.37s/it]loss_total_epoch 329.0216778423637
Training tokenizer:  76% 6111/8047 [5:02:15<2:20:49,  4.36s/it]loss_total_epoch 329.0796656589955
Training tokenizer:  76% 6112/8047 [5:02:20<2:21:16,  4.38s/it]loss_total_epoch 329.1312973815948
Training tokenizer:  76% 6113/8047 [5:02:24<2:21:08,  4.38s/it]loss_total_epoch 329.1686161328107
Training tokenizer:  76% 6114/8047 [5:02:28<2:21:00,  4.38s/it]loss_total_epoch 329.20483806170523
Training tokenizer:  76% 6115/8047 [5:02:33<2:20:58,  4.38s/it]loss_total_epoch 329.25466697476804
Training tokenizer:  76% 6116/8047 [5:02:37<2:20:38,  4.37s/it]loss_total_epoch 329.29821930266917
Training tokenizer:  76% 6117/8047 [5:02:42<2:20:27,  4.37s/it]loss_total_epoch 329.34755706600845
Training tokenizer:  76% 6118/8047 [5:02:46<2:20:00,  4.36s/it]loss_total_epoch 329.3947445843369
Training tokenizer:  76% 6119/8047 [5:02:50<2:19:40,  4.35s/it]loss_total_epoch 329.44506748206913
Training tokenizer:  76% 6120/8047 [5:02:55<2:19:46,  4.35s/it]loss_total_epoch 329.4846466872841
Training tokenizer:  76% 6121/8047 [5:02:59<2:20:03,  4.36s/it]loss_total_epoch 329.52603381313384
Training tokenizer:  76% 6122/8047 [5:03:03<2:20:35,  4.38s/it]loss_total_epoch 329.5727066602558
Training tokenizer:  76% 6123/8047 [5:03:08<2:20:18,  4.38s/it]loss_total_epoch 329.60672078095376
Training tokenizer:  76% 6124/8047 [5:03:12<2:20:05,  4.37s/it]loss_total_epoch 329.64688323251903
Training tokenizer:  76% 6125/8047 [5:03:16<2:20:25,  4.38s/it]loss_total_epoch 329.6851795259863
Training tokenizer:  76% 6126/8047 [5:03:21<2:19:53,  4.37s/it]loss_total_epoch 329.72046571038663
Training tokenizer:  76% 6127/8047 [5:03:25<2:19:33,  4.36s/it]loss_total_epoch 329.7688271161169
Training tokenizer:  76% 6128/8047 [5:03:30<2:19:30,  4.36s/it]loss_total_epoch 329.8203091416508
Training tokenizer:  76% 6129/8047 [5:03:34<2:19:42,  4.37s/it]loss_total_epoch 329.8664762172848
Training tokenizer:  76% 6130/8047 [5:03:38<2:19:31,  4.37s/it]loss_total_epoch 329.90977202914655
Training tokenizer:  76% 6131/8047 [5:03:43<2:19:37,  4.37s/it]loss_total_epoch 329.94926456175745
Training tokenizer:  76% 6132/8047 [5:03:47<2:19:50,  4.38s/it]loss_total_epoch 329.9882358144969
Training tokenizer:  76% 6133/8047 [5:03:51<2:19:19,  4.37s/it]loss_total_epoch 330.0329957101494
Training tokenizer:  76% 6134/8047 [5:03:56<2:19:25,  4.37s/it]loss_total_epoch 330.0722173769027
Training tokenizer:  76% 6135/8047 [5:04:00<2:19:38,  4.38s/it]loss_total_epoch 330.1130477990955
Training tokenizer:  76% 6136/8047 [5:04:05<2:19:42,  4.39s/it]loss_total_epoch 330.15454853512347
Training tokenizer:  76% 6137/8047 [5:04:09<2:19:43,  4.39s/it]loss_total_epoch 330.1937041748315
Training tokenizer:  76% 6138/8047 [5:04:13<2:19:01,  4.37s/it]loss_total_epoch 330.23726011253893
Training tokenizer:  76% 6139/8047 [5:04:18<2:18:44,  4.36s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-c6v5xh9_'
loss_total_epoch 330.2756649386138
Training tokenizer:  76% 6140/8047 [5:04:22<2:18:31,  4.36s/it]loss_total_epoch 330.3210325744003
Training tokenizer:  76% 6141/8047 [5:04:26<2:18:13,  4.35s/it]loss_total_epoch 330.3666655626148
Training tokenizer:  76% 6142/8047 [5:04:31<2:18:31,  4.36s/it]loss_total_epoch 330.4177570473403
Training tokenizer:  76% 6143/8047 [5:04:35<2:18:26,  4.36s/it]loss_total_epoch 330.4717272091657
Training tokenizer:  76% 6144/8047 [5:04:39<2:18:26,  4.36s/it]loss_total_epoch 330.51489864103496
Training tokenizer:  76% 6145/8047 [5:04:44<2:17:40,  4.34s/it]loss_total_epoch 330.55281274951994
Training tokenizer:  76% 6146/8047 [5:04:48<2:18:54,  4.38s/it]loss_total_epoch 330.5887816976756
Training tokenizer:  76% 6147/8047 [5:04:53<2:18:53,  4.39s/it]loss_total_epoch 330.62843625433743
Training tokenizer:  76% 6148/8047 [5:04:57<2:18:41,  4.38s/it]loss_total_epoch 330.67502943240106
Training tokenizer:  76% 6149/8047 [5:05:01<2:18:24,  4.38s/it]loss_total_epoch 330.7271672729403
Training tokenizer:  76% 6150/8047 [5:05:06<2:18:25,  4.38s/it]loss_total_epoch 330.7696458417922
Training tokenizer:  76% 6151/8047 [5:05:10<2:18:05,  4.37s/it]loss_total_epoch 330.8060411233455
Training tokenizer:  76% 6152/8047 [5:05:15<2:18:22,  4.38s/it]loss_total_epoch 330.8483756426722
Training tokenizer:  76% 6153/8047 [5:05:19<2:18:24,  4.38s/it]loss_total_epoch 330.8999268319458
Training tokenizer:  76% 6154/8047 [5:05:23<2:17:38,  4.36s/it]loss_total_epoch 330.9449582528323
Training tokenizer:  76% 6155/8047 [5:05:28<2:17:31,  4.36s/it]loss_total_epoch 330.99642076157033
Training tokenizer:  77% 6156/8047 [5:05:32<2:17:27,  4.36s/it]loss_total_epoch 331.03899591974914
Training tokenizer:  77% 6157/8047 [5:05:36<2:17:16,  4.36s/it]loss_total_epoch 331.078811949119
Training tokenizer:  77% 6158/8047 [5:05:41<2:17:39,  4.37s/it]loss_total_epoch 331.12574029527605
Training tokenizer:  77% 6159/8047 [5:05:45<2:17:31,  4.37s/it]loss_total_epoch 331.16678114049137
Training tokenizer:  77% 6160/8047 [5:05:50<2:19:32,  4.44s/it]loss_total_epoch 331.2118514981121
Training tokenizer:  77% 6161/8047 [5:05:54<2:19:08,  4.43s/it]loss_total_epoch 331.25960684008896
Training tokenizer:  77% 6162/8047 [5:05:58<2:18:27,  4.41s/it]loss_total_epoch 331.2946435455233
Training tokenizer:  77% 6163/8047 [5:06:03<2:18:29,  4.41s/it]loss_total_epoch 331.33739247359335
Training tokenizer:  77% 6164/8047 [5:06:07<2:17:52,  4.39s/it]loss_total_epoch 331.3747395630926
Training tokenizer:  77% 6165/8047 [5:06:12<2:17:07,  4.37s/it]loss_total_epoch 331.4212200175971
Training tokenizer:  77% 6166/8047 [5:06:16<2:16:56,  4.37s/it]loss_total_epoch 331.4681306127459
Training tokenizer:  77% 6167/8047 [5:06:20<2:16:47,  4.37s/it]loss_total_epoch 331.5142514500767
Training tokenizer:  77% 6168/8047 [5:06:25<2:16:44,  4.37s/it]loss_total_epoch 331.5550719667226
Training tokenizer:  77% 6169/8047 [5:06:29<2:17:51,  4.40s/it]loss_total_epoch 331.5935156773776
Training tokenizer:  77% 6170/8047 [5:06:33<2:17:37,  4.40s/it]loss_total_epoch 331.6327420528978
Training tokenizer:  77% 6171/8047 [5:06:38<2:17:24,  4.39s/it]loss_total_epoch 331.67153712548316
Training tokenizer:  77% 6172/8047 [5:06:42<2:16:46,  4.38s/it]loss_total_epoch 331.711379179731
Training tokenizer:  77% 6173/8047 [5:06:47<2:16:28,  4.37s/it]loss_total_epoch 331.7555800061673
Training tokenizer:  77% 6174/8047 [5:06:51<2:16:44,  4.38s/it]loss_total_epoch 331.8023629244417
Training tokenizer:  77% 6175/8047 [5:06:55<2:16:21,  4.37s/it]loss_total_epoch 331.84237039647996
Training tokenizer:  77% 6176/8047 [5:07:00<2:16:16,  4.37s/it]loss_total_epoch 331.8887135293335
Training tokenizer:  77% 6177/8047 [5:07:04<2:16:35,  4.38s/it]loss_total_epoch 331.92708849720657
Training tokenizer:  77% 6178/8047 [5:07:08<2:16:46,  4.39s/it]loss_total_epoch 331.9659312982112
Training tokenizer:  77% 6179/8047 [5:07:13<2:17:04,  4.40s/it]loss_total_epoch 332.00366914458573
Training tokenizer:  77% 6180/8047 [5:07:17<2:17:11,  4.41s/it]loss_total_epoch 332.0479813795537
Training tokenizer:  77% 6181/8047 [5:07:22<2:17:23,  4.42s/it]loss_total_epoch 332.08864182792604
Training tokenizer:  77% 6182/8047 [5:07:26<2:16:54,  4.40s/it]loss_total_epoch 332.1334426160902
Training tokenizer:  77% 6183/8047 [5:07:31<2:16:45,  4.40s/it]loss_total_epoch 332.17702720128
Training tokenizer:  77% 6184/8047 [5:07:35<2:16:36,  4.40s/it]loss_total_epoch 332.221539163962
Training tokenizer:  77% 6185/8047 [5:07:39<2:16:18,  4.39s/it]loss_total_epoch 332.2640700209886
Training tokenizer:  77% 6186/8047 [5:07:44<2:16:21,  4.40s/it]loss_total_epoch 332.2979307938367
Training tokenizer:  77% 6187/8047 [5:07:48<2:15:56,  4.39s/it]loss_total_epoch 332.34302491880953
Training tokenizer:  77% 6188/8047 [5:07:52<2:15:41,  4.38s/it]loss_total_epoch 332.38822936452925
Training tokenizer:  77% 6189/8047 [5:07:57<2:16:08,  4.40s/it]loss_total_epoch 332.4353077020496
Training tokenizer:  77% 6190/8047 [5:08:01<2:16:05,  4.40s/it]loss_total_epoch 332.4854454379529
Training tokenizer:  77% 6191/8047 [5:08:06<2:15:59,  4.40s/it]loss_total_epoch 332.52659634687006
Training tokenizer:  77% 6192/8047 [5:08:10<2:16:07,  4.40s/it]loss_total_epoch 332.5719350744039
Training tokenizer:  77% 6193/8047 [5:08:14<2:15:52,  4.40s/it]loss_total_epoch 332.60621933452785
Training tokenizer:  77% 6194/8047 [5:08:19<2:15:43,  4.39s/it]loss_total_epoch 332.645489981398
Training tokenizer:  77% 6195/8047 [5:08:23<2:16:23,  4.42s/it]loss_total_epoch 332.68266452662647
Training tokenizer:  77% 6196/8047 [5:08:28<2:16:15,  4.42s/it]loss_total_epoch 332.7274353187531
Training tokenizer:  77% 6197/8047 [5:08:32<2:16:04,  4.41s/it]loss_total_epoch 332.7709219735116
Training tokenizer:  77% 6198/8047 [5:08:37<2:16:09,  4.42s/it]loss_total_epoch 332.80930216796696
Training tokenizer:  77% 6199/8047 [5:08:41<2:15:43,  4.41s/it]loss_total_epoch 332.84942367486656
Training tokenizer:  77% 6200/8047 [5:08:45<2:15:20,  4.40s/it]loss_total_epoch 332.8945218939334
Training tokenizer:  77% 6201/8047 [5:08:50<2:15:52,  4.42s/it]loss_total_epoch 332.93509192205966
Training tokenizer:  77% 6202/8047 [5:08:54<2:16:18,  4.43s/it]loss_total_epoch 332.97949414886534
Training tokenizer:  77% 6203/8047 [5:08:59<2:16:10,  4.43s/it]loss_total_epoch 333.0212155301124
Training tokenizer:  77% 6204/8047 [5:09:03<2:15:28,  4.41s/it]loss_total_epoch 333.0614074487239
Training tokenizer:  77% 6205/8047 [5:09:08<2:15:46,  4.42s/it]loss_total_epoch 333.1107797008008
Training tokenizer:  77% 6206/8047 [5:09:12<2:15:26,  4.41s/it]loss_total_epoch 333.1505829375237
Training tokenizer:  77% 6207/8047 [5:09:16<2:14:04,  4.37s/it]loss_total_epoch 333.1864286866039
Training tokenizer:  77% 6208/8047 [5:09:21<2:14:23,  4.38s/it]loss_total_epoch 333.22577152587473
Training tokenizer:  77% 6209/8047 [5:09:25<2:14:36,  4.39s/it]loss_total_epoch 333.2548424638808
Training tokenizer:  77% 6210/8047 [5:09:29<2:13:29,  4.36s/it]loss_total_epoch 333.30228984728456
Training tokenizer:  77% 6211/8047 [5:09:34<2:13:52,  4.38s/it]loss_total_epoch 333.3411325253546
Training tokenizer:  77% 6212/8047 [5:09:38<2:14:22,  4.39s/it]loss_total_epoch 333.39411660656333
Training tokenizer:  77% 6213/8047 [5:09:43<2:14:16,  4.39s/it]loss_total_epoch 333.4418868124485
Training tokenizer:  77% 6214/8047 [5:09:47<2:14:04,  4.39s/it]loss_total_epoch 333.4849828183651
Training tokenizer:  77% 6215/8047 [5:09:51<2:14:05,  4.39s/it]loss_total_epoch 333.5284873060882
Training tokenizer:  77% 6216/8047 [5:09:56<2:14:04,  4.39s/it]loss_total_epoch 333.56469843536615
Training tokenizer:  77% 6217/8047 [5:10:00<2:14:06,  4.40s/it]loss_total_epoch 333.6082643195987
Training tokenizer:  77% 6218/8047 [5:10:05<2:14:58,  4.43s/it]loss_total_epoch 333.66181219369173
Training tokenizer:  77% 6219/8047 [5:10:09<2:15:11,  4.44s/it]loss_total_epoch 333.70614076033235
Training tokenizer:  77% 6220/8047 [5:10:13<2:14:57,  4.43s/it]loss_total_epoch 333.7495726644993
Training tokenizer:  77% 6221/8047 [5:10:18<2:14:47,  4.43s/it]loss_total_epoch 333.7952758073807
Training tokenizer:  77% 6222/8047 [5:10:22<2:14:45,  4.43s/it]loss_total_epoch 333.8408051133156
Training tokenizer:  77% 6223/8047 [5:10:27<2:14:50,  4.44s/it]loss_total_epoch 333.88932522013783
Training tokenizer:  77% 6224/8047 [5:10:31<2:15:08,  4.45s/it]loss_total_epoch 333.9384091719985
Training tokenizer:  77% 6225/8047 [5:10:36<2:14:52,  4.44s/it]loss_total_epoch 333.98258159682155
Training tokenizer:  77% 6226/8047 [5:10:40<2:14:26,  4.43s/it]loss_total_epoch 334.0342161767185
Training tokenizer:  77% 6227/8047 [5:10:45<2:14:32,  4.44s/it]loss_total_epoch 334.07509431988
Training tokenizer:  77% 6228/8047 [5:10:49<2:14:31,  4.44s/it]loss_total_epoch 334.12074295431376
Training tokenizer:  77% 6229/8047 [5:10:53<2:12:39,  4.38s/it]loss_total_epoch 334.1645172163844
Training tokenizer:  77% 6230/8047 [5:10:58<2:12:49,  4.39s/it]loss_total_epoch 334.2004224136472
Training tokenizer:  77% 6231/8047 [5:11:02<2:13:00,  4.39s/it]loss_total_epoch 334.2391651533544
Training tokenizer:  77% 6232/8047 [5:11:06<2:13:08,  4.40s/it]loss_total_epoch 334.2824731245637
Training tokenizer:  77% 6233/8047 [5:11:11<2:13:00,  4.40s/it]loss_total_epoch 334.3207934498787
Training tokenizer:  77% 6234/8047 [5:11:15<2:13:07,  4.41s/it]loss_total_epoch 334.36245533823967
Training tokenizer:  77% 6235/8047 [5:11:20<2:12:46,  4.40s/it]loss_total_epoch 334.40317828580737
Training tokenizer:  77% 6236/8047 [5:11:24<2:13:05,  4.41s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-72xku37u'
loss_total_epoch 334.4450879059732
Training tokenizer:  78% 6237/8047 [5:11:29<2:13:15,  4.42s/it]loss_total_epoch 334.4902127534151
Training tokenizer:  78% 6238/8047 [5:11:33<2:12:55,  4.41s/it]loss_total_epoch 334.5349721722305
Training tokenizer:  78% 6239/8047 [5:11:37<2:13:06,  4.42s/it]loss_total_epoch 334.5774422958493
Training tokenizer:  78% 6240/8047 [5:11:42<2:12:59,  4.42s/it]loss_total_epoch 334.61668261885643
Training tokenizer:  78% 6241/8047 [5:11:46<2:13:03,  4.42s/it]loss_total_epoch 334.6634478904307
Training tokenizer:  78% 6242/8047 [5:11:51<2:13:17,  4.43s/it]loss_total_epoch 334.70380018651485
Training tokenizer:  78% 6243/8047 [5:11:55<2:12:25,  4.40s/it]loss_total_epoch 334.74825965985656
Training tokenizer:  78% 6244/8047 [5:11:59<2:12:35,  4.41s/it]loss_total_epoch 334.7843890301883
Training tokenizer:  78% 6245/8047 [5:12:04<2:13:02,  4.43s/it]loss_total_epoch 334.8263131529093
Training tokenizer:  78% 6246/8047 [5:12:08<2:12:47,  4.42s/it]loss_total_epoch 334.87105384096503
Training tokenizer:  78% 6247/8047 [5:12:13<2:12:45,  4.43s/it]loss_total_epoch 334.9025428853929
Training tokenizer:  78% 6248/8047 [5:12:17<2:12:38,  4.42s/it]loss_total_epoch 334.9516110792756
Training tokenizer:  78% 6249/8047 [5:12:22<2:12:20,  4.42s/it]loss_total_epoch 334.9850445985794
Training tokenizer:  78% 6250/8047 [5:12:26<2:12:23,  4.42s/it]loss_total_epoch 335.0271914228797
Training tokenizer:  78% 6251/8047 [5:12:30<2:12:34,  4.43s/it]loss_total_epoch 335.067830555141
Training tokenizer:  78% 6252/8047 [5:12:35<2:12:15,  4.42s/it]loss_total_epoch 335.11149302870035
Training tokenizer:  78% 6253/8047 [5:12:39<2:12:41,  4.44s/it]loss_total_epoch 335.15379267930984
Training tokenizer:  78% 6254/8047 [5:12:44<2:12:17,  4.43s/it]loss_total_epoch 335.1994469836354
Training tokenizer:  78% 6255/8047 [5:12:48<2:12:27,  4.43s/it]loss_total_epoch 335.24161135032773
Training tokenizer:  78% 6256/8047 [5:12:53<2:11:43,  4.41s/it]loss_total_epoch 335.28542044758797
Training tokenizer:  78% 6257/8047 [5:12:57<2:11:59,  4.42s/it]loss_total_epoch 335.3250009045005
Training tokenizer:  78% 6258/8047 [5:13:01<2:12:01,  4.43s/it]loss_total_epoch 335.3678958080709
Training tokenizer:  78% 6259/8047 [5:13:06<2:12:28,  4.45s/it]loss_total_epoch 335.41728449240327
Training tokenizer:  78% 6260/8047 [5:13:10<2:12:11,  4.44s/it]loss_total_epoch 335.45850106328726
Training tokenizer:  78% 6261/8047 [5:13:15<2:12:11,  4.44s/it]loss_total_epoch 335.4942523278296
Training tokenizer:  78% 6262/8047 [5:13:19<2:12:13,  4.44s/it]loss_total_epoch 335.5439022779465
Training tokenizer:  78% 6263/8047 [5:13:24<2:12:04,  4.44s/it]loss_total_epoch 335.5975251942873
Training tokenizer:  78% 6264/8047 [5:13:28<2:11:47,  4.43s/it]loss_total_epoch 335.63864931836724
Training tokenizer:  78% 6265/8047 [5:13:33<2:11:50,  4.44s/it]loss_total_epoch 335.68361048027873
Training tokenizer:  78% 6266/8047 [5:13:37<2:11:48,  4.44s/it]loss_total_epoch 335.72506565600634
Training tokenizer:  78% 6267/8047 [5:13:41<2:11:18,  4.43s/it]loss_total_epoch 335.7675802856684
Training tokenizer:  78% 6268/8047 [5:13:46<2:11:19,  4.43s/it]loss_total_epoch 335.8133359514177
Training tokenizer:  78% 6269/8047 [5:13:50<2:11:04,  4.42s/it]loss_total_epoch 335.8535277917981
Training tokenizer:  78% 6270/8047 [5:13:55<2:10:58,  4.42s/it]loss_total_epoch 335.89685882627964
Training tokenizer:  78% 6271/8047 [5:13:59<2:11:05,  4.43s/it]loss_total_epoch 335.9390220269561
Training tokenizer:  78% 6272/8047 [5:14:04<2:11:05,  4.43s/it]loss_total_epoch 335.97146463766694
Training tokenizer:  78% 6273/8047 [5:14:08<2:11:09,  4.44s/it]loss_total_epoch 336.0130940862
Training tokenizer:  78% 6274/8047 [5:14:12<2:11:01,  4.43s/it]loss_total_epoch 336.04436057060957
Training tokenizer:  78% 6275/8047 [5:14:17<2:10:48,  4.43s/it]loss_total_epoch 336.0808598846197
Training tokenizer:  78% 6276/8047 [5:14:21<2:11:18,  4.45s/it]loss_total_epoch 336.13204980269074
Training tokenizer:  78% 6277/8047 [5:14:26<2:11:33,  4.46s/it]loss_total_epoch 336.1677237302065
Training tokenizer:  78% 6278/8047 [5:14:30<2:11:04,  4.45s/it]loss_total_epoch 336.21398343890905
Training tokenizer:  78% 6279/8047 [5:14:35<2:10:45,  4.44s/it]loss_total_epoch 336.25379414856434
Training tokenizer:  78% 6280/8047 [5:14:39<2:10:44,  4.44s/it]loss_total_epoch 336.2895907461643
Training tokenizer:  78% 6281/8047 [5:14:44<2:10:40,  4.44s/it]loss_total_epoch 336.3306405991316
Training tokenizer:  78% 6282/8047 [5:14:48<2:10:28,  4.44s/it]loss_total_epoch 336.37781258299947
Training tokenizer:  78% 6283/8047 [5:14:52<2:10:08,  4.43s/it]loss_total_epoch 336.41189789026976
Training tokenizer:  78% 6284/8047 [5:14:57<2:10:02,  4.43s/it]loss_total_epoch 336.45782586559653
Training tokenizer:  78% 6285/8047 [5:15:01<2:09:36,  4.41s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-o9knbfdw'
loss_total_epoch 336.5079205930233
Training tokenizer:  78% 6286/8047 [5:15:06<2:10:09,  4.43s/it]loss_total_epoch 336.5452941060066
Training tokenizer:  78% 6287/8047 [5:15:10<2:09:58,  4.43s/it]loss_total_epoch 336.5911682918668
Training tokenizer:  78% 6288/8047 [5:15:15<2:10:05,  4.44s/it]loss_total_epoch 336.63469322770834
Training tokenizer:  78% 6289/8047 [5:15:19<2:09:58,  4.44s/it]loss_total_epoch 336.68320447951555
Training tokenizer:  78% 6290/8047 [5:15:23<2:09:49,  4.43s/it]loss_total_epoch 336.7211524248123
Training tokenizer:  78% 6291/8047 [5:15:28<2:10:14,  4.45s/it]loss_total_epoch 336.76015324145555
Training tokenizer:  78% 6292/8047 [5:15:32<2:09:48,  4.44s/it]loss_total_epoch 336.7994976118207
Training tokenizer:  78% 6293/8047 [5:15:37<2:09:57,  4.45s/it]loss_total_epoch 336.84309682250023
Training tokenizer:  78% 6294/8047 [5:15:41<2:09:53,  4.45s/it]loss_total_epoch 336.8866925239563
Training tokenizer:  78% 6295/8047 [5:15:46<2:09:28,  4.43s/it]loss_total_epoch 336.9158181305975
Training tokenizer:  78% 6296/8047 [5:15:50<2:09:19,  4.43s/it]loss_total_epoch 336.9583589155227
Training tokenizer:  78% 6297/8047 [5:15:54<2:09:34,  4.44s/it]loss_total_epoch 336.9960951898247
Training tokenizer:  78% 6298/8047 [5:15:59<2:09:13,  4.43s/it]loss_total_epoch 337.03881451301277
Training tokenizer:  78% 6299/8047 [5:16:03<2:09:50,  4.46s/it]loss_total_epoch 337.0726432967931
Training tokenizer:  78% 6300/8047 [5:16:08<2:09:21,  4.44s/it]loss_total_epoch 337.11625348962843
Training tokenizer:  78% 6301/8047 [5:16:12<2:09:28,  4.45s/it]loss_total_epoch 337.15801186300814
Training tokenizer:  78% 6302/8047 [5:16:17<2:09:27,  4.45s/it]loss_total_epoch 337.2019639220089
Training tokenizer:  78% 6303/8047 [5:16:21<2:08:02,  4.41s/it]loss_total_epoch 337.2398987803608
Training tokenizer:  78% 6304/8047 [5:16:26<2:08:42,  4.43s/it]loss_total_epoch 337.2800484728068
Training tokenizer:  78% 6305/8047 [5:16:30<2:09:02,  4.44s/it]loss_total_epoch 337.31699241138995
Training tokenizer:  78% 6306/8047 [5:16:34<2:09:15,  4.45s/it]loss_total_epoch 337.35965139977634
Training tokenizer:  78% 6307/8047 [5:16:39<2:09:11,  4.46s/it]loss_total_epoch 337.4024726655334
Training tokenizer:  78% 6308/8047 [5:16:43<2:09:09,  4.46s/it]loss_total_epoch 337.4421943705529
Training tokenizer:  78% 6309/8047 [5:16:48<2:09:04,  4.46s/it]loss_total_epoch 337.4797426927835
Training tokenizer:  78% 6310/8047 [5:16:52<2:09:02,  4.46s/it]loss_total_epoch 337.51054443232715
Training tokenizer:  78% 6311/8047 [5:16:57<2:09:09,  4.46s/it]loss_total_epoch 337.54890143312514
Training tokenizer:  78% 6312/8047 [5:17:01<2:08:37,  4.45s/it]loss_total_epoch 337.5970737207681
Training tokenizer:  78% 6313/8047 [5:17:06<2:08:30,  4.45s/it]loss_total_epoch 337.63419417478144
Training tokenizer:  78% 6314/8047 [5:17:10<2:08:14,  4.44s/it]loss_total_epoch 337.6875989343971
Training tokenizer:  78% 6315/8047 [5:17:15<2:08:33,  4.45s/it]loss_total_epoch 337.7236500773579
Training tokenizer:  78% 6316/8047 [5:17:19<2:08:44,  4.46s/it]loss_total_epoch 337.76081515289843
Training tokenizer:  79% 6317/8047 [5:17:24<2:08:59,  4.47s/it]loss_total_epoch 337.8012420255691
Training tokenizer:  79% 6318/8047 [5:17:28<2:08:39,  4.46s/it]loss_total_epoch 337.8477989491075
Training tokenizer:  79% 6319/8047 [5:17:32<2:08:21,  4.46s/it]loss_total_epoch 337.8903275858611
Training tokenizer:  79% 6320/8047 [5:17:37<2:09:16,  4.49s/it]loss_total_epoch 337.9386665839702
Training tokenizer:  79% 6321/8047 [5:17:41<2:08:35,  4.47s/it]loss_total_epoch 337.98837781883776
Training tokenizer:  79% 6322/8047 [5:17:46<2:08:50,  4.48s/it]loss_total_epoch 338.0367360431701
Training tokenizer:  79% 6323/8047 [5:17:50<2:08:23,  4.47s/it]loss_total_epoch 338.0810373481363
Training tokenizer:  79% 6324/8047 [5:17:55<2:08:55,  4.49s/it]loss_total_epoch 338.1300291251391
Training tokenizer:  79% 6325/8047 [5:17:59<2:08:54,  4.49s/it]loss_total_epoch 338.16588464193046
Training tokenizer:  79% 6326/8047 [5:18:04<2:08:56,  4.50s/it]loss_total_epoch 338.211975550279
Training tokenizer:  79% 6327/8047 [5:18:08<2:08:26,  4.48s/it]loss_total_epoch 338.25871211476624
Training tokenizer:  79% 6328/8047 [5:18:13<2:08:20,  4.48s/it]loss_total_epoch 338.29890461079776
Training tokenizer:  79% 6329/8047 [5:18:17<2:08:19,  4.48s/it]loss_total_epoch 338.3399620819837
Training tokenizer:  79% 6330/8047 [5:18:22<2:07:42,  4.46s/it]loss_total_epoch 338.3903866726905
Training tokenizer:  79% 6331/8047 [5:18:26<2:07:38,  4.46s/it]loss_total_epoch 338.43428611569107
Training tokenizer:  79% 6332/8047 [5:18:31<2:08:17,  4.49s/it]loss_total_epoch 338.4772917646915
Training tokenizer:  79% 6333/8047 [5:18:35<2:07:46,  4.47s/it]loss_total_epoch 338.5159287583083
Training tokenizer:  79% 6334/8047 [5:18:40<2:07:52,  4.48s/it]loss_total_epoch 338.55323950760067
Training tokenizer:  79% 6335/8047 [5:18:44<2:07:45,  4.48s/it]loss_total_epoch 338.5960796419531
Training tokenizer:  79% 6336/8047 [5:18:49<2:07:57,  4.49s/it]loss_total_epoch 338.6332971807569
Training tokenizer:  79% 6337/8047 [5:18:53<2:08:21,  4.50s/it]loss_total_epoch 338.67321781255305
Training tokenizer:  79% 6338/8047 [5:18:58<2:08:24,  4.51s/it]loss_total_epoch 338.7101086359471
Training tokenizer:  79% 6339/8047 [5:19:02<2:07:37,  4.48s/it]loss_total_epoch 338.74501158855855
Training tokenizer:  79% 6340/8047 [5:19:07<2:07:35,  4.48s/it]loss_total_epoch 338.77949066646397
Training tokenizer:  79% 6341/8047 [5:19:11<2:07:35,  4.49s/it]loss_total_epoch 338.8276172671467
Training tokenizer:  79% 6342/8047 [5:19:16<2:07:32,  4.49s/it]loss_total_epoch 338.8717720154673
Training tokenizer:  79% 6343/8047 [5:19:20<2:07:41,  4.50s/it]loss_total_epoch 338.907870138064
Training tokenizer:  79% 6344/8047 [5:19:25<2:07:21,  4.49s/it]loss_total_epoch 338.9519187156111
Training tokenizer:  79% 6345/8047 [5:19:29<2:07:30,  4.49s/it]loss_total_epoch 338.9964207392186
Training tokenizer:  79% 6346/8047 [5:19:34<2:07:34,  4.50s/it]loss_total_epoch 339.0428460147232
Training tokenizer:  79% 6347/8047 [5:19:38<2:07:22,  4.50s/it]loss_total_epoch 339.08543454296887
Training tokenizer:  79% 6348/8047 [5:19:43<2:07:10,  4.49s/it]loss_total_epoch 339.13344275020063
Training tokenizer:  79% 6349/8047 [5:19:47<2:07:14,  4.50s/it]loss_total_epoch 339.170691376552
Training tokenizer:  79% 6350/8047 [5:19:52<2:06:57,  4.49s/it]loss_total_epoch 339.2061632219702
Training tokenizer:  79% 6351/8047 [5:19:56<2:07:00,  4.49s/it]loss_total_epoch 339.2518462520093
Training tokenizer:  79% 6352/8047 [5:20:01<2:06:40,  4.48s/it]loss_total_epoch 339.2884023543447
Training tokenizer:  79% 6353/8047 [5:20:05<2:06:39,  4.49s/it]loss_total_epoch 339.3249138649553
Training tokenizer:  79% 6354/8047 [5:20:09<2:05:35,  4.45s/it]loss_total_epoch 339.37542220391333
Training tokenizer:  79% 6355/8047 [5:20:14<2:06:18,  4.48s/it]loss_total_epoch 339.41426136158407
Training tokenizer:  79% 6356/8047 [5:20:18<2:06:24,  4.49s/it]loss_total_epoch 339.4540567304939
Training tokenizer:  79% 6357/8047 [5:20:23<2:06:19,  4.48s/it]loss_total_epoch 339.4972364138812
Training tokenizer:  79% 6358/8047 [5:20:27<2:06:20,  4.49s/it]loss_total_epoch 339.54291115142405
Training tokenizer:  79% 6359/8047 [5:20:32<2:06:19,  4.49s/it]loss_total_epoch 339.59776070155203
Training tokenizer:  79% 6360/8047 [5:20:36<2:06:02,  4.48s/it]loss_total_epoch 339.63389616273344
Training tokenizer:  79% 6361/8047 [5:20:41<2:06:09,  4.49s/it]loss_total_epoch 339.6801081430167
Training tokenizer:  79% 6362/8047 [5:20:45<2:06:15,  4.50s/it]loss_total_epoch 339.7103933226317
Training tokenizer:  79% 6363/8047 [5:20:50<2:06:05,  4.49s/it]loss_total_epoch 339.76213606260717
Training tokenizer:  79% 6364/8047 [5:20:54<2:06:06,  4.50s/it]loss_total_epoch 339.80207014270127
Training tokenizer:  79% 6365/8047 [5:20:59<2:06:30,  4.51s/it]loss_total_epoch 339.84845049120486
Training tokenizer:  79% 6366/8047 [5:21:03<2:06:15,  4.51s/it]loss_total_epoch 339.8815731499344
Training tokenizer:  79% 6367/8047 [5:21:08<2:06:05,  4.50s/it]loss_total_epoch 339.9294867981225
Training tokenizer:  79% 6368/8047 [5:21:12<2:05:40,  4.49s/it]loss_total_epoch 339.9722328800708
Training tokenizer:  79% 6369/8047 [5:21:17<2:06:02,  4.51s/it]loss_total_epoch 340.0027209725231
Training tokenizer:  79% 6370/8047 [5:21:21<2:06:25,  4.52s/it]loss_total_epoch 340.0432119462639
Training tokenizer:  79% 6371/8047 [5:21:26<2:06:24,  4.53s/it]loss_total_epoch 340.08187235705554
Training tokenizer:  79% 6372/8047 [5:21:31<2:06:08,  4.52s/it]loss_total_epoch 340.1287896987051
Training tokenizer:  79% 6373/8047 [5:21:35<2:06:01,  4.52s/it]loss_total_epoch 340.16510515101254
Training tokenizer:  79% 6374/8047 [5:21:40<2:05:51,  4.51s/it]loss_total_epoch 340.20680209062994
Training tokenizer:  79% 6375/8047 [5:21:44<2:05:52,  4.52s/it]loss_total_epoch 340.2534310873598
Training tokenizer:  79% 6376/8047 [5:21:49<2:05:45,  4.52s/it]loss_total_epoch 340.29176802001894
Training tokenizer:  79% 6377/8047 [5:21:53<2:06:32,  4.55s/it]loss_total_epoch 340.3348054755479
Training tokenizer:  79% 6378/8047 [5:21:58<2:06:33,  4.55s/it]loss_total_epoch 340.37506769783795
Training tokenizer:  79% 6379/8047 [5:22:02<2:06:07,  4.54s/it]loss_total_epoch 340.4178727027029
Training tokenizer:  79% 6380/8047 [5:22:07<2:06:00,  4.54s/it]loss_total_epoch 340.46351925097406
Training tokenizer:  79% 6381/8047 [5:22:11<2:06:05,  4.54s/it]loss_total_epoch 340.50909655727446
Training tokenizer:  79% 6382/8047 [5:22:16<2:05:48,  4.53s/it]loss_total_epoch 340.55413363687694
Training tokenizer:  79% 6383/8047 [5:22:20<2:05:34,  4.53s/it]loss_total_epoch 340.5942827295512
Training tokenizer:  79% 6384/8047 [5:22:25<2:05:04,  4.51s/it]loss_total_epoch 340.6314897593111
Training tokenizer:  79% 6385/8047 [5:22:29<2:05:02,  4.51s/it]loss_total_epoch 340.67530391924083
Training tokenizer:  79% 6386/8047 [5:22:34<2:04:46,  4.51s/it]loss_total_epoch 340.73005522973835
Training tokenizer:  79% 6387/8047 [5:22:38<2:05:05,  4.52s/it]loss_total_epoch 340.7691838685423
Training tokenizer:  79% 6388/8047 [5:22:43<2:04:52,  4.52s/it]loss_total_epoch 340.81626409851015
Training tokenizer:  79% 6389/8047 [5:22:47<2:05:04,  4.53s/it]loss_total_epoch 340.8552439492196
Training tokenizer:  79% 6390/8047 [5:22:52<2:04:49,  4.52s/it]loss_total_epoch 340.8984922859818
Training tokenizer:  79% 6391/8047 [5:22:57<2:04:56,  4.53s/it]loss_total_epoch 340.93315069563687
Training tokenizer:  79% 6392/8047 [5:23:01<2:04:38,  4.52s/it]loss_total_epoch 340.97021139599383
Training tokenizer:  79% 6393/8047 [5:23:06<2:04:20,  4.51s/it]loss_total_epoch 341.01825275085866
Training tokenizer:  79% 6394/8047 [5:23:10<2:04:10,  4.51s/it]loss_total_epoch 341.0612371135503
Training tokenizer:  79% 6395/8047 [5:23:15<2:04:14,  4.51s/it]loss_total_epoch 341.09116524271667
Training tokenizer:  79% 6396/8047 [5:23:19<2:04:13,  4.51s/it]loss_total_epoch 341.13397034443915
Training tokenizer:  79% 6397/8047 [5:23:24<2:04:03,  4.51s/it]loss_total_epoch 341.18514828570187
Training tokenizer:  80% 6398/8047 [5:23:28<2:04:00,  4.51s/it]loss_total_epoch 341.23621372692287
Training tokenizer:  80% 6399/8047 [5:23:33<2:04:26,  4.53s/it]loss_total_epoch 341.27424978651106
Training tokenizer:  80% 6400/8047 [5:23:37<2:04:05,  4.52s/it]loss_total_epoch 341.3148508500308
Training tokenizer:  80% 6401/8047 [5:23:42<2:04:07,  4.52s/it]loss_total_epoch 341.35430414043367
Training tokenizer:  80% 6402/8047 [5:23:46<2:04:24,  4.54s/it]loss_total_epoch 341.4004180971533
Training tokenizer:  80% 6403/8047 [5:23:51<2:04:22,  4.54s/it]loss_total_epoch 341.4321874175221
Training tokenizer:  80% 6404/8047 [5:23:55<2:04:21,  4.54s/it]loss_total_epoch 341.48374749161303
Training tokenizer:  80% 6405/8047 [5:24:00<2:04:20,  4.54s/it]loss_total_epoch 341.5123190395534
Training tokenizer:  80% 6406/8047 [5:24:04<2:04:22,  4.55s/it]loss_total_epoch 341.5498642362654
Training tokenizer:  80% 6407/8047 [5:24:09<2:04:09,  4.54s/it]loss_total_epoch 341.6011086963117
Training tokenizer:  80% 6408/8047 [5:24:13<2:03:49,  4.53s/it]loss_total_epoch 341.6366792321205
Training tokenizer:  80% 6409/8047 [5:24:18<2:03:37,  4.53s/it]loss_total_epoch 341.6812031939626
Training tokenizer:  80% 6410/8047 [5:24:22<2:03:17,  4.52s/it]loss_total_epoch 341.72283605486155
Training tokenizer:  80% 6411/8047 [5:24:27<2:03:38,  4.53s/it]loss_total_epoch 341.76286531984806
Training tokenizer:  80% 6412/8047 [5:24:32<2:03:37,  4.54s/it]loss_total_epoch 341.8030815497041
Training tokenizer:  80% 6413/8047 [5:24:36<2:03:22,  4.53s/it]loss_total_epoch 341.8492603376508
Training tokenizer:  80% 6414/8047 [5:24:41<2:03:25,  4.53s/it]loss_total_epoch 341.8964098319411
Training tokenizer:  80% 6415/8047 [5:24:45<2:03:18,  4.53s/it]loss_total_epoch 341.9345943033695
Training tokenizer:  80% 6416/8047 [5:24:50<2:03:39,  4.55s/it]loss_total_epoch 341.9865508824587
Training tokenizer:  80% 6417/8047 [5:24:54<2:03:37,  4.55s/it]loss_total_epoch 342.01944974064827
Training tokenizer:  80% 6418/8047 [5:24:59<2:03:06,  4.53s/it]loss_total_epoch 342.06235555931926
Training tokenizer:  80% 6419/8047 [5:25:03<2:02:48,  4.53s/it]loss_total_epoch 342.10207176953554
Training tokenizer:  80% 6420/8047 [5:25:08<2:02:28,  4.52s/it]loss_total_epoch 342.1445150747895
Training tokenizer:  80% 6421/8047 [5:25:12<2:02:41,  4.53s/it]loss_total_epoch 342.1806294992566
Training tokenizer:  80% 6422/8047 [5:25:17<2:02:23,  4.52s/it]loss_total_epoch 342.2207712046802
Training tokenizer:  80% 6423/8047 [5:25:21<2:02:24,  4.52s/it]loss_total_epoch 342.2600708156824
Training tokenizer:  80% 6424/8047 [5:25:26<2:02:26,  4.53s/it]loss_total_epoch 342.30169550701976
Training tokenizer:  80% 6425/8047 [5:25:30<2:01:55,  4.51s/it]loss_total_epoch 342.3535248450935
Training tokenizer:  80% 6426/8047 [5:25:35<2:01:52,  4.51s/it]loss_total_epoch 342.3979674912989
Training tokenizer:  80% 6427/8047 [5:25:40<2:02:28,  4.54s/it]loss_total_epoch 342.44776110351086
Training tokenizer:  80% 6428/8047 [5:25:44<2:01:54,  4.52s/it]loss_total_epoch 342.48967783898115
Training tokenizer:  80% 6429/8047 [5:25:49<2:01:50,  4.52s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-ywuczph7'
loss_total_epoch 342.5399589315057
Training tokenizer:  80% 6430/8047 [5:25:53<2:01:48,  4.52s/it]loss_total_epoch 342.5684961285442
Training tokenizer:  80% 6431/8047 [5:25:58<2:02:01,  4.53s/it]loss_total_epoch 342.6025153193623
Training tokenizer:  80% 6432/8047 [5:26:02<2:01:56,  4.53s/it]loss_total_epoch 342.64538953639567
Training tokenizer:  80% 6433/8047 [5:26:07<2:02:35,  4.56s/it]loss_total_epoch 342.6819924954325
Training tokenizer:  80% 6434/8047 [5:26:11<2:03:27,  4.59s/it]loss_total_epoch 342.73239741288126
Training tokenizer:  80% 6435/8047 [5:26:16<2:03:11,  4.59s/it]loss_total_epoch 342.7733881045133
Training tokenizer:  80% 6436/8047 [5:26:21<2:04:18,  4.63s/it]loss_total_epoch 342.816839100793
Training tokenizer:  80% 6437/8047 [5:26:25<2:03:50,  4.62s/it]loss_total_epoch 342.86023106612265
Training tokenizer:  80% 6438/8047 [5:26:30<2:03:10,  4.59s/it]loss_total_epoch 342.9062611591071
Training tokenizer:  80% 6439/8047 [5:26:34<2:02:21,  4.57s/it]loss_total_epoch 342.95103586278856
Training tokenizer:  80% 6440/8047 [5:26:39<2:02:15,  4.56s/it]loss_total_epoch 343.00171405263245
Training tokenizer:  80% 6441/8047 [5:26:43<2:02:04,  4.56s/it]loss_total_epoch 343.04561182297766
Training tokenizer:  80% 6442/8047 [5:26:48<2:01:49,  4.55s/it]loss_total_epoch 343.0786898713559
Training tokenizer:  80% 6443/8047 [5:26:53<2:01:19,  4.54s/it]loss_total_epoch 343.1344252470881
Training tokenizer:  80% 6444/8047 [5:26:57<2:01:47,  4.56s/it]loss_total_epoch 343.1774283889681
Training tokenizer:  80% 6445/8047 [5:27:02<2:01:58,  4.57s/it]loss_total_epoch 343.218529695645
Training tokenizer:  80% 6446/8047 [5:27:06<2:01:45,  4.56s/it]loss_total_epoch 343.25133385695517
Training tokenizer:  80% 6447/8047 [5:27:11<2:01:27,  4.55s/it]loss_total_epoch 343.2861192319542
Training tokenizer:  80% 6448/8047 [5:27:15<2:01:39,  4.57s/it]loss_total_epoch 343.3164455611259
Training tokenizer:  80% 6449/8047 [5:27:20<2:00:49,  4.54s/it]loss_total_epoch 343.3479929026216
Training tokenizer:  80% 6450/8047 [5:27:24<2:01:01,  4.55s/it]loss_total_epoch 343.3797824513167
Training tokenizer:  80% 6451/8047 [5:27:29<2:01:33,  4.57s/it]loss_total_epoch 343.42132473923266
Training tokenizer:  80% 6452/8047 [5:27:34<2:00:49,  4.54s/it]loss_total_epoch 343.46151924319565
Training tokenizer:  80% 6453/8047 [5:27:38<2:01:02,  4.56s/it]loss_total_epoch 343.50434249825776
Training tokenizer:  80% 6454/8047 [5:27:43<2:01:01,  4.56s/it]loss_total_epoch 343.5435409527272
Training tokenizer:  80% 6455/8047 [5:27:47<2:01:00,  4.56s/it]loss_total_epoch 343.5842186566442
Training tokenizer:  80% 6456/8047 [5:27:52<2:00:03,  4.53s/it]loss_total_epoch 343.6236475389451
Training tokenizer:  80% 6457/8047 [5:27:56<2:00:28,  4.55s/it]loss_total_epoch 343.666038421914
Training tokenizer:  80% 6458/8047 [5:28:01<2:00:42,  4.56s/it]loss_total_epoch 343.70586100406945
Training tokenizer:  80% 6459/8047 [5:28:05<2:00:38,  4.56s/it]loss_total_epoch 343.73680122196674
Training tokenizer:  80% 6460/8047 [5:28:10<2:00:27,  4.55s/it]loss_total_epoch 343.7769853770733
Training tokenizer:  80% 6461/8047 [5:28:15<2:00:33,  4.56s/it]loss_total_epoch 343.8180692791939
Training tokenizer:  80% 6462/8047 [5:28:19<2:00:18,  4.55s/it]loss_total_epoch 343.8598832562566
Training tokenizer:  80% 6463/8047 [5:28:24<2:00:01,  4.55s/it]loss_total_epoch 343.89480536058545
Training tokenizer:  80% 6464/8047 [5:28:28<2:00:17,  4.56s/it]loss_total_epoch 343.94407415017486
Training tokenizer:  80% 6465/8047 [5:28:33<2:00:02,  4.55s/it]loss_total_epoch 343.9856284968555
Training tokenizer:  80% 6466/8047 [5:28:37<1:59:54,  4.55s/it]loss_total_epoch 344.03505543619394
Training tokenizer:  80% 6467/8047 [5:28:42<1:59:55,  4.55s/it]loss_total_epoch 344.06639704853296
Training tokenizer:  80% 6468/8047 [5:28:46<2:00:05,  4.56s/it]loss_total_epoch 344.1101189740002
Training tokenizer:  80% 6469/8047 [5:28:51<2:00:13,  4.57s/it]loss_total_epoch 344.1532718576491
Training tokenizer:  80% 6470/8047 [5:28:56<2:00:26,  4.58s/it]loss_total_epoch 344.1936815343797
Training tokenizer:  80% 6471/8047 [5:29:00<2:00:09,  4.57s/it]loss_total_epoch 344.23322931304574
Training tokenizer:  80% 6472/8047 [5:29:05<2:00:14,  4.58s/it]loss_total_epoch 344.26731557771564
Training tokenizer:  80% 6473/8047 [5:29:09<2:00:23,  4.59s/it]loss_total_epoch 344.30650640651584
Training tokenizer:  80% 6474/8047 [5:29:14<2:00:05,  4.58s/it]loss_total_epoch 344.3540109395981
Training tokenizer:  80% 6475/8047 [5:29:19<2:00:06,  4.58s/it]loss_total_epoch 344.3965918086469
Training tokenizer:  80% 6476/8047 [5:29:23<1:59:44,  4.57s/it]loss_total_epoch 344.4469997026026
Training tokenizer:  80% 6477/8047 [5:29:28<1:59:26,  4.56s/it]loss_total_epoch 344.4919490739703
Training tokenizer:  81% 6478/8047 [5:29:32<1:59:15,  4.56s/it]loss_total_epoch 344.5329041108489
Training tokenizer:  81% 6479/8047 [5:29:37<1:58:49,  4.55s/it]loss_total_epoch 344.57161247357726
Training tokenizer:  81% 6480/8047 [5:29:41<1:58:23,  4.53s/it]loss_total_epoch 344.61216927319765
Training tokenizer:  81% 6481/8047 [5:29:46<1:58:41,  4.55s/it]loss_total_epoch 344.644728820771
Training tokenizer:  81% 6482/8047 [5:29:50<1:59:05,  4.57s/it]loss_total_epoch 344.6992846392095
Training tokenizer:  81% 6483/8047 [5:29:55<1:58:58,  4.56s/it]loss_total_epoch 344.73182651028037
Training tokenizer:  81% 6484/8047 [5:30:00<1:58:56,  4.57s/it]loss_total_epoch 344.76743610203266
Training tokenizer:  81% 6485/8047 [5:30:04<1:58:58,  4.57s/it]loss_total_epoch 344.80863028019667
Training tokenizer:  81% 6486/8047 [5:30:09<1:58:47,  4.57s/it]loss_total_epoch 344.8439617715776
Training tokenizer:  81% 6487/8047 [5:30:13<1:58:39,  4.56s/it]loss_total_epoch 344.88058569282293
Training tokenizer:  81% 6488/8047 [5:30:18<1:58:52,  4.58s/it]loss_total_epoch 344.9303938522935
Training tokenizer:  81% 6489/8047 [5:30:22<1:58:37,  4.57s/it]loss_total_epoch 344.97433691471815
Training tokenizer:  81% 6490/8047 [5:30:27<1:58:23,  4.56s/it]loss_total_epoch 345.01881853863597
Training tokenizer:  81% 6491/8047 [5:30:32<1:58:36,  4.57s/it]loss_total_epoch 345.0555788502097
Training tokenizer:  81% 6492/8047 [5:30:36<1:58:59,  4.59s/it]loss_total_epoch 345.09362872317433
Training tokenizer:  81% 6493/8047 [5:30:41<1:59:22,  4.61s/it]loss_total_epoch 345.12802808359265
Training tokenizer:  81% 6494/8047 [5:30:45<1:59:16,  4.61s/it]loss_total_epoch 345.16979394480586
Training tokenizer:  81% 6495/8047 [5:30:50<1:59:31,  4.62s/it]loss_total_epoch 345.20877972245216
Training tokenizer:  81% 6496/8047 [5:30:55<1:59:04,  4.61s/it]loss_total_epoch 345.24948915466666
Training tokenizer:  81% 6497/8047 [5:30:59<1:58:41,  4.59s/it]loss_total_epoch 345.28743763640523
Training tokenizer:  81% 6498/8047 [5:31:04<1:58:27,  4.59s/it]loss_total_epoch 345.3190568871796
Training tokenizer:  81% 6499/8047 [5:31:08<1:58:24,  4.59s/it]loss_total_epoch 345.35181798785925
Training tokenizer:  81% 6500/8047 [5:31:13<1:58:13,  4.59s/it]loss_total_epoch 345.38770853728056
Training tokenizer:  81% 6501/8047 [5:31:18<1:58:16,  4.59s/it]loss_total_epoch 345.4338044151664
Training tokenizer:  81% 6502/8047 [5:31:22<1:58:11,  4.59s/it]loss_total_epoch 345.4695042707026
Training tokenizer:  81% 6503/8047 [5:31:27<1:58:20,  4.60s/it]loss_total_epoch 345.5053217038512
Training tokenizer:  81% 6504/8047 [5:31:31<1:58:00,  4.59s/it]loss_total_epoch 345.5462182611227
Training tokenizer:  81% 6505/8047 [5:31:36<1:58:09,  4.60s/it]loss_total_epoch 345.5929450467229
Training tokenizer:  81% 6506/8047 [5:31:41<1:57:56,  4.59s/it]loss_total_epoch 345.6326794400811
Training tokenizer:  81% 6507/8047 [5:31:45<1:57:33,  4.58s/it]loss_total_epoch 345.68287697806954
Training tokenizer:  81% 6508/8047 [5:31:50<1:57:43,  4.59s/it]loss_total_epoch 345.73637172952294
Training tokenizer:  81% 6509/8047 [5:31:54<1:57:37,  4.59s/it]loss_total_epoch 345.77596504986286
Training tokenizer:  81% 6510/8047 [5:31:59<1:57:36,  4.59s/it]loss_total_epoch 345.81462494283915
Training tokenizer:  81% 6511/8047 [5:32:03<1:57:43,  4.60s/it]loss_total_epoch 345.8488706946373
Training tokenizer:  81% 6512/8047 [5:32:08<1:57:49,  4.61s/it]loss_total_epoch 345.8860700689256
Training tokenizer:  81% 6513/8047 [5:32:13<1:57:34,  4.60s/it]loss_total_epoch 345.9311103411019
Training tokenizer:  81% 6514/8047 [5:32:17<1:57:40,  4.61s/it]loss_total_epoch 345.97455298528075
Training tokenizer:  81% 6515/8047 [5:32:22<1:57:11,  4.59s/it]loss_total_epoch 346.0212678089738
Training tokenizer:  81% 6516/8047 [5:32:26<1:56:54,  4.58s/it]loss_total_epoch 346.07546576857567
Training tokenizer:  81% 6517/8047 [5:32:31<1:57:10,  4.60s/it]loss_total_epoch 346.1196994856
Training tokenizer:  81% 6518/8047 [5:32:36<1:56:56,  4.59s/it]loss_total_epoch 346.16031731665134
Training tokenizer:  81% 6519/8047 [5:32:40<1:56:41,  4.58s/it]loss_total_epoch 346.1989091001451
Training tokenizer:  81% 6520/8047 [5:32:45<1:56:52,  4.59s/it]loss_total_epoch 346.23546670749784
Training tokenizer:  81% 6521/8047 [5:32:49<1:56:33,  4.58s/it]loss_total_epoch 346.2833405956626
Training tokenizer:  81% 6522/8047 [5:32:54<1:56:51,  4.60s/it]loss_total_epoch 346.34226286411285
Training tokenizer:  81% 6523/8047 [5:32:59<1:57:05,  4.61s/it]loss_total_epoch 346.37600726634264
Training tokenizer:  81% 6524/8047 [5:33:03<1:56:51,  4.60s/it]loss_total_epoch 346.4113723896444
Training tokenizer:  81% 6525/8047 [5:33:08<1:56:45,  4.60s/it]loss_total_epoch 346.4521937556565
Training tokenizer:  81% 6526/8047 [5:33:12<1:56:05,  4.58s/it]loss_total_epoch 346.5038184262812
Training tokenizer:  81% 6527/8047 [5:33:17<1:55:56,  4.58s/it]loss_total_epoch 346.54012336581945
Training tokenizer:  81% 6528/8047 [5:33:22<1:56:10,  4.59s/it]loss_total_epoch 346.58210587874055
Training tokenizer:  81% 6529/8047 [5:33:26<1:56:17,  4.60s/it]loss_total_epoch 346.6368415169418
Training tokenizer:  81% 6530/8047 [5:33:31<1:56:16,  4.60s/it]loss_total_epoch 346.6742033548653
Training tokenizer:  81% 6531/8047 [5:33:35<1:55:52,  4.59s/it]loss_total_epoch 346.7180425077677
Training tokenizer:  81% 6532/8047 [5:33:40<1:55:28,  4.57s/it]loss_total_epoch 346.7543236762285
Training tokenizer:  81% 6533/8047 [5:33:44<1:55:49,  4.59s/it]loss_total_epoch 346.7921120747924
Training tokenizer:  81% 6534/8047 [5:33:49<1:55:49,  4.59s/it]loss_total_epoch 346.84097999706864
Training tokenizer:  81% 6535/8047 [5:33:54<1:55:10,  4.57s/it]loss_total_epoch 346.88067546859384
Training tokenizer:  81% 6536/8047 [5:33:58<1:55:08,  4.57s/it]loss_total_epoch 346.924661077559
Training tokenizer:  81% 6537/8047 [5:34:03<1:55:45,  4.60s/it]loss_total_epoch 346.96350315213203
Training tokenizer:  81% 6538/8047 [5:34:07<1:55:28,  4.59s/it]loss_total_epoch 347.0132776685059
Training tokenizer:  81% 6539/8047 [5:34:12<1:55:21,  4.59s/it]loss_total_epoch 347.06177339702845
Training tokenizer:  81% 6540/8047 [5:34:17<1:55:06,  4.58s/it]loss_total_epoch 347.09561567008495
Training tokenizer:  81% 6541/8047 [5:34:21<1:55:02,  4.58s/it]loss_total_epoch 347.13662426918745
Training tokenizer:  81% 6542/8047 [5:34:26<1:55:38,  4.61s/it]loss_total_epoch 347.1824656315148
Training tokenizer:  81% 6543/8047 [5:34:30<1:55:25,  4.60s/it]loss_total_epoch 347.2291603535414
Training tokenizer:  81% 6544/8047 [5:34:35<1:55:15,  4.60s/it]loss_total_epoch 347.27589363604784
Training tokenizer:  81% 6545/8047 [5:34:40<1:54:22,  4.57s/it]loss_total_epoch 347.32240933552384
Training tokenizer:  81% 6546/8047 [5:34:44<1:54:38,  4.58s/it]loss_total_epoch 347.36934653669596
Training tokenizer:  81% 6547/8047 [5:34:49<1:54:37,  4.58s/it]loss_total_epoch 347.41439120844007
Training tokenizer:  81% 6548/8047 [5:34:53<1:54:45,  4.59s/it]loss_total_epoch 347.4627399407327
Training tokenizer:  81% 6549/8047 [5:34:58<1:54:35,  4.59s/it]loss_total_epoch 347.5007104501128
Training tokenizer:  81% 6550/8047 [5:35:03<1:54:53,  4.61s/it]loss_total_epoch 347.53649803996086
Training tokenizer:  81% 6551/8047 [5:35:07<1:55:09,  4.62s/it]loss_total_epoch 347.58494836091995
Training tokenizer:  81% 6552/8047 [5:35:12<1:54:25,  4.59s/it]loss_total_epoch 347.6168550774455
Training tokenizer:  81% 6553/8047 [5:35:16<1:54:42,  4.61s/it]loss_total_epoch 347.65734262019396
Training tokenizer:  81% 6554/8047 [5:35:21<1:54:38,  4.61s/it]loss_total_epoch 347.6994435787201
Training tokenizer:  81% 6555/8047 [5:35:26<1:54:30,  4.61s/it]loss_total_epoch 347.74472189694643
Training tokenizer:  81% 6556/8047 [5:35:30<1:54:36,  4.61s/it]loss_total_epoch 347.7895053885877
Training tokenizer:  81% 6557/8047 [5:35:35<1:53:17,  4.56s/it]loss_total_epoch 347.827110491693
Training tokenizer:  81% 6558/8047 [5:35:39<1:53:24,  4.57s/it]loss_total_epoch 347.8664646074176
Training tokenizer:  82% 6559/8047 [5:35:44<1:53:23,  4.57s/it]loss_total_epoch 347.9088551811874
Training tokenizer:  82% 6560/8047 [5:35:48<1:53:56,  4.60s/it]loss_total_epoch 347.94858955591917
Training tokenizer:  82% 6561/8047 [5:35:53<1:53:46,  4.59s/it]loss_total_epoch 347.98120568320155
Training tokenizer:  82% 6562/8047 [5:35:58<1:54:04,  4.61s/it]loss_total_epoch 348.0229225344956
Training tokenizer:  82% 6563/8047 [5:36:02<1:53:46,  4.60s/it]loss_total_epoch 348.0681822448969
Training tokenizer:  82% 6564/8047 [5:36:07<1:54:09,  4.62s/it]loss_total_epoch 348.10936772823334
Training tokenizer:  82% 6565/8047 [5:36:11<1:52:51,  4.57s/it]loss_total_epoch 348.14195960760117
Training tokenizer:  82% 6566/8047 [5:36:16<1:52:44,  4.57s/it]loss_total_epoch 348.1868006102741
Training tokenizer:  82% 6567/8047 [5:36:21<1:53:01,  4.58s/it]loss_total_epoch 348.2252977192402
Training tokenizer:  82% 6568/8047 [5:36:25<1:53:36,  4.61s/it]loss_total_epoch 348.26217107847333
Training tokenizer:  82% 6569/8047 [5:36:30<1:53:49,  4.62s/it]loss_total_epoch 348.3080173358321
Training tokenizer:  82% 6570/8047 [5:36:35<1:53:39,  4.62s/it]loss_total_epoch 348.3466907441616
Training tokenizer:  82% 6571/8047 [5:36:39<1:53:51,  4.63s/it]loss_total_epoch 348.39368550106883
Training tokenizer:  82% 6572/8047 [5:36:44<1:53:58,  4.64s/it]loss_total_epoch 348.4371621198952
Training tokenizer:  82% 6573/8047 [5:36:48<1:54:02,  4.64s/it]loss_total_epoch 348.4858510121703
Training tokenizer:  82% 6574/8047 [5:36:53<1:53:58,  4.64s/it]loss_total_epoch 348.52856012806296
Training tokenizer:  82% 6575/8047 [5:36:58<1:53:10,  4.61s/it]loss_total_epoch 348.57039511576295
Training tokenizer:  82% 6576/8047 [5:37:03<1:55:03,  4.69s/it]loss_total_epoch 348.6174916662276
Training tokenizer:  82% 6577/8047 [5:37:07<1:54:25,  4.67s/it]loss_total_epoch 348.663714479655
Training tokenizer:  82% 6578/8047 [5:37:12<1:53:45,  4.65s/it]loss_total_epoch 348.7096739783883
Training tokenizer:  82% 6579/8047 [5:37:16<1:53:31,  4.64s/it]loss_total_epoch 348.75170658901334
Training tokenizer:  82% 6580/8047 [5:37:21<1:53:28,  4.64s/it]loss_total_epoch 348.79590336605906
Training tokenizer:  82% 6581/8047 [5:37:26<1:53:03,  4.63s/it]loss_total_epoch 348.8478893749416
Training tokenizer:  82% 6582/8047 [5:37:30<1:52:48,  4.62s/it]loss_total_epoch 348.88326263800263
Training tokenizer:  82% 6583/8047 [5:37:35<1:53:14,  4.64s/it]loss_total_epoch 348.9182844981551
Training tokenizer:  82% 6584/8047 [5:37:40<1:53:21,  4.65s/it]loss_total_epoch 348.9567523598671
Training tokenizer:  82% 6585/8047 [5:37:44<1:53:09,  4.64s/it]loss_total_epoch 348.9990849532187
Training tokenizer:  82% 6586/8047 [5:37:49<1:52:38,  4.63s/it]loss_total_epoch 349.03773053735495
Training tokenizer:  82% 6587/8047 [5:37:53<1:52:20,  4.62s/it]loss_total_epoch 349.0718448795378
Training tokenizer:  82% 6588/8047 [5:37:58<1:52:26,  4.62s/it]loss_total_epoch 349.1141202673316
Training tokenizer:  82% 6589/8047 [5:38:03<1:52:34,  4.63s/it]loss_total_epoch 349.143938081339
Training tokenizer:  82% 6590/8047 [5:38:07<1:52:45,  4.64s/it]loss_total_epoch 349.1822249609977
Training tokenizer:  82% 6591/8047 [5:38:12<1:52:40,  4.64s/it]loss_total_epoch 349.2286757398397
Training tokenizer:  82% 6592/8047 [5:38:17<1:52:28,  4.64s/it]loss_total_epoch 349.264522055164
Training tokenizer:  82% 6593/8047 [5:38:21<1:52:13,  4.63s/it]loss_total_epoch 349.30539700202644
Training tokenizer:  82% 6594/8047 [5:38:26<1:52:12,  4.63s/it]loss_total_epoch 349.352952869609
Training tokenizer:  82% 6595/8047 [5:38:31<1:52:09,  4.63s/it]loss_total_epoch 349.3913068789989
Training tokenizer:  82% 6596/8047 [5:38:35<1:52:20,  4.65s/it]loss_total_epoch 349.42834231071174
Training tokenizer:  82% 6597/8047 [5:38:40<1:52:08,  4.64s/it]loss_total_epoch 349.4678004067391
Training tokenizer:  82% 6598/8047 [5:38:44<1:52:16,  4.65s/it]loss_total_epoch 349.5104207005352
Training tokenizer:  82% 6599/8047 [5:38:49<1:52:01,  4.64s/it]loss_total_epoch 349.5533137526363
Training tokenizer:  82% 6600/8047 [5:38:54<1:52:08,  4.65s/it]loss_total_epoch 349.59399298392236
Training tokenizer:  82% 6601/8047 [5:38:58<1:51:39,  4.63s/it]loss_total_epoch 349.635672705248
Training tokenizer:  82% 6602/8047 [5:39:03<1:51:26,  4.63s/it]loss_total_epoch 349.6801521945745
Training tokenizer:  82% 6603/8047 [5:39:08<1:51:25,  4.63s/it]loss_total_epoch 349.7146773580462
Training tokenizer:  82% 6604/8047 [5:39:12<1:51:30,  4.64s/it]loss_total_epoch 349.75780113972723
Training tokenizer:  82% 6605/8047 [5:39:17<1:51:18,  4.63s/it]loss_total_epoch 349.80035344697535
Training tokenizer:  82% 6606/8047 [5:39:21<1:51:00,  4.62s/it]loss_total_epoch 349.84633164294064
Training tokenizer:  82% 6607/8047 [5:39:26<1:51:08,  4.63s/it]loss_total_epoch 349.8923846203834
Training tokenizer:  82% 6608/8047 [5:39:31<1:51:05,  4.63s/it]loss_total_epoch 349.94290369190276
Training tokenizer:  82% 6609/8047 [5:39:35<1:51:07,  4.64s/it]loss_total_epoch 349.9805965889245
Training tokenizer:  82% 6610/8047 [5:39:40<1:51:13,  4.64s/it]loss_total_epoch 350.028048356995
Training tokenizer:  82% 6611/8047 [5:39:45<1:50:44,  4.63s/it]loss_total_epoch 350.0670339334756
Training tokenizer:  82% 6612/8047 [5:39:49<1:50:54,  4.64s/it]loss_total_epoch 350.1053041573614
Training tokenizer:  82% 6613/8047 [5:39:54<1:50:08,  4.61s/it]loss_total_epoch 350.14087597094476
Training tokenizer:  82% 6614/8047 [5:39:58<1:50:10,  4.61s/it]loss_total_epoch 350.182288819924
Training tokenizer:  82% 6615/8047 [5:40:03<1:50:06,  4.61s/it]loss_total_epoch 350.2235154155642
Training tokenizer:  82% 6616/8047 [5:40:08<1:50:25,  4.63s/it]loss_total_epoch 350.270973386243
Training tokenizer:  82% 6617/8047 [5:40:12<1:50:05,  4.62s/it]loss_total_epoch 350.31399667076766
Training tokenizer:  82% 6618/8047 [5:40:17<1:49:52,  4.61s/it]loss_total_epoch 350.3624708484858
Training tokenizer:  82% 6619/8047 [5:40:22<1:49:46,  4.61s/it]loss_total_epoch 350.39657188393176
Training tokenizer:  82% 6620/8047 [5:40:26<1:50:00,  4.63s/it]loss_total_epoch 350.4314542952925
Training tokenizer:  82% 6621/8047 [5:40:31<1:49:44,  4.62s/it]loss_total_epoch 350.4742723312229
Training tokenizer:  82% 6622/8047 [5:40:36<1:50:09,  4.64s/it]loss_total_epoch 350.5185416024178
Training tokenizer:  82% 6623/8047 [5:40:40<1:49:38,  4.62s/it]loss_total_epoch 350.5559788700193
Training tokenizer:  82% 6624/8047 [5:40:45<1:49:42,  4.63s/it]loss_total_epoch 350.594317143783
Training tokenizer:  82% 6625/8047 [5:40:49<1:49:53,  4.64s/it]loss_total_epoch 350.63276715017855
Training tokenizer:  82% 6626/8047 [5:40:54<1:49:57,  4.64s/it]loss_total_epoch 350.6687314901501
Training tokenizer:  82% 6627/8047 [5:40:59<1:49:58,  4.65s/it]loss_total_epoch 350.71156164072454
Training tokenizer:  82% 6628/8047 [5:41:03<1:49:17,  4.62s/it]loss_total_epoch 350.7599498163909
Training tokenizer:  82% 6629/8047 [5:41:08<1:49:25,  4.63s/it]loss_total_epoch 350.80004986561835
Training tokenizer:  82% 6630/8047 [5:41:13<1:49:39,  4.64s/it]loss_total_epoch 350.84543194808066
Training tokenizer:  82% 6631/8047 [5:41:17<1:49:44,  4.65s/it]loss_total_epoch 350.8872233759612
Training tokenizer:  82% 6632/8047 [5:41:22<1:49:36,  4.65s/it]loss_total_epoch 350.92729505710304
Training tokenizer:  82% 6633/8047 [5:41:27<1:49:54,  4.66s/it]loss_total_epoch 350.9644166249782
Training tokenizer:  82% 6634/8047 [5:41:31<1:49:53,  4.67s/it]loss_total_epoch 351.0055307652801
Training tokenizer:  82% 6635/8047 [5:41:36<1:49:48,  4.67s/it]loss_total_epoch 351.04968972690403
Training tokenizer:  82% 6636/8047 [5:41:41<1:49:32,  4.66s/it]loss_total_epoch 351.0854739341885
Training tokenizer:  82% 6637/8047 [5:41:45<1:49:37,  4.66s/it]loss_total_epoch 351.1280739773065
Training tokenizer:  82% 6638/8047 [5:41:50<1:49:24,  4.66s/it]loss_total_epoch 351.1714999396354
Training tokenizer:  83% 6639/8047 [5:41:55<1:49:16,  4.66s/it]loss_total_epoch 351.20367451943457
Training tokenizer:  83% 6640/8047 [5:41:59<1:49:14,  4.66s/it]loss_total_epoch 351.2409768793732
Training tokenizer:  83% 6641/8047 [5:42:04<1:50:37,  4.72s/it]loss_total_epoch 351.28378702513874
Training tokenizer:  83% 6642/8047 [5:42:09<1:50:13,  4.71s/it]loss_total_epoch 351.3217630404979
Training tokenizer:  83% 6643/8047 [5:42:13<1:50:13,  4.71s/it]loss_total_epoch 351.35908340476453
Training tokenizer:  83% 6644/8047 [5:42:18<1:50:03,  4.71s/it]loss_total_epoch 351.39503013528883
Training tokenizer:  83% 6645/8047 [5:42:23<1:49:33,  4.69s/it]loss_total_epoch 351.440880080685
Training tokenizer:  83% 6646/8047 [5:42:28<1:49:32,  4.69s/it]loss_total_epoch 351.4810811113566
Training tokenizer:  83% 6647/8047 [5:42:32<1:49:22,  4.69s/it]loss_total_epoch 351.525934452191
Training tokenizer:  83% 6648/8047 [5:42:37<1:48:52,  4.67s/it]loss_total_epoch 351.56613753549755
Training tokenizer:  83% 6649/8047 [5:42:41<1:47:39,  4.62s/it]loss_total_epoch 351.6117441635579
Training tokenizer:  83% 6650/8047 [5:42:46<1:47:31,  4.62s/it]loss_total_epoch 351.6466144975275
Training tokenizer:  83% 6651/8047 [5:42:51<1:47:12,  4.61s/it]loss_total_epoch 351.6837846878916
Training tokenizer:  83% 6652/8047 [5:42:55<1:47:29,  4.62s/it]loss_total_epoch 351.7237652782351
Training tokenizer:  83% 6653/8047 [5:43:00<1:48:06,  4.65s/it]loss_total_epoch 351.773629212752
Training tokenizer:  83% 6654/8047 [5:43:05<1:47:52,  4.65s/it]loss_total_epoch 351.8236541915685
Training tokenizer:  83% 6655/8047 [5:43:09<1:47:39,  4.64s/it]loss_total_epoch 351.8590951990336
Training tokenizer:  83% 6656/8047 [5:43:14<1:47:46,  4.65s/it]loss_total_epoch 351.90292411483824
Training tokenizer:  83% 6657/8047 [5:43:18<1:47:38,  4.65s/it]loss_total_epoch 351.9391948375851
Training tokenizer:  83% 6658/8047 [5:43:23<1:47:25,  4.64s/it]loss_total_epoch 351.97860140539706
Training tokenizer:  83% 6659/8047 [5:43:28<1:47:24,  4.64s/it]loss_total_epoch 352.0150544587523
Training tokenizer:  83% 6660/8047 [5:43:32<1:47:41,  4.66s/it]loss_total_epoch 352.0595932137221
Training tokenizer:  83% 6661/8047 [5:43:37<1:47:30,  4.65s/it]loss_total_epoch 352.09269476495683
Training tokenizer:  83% 6662/8047 [5:43:42<1:47:10,  4.64s/it]loss_total_epoch 352.1349611226469
Training tokenizer:  83% 6663/8047 [5:43:46<1:47:14,  4.65s/it]loss_total_epoch 352.1824647318572
Training tokenizer:  83% 6664/8047 [5:43:51<1:47:23,  4.66s/it]loss_total_epoch 352.2151050698012
Training tokenizer:  83% 6665/8047 [5:43:56<1:47:05,  4.65s/it]loss_total_epoch 352.2567110527307
Training tokenizer:  83% 6666/8047 [5:44:00<1:47:22,  4.66s/it]loss_total_epoch 352.30316697992384
Training tokenizer:  83% 6667/8047 [5:44:05<1:47:18,  4.67s/it]loss_total_epoch 352.3545059617609
Training tokenizer:  83% 6668/8047 [5:44:10<1:46:14,  4.62s/it]loss_total_epoch 352.4029749054462
Training tokenizer:  83% 6669/8047 [5:44:14<1:46:26,  4.63s/it]loss_total_epoch 352.457689916715
Training tokenizer:  83% 6670/8047 [5:44:19<1:46:45,  4.65s/it]loss_total_epoch 352.50635602883995
Training tokenizer:  83% 6671/8047 [5:44:24<1:46:38,  4.65s/it]loss_total_epoch 352.53277065604925
Training tokenizer:  83% 6672/8047 [5:44:28<1:46:55,  4.67s/it]loss_total_epoch 352.5790499225259
Training tokenizer:  83% 6673/8047 [5:44:33<1:46:58,  4.67s/it]loss_total_epoch 352.62254122272134
Training tokenizer:  83% 6674/8047 [5:44:38<1:46:48,  4.67s/it]loss_total_epoch 352.676790650934
Training tokenizer:  83% 6675/8047 [5:44:42<1:46:44,  4.67s/it]loss_total_epoch 352.71774247288704
Training tokenizer:  83% 6676/8047 [5:44:47<1:46:36,  4.67s/it]loss_total_epoch 352.76498084142804
Training tokenizer:  83% 6677/8047 [5:44:52<1:46:36,  4.67s/it]loss_total_epoch 352.80224714428186
Training tokenizer:  83% 6678/8047 [5:44:56<1:46:25,  4.66s/it]loss_total_epoch 352.8382168337703
Training tokenizer:  83% 6679/8047 [5:45:01<1:46:04,  4.65s/it]loss_total_epoch 352.88311495631933
Training tokenizer:  83% 6680/8047 [5:45:06<1:45:59,  4.65s/it]loss_total_epoch 352.9272389113903
Training tokenizer:  83% 6681/8047 [5:45:10<1:45:01,  4.61s/it]loss_total_epoch 352.96384621039033
Training tokenizer:  83% 6682/8047 [5:45:15<1:45:22,  4.63s/it]loss_total_epoch 352.99999668449163
Training tokenizer:  83% 6683/8047 [5:45:19<1:45:38,  4.65s/it]loss_total_epoch 353.03391548991203
Training tokenizer:  83% 6684/8047 [5:45:24<1:45:19,  4.64s/it]loss_total_epoch 353.0731539390981
Training tokenizer:  83% 6685/8047 [5:45:29<1:45:24,  4.64s/it]loss_total_epoch 353.1139541231096
Training tokenizer:  83% 6686/8047 [5:45:33<1:45:24,  4.65s/it]loss_total_epoch 353.1437681764364
Training tokenizer:  83% 6687/8047 [5:45:38<1:45:02,  4.63s/it]loss_total_epoch 353.17754350975156
Training tokenizer:  83% 6688/8047 [5:45:43<1:45:07,  4.64s/it]loss_total_epoch 353.2174090668559
Training tokenizer:  83% 6689/8047 [5:45:47<1:45:25,  4.66s/it]loss_total_epoch 353.2663587257266
Training tokenizer:  83% 6690/8047 [5:45:52<1:45:32,  4.67s/it]loss_total_epoch 353.30324440449476
Training tokenizer:  83% 6691/8047 [5:45:57<1:45:27,  4.67s/it]loss_total_epoch 353.33294808119535
Training tokenizer:  83% 6692/8047 [5:46:01<1:45:25,  4.67s/it]loss_total_epoch 353.3784843496978
Training tokenizer:  83% 6693/8047 [5:46:06<1:45:22,  4.67s/it]loss_total_epoch 353.4231312684715
Training tokenizer:  83% 6694/8047 [5:46:11<1:45:05,  4.66s/it]loss_total_epoch 353.46523121371865
Training tokenizer:  83% 6695/8047 [5:46:15<1:45:06,  4.66s/it]loss_total_epoch 353.50455727800727
Training tokenizer:  83% 6696/8047 [5:46:20<1:45:15,  4.67s/it]loss_total_epoch 353.5545057170093
Training tokenizer:  83% 6697/8047 [5:46:25<1:45:06,  4.67s/it]loss_total_epoch 353.5913084074855
Training tokenizer:  83% 6698/8047 [5:46:29<1:45:16,  4.68s/it]loss_total_epoch 353.62666138261557
Training tokenizer:  83% 6699/8047 [5:46:34<1:44:50,  4.67s/it]loss_total_epoch 353.6658235602081
Training tokenizer:  83% 6700/8047 [5:46:39<1:45:04,  4.68s/it]loss_total_epoch 353.697967171669
Training tokenizer:  83% 6701/8047 [5:46:43<1:44:43,  4.67s/it]loss_total_epoch 353.73435390368104
Training tokenizer:  83% 6702/8047 [5:46:48<1:44:39,  4.67s/it]loss_total_epoch 353.7805933021009
Training tokenizer:  83% 6703/8047 [5:46:53<1:44:37,  4.67s/it]loss_total_epoch 353.8291782028973
Training tokenizer:  83% 6704/8047 [5:46:57<1:44:23,  4.66s/it]loss_total_epoch 353.8726354762912
Training tokenizer:  83% 6705/8047 [5:47:02<1:44:16,  4.66s/it]loss_total_epoch 353.9112041220069
Training tokenizer:  83% 6706/8047 [5:47:07<1:44:21,  4.67s/it]loss_total_epoch 353.94612161815166
Training tokenizer:  83% 6707/8047 [5:47:11<1:44:31,  4.68s/it]loss_total_epoch 353.9849325083196
Training tokenizer:  83% 6708/8047 [5:47:16<1:44:34,  4.69s/it]loss_total_epoch 354.0224032625556
Training tokenizer:  83% 6709/8047 [5:47:21<1:44:28,  4.69s/it]loss_total_epoch 354.0690111555159
Training tokenizer:  83% 6710/8047 [5:47:25<1:44:22,  4.68s/it]loss_total_epoch 354.10193217918277
Training tokenizer:  83% 6711/8047 [5:47:30<1:43:59,  4.67s/it]loss_total_epoch 354.14156144857407
Training tokenizer:  83% 6712/8047 [5:47:35<1:43:56,  4.67s/it]loss_total_epoch 354.196094840765
Training tokenizer:  83% 6713/8047 [5:47:39<1:43:36,  4.66s/it]loss_total_epoch 354.2298921048641
Training tokenizer:  83% 6714/8047 [5:47:44<1:43:49,  4.67s/it]loss_total_epoch 354.270337741822
Training tokenizer:  83% 6715/8047 [5:47:49<1:43:54,  4.68s/it]loss_total_epoch 354.31632329151034
Training tokenizer:  83% 6716/8047 [5:47:54<1:43:52,  4.68s/it]loss_total_epoch 354.36863197386265
Training tokenizer:  83% 6717/8047 [5:47:58<1:44:04,  4.70s/it]loss_total_epoch 354.41091671586037
Training tokenizer:  83% 6718/8047 [5:48:03<1:44:07,  4.70s/it]loss_total_epoch 354.43928323313594
Training tokenizer:  83% 6719/8047 [5:48:08<1:44:17,  4.71s/it]loss_total_epoch 354.4784722290933
Training tokenizer:  84% 6720/8047 [5:48:12<1:44:10,  4.71s/it]loss_total_epoch 354.5272257402539
Training tokenizer:  84% 6721/8047 [5:48:17<1:44:04,  4.71s/it]loss_total_epoch 354.56927911937237
Training tokenizer:  84% 6722/8047 [5:48:22<1:43:45,  4.70s/it]loss_total_epoch 354.60567977651954
Training tokenizer:  84% 6723/8047 [5:48:26<1:43:27,  4.69s/it]loss_total_epoch 354.64530235156417
Training tokenizer:  84% 6724/8047 [5:48:31<1:43:44,  4.70s/it]loss_total_epoch 354.6847705952823
Training tokenizer:  84% 6725/8047 [5:48:36<1:43:26,  4.69s/it]loss_total_epoch 354.7162556834519
Training tokenizer:  84% 6726/8047 [5:48:41<1:43:39,  4.71s/it]loss_total_epoch 354.7658413015306
Training tokenizer:  84% 6727/8047 [5:48:45<1:43:46,  4.72s/it]loss_total_epoch 354.80811620503664
Training tokenizer:  84% 6728/8047 [5:48:50<1:43:48,  4.72s/it]loss_total_epoch 354.84396251663566
Training tokenizer:  84% 6729/8047 [5:48:55<1:43:29,  4.71s/it]loss_total_epoch 354.88250736147165
Training tokenizer:  84% 6730/8047 [5:48:59<1:43:22,  4.71s/it]loss_total_epoch 354.9235985837877
Training tokenizer:  84% 6731/8047 [5:49:04<1:42:54,  4.69s/it]loss_total_epoch 354.97085066884756
Training tokenizer:  84% 6732/8047 [5:49:09<1:42:42,  4.69s/it]loss_total_epoch 355.0123911611736
Training tokenizer:  84% 6733/8047 [5:49:13<1:42:40,  4.69s/it]loss_total_epoch 355.0638221129775
Training tokenizer:  84% 6734/8047 [5:49:18<1:42:37,  4.69s/it]loss_total_epoch 355.0990145839751
Training tokenizer:  84% 6735/8047 [5:49:23<1:42:57,  4.71s/it]loss_total_epoch 355.13738833367825
Training tokenizer:  84% 6736/8047 [5:49:28<1:42:46,  4.70s/it]loss_total_epoch 355.17870362475514
Training tokenizer:  84% 6737/8047 [5:49:32<1:42:41,  4.70s/it]loss_total_epoch 355.2263385280967
Training tokenizer:  84% 6738/8047 [5:49:37<1:42:40,  4.71s/it]loss_total_epoch 355.27846129983664
Training tokenizer:  84% 6739/8047 [5:49:42<1:42:32,  4.70s/it]loss_total_epoch 355.3260736837983
Training tokenizer:  84% 6740/8047 [5:49:46<1:42:20,  4.70s/it]loss_total_epoch 355.37399170175195
Training tokenizer:  84% 6741/8047 [5:49:51<1:42:15,  4.70s/it]loss_total_epoch 355.41377798467875
Training tokenizer:  84% 6742/8047 [5:49:56<1:41:57,  4.69s/it]loss_total_epoch 355.45314402133226
Training tokenizer:  84% 6743/8047 [5:50:00<1:41:27,  4.67s/it]loss_total_epoch 355.48833472281694
Training tokenizer:  84% 6744/8047 [5:50:05<1:41:48,  4.69s/it]loss_total_epoch 355.52994810789824
Training tokenizer:  84% 6745/8047 [5:50:10<1:41:47,  4.69s/it]loss_total_epoch 355.5737358368933
Training tokenizer:  84% 6746/8047 [5:50:14<1:41:21,  4.67s/it]loss_total_epoch 355.6199278719723
Training tokenizer:  84% 6747/8047 [5:50:19<1:41:31,  4.69s/it]loss_total_epoch 355.6570423133671
Training tokenizer:  84% 6748/8047 [5:50:24<1:41:43,  4.70s/it]loss_total_epoch 355.6988756842911
Training tokenizer:  84% 6749/8047 [5:50:29<1:41:38,  4.70s/it]loss_total_epoch 355.7381930537522
Training tokenizer:  84% 6750/8047 [5:50:33<1:41:23,  4.69s/it]loss_total_epoch 355.7837354838848
Training tokenizer:  84% 6751/8047 [5:50:38<1:41:16,  4.69s/it]loss_total_epoch 355.8265306353569
Training tokenizer:  84% 6752/8047 [5:50:43<1:41:02,  4.68s/it]loss_total_epoch 355.871549051255
Training tokenizer:  84% 6753/8047 [5:50:47<1:41:14,  4.69s/it]loss_total_epoch 355.9189091473818
Training tokenizer:  84% 6754/8047 [5:50:52<1:41:15,  4.70s/it]loss_total_epoch 355.961885407567
Training tokenizer:  84% 6755/8047 [5:50:57<1:41:00,  4.69s/it]loss_total_epoch 356.0074862577021
Training tokenizer:  84% 6756/8047 [5:51:01<1:40:59,  4.69s/it]loss_total_epoch 356.0381324030459
Training tokenizer:  84% 6757/8047 [5:51:06<1:41:04,  4.70s/it]loss_total_epoch 356.07747265323997
Training tokenizer:  84% 6758/8047 [5:51:11<1:40:50,  4.69s/it]loss_total_epoch 356.1145612746477
Training tokenizer:  84% 6759/8047 [5:51:16<1:41:00,  4.71s/it]loss_total_epoch 356.1499754227698
Training tokenizer:  84% 6760/8047 [5:51:20<1:41:00,  4.71s/it]loss_total_epoch 356.1907321996987
Training tokenizer:  84% 6761/8047 [5:51:25<1:41:11,  4.72s/it]loss_total_epoch 356.24064361676574
Training tokenizer:  84% 6762/8047 [5:51:30<1:40:32,  4.69s/it]loss_total_epoch 356.2815840989351
Training tokenizer:  84% 6763/8047 [5:51:34<1:40:29,  4.70s/it]loss_total_epoch 356.32492926716805
Training tokenizer:  84% 6764/8047 [5:51:39<1:40:18,  4.69s/it]loss_total_epoch 356.36146841570735
Training tokenizer:  84% 6765/8047 [5:51:44<1:40:23,  4.70s/it]loss_total_epoch 356.3967025093734
Training tokenizer:  84% 6766/8047 [5:51:49<1:40:32,  4.71s/it]loss_total_epoch 356.4420058131218
Training tokenizer:  84% 6767/8047 [5:51:53<1:40:27,  4.71s/it]loss_total_epoch 356.483809158206
Training tokenizer:  84% 6768/8047 [5:51:58<1:39:56,  4.69s/it]loss_total_epoch 356.52079655975103
Training tokenizer:  84% 6769/8047 [5:52:03<1:39:47,  4.69s/it]loss_total_epoch 356.5638181194663
Training tokenizer:  84% 6770/8047 [5:52:07<1:38:53,  4.65s/it]loss_total_epoch 356.6080464683473
Training tokenizer:  84% 6771/8047 [5:52:12<1:39:07,  4.66s/it]loss_total_epoch 356.6332311350852
Training tokenizer:  84% 6772/8047 [5:52:17<1:39:32,  4.68s/it]loss_total_epoch 356.682595776394
Training tokenizer:  84% 6773/8047 [5:52:21<1:39:20,  4.68s/it]loss_total_epoch 356.7230626884848
Training tokenizer:  84% 6774/8047 [5:52:26<1:39:42,  4.70s/it]loss_total_epoch 356.7594340015203
Training tokenizer:  84% 6775/8047 [5:52:31<1:39:31,  4.69s/it]loss_total_epoch 356.7960221860558
Training tokenizer:  84% 6776/8047 [5:52:35<1:39:22,  4.69s/it]loss_total_epoch 356.83605231530964
Training tokenizer:  84% 6777/8047 [5:52:40<1:39:29,  4.70s/it]loss_total_epoch 356.86386024206877
Training tokenizer:  84% 6778/8047 [5:52:45<1:39:32,  4.71s/it]loss_total_epoch 356.9060566574335
Training tokenizer:  84% 6779/8047 [5:52:50<1:40:01,  4.73s/it]loss_total_epoch 356.93941301479936
Training tokenizer:  84% 6780/8047 [5:52:54<1:39:46,  4.72s/it]loss_total_epoch 356.9826053082943
Training tokenizer:  84% 6781/8047 [5:52:59<1:40:05,  4.74s/it]loss_total_epoch 357.0219434648752
Training tokenizer:  84% 6782/8047 [5:53:04<1:39:49,  4.73s/it]loss_total_epoch 357.05328810214996
Training tokenizer:  84% 6783/8047 [5:53:08<1:38:44,  4.69s/it]loss_total_epoch 357.1015793122351
Training tokenizer:  84% 6784/8047 [5:53:13<1:38:52,  4.70s/it]loss_total_epoch 357.15006982907653
Training tokenizer:  84% 6785/8047 [5:53:18<1:37:50,  4.65s/it]loss_total_epoch 357.18239964544773
Training tokenizer:  84% 6786/8047 [5:53:22<1:38:08,  4.67s/it]loss_total_epoch 357.22353645414114
Training tokenizer:  84% 6787/8047 [5:53:27<1:38:13,  4.68s/it]loss_total_epoch 357.26370953023434
Training tokenizer:  84% 6788/8047 [5:53:32<1:38:24,  4.69s/it]loss_total_epoch 357.3027685210109
Training tokenizer:  84% 6789/8047 [5:53:36<1:38:24,  4.69s/it]loss_total_epoch 357.3404063358903
Training tokenizer:  84% 6790/8047 [5:53:41<1:38:47,  4.72s/it]loss_total_epoch 357.39042434841394
Training tokenizer:  84% 6791/8047 [5:53:46<1:38:34,  4.71s/it]loss_total_epoch 357.4378415308893
Training tokenizer:  84% 6792/8047 [5:53:51<1:38:50,  4.73s/it]loss_total_epoch 357.4776507616043
Training tokenizer:  84% 6793/8047 [5:53:55<1:38:28,  4.71s/it]loss_total_epoch 357.5071093477309
Training tokenizer:  84% 6794/8047 [5:54:00<1:38:05,  4.70s/it]loss_total_epoch 357.5458842255175
Training tokenizer:  84% 6795/8047 [5:54:05<1:38:20,  4.71s/it]loss_total_epoch 357.58981804922223
Training tokenizer:  84% 6796/8047 [5:54:09<1:38:12,  4.71s/it]loss_total_epoch 357.6361781768501
Training tokenizer:  84% 6797/8047 [5:54:14<1:37:52,  4.70s/it]loss_total_epoch 357.6802103891969
Training tokenizer:  84% 6798/8047 [5:54:19<1:38:05,  4.71s/it]loss_total_epoch 357.7221870608628
Training tokenizer:  84% 6799/8047 [5:54:24<1:38:03,  4.71s/it]loss_total_epoch 357.7639633603394
Training tokenizer:  85% 6800/8047 [5:54:28<1:37:47,  4.71s/it]loss_total_epoch 357.80249324813485
Training tokenizer:  85% 6801/8047 [5:54:33<1:38:05,  4.72s/it]loss_total_epoch 357.8380525633693
Training tokenizer:  85% 6802/8047 [5:54:38<1:37:58,  4.72s/it]loss_total_epoch 357.87402085214853
Training tokenizer:  85% 6803/8047 [5:54:42<1:37:30,  4.70s/it]loss_total_epoch 357.90816067904234
Training tokenizer:  85% 6804/8047 [5:54:47<1:37:30,  4.71s/it]loss_total_epoch 357.9522685967386
Training tokenizer:  85% 6805/8047 [5:54:52<1:37:18,  4.70s/it]loss_total_epoch 357.99369471520185
Training tokenizer:  85% 6806/8047 [5:54:57<1:37:42,  4.72s/it]loss_total_epoch 358.03470354899764
Training tokenizer:  85% 6807/8047 [5:55:01<1:37:31,  4.72s/it]loss_total_epoch 358.0636209100485
Training tokenizer:  85% 6808/8047 [5:55:06<1:37:12,  4.71s/it]loss_total_epoch 358.10741982236505
Training tokenizer:  85% 6809/8047 [5:55:11<1:37:17,  4.71s/it]loss_total_epoch 358.1521901451051
Training tokenizer:  85% 6810/8047 [5:55:15<1:37:20,  4.72s/it]loss_total_epoch 358.1858717724681
Training tokenizer:  85% 6811/8047 [5:55:20<1:37:24,  4.73s/it]loss_total_epoch 358.2324166186154
Training tokenizer:  85% 6812/8047 [5:55:25<1:37:14,  4.72s/it]loss_total_epoch 358.2849905155599
Training tokenizer:  85% 6813/8047 [5:55:30<1:37:17,  4.73s/it]loss_total_epoch 358.3282860815525
Training tokenizer:  85% 6814/8047 [5:55:34<1:37:13,  4.73s/it]loss_total_epoch 358.363339047879
Training tokenizer:  85% 6815/8047 [5:55:39<1:36:53,  4.72s/it]loss_total_epoch 358.4029627852142
Training tokenizer:  85% 6816/8047 [5:55:44<1:36:33,  4.71s/it]loss_total_epoch 358.4428736232221
Training tokenizer:  85% 6817/8047 [5:55:48<1:36:36,  4.71s/it]loss_total_epoch 358.4877362251282
Training tokenizer:  85% 6818/8047 [5:55:53<1:36:40,  4.72s/it]loss_total_epoch 358.5240189470351
Training tokenizer:  85% 6819/8047 [5:55:58<1:36:39,  4.72s/it]loss_total_epoch 358.56645350158215
Training tokenizer:  85% 6820/8047 [5:56:03<1:36:31,  4.72s/it]loss_total_epoch 358.60910987481475
Training tokenizer:  85% 6821/8047 [5:56:07<1:36:23,  4.72s/it]loss_total_epoch 358.6472340077162
Training tokenizer:  85% 6822/8047 [5:56:12<1:36:25,  4.72s/it]loss_total_epoch 358.6891899853945
Training tokenizer:  85% 6823/8047 [5:56:17<1:36:16,  4.72s/it]loss_total_epoch 358.7257358916104
Training tokenizer:  85% 6824/8047 [5:56:22<1:36:11,  4.72s/it]loss_total_epoch 358.7726101540029
Training tokenizer:  85% 6825/8047 [5:56:26<1:36:05,  4.72s/it]loss_total_epoch 358.8128397911787
Training tokenizer:  85% 6826/8047 [5:56:31<1:36:03,  4.72s/it]loss_total_epoch 358.8509640581906
Training tokenizer:  85% 6827/8047 [5:56:36<1:36:01,  4.72s/it]loss_total_epoch 358.8830788247287
Training tokenizer:  85% 6828/8047 [5:56:40<1:36:06,  4.73s/it]loss_total_epoch 358.91311356052756
Training tokenizer:  85% 6829/8047 [5:56:45<1:36:18,  4.74s/it]loss_total_epoch 358.9579393789172
Training tokenizer:  85% 6830/8047 [5:56:50<1:36:12,  4.74s/it]loss_total_epoch 359.00102093815804
Training tokenizer:  85% 6831/8047 [5:56:55<1:36:38,  4.77s/it]loss_total_epoch 359.04307583346963
Training tokenizer:  85% 6832/8047 [5:57:00<1:36:36,  4.77s/it]loss_total_epoch 359.08218874037266
Training tokenizer:  85% 6833/8047 [5:57:04<1:36:34,  4.77s/it]loss_total_epoch 359.1253658980131
Training tokenizer:  85% 6834/8047 [5:57:09<1:36:24,  4.77s/it]loss_total_epoch 359.1573062762618
Training tokenizer:  85% 6835/8047 [5:57:14<1:36:16,  4.77s/it]loss_total_epoch 359.1943130977452
Training tokenizer:  85% 6836/8047 [5:57:19<1:35:52,  4.75s/it]loss_total_epoch 359.24396819248796
Training tokenizer:  85% 6837/8047 [5:57:23<1:34:43,  4.70s/it]loss_total_epoch 359.28638518229127
Training tokenizer:  85% 6838/8047 [5:57:28<1:34:40,  4.70s/it]loss_total_epoch 359.3164206016809
Training tokenizer:  85% 6839/8047 [5:57:33<1:34:54,  4.71s/it]loss_total_epoch 359.3556192126125
Training tokenizer:  85% 6840/8047 [5:57:37<1:34:51,  4.72s/it]loss_total_epoch 359.39063541404903
Training tokenizer:  85% 6841/8047 [5:57:42<1:35:09,  4.73s/it]loss_total_epoch 359.4294418375939
Training tokenizer:  85% 6842/8047 [5:57:47<1:35:17,  4.74s/it]loss_total_epoch 359.4724952634424
Training tokenizer:  85% 6843/8047 [5:57:52<1:35:00,  4.73s/it]loss_total_epoch 359.5143355857581
Training tokenizer:  85% 6844/8047 [5:57:56<1:34:48,  4.73s/it]loss_total_epoch 359.565450521186
Training tokenizer:  85% 6845/8047 [5:58:01<1:34:47,  4.73s/it]loss_total_epoch 359.6154985371977
Training tokenizer:  85% 6846/8047 [5:58:06<1:34:52,  4.74s/it]loss_total_epoch 359.65608492307365
Training tokenizer:  85% 6847/8047 [5:58:10<1:34:36,  4.73s/it]loss_total_epoch 359.6886594425887
Training tokenizer:  85% 6848/8047 [5:58:15<1:34:30,  4.73s/it]loss_total_epoch 359.7303409446031
Training tokenizer:  85% 6849/8047 [5:58:20<1:34:04,  4.71s/it]loss_total_epoch 359.77588235400617
Training tokenizer:  85% 6850/8047 [5:58:25<1:34:09,  4.72s/it]loss_total_epoch 359.8175835516304
Training tokenizer:  85% 6851/8047 [5:58:29<1:34:00,  4.72s/it]loss_total_epoch 359.8536008168012
Training tokenizer:  85% 6852/8047 [5:58:34<1:33:53,  4.71s/it]loss_total_epoch 359.90029244311154
Training tokenizer:  85% 6853/8047 [5:58:39<1:34:22,  4.74s/it]loss_total_epoch 359.94231159053743
Training tokenizer:  85% 6854/8047 [5:58:44<1:34:30,  4.75s/it]loss_total_epoch 359.9965904187411
Training tokenizer:  85% 6855/8047 [5:58:48<1:34:23,  4.75s/it]loss_total_epoch 360.04333788342774
Training tokenizer:  85% 6856/8047 [5:58:53<1:34:09,  4.74s/it]loss_total_epoch 360.07274184376
Training tokenizer:  85% 6857/8047 [5:58:58<1:34:12,  4.75s/it]loss_total_epoch 360.1177657097578
Training tokenizer:  85% 6858/8047 [5:59:03<1:34:08,  4.75s/it]loss_total_epoch 360.1581363119185
Training tokenizer:  85% 6859/8047 [5:59:07<1:33:49,  4.74s/it]loss_total_epoch 360.1943853609264
Training tokenizer:  85% 6860/8047 [5:59:12<1:33:52,  4.75s/it]loss_total_epoch 360.2387922294438
Training tokenizer:  85% 6861/8047 [5:59:17<1:33:35,  4.73s/it]loss_total_epoch 360.28750639781356
Training tokenizer:  85% 6862/8047 [5:59:22<1:33:36,  4.74s/it]loss_total_epoch 360.33725179359317
Training tokenizer:  85% 6863/8047 [5:59:26<1:33:33,  4.74s/it]loss_total_epoch 360.38251227140427
Training tokenizer:  85% 6864/8047 [5:59:31<1:33:11,  4.73s/it]loss_total_epoch 360.4210681580007
Training tokenizer:  85% 6865/8047 [5:59:36<1:33:27,  4.74s/it]loss_total_epoch 360.468128927052
Training tokenizer:  85% 6866/8047 [5:59:41<1:33:30,  4.75s/it]loss_total_epoch 360.50181033834815
Training tokenizer:  85% 6867/8047 [5:59:45<1:33:30,  4.75s/it]loss_total_epoch 360.5381302423775
Training tokenizer:  85% 6868/8047 [5:59:50<1:33:14,  4.75s/it]loss_total_epoch 360.56808470748365
Training tokenizer:  85% 6869/8047 [5:59:55<1:33:32,  4.76s/it]loss_total_epoch 360.6121618207544
Training tokenizer:  85% 6870/8047 [6:00:00<1:33:42,  4.78s/it]loss_total_epoch 360.64755847491324
Training tokenizer:  85% 6871/8047 [6:00:04<1:32:30,  4.72s/it]loss_total_epoch 360.6870479453355
Training tokenizer:  85% 6872/8047 [6:00:09<1:32:34,  4.73s/it]loss_total_epoch 360.72719942219555
Training tokenizer:  85% 6873/8047 [6:00:14<1:32:50,  4.74s/it]loss_total_epoch 360.76494707353413
Training tokenizer:  85% 6874/8047 [6:00:18<1:32:09,  4.71s/it]loss_total_epoch 360.8151430506259
Training tokenizer:  85% 6875/8047 [6:00:23<1:32:05,  4.71s/it]loss_total_epoch 360.86305754445493
Training tokenizer:  85% 6876/8047 [6:00:28<1:32:16,  4.73s/it]loss_total_epoch 360.9010901134461
Training tokenizer:  85% 6877/8047 [6:00:33<1:32:27,  4.74s/it]loss_total_epoch 360.9465849790722
Training tokenizer:  85% 6878/8047 [6:00:37<1:32:26,  4.74s/it]loss_total_epoch 360.9960267227143
Training tokenizer:  85% 6879/8047 [6:00:42<1:32:03,  4.73s/it]loss_total_epoch 361.0334078129381
Training tokenizer:  85% 6880/8047 [6:00:47<1:31:54,  4.73s/it]loss_total_epoch 361.07697657682
Training tokenizer:  86% 6881/8047 [6:00:52<1:32:05,  4.74s/it]loss_total_epoch 361.1211787555367
Training tokenizer:  86% 6882/8047 [6:00:56<1:32:04,  4.74s/it]loss_total_epoch 361.1645393129438
Training tokenizer:  86% 6883/8047 [6:01:01<1:31:42,  4.73s/it]loss_total_epoch 361.2073899861425
Training tokenizer:  86% 6884/8047 [6:01:06<1:31:46,  4.74s/it]loss_total_epoch 361.2455795351416
Training tokenizer:  86% 6885/8047 [6:01:11<1:31:59,  4.75s/it]loss_total_epoch 361.29543488658965
Training tokenizer:  86% 6886/8047 [6:01:15<1:31:56,  4.75s/it]loss_total_epoch 361.3390885461122
Training tokenizer:  86% 6887/8047 [6:01:20<1:32:07,  4.77s/it]loss_total_epoch 361.38218674622476
Training tokenizer:  86% 6888/8047 [6:01:25<1:31:49,  4.75s/it]loss_total_epoch 361.4301246460527
Training tokenizer:  86% 6889/8047 [6:01:30<1:31:47,  4.76s/it]loss_total_epoch 361.4723394047469
Training tokenizer:  86% 6890/8047 [6:01:34<1:31:46,  4.76s/it]loss_total_epoch 361.5164387729019
Training tokenizer:  86% 6891/8047 [6:01:39<1:31:39,  4.76s/it]loss_total_epoch 361.5570684093982
Training tokenizer:  86% 6892/8047 [6:01:44<1:31:54,  4.77s/it]loss_total_epoch 361.5857529398054
Training tokenizer:  86% 6893/8047 [6:01:49<1:31:47,  4.77s/it]loss_total_epoch 361.62874163500965
Training tokenizer:  86% 6894/8047 [6:01:53<1:31:43,  4.77s/it]loss_total_epoch 361.66156180389225
Training tokenizer:  86% 6895/8047 [6:01:58<1:31:16,  4.75s/it]loss_total_epoch 361.6987997125834
Training tokenizer:  86% 6896/8047 [6:02:03<1:30:59,  4.74s/it]loss_total_epoch 361.7321755159646
Training tokenizer:  86% 6897/8047 [6:02:08<1:31:01,  4.75s/it]loss_total_epoch 361.7811396513134
Training tokenizer:  86% 6898/8047 [6:02:12<1:30:53,  4.75s/it]loss_total_epoch 361.8288961108774
Training tokenizer:  86% 6899/8047 [6:02:17<1:30:41,  4.74s/it]loss_total_epoch 361.8735831659287
Training tokenizer:  86% 6900/8047 [6:02:22<1:30:25,  4.73s/it]loss_total_epoch 361.91183258779347
Training tokenizer:  86% 6901/8047 [6:02:27<1:30:25,  4.73s/it]loss_total_epoch 361.95772722922266
Training tokenizer:  86% 6902/8047 [6:02:31<1:30:26,  4.74s/it]loss_total_epoch 362.0069912131876
Training tokenizer:  86% 6903/8047 [6:02:36<1:30:32,  4.75s/it]loss_total_epoch 362.0514428894967
Training tokenizer:  86% 6904/8047 [6:02:41<1:30:34,  4.75s/it]loss_total_epoch 362.082029895857
Training tokenizer:  86% 6905/8047 [6:02:46<1:30:20,  4.75s/it]loss_total_epoch 362.1259405184537
Training tokenizer:  86% 6906/8047 [6:02:50<1:29:52,  4.73s/it]loss_total_epoch 362.1590042617172
Training tokenizer:  86% 6907/8047 [6:02:55<1:29:44,  4.72s/it]loss_total_epoch 362.2085476536304
Training tokenizer:  86% 6908/8047 [6:03:00<1:29:39,  4.72s/it]loss_total_epoch 362.2466922905296
Training tokenizer:  86% 6909/8047 [6:03:04<1:29:19,  4.71s/it]loss_total_epoch 362.2787096593529
Training tokenizer:  86% 6910/8047 [6:03:09<1:29:34,  4.73s/it]loss_total_epoch 362.32615808956325
Training tokenizer:  86% 6911/8047 [6:03:14<1:29:44,  4.74s/it]loss_total_epoch 362.3541828598827
Training tokenizer:  86% 6912/8047 [6:03:19<1:29:35,  4.74s/it]loss_total_epoch 362.3963326420635
Training tokenizer:  86% 6913/8047 [6:03:23<1:29:41,  4.75s/it]loss_total_epoch 362.445297813043
Training tokenizer:  86% 6914/8047 [6:03:28<1:29:32,  4.74s/it]loss_total_epoch 362.4915299024433
Training tokenizer:  86% 6915/8047 [6:03:33<1:29:17,  4.73s/it]loss_total_epoch 362.5274899918586
Training tokenizer:  86% 6916/8047 [6:03:38<1:29:15,  4.74s/it]loss_total_epoch 362.56602230481803
Training tokenizer:  86% 6917/8047 [6:03:42<1:29:10,  4.74s/it]loss_total_epoch 362.6114221755415
Training tokenizer:  86% 6918/8047 [6:03:47<1:29:07,  4.74s/it]loss_total_epoch 362.649404944852
Training tokenizer:  86% 6919/8047 [6:03:52<1:28:57,  4.73s/it]loss_total_epoch 362.68126377649605
Training tokenizer:  86% 6920/8047 [6:03:57<1:29:16,  4.75s/it]loss_total_epoch 362.7233900334686
Training tokenizer:  86% 6921/8047 [6:04:01<1:29:14,  4.76s/it]loss_total_epoch 362.7602941375226
Training tokenizer:  86% 6922/8047 [6:04:06<1:29:03,  4.75s/it]loss_total_epoch 362.7965257372707
Training tokenizer:  86% 6923/8047 [6:04:11<1:28:50,  4.74s/it]loss_total_epoch 362.8437286373228
Training tokenizer:  86% 6924/8047 [6:04:16<1:28:42,  4.74s/it]loss_total_epoch 362.875223653391
Training tokenizer:  86% 6925/8047 [6:04:20<1:28:37,  4.74s/it]loss_total_epoch 362.9173984248191
Training tokenizer:  86% 6926/8047 [6:04:25<1:28:29,  4.74s/it]loss_total_epoch 362.96058933250606
Training tokenizer:  86% 6927/8047 [6:04:30<1:28:28,  4.74s/it]loss_total_epoch 362.9952452573925
Training tokenizer:  86% 6928/8047 [6:04:35<1:28:46,  4.76s/it]loss_total_epoch 363.02978124283254
Training tokenizer:  86% 6929/8047 [6:04:39<1:28:52,  4.77s/it]loss_total_epoch 363.06597906164825
Training tokenizer:  86% 6930/8047 [6:04:44<1:29:12,  4.79s/it]loss_total_epoch 363.10210918821394
Training tokenizer:  86% 6931/8047 [6:04:49<1:28:47,  4.77s/it]loss_total_epoch 363.1381619218737
Training tokenizer:  86% 6932/8047 [6:04:54<1:28:37,  4.77s/it]loss_total_epoch 363.17536530829966
Training tokenizer:  86% 6933/8047 [6:04:58<1:28:14,  4.75s/it]loss_total_epoch 363.22320333682
Training tokenizer:  86% 6934/8047 [6:05:03<1:28:06,  4.75s/it]loss_total_epoch 363.25318185053766
Training tokenizer:  86% 6935/8047 [6:05:08<1:28:22,  4.77s/it]loss_total_epoch 363.29298268817365
Training tokenizer:  86% 6936/8047 [6:05:13<1:28:20,  4.77s/it]loss_total_epoch 363.335519393906
Training tokenizer:  86% 6937/8047 [6:05:18<1:28:16,  4.77s/it]loss_total_epoch 363.37491209991276
Training tokenizer:  86% 6938/8047 [6:05:22<1:27:40,  4.74s/it]loss_total_epoch 363.41397617571056
Training tokenizer:  86% 6939/8047 [6:05:27<1:27:34,  4.74s/it]loss_total_epoch 363.46918363310397
Training tokenizer:  86% 6940/8047 [6:05:32<1:27:41,  4.75s/it]loss_total_epoch 363.51158165745437
Training tokenizer:  86% 6941/8047 [6:05:36<1:27:35,  4.75s/it]loss_total_epoch 363.5437911134213
Training tokenizer:  86% 6942/8047 [6:05:41<1:27:08,  4.73s/it]loss_total_epoch 363.5879817735404
Training tokenizer:  86% 6943/8047 [6:05:46<1:26:50,  4.72s/it]loss_total_epoch 363.6265398990363
Training tokenizer:  86% 6944/8047 [6:05:51<1:26:58,  4.73s/it]loss_total_epoch 363.6649205889553
Training tokenizer:  86% 6945/8047 [6:05:55<1:26:50,  4.73s/it]loss_total_epoch 363.70633426494896
Training tokenizer:  86% 6946/8047 [6:06:00<1:26:40,  4.72s/it]loss_total_epoch 363.74205471388996
Training tokenizer:  86% 6947/8047 [6:06:05<1:27:01,  4.75s/it]loss_total_epoch 363.78333261422813
Training tokenizer:  86% 6948/8047 [6:06:10<1:26:49,  4.74s/it]loss_total_epoch 363.82625217549503
Training tokenizer:  86% 6949/8047 [6:06:14<1:26:43,  4.74s/it]loss_total_epoch 363.861269114539
Training tokenizer:  86% 6950/8047 [6:06:19<1:26:33,  4.73s/it]loss_total_epoch 363.9072737004608
Training tokenizer:  86% 6951/8047 [6:06:24<1:26:31,  4.74s/it]loss_total_epoch 363.95742023177445
Training tokenizer:  86% 6952/8047 [6:06:29<1:26:26,  4.74s/it]loss_total_epoch 363.99650749377906
Training tokenizer:  86% 6953/8047 [6:06:33<1:26:41,  4.75s/it]loss_total_epoch 364.04265226610005
Training tokenizer:  86% 6954/8047 [6:06:38<1:26:38,  4.76s/it]loss_total_epoch 364.08392024599016
Training tokenizer:  86% 6955/8047 [6:06:43<1:26:38,  4.76s/it]loss_total_epoch 364.11647243238986
Training tokenizer:  86% 6956/8047 [6:06:48<1:26:30,  4.76s/it]loss_total_epoch 364.15306124277413
Training tokenizer:  86% 6957/8047 [6:06:52<1:26:03,  4.74s/it]loss_total_epoch 364.1893288511783
Training tokenizer:  86% 6958/8047 [6:06:57<1:25:51,  4.73s/it]loss_total_epoch 364.23266494460404
Training tokenizer:  86% 6959/8047 [6:07:02<1:25:34,  4.72s/it]loss_total_epoch 364.2747287917882
Training tokenizer:  86% 6960/8047 [6:07:06<1:25:06,  4.70s/it]loss_total_epoch 364.31912302039564
Training tokenizer:  87% 6961/8047 [6:07:11<1:25:39,  4.73s/it]loss_total_epoch 364.3566028792411
Training tokenizer:  87% 6962/8047 [6:07:16<1:25:52,  4.75s/it]loss_total_epoch 364.40272025577724
Training tokenizer:  87% 6963/8047 [6:07:21<1:26:02,  4.76s/it]loss_total_epoch 364.4329681713134
Training tokenizer:  87% 6964/8047 [6:07:26<1:26:00,  4.77s/it]loss_total_epoch 364.4769974742085
Training tokenizer:  87% 6965/8047 [6:07:30<1:25:51,  4.76s/it]loss_total_epoch 364.52168416790664
Training tokenizer:  87% 6966/8047 [6:07:35<1:25:48,  4.76s/it]loss_total_epoch 364.55443203635514
Training tokenizer:  87% 6967/8047 [6:07:40<1:25:43,  4.76s/it]loss_total_epoch 364.59537186287344
Training tokenizer:  87% 6968/8047 [6:07:45<1:25:41,  4.77s/it]loss_total_epoch 364.6362760383636
Training tokenizer:  87% 6969/8047 [6:07:49<1:25:17,  4.75s/it]loss_total_epoch 364.67096006684005
Training tokenizer:  87% 6970/8047 [6:07:54<1:25:08,  4.74s/it]loss_total_epoch 364.7201521527022
Training tokenizer:  87% 6971/8047 [6:07:59<1:24:54,  4.73s/it]loss_total_epoch 364.7639951761812
Training tokenizer:  87% 6972/8047 [6:08:03<1:24:56,  4.74s/it]loss_total_epoch 364.7987316120416
Training tokenizer:  87% 6973/8047 [6:08:08<1:24:55,  4.74s/it]loss_total_epoch 364.834546437487
Training tokenizer:  87% 6974/8047 [6:08:13<1:24:34,  4.73s/it]loss_total_epoch 364.86135024763644
Training tokenizer:  87% 6975/8047 [6:08:18<1:24:46,  4.74s/it]loss_total_epoch 364.91500042937696
Training tokenizer:  87% 6976/8047 [6:08:22<1:24:31,  4.73s/it]loss_total_epoch 364.9511732365936
Training tokenizer:  87% 6977/8047 [6:08:27<1:24:26,  4.74s/it]loss_total_epoch 364.98786776326597
Training tokenizer:  87% 6978/8047 [6:08:32<1:24:24,  4.74s/it]loss_total_epoch 365.02156709320843
Training tokenizer:  87% 6979/8047 [6:08:37<1:24:08,  4.73s/it]loss_total_epoch 365.05797507055104
Training tokenizer:  87% 6980/8047 [6:08:41<1:24:16,  4.74s/it]loss_total_epoch 365.08703895285726
Training tokenizer:  87% 6981/8047 [6:08:46<1:24:19,  4.75s/it]loss_total_epoch 365.1236694306135
Training tokenizer:  87% 6982/8047 [6:08:51<1:24:04,  4.74s/it]loss_total_epoch 365.16374922916293
Training tokenizer:  87% 6983/8047 [6:08:56<1:24:15,  4.75s/it]loss_total_epoch 365.20454344153404
Training tokenizer:  87% 6984/8047 [6:09:00<1:23:58,  4.74s/it]loss_total_epoch 365.2494315095246
Training tokenizer:  87% 6985/8047 [6:09:05<1:23:50,  4.74s/it]loss_total_epoch 365.2891368493438
Training tokenizer:  87% 6986/8047 [6:09:10<1:23:55,  4.75s/it]loss_total_epoch 365.33321333676577
Training tokenizer:  87% 6987/8047 [6:09:14<1:23:25,  4.72s/it]loss_total_epoch 365.37801795452833
Training tokenizer:  87% 6988/8047 [6:09:19<1:23:26,  4.73s/it]loss_total_epoch 365.4170591197908
Training tokenizer:  87% 6989/8047 [6:09:24<1:23:09,  4.72s/it]loss_total_epoch 365.4600808247924
Training tokenizer:  87% 6990/8047 [6:09:29<1:23:10,  4.72s/it]loss_total_epoch 365.50862684845924
Training tokenizer:  87% 6991/8047 [6:09:33<1:22:56,  4.71s/it]loss_total_epoch 365.55494947358966
Training tokenizer:  87% 6992/8047 [6:09:38<1:23:02,  4.72s/it]loss_total_epoch 365.5878546126187
Training tokenizer:  87% 6993/8047 [6:09:43<1:22:54,  4.72s/it]loss_total_epoch 365.633209630847
Training tokenizer:  87% 6994/8047 [6:09:48<1:23:05,  4.74s/it]loss_total_epoch 365.66937512531877
Training tokenizer:  87% 6995/8047 [6:09:52<1:23:03,  4.74s/it]loss_total_epoch 365.7056289277971
Training tokenizer:  87% 6996/8047 [6:09:57<1:23:05,  4.74s/it]loss_total_epoch 365.75411054491997
Training tokenizer:  87% 6997/8047 [6:10:02<1:23:00,  4.74s/it]loss_total_epoch 365.788987737149
Training tokenizer:  87% 6998/8047 [6:10:07<1:22:53,  4.74s/it]loss_total_epoch 365.8212245814502
Training tokenizer:  87% 6999/8047 [6:10:11<1:22:41,  4.73s/it]loss_total_epoch 365.858801137656
Training tokenizer:  87% 7000/8047 [6:10:16<1:22:48,  4.75s/it]loss_total_epoch 365.8989907465875
Training tokenizer:  87% 7001/8047 [6:10:21<1:22:47,  4.75s/it]loss_total_epoch 365.93897680565715
Training tokenizer:  87% 7002/8047 [6:10:26<1:22:41,  4.75s/it]loss_total_epoch 365.97582211717963
Training tokenizer:  87% 7003/8047 [6:10:30<1:22:49,  4.76s/it]loss_total_epoch 366.021336723119
Training tokenizer:  87% 7004/8047 [6:10:35<1:22:52,  4.77s/it]loss_total_epoch 366.0628259703517
Training tokenizer:  87% 7005/8047 [6:10:40<1:22:32,  4.75s/it]loss_total_epoch 366.10408974811435
Training tokenizer:  87% 7006/8047 [6:10:44<1:21:18,  4.69s/it]loss_total_epoch 366.14305187016726
Training tokenizer:  87% 7007/8047 [6:10:49<1:21:26,  4.70s/it]loss_total_epoch 366.18582171574235
Training tokenizer:  87% 7008/8047 [6:10:54<1:21:20,  4.70s/it]loss_total_epoch 366.22433096542954
Training tokenizer:  87% 7009/8047 [6:10:58<1:20:25,  4.65s/it]loss_total_epoch 366.2593422085047
Training tokenizer:  87% 7010/8047 [6:11:03<1:20:42,  4.67s/it]loss_total_epoch 366.30194974690676
Training tokenizer:  87% 7011/8047 [6:11:08<1:21:07,  4.70s/it]loss_total_epoch 366.33883057907224
Training tokenizer:  87% 7012/8047 [6:11:13<1:21:22,  4.72s/it]loss_total_epoch 366.3803076930344
Training tokenizer:  87% 7013/8047 [6:11:17<1:21:23,  4.72s/it]loss_total_epoch 366.4261978827417
Training tokenizer:  87% 7014/8047 [6:11:22<1:21:16,  4.72s/it]loss_total_epoch 366.46274484694004
Training tokenizer:  87% 7015/8047 [6:11:27<1:21:28,  4.74s/it]loss_total_epoch 366.5079710073769
Training tokenizer:  87% 7016/8047 [6:11:32<1:21:13,  4.73s/it]loss_total_epoch 366.54031102359295
Training tokenizer:  87% 7017/8047 [6:11:36<1:21:18,  4.74s/it]loss_total_epoch 366.57506278157234
Training tokenizer:  87% 7018/8047 [6:11:41<1:20:42,  4.71s/it]loss_total_epoch 366.6153030768037
Training tokenizer:  87% 7019/8047 [6:11:46<1:21:00,  4.73s/it]loss_total_epoch 366.658029448241
Training tokenizer:  87% 7020/8047 [6:11:50<1:20:48,  4.72s/it]loss_total_epoch 366.6979112550616
Training tokenizer:  87% 7021/8047 [6:11:55<1:20:54,  4.73s/it]loss_total_epoch 366.73402547836304
Training tokenizer:  87% 7022/8047 [6:12:00<1:20:54,  4.74s/it]loss_total_epoch 366.77039662376046
Training tokenizer:  87% 7023/8047 [6:12:05<1:21:02,  4.75s/it]loss_total_epoch 366.81411534547806
Training tokenizer:  87% 7024/8047 [6:12:09<1:20:58,  4.75s/it]loss_total_epoch 366.85260724648833
Training tokenizer:  87% 7025/8047 [6:12:14<1:20:50,  4.75s/it]loss_total_epoch 366.90338337793946
Training tokenizer:  87% 7026/8047 [6:12:19<1:20:41,  4.74s/it]loss_total_epoch 366.93620179221034
Training tokenizer:  87% 7027/8047 [6:12:24<1:20:44,  4.75s/it]loss_total_epoch 366.9695017263293
Training tokenizer:  87% 7028/8047 [6:12:28<1:20:39,  4.75s/it]loss_total_epoch 367.0113261193037
Training tokenizer:  87% 7029/8047 [6:12:33<1:20:40,  4.75s/it]loss_total_epoch 367.04519963264465
Training tokenizer:  87% 7030/8047 [6:12:38<1:20:36,  4.76s/it]loss_total_epoch 367.0806407779455
Training tokenizer:  87% 7031/8047 [6:12:43<1:20:20,  4.74s/it]loss_total_epoch 367.120685916394
Training tokenizer:  87% 7032/8047 [6:12:47<1:20:21,  4.75s/it]loss_total_epoch 367.16817904636264
Training tokenizer:  87% 7033/8047 [6:12:52<1:20:18,  4.75s/it]loss_total_epoch 367.2024533338845
Training tokenizer:  87% 7034/8047 [6:12:57<1:19:18,  4.70s/it]loss_total_epoch 367.23961539566517
Training tokenizer:  87% 7035/8047 [6:13:01<1:19:28,  4.71s/it]loss_total_epoch 367.28157437592745
Training tokenizer:  87% 7036/8047 [6:13:06<1:19:22,  4.71s/it]loss_total_epoch 367.322816811502
Training tokenizer:  87% 7037/8047 [6:13:11<1:19:30,  4.72s/it]loss_total_epoch 367.3674626387656
Training tokenizer:  87% 7038/8047 [6:13:16<1:19:27,  4.73s/it]loss_total_epoch 367.4162589162588
Training tokenizer:  87% 7039/8047 [6:13:20<1:19:35,  4.74s/it]loss_total_epoch 367.46494870632887
Training tokenizer:  87% 7040/8047 [6:13:25<1:19:23,  4.73s/it]loss_total_epoch 367.51334262266755
Training tokenizer:  87% 7041/8047 [6:13:30<1:19:15,  4.73s/it]loss_total_epoch 367.55009542033076
Training tokenizer:  88% 7042/8047 [6:13:35<1:19:18,  4.73s/it]loss_total_epoch 367.58478927239776
Training tokenizer:  88% 7043/8047 [6:13:39<1:19:18,  4.74s/it]loss_total_epoch 367.62421115860343
Training tokenizer:  88% 7044/8047 [6:13:44<1:19:32,  4.76s/it]loss_total_epoch 367.6593927554786
Training tokenizer:  88% 7045/8047 [6:13:49<1:19:47,  4.78s/it]loss_total_epoch 367.70664915814996
Training tokenizer:  88% 7046/8047 [6:13:54<1:19:18,  4.75s/it]loss_total_epoch 367.7505726441741
Training tokenizer:  88% 7047/8047 [6:13:58<1:19:05,  4.75s/it]loss_total_epoch 367.78440394997597
Training tokenizer:  88% 7048/8047 [6:14:03<1:18:48,  4.73s/it]loss_total_epoch 367.81889321655035
Training tokenizer:  88% 7049/8047 [6:14:08<1:18:40,  4.73s/it]loss_total_epoch 367.855836275965
Training tokenizer:  88% 7050/8047 [6:14:13<1:18:53,  4.75s/it]loss_total_epoch 367.8954597450793
Training tokenizer:  88% 7051/8047 [6:14:17<1:18:51,  4.75s/it]loss_total_epoch 367.9323611855507
Training tokenizer:  88% 7052/8047 [6:14:22<1:19:11,  4.78s/it]loss_total_epoch 367.966230802238
Training tokenizer:  88% 7053/8047 [6:14:27<1:18:57,  4.77s/it]loss_total_epoch 368.0153085999191
Training tokenizer:  88% 7054/8047 [6:14:32<1:18:29,  4.74s/it]loss_total_epoch 368.0566047169268
Training tokenizer:  88% 7055/8047 [6:14:36<1:18:13,  4.73s/it]loss_total_epoch 368.0967532284558
Training tokenizer:  88% 7056/8047 [6:14:41<1:18:07,  4.73s/it]loss_total_epoch 368.13136365637183
Training tokenizer:  88% 7057/8047 [6:14:46<1:18:05,  4.73s/it]loss_total_epoch 368.17418748885393
Training tokenizer:  88% 7058/8047 [6:14:51<1:18:20,  4.75s/it]loss_total_epoch 368.21458487585187
Training tokenizer:  88% 7059/8047 [6:14:55<1:18:25,  4.76s/it]loss_total_epoch 368.24729910865426
Training tokenizer:  88% 7060/8047 [6:15:00<1:18:11,  4.75s/it]loss_total_epoch 368.29099303483963
Training tokenizer:  88% 7061/8047 [6:15:05<1:18:06,  4.75s/it]loss_total_epoch 368.3258882276714
Training tokenizer:  88% 7062/8047 [6:15:10<1:17:56,  4.75s/it]loss_total_epoch 368.36950977519155
Training tokenizer:  88% 7063/8047 [6:15:14<1:17:55,  4.75s/it]loss_total_epoch 368.4041169025004
Training tokenizer:  88% 7064/8047 [6:15:19<1:17:52,  4.75s/it]loss_total_epoch 368.44509622454643
Training tokenizer:  88% 7065/8047 [6:15:24<1:17:52,  4.76s/it]loss_total_epoch 368.48319943621755
Training tokenizer:  88% 7066/8047 [6:15:29<1:17:39,  4.75s/it]loss_total_epoch 368.5309611931443
Training tokenizer:  88% 7067/8047 [6:15:33<1:17:24,  4.74s/it]loss_total_epoch 368.5635609216988
Training tokenizer:  88% 7068/8047 [6:15:38<1:17:19,  4.74s/it]loss_total_epoch 368.6081885881722
Training tokenizer:  88% 7069/8047 [6:15:43<1:17:06,  4.73s/it]loss_total_epoch 368.65669740736485
Training tokenizer:  88% 7070/8047 [6:15:48<1:16:57,  4.73s/it]loss_total_epoch 368.69092724472284
Training tokenizer:  88% 7071/8047 [6:15:52<1:16:57,  4.73s/it]loss_total_epoch 368.7271018847823
Training tokenizer:  88% 7072/8047 [6:15:57<1:17:02,  4.74s/it]loss_total_epoch 368.7592680417001
Training tokenizer:  88% 7073/8047 [6:16:02<1:17:20,  4.76s/it]loss_total_epoch 368.80465162172914
Training tokenizer:  88% 7074/8047 [6:16:07<1:17:01,  4.75s/it]loss_total_epoch 368.842143882066
Training tokenizer:  88% 7075/8047 [6:16:11<1:16:46,  4.74s/it]loss_total_epoch 368.89154494553804
Training tokenizer:  88% 7076/8047 [6:16:16<1:15:44,  4.68s/it]loss_total_epoch 368.9314396940172
Training tokenizer:  88% 7077/8047 [6:16:21<1:15:53,  4.69s/it]loss_total_epoch 368.97620835900307
Training tokenizer:  88% 7078/8047 [6:16:25<1:16:13,  4.72s/it]loss_total_epoch 369.02730637416244
Training tokenizer:  88% 7079/8047 [6:16:30<1:16:25,  4.74s/it]loss_total_epoch 369.0696986168623
Training tokenizer:  88% 7080/8047 [6:16:35<1:16:12,  4.73s/it]loss_total_epoch 369.1037505194545
Training tokenizer:  88% 7081/8047 [6:16:40<1:16:21,  4.74s/it]loss_total_epoch 369.14735823124647
Training tokenizer:  88% 7082/8047 [6:16:44<1:16:18,  4.74s/it]loss_total_epoch 369.18999714404345
Training tokenizer:  88% 7083/8047 [6:16:49<1:16:24,  4.76s/it]loss_total_epoch 369.2318188212812
Training tokenizer:  88% 7084/8047 [6:16:54<1:16:09,  4.75s/it]loss_total_epoch 369.2638545073569
Training tokenizer:  88% 7085/8047 [6:16:59<1:15:57,  4.74s/it]loss_total_epoch 369.3071214854717
Training tokenizer:  88% 7086/8047 [6:17:03<1:15:47,  4.73s/it]loss_total_epoch 369.3514003418386
Training tokenizer:  88% 7087/8047 [6:17:08<1:15:50,  4.74s/it]loss_total_epoch 369.3970249965787
Training tokenizer:  88% 7088/8047 [6:17:13<1:15:25,  4.72s/it]loss_total_epoch 369.4445216320455
Training tokenizer:  88% 7089/8047 [6:17:18<1:15:51,  4.75s/it]loss_total_epoch 369.4882763288915
Training tokenizer:  88% 7090/8047 [6:17:22<1:15:40,  4.74s/it]loss_total_epoch 369.52444713562727
Training tokenizer:  88% 7091/8047 [6:17:27<1:15:19,  4.73s/it]loss_total_epoch 369.56549885869026
Training tokenizer:  88% 7092/8047 [6:17:32<1:14:34,  4.69s/it]loss_total_epoch 369.6081539019942
Training tokenizer:  88% 7093/8047 [6:17:36<1:14:36,  4.69s/it]loss_total_epoch 369.6518837623298
Training tokenizer:  88% 7094/8047 [6:17:41<1:14:36,  4.70s/it]loss_total_epoch 369.7016242556274
Training tokenizer:  88% 7095/8047 [6:17:46<1:14:49,  4.72s/it]loss_total_epoch 369.7531711310148
Training tokenizer:  88% 7096/8047 [6:17:50<1:14:56,  4.73s/it]loss_total_epoch 369.8030719421804
Training tokenizer:  88% 7097/8047 [6:17:55<1:15:25,  4.76s/it]loss_total_epoch 369.8446035720408
Training tokenizer:  88% 7098/8047 [6:18:00<1:15:07,  4.75s/it]loss_total_epoch 369.8926625289023
Training tokenizer:  88% 7099/8047 [6:18:05<1:14:02,  4.69s/it]loss_total_epoch 369.93103479221463
Training tokenizer:  88% 7100/8047 [6:18:09<1:14:05,  4.69s/it]loss_total_epoch 369.96433770284057
Training tokenizer:  88% 7101/8047 [6:18:14<1:13:56,  4.69s/it]loss_total_epoch 370.01096131280065
Training tokenizer:  88% 7102/8047 [6:18:19<1:14:06,  4.71s/it]loss_total_epoch 370.0516497977078
Training tokenizer:  88% 7103/8047 [6:18:23<1:14:21,  4.73s/it]loss_total_epoch 370.08780847862363
Training tokenizer:  88% 7104/8047 [6:18:28<1:14:24,  4.73s/it]loss_total_epoch 370.1308436207473
Training tokenizer:  88% 7105/8047 [6:18:33<1:14:29,  4.74s/it]loss_total_epoch 370.17532435432076
Training tokenizer:  88% 7106/8047 [6:18:38<1:14:17,  4.74s/it]loss_total_epoch 370.21387242153287
Training tokenizer:  88% 7107/8047 [6:18:42<1:14:13,  4.74s/it]loss_total_epoch 370.2562552280724
Training tokenizer:  88% 7108/8047 [6:18:47<1:14:31,  4.76s/it]loss_total_epoch 370.2973645068705
Training tokenizer:  88% 7109/8047 [6:18:52<1:13:50,  4.72s/it]loss_total_epoch 370.3362307064235
Training tokenizer:  88% 7110/8047 [6:18:57<1:13:37,  4.71s/it]loss_total_epoch 370.3787683919072
Training tokenizer:  88% 7111/8047 [6:19:01<1:13:30,  4.71s/it]loss_total_epoch 370.4120582193136
Training tokenizer:  88% 7112/8047 [6:19:06<1:13:47,  4.74s/it]loss_total_epoch 370.4490136653185
Training tokenizer:  88% 7113/8047 [6:19:11<1:13:57,  4.75s/it]loss_total_epoch 370.49521470814943
Training tokenizer:  88% 7114/8047 [6:19:16<1:13:31,  4.73s/it]loss_total_epoch 370.53442257270217
Training tokenizer:  88% 7115/8047 [6:19:20<1:13:30,  4.73s/it]loss_total_epoch 370.57903615385294
Training tokenizer:  88% 7116/8047 [6:19:25<1:13:31,  4.74s/it]loss_total_epoch 370.614383533597
Training tokenizer:  88% 7117/8047 [6:19:30<1:13:37,  4.75s/it]loss_total_epoch 370.6463567018509
Training tokenizer:  88% 7118/8047 [6:19:35<1:13:20,  4.74s/it]loss_total_epoch 370.685260809958
Training tokenizer:  88% 7119/8047 [6:19:39<1:13:34,  4.76s/it]loss_total_epoch 370.7278789617121
Training tokenizer:  88% 7120/8047 [6:19:44<1:13:20,  4.75s/it]loss_total_epoch 370.7715187035501
Training tokenizer:  88% 7121/8047 [6:19:49<1:13:11,  4.74s/it]loss_total_epoch 370.82247300073504
Training tokenizer:  89% 7122/8047 [6:19:54<1:13:00,  4.74s/it]loss_total_epoch 370.8639905899763
Training tokenizer:  89% 7123/8047 [6:19:58<1:12:31,  4.71s/it]loss_total_epoch 370.9021505750716
Training tokenizer:  89% 7124/8047 [6:20:03<1:12:45,  4.73s/it]loss_total_epoch 370.9330088496208
Training tokenizer:  89% 7125/8047 [6:20:08<1:12:46,  4.74s/it]loss_total_epoch 370.96927854791284
Training tokenizer:  89% 7126/8047 [6:20:12<1:12:33,  4.73s/it]loss_total_epoch 371.006697114557
Training tokenizer:  89% 7127/8047 [6:20:17<1:12:31,  4.73s/it]loss_total_epoch 371.0591737963259
Training tokenizer:  89% 7128/8047 [6:20:22<1:12:32,  4.74s/it]loss_total_epoch 371.10282941907644
Training tokenizer:  89% 7129/8047 [6:20:27<1:12:17,  4.73s/it]loss_total_epoch 371.14584562182426
Training tokenizer:  89% 7130/8047 [6:20:31<1:12:19,  4.73s/it]loss_total_epoch 371.18041006475687
Training tokenizer:  89% 7131/8047 [6:20:36<1:12:07,  4.72s/it]loss_total_epoch 371.22273943573236
Training tokenizer:  89% 7132/8047 [6:20:41<1:12:02,  4.72s/it]loss_total_epoch 371.2646692097187
Training tokenizer:  89% 7133/8047 [6:20:46<1:12:03,  4.73s/it]loss_total_epoch 371.31199695542455
Training tokenizer:  89% 7134/8047 [6:20:50<1:12:07,  4.74s/it]loss_total_epoch 371.35924384742975
Training tokenizer:  89% 7135/8047 [6:20:55<1:12:04,  4.74s/it]loss_total_epoch 371.3978380523622
Training tokenizer:  89% 7136/8047 [6:21:00<1:11:49,  4.73s/it]loss_total_epoch 371.44995614513755
Training tokenizer:  89% 7137/8047 [6:21:05<1:11:52,  4.74s/it]loss_total_epoch 371.4907381795347
Training tokenizer:  89% 7138/8047 [6:21:09<1:11:48,  4.74s/it]loss_total_epoch 371.5275280997157
Training tokenizer:  89% 7139/8047 [6:21:14<1:11:51,  4.75s/it]loss_total_epoch 371.57299061119556
Training tokenizer:  89% 7140/8047 [6:21:19<1:11:47,  4.75s/it]loss_total_epoch 371.60962907224894
Training tokenizer:  89% 7141/8047 [6:21:24<1:11:55,  4.76s/it]loss_total_epoch 371.649708468467
Training tokenizer:  89% 7142/8047 [6:21:28<1:11:53,  4.77s/it]loss_total_epoch 371.69403987005353
Training tokenizer:  89% 7143/8047 [6:21:33<1:11:37,  4.75s/it]loss_total_epoch 371.73656257614493
Training tokenizer:  89% 7144/8047 [6:21:38<1:11:39,  4.76s/it]loss_total_epoch 371.76630726642907
Training tokenizer:  89% 7145/8047 [6:21:43<1:11:27,  4.75s/it]loss_total_epoch 371.8092510346323
Training tokenizer:  89% 7146/8047 [6:21:47<1:11:16,  4.75s/it]loss_total_epoch 371.8464587274939
Training tokenizer:  89% 7147/8047 [6:21:52<1:11:26,  4.76s/it]loss_total_epoch 371.89316436462104
Training tokenizer:  89% 7148/8047 [6:21:57<1:11:03,  4.74s/it]loss_total_epoch 371.9280400183052
Training tokenizer:  89% 7149/8047 [6:22:02<1:10:58,  4.74s/it]loss_total_epoch 371.96520328335464
Training tokenizer:  89% 7150/8047 [6:22:06<1:10:52,  4.74s/it]loss_total_epoch 372.006972393021
Training tokenizer:  89% 7151/8047 [6:22:11<1:10:53,  4.75s/it]loss_total_epoch 372.0516780447215
Training tokenizer:  89% 7152/8047 [6:22:16<1:10:46,  4.75s/it]loss_total_epoch 372.0992434602231
Training tokenizer:  89% 7153/8047 [6:22:20<1:10:34,  4.74s/it]loss_total_epoch 372.1469048317522
Training tokenizer:  89% 7154/8047 [6:22:25<1:10:30,  4.74s/it]loss_total_epoch 372.17837682925165
Training tokenizer:  89% 7155/8047 [6:22:30<1:10:37,  4.75s/it]loss_total_epoch 372.2198112215847
Training tokenizer:  89% 7156/8047 [6:22:35<1:10:20,  4.74s/it]loss_total_epoch 372.2552748415619
Training tokenizer:  89% 7157/8047 [6:22:39<1:10:23,  4.75s/it]loss_total_epoch 372.30592917464674
Training tokenizer:  89% 7158/8047 [6:22:44<1:10:27,  4.76s/it]loss_total_epoch 372.35584801994264
Training tokenizer:  89% 7159/8047 [6:22:49<1:10:18,  4.75s/it]loss_total_epoch 372.3945405203849
Training tokenizer:  89% 7160/8047 [6:22:54<1:10:17,  4.75s/it]loss_total_epoch 372.4326474759728
Training tokenizer:  89% 7161/8047 [6:22:58<1:10:03,  4.74s/it]loss_total_epoch 372.47726872749627
Training tokenizer:  89% 7162/8047 [6:23:03<1:10:13,  4.76s/it]loss_total_epoch 372.5210188794881
Training tokenizer:  89% 7163/8047 [6:23:08<1:10:14,  4.77s/it]loss_total_epoch 372.56677587516606
Training tokenizer:  89% 7164/8047 [6:23:13<1:10:01,  4.76s/it]loss_total_epoch 372.5998815651983
Training tokenizer:  89% 7165/8047 [6:23:18<1:09:41,  4.74s/it]loss_total_epoch 372.6474304366857
Training tokenizer:  89% 7166/8047 [6:23:22<1:09:38,  4.74s/it]loss_total_epoch 372.68301170505583
Training tokenizer:  89% 7167/8047 [6:23:27<1:09:19,  4.73s/it]loss_total_epoch 372.72387142665684
Training tokenizer:  89% 7168/8047 [6:23:32<1:09:25,  4.74s/it]loss_total_epoch 372.7732597347349
Training tokenizer:  89% 7169/8047 [6:23:36<1:09:14,  4.73s/it]loss_total_epoch 372.8107123170048
Training tokenizer:  89% 7170/8047 [6:23:41<1:09:25,  4.75s/it]loss_total_epoch 372.85207800753415
Training tokenizer:  89% 7171/8047 [6:23:46<1:09:19,  4.75s/it]loss_total_epoch 372.894721256569
Training tokenizer:  89% 7172/8047 [6:23:51<1:09:21,  4.76s/it]loss_total_epoch 372.93441629223526
Training tokenizer:  89% 7173/8047 [6:23:55<1:08:55,  4.73s/it]loss_total_epoch 372.9751813802868
Training tokenizer:  89% 7174/8047 [6:24:00<1:08:58,  4.74s/it]loss_total_epoch 373.0163635071367
Training tokenizer:  89% 7175/8047 [6:24:05<1:08:42,  4.73s/it]loss_total_epoch 373.0563431289047
Training tokenizer:  89% 7176/8047 [6:24:10<1:08:36,  4.73s/it]loss_total_epoch 373.10393172316253
Training tokenizer:  89% 7177/8047 [6:24:14<1:08:40,  4.74s/it]loss_total_epoch 373.14779766462743
Training tokenizer:  89% 7178/8047 [6:24:19<1:08:42,  4.74s/it]loss_total_epoch 373.19595968164504
Training tokenizer:  89% 7179/8047 [6:24:24<1:08:52,  4.76s/it]loss_total_epoch 373.24587802775204
Training tokenizer:  89% 7180/8047 [6:24:29<1:08:41,  4.75s/it]loss_total_epoch 373.29371569491923
Training tokenizer:  89% 7181/8047 [6:24:33<1:08:33,  4.75s/it]loss_total_epoch 373.3225466515869
Training tokenizer:  89% 7182/8047 [6:24:38<1:08:26,  4.75s/it]loss_total_epoch 373.35734896175563
Training tokenizer:  89% 7183/8047 [6:24:43<1:08:23,  4.75s/it]loss_total_epoch 373.3960030619055
Training tokenizer:  89% 7184/8047 [6:24:48<1:08:08,  4.74s/it]loss_total_epoch 373.4385107886046
Training tokenizer:  89% 7185/8047 [6:24:52<1:08:04,  4.74s/it]loss_total_epoch 373.481021149084
Training tokenizer:  89% 7186/8047 [6:24:57<1:08:00,  4.74s/it]loss_total_epoch 373.52135958336294
Training tokenizer:  89% 7187/8047 [6:25:02<1:08:04,  4.75s/it]loss_total_epoch 373.56007794849575
Training tokenizer:  89% 7188/8047 [6:25:07<1:07:56,  4.75s/it]loss_total_epoch 373.590620322153
Training tokenizer:  89% 7189/8047 [6:25:11<1:07:47,  4.74s/it]loss_total_epoch 373.63784407265484
Training tokenizer:  89% 7190/8047 [6:25:16<1:07:37,  4.73s/it]loss_total_epoch 373.67234518937767
Training tokenizer:  89% 7191/8047 [6:25:21<1:07:20,  4.72s/it]loss_total_epoch 373.7132638525218
Training tokenizer:  89% 7192/8047 [6:25:25<1:07:16,  4.72s/it]loss_total_epoch 373.76077674143016
Training tokenizer:  89% 7193/8047 [6:25:30<1:07:20,  4.73s/it]loss_total_epoch 373.80077272839844
Training tokenizer:  89% 7194/8047 [6:25:35<1:07:21,  4.74s/it]loss_total_epoch 373.84491883404553
Training tokenizer:  89% 7195/8047 [6:25:40<1:07:13,  4.73s/it]loss_total_epoch 373.87857663072646
Training tokenizer:  89% 7196/8047 [6:25:44<1:07:06,  4.73s/it]loss_total_epoch 373.92794351466
Training tokenizer:  89% 7197/8047 [6:25:49<1:07:07,  4.74s/it]loss_total_epoch 373.9666852969676
Training tokenizer:  89% 7198/8047 [6:25:54<1:07:04,  4.74s/it]loss_total_epoch 374.0119618307799
Training tokenizer:  89% 7199/8047 [6:25:59<1:07:08,  4.75s/it]loss_total_epoch 374.0476894918829
Training tokenizer:  89% 7200/8047 [6:26:03<1:07:00,  4.75s/it]loss_total_epoch 374.0949914995581
Training tokenizer:  89% 7201/8047 [6:26:08<1:07:08,  4.76s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-559wkujj'
loss_total_epoch 374.13491045869887
Training tokenizer:  89% 7202/8047 [6:26:13<1:06:46,  4.74s/it]loss_total_epoch 374.17698164470494
Training tokenizer:  90% 7203/8047 [6:26:18<1:06:40,  4.74s/it]loss_total_epoch 374.2193969618529
Training tokenizer:  90% 7204/8047 [6:26:22<1:06:21,  4.72s/it]loss_total_epoch 374.26597025059164
Training tokenizer:  90% 7205/8047 [6:26:27<1:06:31,  4.74s/it]loss_total_epoch 374.30813503079116
Training tokenizer:  90% 7206/8047 [6:26:32<1:06:28,  4.74s/it]loss_total_epoch 374.34755907021463
Training tokenizer:  90% 7207/8047 [6:26:37<1:06:10,  4.73s/it]loss_total_epoch 374.38943993113935
Training tokenizer:  90% 7208/8047 [6:26:41<1:06:12,  4.73s/it]loss_total_epoch 374.4402814973146
Training tokenizer:  90% 7209/8047 [6:26:46<1:06:06,  4.73s/it]loss_total_epoch 374.4673267900944
Training tokenizer:  90% 7210/8047 [6:26:51<1:06:10,  4.74s/it]loss_total_epoch 374.5062715560198
Training tokenizer:  90% 7211/8047 [6:26:55<1:05:48,  4.72s/it]loss_total_epoch 374.5489331893623
Training tokenizer:  90% 7212/8047 [6:27:00<1:05:37,  4.72s/it]loss_total_epoch 374.5857371427119
Training tokenizer:  90% 7213/8047 [6:27:05<1:05:38,  4.72s/it]loss_total_epoch 374.6226145327091
Training tokenizer:  90% 7214/8047 [6:27:10<1:05:51,  4.74s/it]loss_total_epoch 374.66214584186673
Training tokenizer:  90% 7215/8047 [6:27:14<1:05:48,  4.75s/it]loss_total_epoch 374.71446714550257
Training tokenizer:  90% 7216/8047 [6:27:19<1:05:43,  4.75s/it]loss_total_epoch 374.7573876082897
Training tokenizer:  90% 7217/8047 [6:27:24<1:05:37,  4.74s/it]loss_total_epoch 374.7966556735337
Training tokenizer:  90% 7218/8047 [6:27:29<1:05:35,  4.75s/it]loss_total_epoch 374.8371678031981
Training tokenizer:  90% 7219/8047 [6:27:33<1:05:27,  4.74s/it]loss_total_epoch 374.87571896985173
Training tokenizer:  90% 7220/8047 [6:27:38<1:05:19,  4.74s/it]loss_total_epoch 374.9100319631398
Training tokenizer:  90% 7221/8047 [6:27:43<1:05:15,  4.74s/it]loss_total_epoch 374.94784466177225
Training tokenizer:  90% 7222/8047 [6:27:48<1:05:01,  4.73s/it]loss_total_epoch 374.98952570557594
Training tokenizer:  90% 7223/8047 [6:27:52<1:05:00,  4.73s/it]loss_total_epoch 375.0301622748375
Training tokenizer:  90% 7224/8047 [6:27:57<1:04:59,  4.74s/it]loss_total_epoch 375.07535460591316
Training tokenizer:  90% 7225/8047 [6:28:02<1:04:41,  4.72s/it]loss_total_epoch 375.1212133727968
Training tokenizer:  90% 7226/8047 [6:28:07<1:04:36,  4.72s/it]loss_total_epoch 375.164002597332
Training tokenizer:  90% 7227/8047 [6:28:11<1:04:33,  4.72s/it]loss_total_epoch 375.20547452941537
Training tokenizer:  90% 7228/8047 [6:28:16<1:04:41,  4.74s/it]loss_total_epoch 375.2442494407296
Training tokenizer:  90% 7229/8047 [6:28:21<1:04:41,  4.75s/it]loss_total_epoch 375.2815627679229
Training tokenizer:  90% 7230/8047 [6:28:26<1:04:37,  4.75s/it]loss_total_epoch 375.32082338258624
Training tokenizer:  90% 7231/8047 [6:28:30<1:04:27,  4.74s/it]loss_total_epoch 375.3617093116045
Training tokenizer:  90% 7232/8047 [6:28:35<1:04:21,  4.74s/it]loss_total_epoch 375.4128235951066
Training tokenizer:  90% 7233/8047 [6:28:40<1:04:13,  4.73s/it]loss_total_epoch 375.4498386718333
Training tokenizer:  90% 7234/8047 [6:28:44<1:04:17,  4.74s/it]loss_total_epoch 375.48773815482855
Training tokenizer:  90% 7235/8047 [6:28:49<1:03:40,  4.70s/it]loss_total_epoch 375.53048548847437
Training tokenizer:  90% 7236/8047 [6:28:54<1:03:43,  4.71s/it]loss_total_epoch 375.57491924986243
Training tokenizer:  90% 7237/8047 [6:28:59<1:03:42,  4.72s/it]loss_total_epoch 375.61665983498096
Training tokenizer:  90% 7238/8047 [6:29:03<1:03:54,  4.74s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-95upgh7y'
loss_total_epoch 375.65877098962665
Training tokenizer:  90% 7239/8047 [6:29:08<1:03:54,  4.75s/it]loss_total_epoch 375.69456910714507
Training tokenizer:  90% 7240/8047 [6:29:13<1:03:55,  4.75s/it]loss_total_epoch 375.7436383701861
Training tokenizer:  90% 7241/8047 [6:29:18<1:03:52,  4.75s/it]loss_total_epoch 375.7794817984104
Training tokenizer:  90% 7242/8047 [6:29:22<1:03:47,  4.76s/it]loss_total_epoch 375.81026620045304
Training tokenizer:  90% 7243/8047 [6:29:27<1:03:50,  4.76s/it]loss_total_epoch 375.848506372422
Training tokenizer:  90% 7244/8047 [6:29:32<1:03:32,  4.75s/it]loss_total_epoch 375.8851472251117
Training tokenizer:  90% 7245/8047 [6:29:37<1:03:26,  4.75s/it]loss_total_epoch 375.9301198832691
Training tokenizer:  90% 7246/8047 [6:29:41<1:03:19,  4.74s/it]loss_total_epoch 375.9704606346786
Training tokenizer:  90% 7247/8047 [6:29:46<1:03:12,  4.74s/it]loss_total_epoch 376.01442193984985
Training tokenizer:  90% 7248/8047 [6:29:51<1:03:11,  4.74s/it]loss_total_epoch 376.0497335828841
Training tokenizer:  90% 7249/8047 [6:29:56<1:03:04,  4.74s/it]loss_total_epoch 376.0997833125293
Training tokenizer:  90% 7250/8047 [6:30:00<1:02:48,  4.73s/it]loss_total_epoch 376.1485041640699
Training tokenizer:  90% 7251/8047 [6:30:05<1:02:35,  4.72s/it]loss_total_epoch 376.181156296283
Training tokenizer:  90% 7252/8047 [6:30:10<1:02:48,  4.74s/it]loss_total_epoch 376.2235212177038
Training tokenizer:  90% 7253/8047 [6:30:14<1:02:37,  4.73s/it]loss_total_epoch 376.26600201800466
Training tokenizer:  90% 7254/8047 [6:30:19<1:02:42,  4.74s/it]loss_total_epoch 376.30252373963594
Training tokenizer:  90% 7255/8047 [6:30:24<1:02:35,  4.74s/it]loss_total_epoch 376.3496244326234
Training tokenizer:  90% 7256/8047 [6:30:29<1:02:25,  4.73s/it]loss_total_epoch 376.3917029686272
Training tokenizer:  90% 7257/8047 [6:30:33<1:02:18,  4.73s/it]loss_total_epoch 376.42515160888433
Training tokenizer:  90% 7258/8047 [6:30:38<1:02:08,  4.73s/it]loss_total_epoch 376.4572430625558
Training tokenizer:  90% 7259/8047 [6:30:43<1:01:56,  4.72s/it]loss_total_epoch 376.49230163544416
Training tokenizer:  90% 7260/8047 [6:30:48<1:01:54,  4.72s/it]loss_total_epoch 376.5237683355808
Training tokenizer:  90% 7261/8047 [6:30:52<1:01:59,  4.73s/it]loss_total_epoch 376.56046226248145
Training tokenizer:  90% 7262/8047 [6:30:57<1:01:31,  4.70s/it]loss_total_epoch 376.60789002105594
Training tokenizer:  90% 7263/8047 [6:31:02<1:01:53,  4.74s/it]loss_total_epoch 376.65077913179994
Training tokenizer:  90% 7264/8047 [6:31:07<1:01:49,  4.74s/it]loss_total_epoch 376.6825869269669
Training tokenizer:  90% 7265/8047 [6:31:11<1:01:35,  4.73s/it]loss_total_epoch 376.72569077834487
Training tokenizer:  90% 7266/8047 [6:31:16<1:01:41,  4.74s/it]loss_total_epoch 376.7651143632829
Training tokenizer:  90% 7267/8047 [6:31:21<1:01:26,  4.73s/it]loss_total_epoch 376.8125423155725
Training tokenizer:  90% 7268/8047 [6:31:25<1:01:30,  4.74s/it]loss_total_epoch 376.8502450995147
Training tokenizer:  90% 7269/8047 [6:31:30<1:01:30,  4.74s/it]loss_total_epoch 376.8870040103793
Training tokenizer:  90% 7270/8047 [6:31:35<1:01:28,  4.75s/it]loss_total_epoch 376.92514691501856
Training tokenizer:  90% 7271/8047 [6:31:40<1:01:19,  4.74s/it]loss_total_epoch 376.9562318623066
Training tokenizer:  90% 7272/8047 [6:31:44<1:01:08,  4.73s/it]loss_total_epoch 376.9908817484975
Training tokenizer:  90% 7273/8047 [6:31:49<1:00:58,  4.73s/it]loss_total_epoch 377.0355200320482
Training tokenizer:  90% 7274/8047 [6:31:54<1:00:44,  4.72s/it]loss_total_epoch 377.06819373369217
Training tokenizer:  90% 7275/8047 [6:31:59<1:00:43,  4.72s/it]loss_total_epoch 377.1150990277529
Training tokenizer:  90% 7276/8047 [6:32:03<1:00:41,  4.72s/it]loss_total_epoch 377.1490122079849
Training tokenizer:  90% 7277/8047 [6:32:08<1:00:37,  4.72s/it]loss_total_epoch 377.18933656439185
Training tokenizer:  90% 7278/8047 [6:32:13<1:00:34,  4.73s/it]loss_total_epoch 377.2259933017194
Training tokenizer:  90% 7279/8047 [6:32:18<1:00:47,  4.75s/it]loss_total_epoch 377.26453134790063
Training tokenizer:  90% 7280/8047 [6:32:22<1:00:34,  4.74s/it]loss_total_epoch 377.31716675683856
Training tokenizer:  90% 7281/8047 [6:32:27<1:00:35,  4.75s/it]loss_total_epoch 377.3535013869405
Training tokenizer:  90% 7282/8047 [6:32:32<1:00:20,  4.73s/it]loss_total_epoch 377.39613408595324
Training tokenizer:  91% 7283/8047 [6:32:36<1:00:23,  4.74s/it]loss_total_epoch 377.4447008818388
Training tokenizer:  91% 7284/8047 [6:32:41<1:00:22,  4.75s/it]loss_total_epoch 377.48341485112906
Training tokenizer:  91% 7285/8047 [6:32:46<1:00:22,  4.75s/it]loss_total_epoch 377.52180241420865
Training tokenizer:  91% 7286/8047 [6:32:51<1:00:19,  4.76s/it]loss_total_epoch 377.5682729817927
Training tokenizer:  91% 7287/8047 [6:32:55<1:00:13,  4.75s/it]loss_total_epoch 377.6191059798002
Training tokenizer:  91% 7288/8047 [6:33:00<1:00:15,  4.76s/it]loss_total_epoch 377.66231383755803
Training tokenizer:  91% 7289/8047 [6:33:05<1:00:09,  4.76s/it]loss_total_epoch 377.70605182275176
Training tokenizer:  91% 7290/8047 [6:33:10<59:50,  4.74s/it]  loss_total_epoch 377.7446305863559
Training tokenizer:  91% 7291/8047 [6:33:14<59:31,  4.72s/it]loss_total_epoch 377.77038794755936
Training tokenizer:  91% 7292/8047 [6:33:19<59:24,  4.72s/it]loss_total_epoch 377.8053332976997
Training tokenizer:  91% 7293/8047 [6:33:24<59:20,  4.72s/it]loss_total_epoch 377.84273555502295
Training tokenizer:  91% 7294/8047 [6:33:29<59:17,  4.72s/it]loss_total_epoch 377.87719286233187
Training tokenizer:  91% 7295/8047 [6:33:33<59:18,  4.73s/it]loss_total_epoch 377.91222282871604
Training tokenizer:  91% 7296/8047 [6:33:38<59:17,  4.74s/it]loss_total_epoch 377.95232608914375
Training tokenizer:  91% 7297/8047 [6:33:43<59:12,  4.74s/it]loss_total_epoch 377.99082240089774
Training tokenizer:  91% 7298/8047 [6:33:48<59:30,  4.77s/it]loss_total_epoch 378.01859468221664
Training tokenizer:  91% 7299/8047 [6:33:52<59:24,  4.77s/it]loss_total_epoch 378.06101570650935
Training tokenizer:  91% 7300/8047 [6:33:57<58:39,  4.71s/it]loss_total_epoch 378.1006842330098
Training tokenizer:  91% 7301/8047 [6:34:02<58:46,  4.73s/it]loss_total_epoch 378.13984927907586
Training tokenizer:  91% 7302/8047 [6:34:07<58:54,  4.74s/it]loss_total_epoch 378.1845166794956
Training tokenizer:  91% 7303/8047 [6:34:11<59:01,  4.76s/it]loss_total_epoch 378.21890299767256
Training tokenizer:  91% 7304/8047 [6:34:16<58:50,  4.75s/it]loss_total_epoch 378.24949007853866
Training tokenizer:  91% 7305/8047 [6:34:21<58:41,  4.75s/it]loss_total_epoch 378.29700994491577
Training tokenizer:  91% 7306/8047 [6:34:26<58:25,  4.73s/it]loss_total_epoch 378.3325452506542
Training tokenizer:  91% 7307/8047 [6:34:30<58:17,  4.73s/it]loss_total_epoch 378.3708118982613
Training tokenizer:  91% 7308/8047 [6:34:35<58:18,  4.73s/it]loss_total_epoch 378.4138812944293
Training tokenizer:  91% 7309/8047 [6:34:40<58:22,  4.75s/it]loss_total_epoch 378.4557702764869
Training tokenizer:  91% 7310/8047 [6:34:44<58:16,  4.74s/it]loss_total_epoch 378.49907079711556
Training tokenizer:  91% 7311/8047 [6:34:49<57:59,  4.73s/it]loss_total_epoch 378.53467020764947
Training tokenizer:  91% 7312/8047 [6:34:54<58:02,  4.74s/it]loss_total_epoch 378.578199647367
Training tokenizer:  91% 7313/8047 [6:34:59<58:07,  4.75s/it]loss_total_epoch 378.62934156507254
Training tokenizer:  91% 7314/8047 [6:35:04<58:07,  4.76s/it]loss_total_epoch 378.67773757502437
Training tokenizer:  91% 7315/8047 [6:35:08<58:06,  4.76s/it]loss_total_epoch 378.7259858213365
Training tokenizer:  91% 7316/8047 [6:35:13<58:15,  4.78s/it]loss_total_epoch 378.766107250005
Training tokenizer:  91% 7317/8047 [6:35:18<57:59,  4.77s/it]loss_total_epoch 378.8018173985183
Training tokenizer:  91% 7318/8047 [6:35:23<57:44,  4.75s/it]loss_total_epoch 378.8402751609683
Training tokenizer:  91% 7319/8047 [6:35:27<57:26,  4.73s/it]loss_total_epoch 378.8895823508501
Training tokenizer:  91% 7320/8047 [6:35:32<57:17,  4.73s/it]loss_total_epoch 378.93278792873025
Training tokenizer:  91% 7321/8047 [6:35:37<57:14,  4.73s/it]loss_total_epoch 378.97936495766044
Training tokenizer:  91% 7322/8047 [6:35:41<57:09,  4.73s/it]loss_total_epoch 379.01379358395934
Training tokenizer:  91% 7323/8047 [6:35:46<57:00,  4.72s/it]loss_total_epoch 379.0586764551699
Training tokenizer:  91% 7324/8047 [6:35:51<57:10,  4.75s/it]loss_total_epoch 379.1001967154443
Training tokenizer:  91% 7325/8047 [6:35:56<57:02,  4.74s/it]loss_total_epoch 379.1335366591811
Training tokenizer:  91% 7326/8047 [6:36:00<56:52,  4.73s/it]loss_total_epoch 379.1815130151808
Training tokenizer:  91% 7327/8047 [6:36:05<56:46,  4.73s/it]loss_total_epoch 379.22197126969695
Training tokenizer:  91% 7328/8047 [6:36:10<56:52,  4.75s/it]loss_total_epoch 379.26450149714947
Training tokenizer:  91% 7329/8047 [6:36:15<56:35,  4.73s/it]loss_total_epoch 379.3077490590513
Training tokenizer:  91% 7330/8047 [6:36:19<56:29,  4.73s/it]loss_total_epoch 379.3505999147892
Training tokenizer:  91% 7331/8047 [6:36:24<56:25,  4.73s/it]loss_total_epoch 379.3896568082273
Training tokenizer:  91% 7332/8047 [6:36:29<56:20,  4.73s/it]loss_total_epoch 379.4275649264455
Training tokenizer:  91% 7333/8047 [6:36:33<56:13,  4.72s/it]loss_total_epoch 379.4707032293081
Training tokenizer:  91% 7334/8047 [6:36:38<56:23,  4.74s/it]loss_total_epoch 379.50331803411245
Training tokenizer:  91% 7335/8047 [6:36:43<56:22,  4.75s/it]loss_total_epoch 379.5457959584892
Training tokenizer:  91% 7336/8047 [6:36:48<56:16,  4.75s/it]loss_total_epoch 379.57900262996554
Training tokenizer:  91% 7337/8047 [6:36:53<56:20,  4.76s/it]loss_total_epoch 379.6055596843362
Training tokenizer:  91% 7338/8047 [6:36:57<56:16,  4.76s/it]loss_total_epoch 379.65007369592786
Training tokenizer:  91% 7339/8047 [6:37:02<56:09,  4.76s/it]loss_total_epoch 379.6973963342607
Training tokenizer:  91% 7340/8047 [6:37:07<55:59,  4.75s/it]loss_total_epoch 379.7464440241456
Training tokenizer:  91% 7341/8047 [6:37:12<55:51,  4.75s/it]loss_total_epoch 379.78334694355726
Training tokenizer:  91% 7342/8047 [6:37:16<55:44,  4.74s/it]loss_total_epoch 379.82027376815677
Training tokenizer:  91% 7343/8047 [6:37:21<55:35,  4.74s/it]loss_total_epoch 379.8556977286935
Training tokenizer:  91% 7344/8047 [6:37:26<55:21,  4.72s/it]loss_total_epoch 379.89104652032256
Training tokenizer:  91% 7345/8047 [6:37:30<55:29,  4.74s/it]loss_total_epoch 379.93068451061845
Training tokenizer:  91% 7346/8047 [6:37:35<55:27,  4.75s/it]loss_total_epoch 379.97429720684886
Training tokenizer:  91% 7347/8047 [6:37:40<55:33,  4.76s/it]loss_total_epoch 379.99717235192657
Training tokenizer:  91% 7348/8047 [6:37:45<55:34,  4.77s/it]loss_total_epoch 380.03306990116835
Training tokenizer:  91% 7349/8047 [6:37:50<55:24,  4.76s/it]loss_total_epoch 380.0877656303346
Training tokenizer:  91% 7350/8047 [6:37:55<56:18,  4.85s/it]loss_total_epoch 380.12507716193795
Training tokenizer:  91% 7351/8047 [6:37:59<55:47,  4.81s/it]loss_total_epoch 380.17032001912594
Training tokenizer:  91% 7352/8047 [6:38:04<55:26,  4.79s/it]loss_total_epoch 380.2143759354949
Training tokenizer:  91% 7353/8047 [6:38:09<55:06,  4.76s/it]loss_total_epoch 380.2593335919082
Training tokenizer:  91% 7354/8047 [6:38:14<54:53,  4.75s/it]loss_total_epoch 380.3039596825838
Training tokenizer:  91% 7355/8047 [6:38:18<54:41,  4.74s/it]loss_total_epoch 380.3483106903732
Training tokenizer:  91% 7356/8047 [6:38:23<54:38,  4.75s/it]loss_total_epoch 380.3810834027827
Training tokenizer:  91% 7357/8047 [6:38:28<54:36,  4.75s/it]loss_total_epoch 380.4156184643507
Training tokenizer:  91% 7358/8047 [6:38:33<54:36,  4.76s/it]loss_total_epoch 380.4601917974651
Training tokenizer:  91% 7359/8047 [6:38:37<54:27,  4.75s/it]loss_total_epoch 380.50008483603597
Training tokenizer:  91% 7360/8047 [6:38:42<54:10,  4.73s/it]loss_total_epoch 380.553122099489
Training tokenizer:  91% 7361/8047 [6:38:47<54:04,  4.73s/it]loss_total_epoch 380.59265926480293
Training tokenizer:  91% 7362/8047 [6:38:51<54:09,  4.74s/it]loss_total_epoch 380.63665498420596
Training tokenizer:  91% 7363/8047 [6:38:56<54:05,  4.74s/it]loss_total_epoch 380.67839255556464
Training tokenizer:  92% 7364/8047 [6:39:01<53:58,  4.74s/it]loss_total_epoch 380.71583346277475
Training tokenizer:  92% 7365/8047 [6:39:06<53:45,  4.73s/it]loss_total_epoch 380.7630637809634
Training tokenizer:  92% 7366/8047 [6:39:10<53:32,  4.72s/it]loss_total_epoch 380.79668927565217
Training tokenizer:  92% 7367/8047 [6:39:15<53:35,  4.73s/it]loss_total_epoch 380.83586783334613
Training tokenizer:  92% 7368/8047 [6:39:20<53:39,  4.74s/it]loss_total_epoch 380.87867023795843
Training tokenizer:  92% 7369/8047 [6:39:25<53:31,  4.74s/it]loss_total_epoch 380.91454343497753
Training tokenizer:  92% 7370/8047 [6:39:29<53:24,  4.73s/it]loss_total_epoch 380.9527728743851
Training tokenizer:  92% 7371/8047 [6:39:34<53:19,  4.73s/it]loss_total_epoch 380.98900386691093
Training tokenizer:  92% 7372/8047 [6:39:39<53:20,  4.74s/it]loss_total_epoch 381.0296814329922
Training tokenizer:  92% 7373/8047 [6:39:44<53:29,  4.76s/it]loss_total_epoch 381.0747719742358
Training tokenizer:  92% 7374/8047 [6:39:48<53:26,  4.76s/it]loss_total_epoch 381.1198525354266
Training tokenizer:  92% 7375/8047 [6:39:53<53:11,  4.75s/it]loss_total_epoch 381.16000908240676
Training tokenizer:  92% 7376/8047 [6:39:58<52:56,  4.73s/it]loss_total_epoch 381.19710924476385
Training tokenizer:  92% 7377/8047 [6:40:03<52:55,  4.74s/it]loss_total_epoch 381.22989244386554
Training tokenizer:  92% 7378/8047 [6:40:07<52:47,  4.74s/it]loss_total_epoch 381.2728320658207
Training tokenizer:  92% 7379/8047 [6:40:12<52:43,  4.74s/it]loss_total_epoch 381.3265529163182
Training tokenizer:  92% 7380/8047 [6:40:17<52:32,  4.73s/it]loss_total_epoch 381.37669726088643
Training tokenizer:  92% 7381/8047 [6:40:21<52:40,  4.75s/it]loss_total_epoch 381.41738064587116
Training tokenizer:  92% 7382/8047 [6:40:26<52:38,  4.75s/it]loss_total_epoch 381.46695297956467
Training tokenizer:  92% 7383/8047 [6:40:31<52:36,  4.75s/it]loss_total_epoch 381.5075330771506
Training tokenizer:  92% 7384/8047 [6:40:36<52:20,  4.74s/it]loss_total_epoch 381.5444494970143
Training tokenizer:  92% 7385/8047 [6:40:40<52:17,  4.74s/it]loss_total_epoch 381.586711242795
Training tokenizer:  92% 7386/8047 [6:40:45<52:13,  4.74s/it]loss_total_epoch 381.63582933321595
Training tokenizer:  92% 7387/8047 [6:40:50<52:11,  4.75s/it]loss_total_epoch 381.6757688112557
Training tokenizer:  92% 7388/8047 [6:40:55<51:59,  4.73s/it]loss_total_epoch 381.71437637135386
Training tokenizer:  92% 7389/8047 [6:40:59<52:03,  4.75s/it]loss_total_epoch 381.75307584553957
Training tokenizer:  92% 7390/8047 [6:41:04<51:48,  4.73s/it]loss_total_epoch 381.7925088852644
Training tokenizer:  92% 7391/8047 [6:41:09<51:34,  4.72s/it]loss_total_epoch 381.83593399450183
Training tokenizer:  92% 7392/8047 [6:41:14<51:38,  4.73s/it]loss_total_epoch 381.8843921981752
Training tokenizer:  92% 7393/8047 [6:41:18<51:36,  4.74s/it]loss_total_epoch 381.92605982348323
Training tokenizer:  92% 7394/8047 [6:41:23<51:34,  4.74s/it]loss_total_epoch 381.9664082489908
Training tokenizer:  92% 7395/8047 [6:41:28<51:31,  4.74s/it]loss_total_epoch 382.0040119178593
Training tokenizer:  92% 7396/8047 [6:41:33<51:54,  4.78s/it]loss_total_epoch 382.04246842861176
Training tokenizer:  92% 7397/8047 [6:41:37<51:35,  4.76s/it]loss_total_epoch 382.08248249441385
Training tokenizer:  92% 7398/8047 [6:41:42<51:24,  4.75s/it]loss_total_epoch 382.11446295306087
Training tokenizer:  92% 7399/8047 [6:41:47<51:13,  4.74s/it]loss_total_epoch 382.15216685831547
Training tokenizer:  92% 7400/8047 [6:41:52<51:18,  4.76s/it]loss_total_epoch 382.1874241270125
Training tokenizer:  92% 7401/8047 [6:41:56<51:06,  4.75s/it]loss_total_epoch 382.2187960855663
Training tokenizer:  92% 7402/8047 [6:42:01<50:45,  4.72s/it]loss_total_epoch 382.2660122998059
Training tokenizer:  92% 7403/8047 [6:42:06<50:34,  4.71s/it]loss_total_epoch 382.30356645956635
Training tokenizer:  92% 7404/8047 [6:42:10<50:32,  4.72s/it]loss_total_epoch 382.3432417176664
Training tokenizer:  92% 7405/8047 [6:42:15<50:24,  4.71s/it]loss_total_epoch 382.3834594003856
Training tokenizer:  92% 7406/8047 [6:42:20<50:37,  4.74s/it]loss_total_epoch 382.4240121431649
Training tokenizer:  92% 7407/8047 [6:42:25<50:06,  4.70s/it]loss_total_epoch 382.46470265090466
Training tokenizer:  92% 7408/8047 [6:42:29<50:01,  4.70s/it]loss_total_epoch 382.5047816261649
Training tokenizer:  92% 7409/8047 [6:42:34<50:10,  4.72s/it]loss_total_epoch 382.52719794772565
Training tokenizer:  92% 7410/8047 [6:42:39<50:14,  4.73s/it]loss_total_epoch 382.5625657271594
Training tokenizer:  92% 7411/8047 [6:42:43<50:03,  4.72s/it]loss_total_epoch 382.6123751346022
Training tokenizer:  92% 7412/8047 [6:42:48<50:08,  4.74s/it]loss_total_epoch 382.6611185166985
Training tokenizer:  92% 7413/8047 [6:42:53<50:12,  4.75s/it]loss_total_epoch 382.70797847397625
Training tokenizer:  92% 7414/8047 [6:42:58<50:10,  4.76s/it]loss_total_epoch 382.7489049527794
Training tokenizer:  92% 7415/8047 [6:43:03<50:09,  4.76s/it]loss_total_epoch 382.78412796370685
Training tokenizer:  92% 7416/8047 [6:43:07<50:04,  4.76s/it]loss_total_epoch 382.81872864998877
Training tokenizer:  92% 7417/8047 [6:43:12<49:57,  4.76s/it]loss_total_epoch 382.8698664326221
Training tokenizer:  92% 7418/8047 [6:43:17<49:45,  4.75s/it]loss_total_epoch 382.9066849965602
Training tokenizer:  92% 7419/8047 [6:43:22<49:37,  4.74s/it]loss_total_epoch 382.9536367524415
Training tokenizer:  92% 7420/8047 [6:43:26<49:29,  4.74s/it]loss_total_epoch 382.98557615838945
Training tokenizer:  92% 7421/8047 [6:43:31<49:28,  4.74s/it]loss_total_epoch 383.0282426122576
Training tokenizer:  92% 7422/8047 [6:43:36<49:20,  4.74s/it]loss_total_epoch 383.06879358924925
Training tokenizer:  92% 7423/8047 [6:43:41<49:24,  4.75s/it]loss_total_epoch 383.1073755789548
Training tokenizer:  92% 7424/8047 [6:43:45<49:14,  4.74s/it]loss_total_epoch 383.1466778051108
Training tokenizer:  92% 7425/8047 [6:43:50<49:05,  4.74s/it]loss_total_epoch 383.17629962973297
Training tokenizer:  92% 7426/8047 [6:43:55<49:05,  4.74s/it]loss_total_epoch 383.2279405388981
Training tokenizer:  92% 7427/8047 [6:43:59<48:51,  4.73s/it]loss_total_epoch 383.27827058546245
Training tokenizer:  92% 7428/8047 [6:44:04<48:44,  4.72s/it]loss_total_epoch 383.31476795114577
Training tokenizer:  92% 7429/8047 [6:44:09<48:36,  4.72s/it]loss_total_epoch 383.3579644281417
Training tokenizer:  92% 7430/8047 [6:44:14<48:39,  4.73s/it]loss_total_epoch 383.3949975129217
Training tokenizer:  92% 7431/8047 [6:44:18<48:43,  4.75s/it]loss_total_epoch 383.4342210162431
Training tokenizer:  92% 7432/8047 [6:44:23<48:27,  4.73s/it]loss_total_epoch 383.48385968990624
Training tokenizer:  92% 7433/8047 [6:44:28<48:29,  4.74s/it]loss_total_epoch 383.5176981780678
Training tokenizer:  92% 7434/8047 [6:44:33<48:29,  4.75s/it]loss_total_epoch 383.561157470569
Training tokenizer:  92% 7435/8047 [6:44:37<48:32,  4.76s/it]loss_total_epoch 383.60797462426126
Training tokenizer:  92% 7436/8047 [6:44:42<48:27,  4.76s/it]loss_total_epoch 383.64061754755676
Training tokenizer:  92% 7437/8047 [6:44:47<48:22,  4.76s/it]loss_total_epoch 383.67805927433074
Training tokenizer:  92% 7438/8047 [6:44:52<48:19,  4.76s/it]loss_total_epoch 383.72513709776103
Training tokenizer:  92% 7439/8047 [6:44:56<48:16,  4.76s/it]loss_total_epoch 383.7652335781604
Training tokenizer:  92% 7440/8047 [6:45:01<48:24,  4.78s/it]loss_total_epoch 383.8004641402513
Training tokenizer:  92% 7441/8047 [6:45:06<47:43,  4.73s/it]loss_total_epoch 383.83445387892425
Training tokenizer:  92% 7442/8047 [6:45:11<47:41,  4.73s/it]loss_total_epoch 383.87446014769375
Training tokenizer:  92% 7443/8047 [6:45:15<47:43,  4.74s/it]loss_total_epoch 383.90136511810124
Training tokenizer:  93% 7444/8047 [6:45:20<47:36,  4.74s/it]loss_total_epoch 383.93857629038393
Training tokenizer:  93% 7445/8047 [6:45:25<47:37,  4.75s/it]loss_total_epoch 383.9799335319549
Training tokenizer:  93% 7446/8047 [6:45:30<47:33,  4.75s/it]loss_total_epoch 384.0185535941273
Training tokenizer:  93% 7447/8047 [6:45:34<47:36,  4.76s/it]loss_total_epoch 384.06057121790946
Training tokenizer:  93% 7448/8047 [6:45:39<47:23,  4.75s/it]loss_total_epoch 384.0983044151217
Training tokenizer:  93% 7449/8047 [6:45:44<47:18,  4.75s/it]loss_total_epoch 384.13969144783914
Training tokenizer:  93% 7450/8047 [6:45:49<47:05,  4.73s/it]loss_total_epoch 384.1903010737151
Training tokenizer:  93% 7451/8047 [6:45:53<47:15,  4.76s/it]loss_total_epoch 384.23124195449054
Training tokenizer:  93% 7452/8047 [6:45:58<47:07,  4.75s/it]loss_total_epoch 384.2787276301533
Training tokenizer:  93% 7453/8047 [6:46:03<47:04,  4.75s/it]loss_total_epoch 384.316621536389
Training tokenizer:  93% 7454/8047 [6:46:08<46:57,  4.75s/it]loss_total_epoch 384.3503507543355
Training tokenizer:  93% 7455/8047 [6:46:12<46:50,  4.75s/it]loss_total_epoch 384.38879767619073
Training tokenizer:  93% 7456/8047 [6:46:17<46:53,  4.76s/it]loss_total_epoch 384.43144728802145
Training tokenizer:  93% 7457/8047 [6:46:22<46:54,  4.77s/it]loss_total_epoch 384.46590663306415
Training tokenizer:  93% 7458/8047 [6:46:27<46:19,  4.72s/it]loss_total_epoch 384.5081915613264
Training tokenizer:  93% 7459/8047 [6:46:31<46:17,  4.72s/it]loss_total_epoch 384.55123139359057
Training tokenizer:  93% 7460/8047 [6:46:36<46:24,  4.74s/it]loss_total_epoch 384.5959480460733
Training tokenizer:  93% 7461/8047 [6:46:41<46:21,  4.75s/it]loss_total_epoch 384.6394977848977
Training tokenizer:  93% 7462/8047 [6:46:46<46:08,  4.73s/it]loss_total_epoch 384.67732288874686
Training tokenizer:  93% 7463/8047 [6:46:50<45:47,  4.71s/it]loss_total_epoch 384.71876961924136
Training tokenizer:  93% 7464/8047 [6:46:55<45:54,  4.72s/it]loss_total_epoch 384.75824279151857
Training tokenizer:  93% 7465/8047 [6:47:00<45:50,  4.73s/it]loss_total_epoch 384.7954683508724
Training tokenizer:  93% 7466/8047 [6:47:04<45:48,  4.73s/it]loss_total_epoch 384.8288658540696
Training tokenizer:  93% 7467/8047 [6:47:09<45:43,  4.73s/it]loss_total_epoch 384.8603914808482
Training tokenizer:  93% 7468/8047 [6:47:14<45:38,  4.73s/it]loss_total_epoch 384.9023399334401
Training tokenizer:  93% 7469/8047 [6:47:19<45:41,  4.74s/it]loss_total_epoch 384.93471566028893
Training tokenizer:  93% 7470/8047 [6:47:23<45:36,  4.74s/it]loss_total_epoch 384.97922927327454
Training tokenizer:  93% 7471/8047 [6:47:28<45:39,  4.76s/it]loss_total_epoch 385.00781662017107
Training tokenizer:  93% 7472/8047 [6:47:33<45:28,  4.75s/it]loss_total_epoch 385.0530450716615
Training tokenizer:  93% 7473/8047 [6:47:38<45:19,  4.74s/it]loss_total_epoch 385.0883035585284
Training tokenizer:  93% 7474/8047 [6:47:42<45:30,  4.77s/it]loss_total_epoch 385.12709357216954
Training tokenizer:  93% 7475/8047 [6:47:47<45:25,  4.76s/it]loss_total_epoch 385.1663603298366
Training tokenizer:  93% 7476/8047 [6:47:52<45:20,  4.76s/it]loss_total_epoch 385.20031221583486
Training tokenizer:  93% 7477/8047 [6:47:57<45:16,  4.77s/it]loss_total_epoch 385.24341589957476
Training tokenizer:  93% 7478/8047 [6:48:01<45:09,  4.76s/it]loss_total_epoch 385.27712497115135
Training tokenizer:  93% 7479/8047 [6:48:06<45:07,  4.77s/it]loss_total_epoch 385.31309293583035
Training tokenizer:  93% 7480/8047 [6:48:11<45:01,  4.76s/it]loss_total_epoch 385.35078627616167
Training tokenizer:  93% 7481/8047 [6:48:16<44:56,  4.76s/it]loss_total_epoch 385.39646251127124
Training tokenizer:  93% 7482/8047 [6:48:21<44:53,  4.77s/it]loss_total_epoch 385.43609472736716
Training tokenizer:  93% 7483/8047 [6:48:25<44:49,  4.77s/it]loss_total_epoch 385.4762492403388
Training tokenizer:  93% 7484/8047 [6:48:30<44:40,  4.76s/it]loss_total_epoch 385.5170471817255
Training tokenizer:  93% 7485/8047 [6:48:35<44:28,  4.75s/it]loss_total_epoch 385.5502923615277
Training tokenizer:  93% 7486/8047 [6:48:40<44:24,  4.75s/it]loss_total_epoch 385.57886596396565
Training tokenizer:  93% 7487/8047 [6:48:44<44:19,  4.75s/it]loss_total_epoch 385.62195894122124
Training tokenizer:  93% 7488/8047 [6:48:49<44:10,  4.74s/it]loss_total_epoch 385.6616070382297
Training tokenizer:  93% 7489/8047 [6:48:54<44:03,  4.74s/it]loss_total_epoch 385.69621282815933
Training tokenizer:  93% 7490/8047 [6:48:59<44:02,  4.74s/it]loss_total_epoch 385.7401068173349
Training tokenizer:  93% 7491/8047 [6:49:03<43:55,  4.74s/it]loss_total_epoch 385.7755906879902
Training tokenizer:  93% 7492/8047 [6:49:08<43:50,  4.74s/it]loss_total_epoch 385.8230635523796
Training tokenizer:  93% 7493/8047 [6:49:13<43:56,  4.76s/it]loss_total_epoch 385.86574172601104
Training tokenizer:  93% 7494/8047 [6:49:18<43:52,  4.76s/it]loss_total_epoch 385.9110269509256
Training tokenizer:  93% 7495/8047 [6:49:22<43:58,  4.78s/it]loss_total_epoch 385.9530116394162
Training tokenizer:  93% 7496/8047 [6:49:27<43:40,  4.76s/it]loss_total_epoch 385.9857199527323
Training tokenizer:  93% 7497/8047 [6:49:32<43:29,  4.74s/it]loss_total_epoch 386.0243357717991
Training tokenizer:  93% 7498/8047 [6:49:37<43:26,  4.75s/it]loss_total_epoch 386.0695333480835
Training tokenizer:  93% 7499/8047 [6:49:41<43:26,  4.76s/it]loss_total_epoch 386.1135074645281
Training tokenizer:  93% 7500/8047 [6:49:46<43:20,  4.75s/it]loss_total_epoch 386.1501715481281
Training tokenizer:  93% 7501/8047 [6:49:51<43:16,  4.76s/it]loss_total_epoch 386.1818266399205
Training tokenizer:  93% 7502/8047 [6:49:56<43:12,  4.76s/it]loss_total_epoch 386.22430581599474
Training tokenizer:  93% 7503/8047 [6:50:00<43:12,  4.77s/it]loss_total_epoch 386.2680996209383
Training tokenizer:  93% 7504/8047 [6:50:05<43:04,  4.76s/it]loss_total_epoch 386.30913634598255
Training tokenizer:  93% 7505/8047 [6:50:10<43:02,  4.76s/it]loss_total_epoch 386.3560007996857
Training tokenizer:  93% 7506/8047 [6:50:15<42:56,  4.76s/it]loss_total_epoch 386.3969578258693
Training tokenizer:  93% 7507/8047 [6:50:19<42:48,  4.76s/it]loss_total_epoch 386.435481030494
Training tokenizer:  93% 7508/8047 [6:50:24<42:46,  4.76s/it]loss_total_epoch 386.47386198863387
Training tokenizer:  93% 7509/8047 [6:50:29<42:43,  4.76s/it]loss_total_epoch 386.5166272446513
Training tokenizer:  93% 7510/8047 [6:50:34<42:40,  4.77s/it]loss_total_epoch 386.55835320055485
Training tokenizer:  93% 7511/8047 [6:50:38<42:33,  4.76s/it]loss_total_epoch 386.5934298634529
Training tokenizer:  93% 7512/8047 [6:50:43<42:20,  4.75s/it]loss_total_epoch 386.63118628412485
Training tokenizer:  93% 7513/8047 [6:50:48<42:07,  4.73s/it]loss_total_epoch 386.6718871742487
Training tokenizer:  93% 7514/8047 [6:50:53<42:02,  4.73s/it]loss_total_epoch 386.7113976292312
Training tokenizer:  93% 7515/8047 [6:50:57<42:00,  4.74s/it]loss_total_epoch 386.7574790529907
Training tokenizer:  93% 7516/8047 [6:51:02<41:56,  4.74s/it]loss_total_epoch 386.8005623035133
Training tokenizer:  93% 7517/8047 [6:51:07<41:51,  4.74s/it]loss_total_epoch 386.8338671065867
Training tokenizer:  93% 7518/8047 [6:51:12<41:52,  4.75s/it]loss_total_epoch 386.87354630604386
Training tokenizer:  93% 7519/8047 [6:51:16<41:50,  4.75s/it]loss_total_epoch 386.9241766370833
Training tokenizer:  93% 7520/8047 [6:51:21<41:52,  4.77s/it]loss_total_epoch 386.9610180966556
Training tokenizer:  93% 7521/8047 [6:51:26<41:50,  4.77s/it]loss_total_epoch 387.0080119408667
Training tokenizer:  93% 7522/8047 [6:51:31<41:41,  4.76s/it]loss_total_epoch 387.0551094599068
Training tokenizer:  93% 7523/8047 [6:51:35<41:35,  4.76s/it]loss_total_epoch 387.08583199232817
Training tokenizer:  94% 7524/8047 [6:51:40<41:30,  4.76s/it]loss_total_epoch 387.1210659183562
Training tokenizer:  94% 7525/8047 [6:51:45<41:18,  4.75s/it]loss_total_epoch 387.1687940508127
Training tokenizer:  94% 7526/8047 [6:51:50<41:18,  4.76s/it]loss_total_epoch 387.20895374566317
Training tokenizer:  94% 7527/8047 [6:51:55<41:15,  4.76s/it]loss_total_epoch 387.2530524209142
Training tokenizer:  94% 7528/8047 [6:51:59<41:18,  4.77s/it]loss_total_epoch 387.3017242923379
Training tokenizer:  94% 7529/8047 [6:52:04<40:56,  4.74s/it]loss_total_epoch 387.3435803428292
Training tokenizer:  94% 7530/8047 [6:52:09<41:02,  4.76s/it]loss_total_epoch 387.3816177509725
Training tokenizer:  94% 7531/8047 [6:52:14<40:56,  4.76s/it]loss_total_epoch 387.42011147737503
Training tokenizer:  94% 7532/8047 [6:52:18<41:02,  4.78s/it]loss_total_epoch 387.45786448568106
Training tokenizer:  94% 7533/8047 [6:52:23<40:55,  4.78s/it]loss_total_epoch 387.5019180215895
Training tokenizer:  94% 7534/8047 [6:52:28<40:42,  4.76s/it]loss_total_epoch 387.5391144603491
Training tokenizer:  94% 7535/8047 [6:52:33<40:42,  4.77s/it]loss_total_epoch 387.573104750365
Training tokenizer:  94% 7536/8047 [6:52:37<40:29,  4.75s/it]loss_total_epoch 387.60461631789804
Training tokenizer:  94% 7537/8047 [6:52:42<40:19,  4.74s/it]loss_total_epoch 387.64098765328526
Training tokenizer:  94% 7538/8047 [6:52:47<40:16,  4.75s/it]loss_total_epoch 387.68735060840845
Training tokenizer:  94% 7539/8047 [6:52:52<40:15,  4.76s/it]loss_total_epoch 387.7293996140361
Training tokenizer:  94% 7540/8047 [6:52:56<40:14,  4.76s/it]loss_total_epoch 387.7687977813184
Training tokenizer:  94% 7541/8047 [6:53:01<40:11,  4.77s/it]loss_total_epoch 387.8152016289532
Training tokenizer:  94% 7542/8047 [6:53:06<40:16,  4.78s/it]loss_total_epoch 387.85584151372313
Training tokenizer:  94% 7543/8047 [6:53:11<39:58,  4.76s/it]loss_total_epoch 387.9000305272639
Training tokenizer:  94% 7544/8047 [6:53:15<39:53,  4.76s/it]loss_total_epoch 387.9402832277119
Training tokenizer:  94% 7545/8047 [6:53:20<39:52,  4.77s/it]loss_total_epoch 387.9815736711025
Training tokenizer:  94% 7546/8047 [6:53:25<39:41,  4.75s/it]loss_total_epoch 388.01564509421587
Training tokenizer:  94% 7547/8047 [6:53:30<39:13,  4.71s/it]loss_total_epoch 388.0555275864899
Training tokenizer:  94% 7548/8047 [6:53:34<39:23,  4.74s/it]loss_total_epoch 388.0979655571282
Training tokenizer:  94% 7549/8047 [6:53:39<39:22,  4.74s/it]loss_total_epoch 388.12963312119246
Training tokenizer:  94% 7550/8047 [6:53:44<39:24,  4.76s/it]loss_total_epoch 388.1735831089318
Training tokenizer:  94% 7551/8047 [6:53:49<39:23,  4.77s/it]loss_total_epoch 388.21566908061504
Training tokenizer:  94% 7552/8047 [6:53:53<39:20,  4.77s/it]loss_total_epoch 388.253727119416
Training tokenizer:  94% 7553/8047 [6:53:58<39:10,  4.76s/it]loss_total_epoch 388.286115873605
Training tokenizer:  94% 7554/8047 [6:54:03<39:04,  4.76s/it]loss_total_epoch 388.31995287537575
Training tokenizer:  94% 7555/8047 [6:54:08<39:03,  4.76s/it]loss_total_epoch 388.3577750697732
Training tokenizer:  94% 7556/8047 [6:54:13<39:00,  4.77s/it]loss_total_epoch 388.39115941151977
Training tokenizer:  94% 7557/8047 [6:54:17<38:55,  4.77s/it]loss_total_epoch 388.42845490947366
Training tokenizer:  94% 7558/8047 [6:54:22<38:44,  4.75s/it]loss_total_epoch 388.460415430367
Training tokenizer:  94% 7559/8047 [6:54:27<38:30,  4.73s/it]loss_total_epoch 388.49460389092565
Training tokenizer:  94% 7560/8047 [6:54:31<38:25,  4.73s/it]loss_total_epoch 388.53264820948243
Training tokenizer:  94% 7561/8047 [6:54:36<38:23,  4.74s/it]loss_total_epoch 388.57140102237463
Training tokenizer:  94% 7562/8047 [6:54:41<38:26,  4.76s/it]loss_total_epoch 388.610052511096
Training tokenizer:  94% 7563/8047 [6:54:46<38:23,  4.76s/it]loss_total_epoch 388.65309680998325
Training tokenizer:  94% 7564/8047 [6:54:50<38:13,  4.75s/it]loss_total_epoch 388.6906383782625
Training tokenizer:  94% 7565/8047 [6:54:55<38:12,  4.76s/it]loss_total_epoch 388.7336188554764
Training tokenizer:  94% 7566/8047 [6:55:00<38:04,  4.75s/it]loss_total_epoch 388.7748527266085
Training tokenizer:  94% 7567/8047 [6:55:05<37:58,  4.75s/it]loss_total_epoch 388.81039648875594
Training tokenizer:  94% 7568/8047 [6:55:09<37:53,  4.75s/it]loss_total_epoch 388.8455209173262
Training tokenizer:  94% 7569/8047 [6:55:14<37:45,  4.74s/it]loss_total_epoch 388.8730677962303
Training tokenizer:  94% 7570/8047 [6:55:19<37:49,  4.76s/it]loss_total_epoch 388.91415030136704
Training tokenizer:  94% 7571/8047 [6:55:24<37:40,  4.75s/it]loss_total_epoch 388.9530016966164
Training tokenizer:  94% 7572/8047 [6:55:28<37:39,  4.76s/it]loss_total_epoch 388.9946988597512
Training tokenizer:  94% 7573/8047 [6:55:33<37:37,  4.76s/it]loss_total_epoch 389.0320356041193
Training tokenizer:  94% 7574/8047 [6:55:38<37:26,  4.75s/it]loss_total_epoch 389.06904750317335
Training tokenizer:  94% 7575/8047 [6:55:43<37:21,  4.75s/it]loss_total_epoch 389.10948419570923
Training tokenizer:  94% 7576/8047 [6:55:47<37:11,  4.74s/it]loss_total_epoch 389.14342745766044
Training tokenizer:  94% 7577/8047 [6:55:52<37:07,  4.74s/it]loss_total_epoch 389.1791008114815
Training tokenizer:  94% 7578/8047 [6:55:57<36:47,  4.71s/it]loss_total_epoch 389.21815983206034
Training tokenizer:  94% 7579/8047 [6:56:02<36:44,  4.71s/it]loss_total_epoch 389.263128913939
Training tokenizer:  94% 7580/8047 [6:56:06<36:41,  4.71s/it]loss_total_epoch 389.3096566386521
Training tokenizer:  94% 7581/8047 [6:56:11<36:51,  4.74s/it]loss_total_epoch 389.34950978681445
Training tokenizer:  94% 7582/8047 [6:56:16<36:43,  4.74s/it]loss_total_epoch 389.38925874978304
Training tokenizer:  94% 7583/8047 [6:56:21<36:38,  4.74s/it]loss_total_epoch 389.42996210604906
Training tokenizer:  94% 7584/8047 [6:56:25<36:34,  4.74s/it]loss_total_epoch 389.47172873839736
Training tokenizer:  94% 7585/8047 [6:56:30<36:34,  4.75s/it]loss_total_epoch 389.5112406723201
Training tokenizer:  94% 7586/8047 [6:56:35<36:25,  4.74s/it]loss_total_epoch 389.55043049529195
Training tokenizer:  94% 7587/8047 [6:56:40<36:27,  4.76s/it]loss_total_epoch 389.59092427417636
Training tokenizer:  94% 7588/8047 [6:56:44<36:09,  4.73s/it]loss_total_epoch 389.63500878587365
Training tokenizer:  94% 7589/8047 [6:56:49<36:01,  4.72s/it]loss_total_epoch 389.6823438592255
Training tokenizer:  94% 7590/8047 [6:56:54<36:09,  4.75s/it]loss_total_epoch 389.7174273431301
Training tokenizer:  94% 7591/8047 [6:56:59<36:07,  4.75s/it]loss_total_epoch 389.7507287077606
Training tokenizer:  94% 7592/8047 [6:57:03<36:01,  4.75s/it]loss_total_epoch 389.7927733659744
Training tokenizer:  94% 7593/8047 [6:57:08<35:57,  4.75s/it]loss_total_epoch 389.83627576753497
Training tokenizer:  94% 7594/8047 [6:57:13<35:52,  4.75s/it]loss_total_epoch 389.8709542863071
Training tokenizer:  94% 7595/8047 [6:57:17<35:42,  4.74s/it]loss_total_epoch 389.90997448936105
Training tokenizer:  94% 7596/8047 [6:57:22<35:40,  4.75s/it]loss_total_epoch 389.9527450315654
Training tokenizer:  94% 7597/8047 [6:57:27<35:32,  4.74s/it]loss_total_epoch 389.9815706796944
Training tokenizer:  94% 7598/8047 [6:57:32<35:36,  4.76s/it]loss_total_epoch 390.02831229940057
Training tokenizer:  94% 7599/8047 [6:57:36<35:26,  4.75s/it]loss_total_epoch 390.0634793564677
Training tokenizer:  94% 7600/8047 [6:57:41<35:27,  4.76s/it]loss_total_epoch 390.0969889052212
Training tokenizer:  94% 7601/8047 [6:57:46<35:25,  4.77s/it]loss_total_epoch 390.1351613625884
Training tokenizer:  94% 7602/8047 [6:57:51<35:26,  4.78s/it]loss_total_epoch 390.1672656163573
Training tokenizer:  94% 7603/8047 [6:57:56<35:21,  4.78s/it]loss_total_epoch 390.2055144235492
Training tokenizer:  94% 7604/8047 [6:58:00<35:11,  4.77s/it]loss_total_epoch 390.24626633897424
Training tokenizer:  95% 7605/8047 [6:58:05<35:14,  4.78s/it]loss_total_epoch 390.28288707509637
Training tokenizer:  95% 7606/8047 [6:58:10<35:00,  4.76s/it]loss_total_epoch 390.3228610418737
Training tokenizer:  95% 7607/8047 [6:58:15<34:57,  4.77s/it]loss_total_epoch 390.3665446229279
Training tokenizer:  95% 7608/8047 [6:58:19<34:56,  4.78s/it]loss_total_epoch 390.41021152213216
Training tokenizer:  95% 7609/8047 [6:58:24<34:51,  4.78s/it]loss_total_epoch 390.45425206050277
Training tokenizer:  95% 7610/8047 [6:58:29<34:41,  4.76s/it]loss_total_epoch 390.4936055801809
Training tokenizer:  95% 7611/8047 [6:58:34<34:36,  4.76s/it]loss_total_epoch 390.5339196547866
Training tokenizer:  95% 7612/8047 [6:58:38<34:25,  4.75s/it]loss_total_epoch 390.5786625929177
Training tokenizer:  95% 7613/8047 [6:58:43<34:21,  4.75s/it]loss_total_epoch 390.61516031995416
Training tokenizer:  95% 7614/8047 [6:58:48<34:18,  4.75s/it]loss_total_epoch 390.6557721346617
Training tokenizer:  95% 7615/8047 [6:58:53<34:13,  4.75s/it]loss_total_epoch 390.6938584186137
Training tokenizer:  95% 7616/8047 [6:58:57<34:07,  4.75s/it]loss_total_epoch 390.7296146452427
Training tokenizer:  95% 7617/8047 [6:59:02<34:03,  4.75s/it]loss_total_epoch 390.7642962709069
Training tokenizer:  95% 7618/8047 [6:59:07<33:54,  4.74s/it]loss_total_epoch 390.7989902123809
Training tokenizer:  95% 7619/8047 [6:59:12<33:55,  4.76s/it]loss_total_epoch 390.8346780911088
Training tokenizer:  95% 7620/8047 [6:59:17<33:52,  4.76s/it]loss_total_epoch 390.8720962032676
Training tokenizer:  95% 7621/8047 [6:59:21<33:46,  4.76s/it]loss_total_epoch 390.907881885767
Training tokenizer:  95% 7622/8047 [6:59:26<33:50,  4.78s/it]loss_total_epoch 390.95130087435246
Training tokenizer:  95% 7623/8047 [6:59:31<33:35,  4.75s/it]loss_total_epoch 390.99264652282
Training tokenizer:  95% 7624/8047 [6:59:36<33:30,  4.75s/it]loss_total_epoch 391.0315935648978
Training tokenizer:  95% 7625/8047 [6:59:40<33:25,  4.75s/it]loss_total_epoch 391.07219702750444
Training tokenizer:  95% 7626/8047 [6:59:45<33:20,  4.75s/it]loss_total_epoch 391.10479760169983
Training tokenizer:  95% 7627/8047 [6:59:50<33:13,  4.75s/it]loss_total_epoch 391.13171127066016
Training tokenizer:  95% 7628/8047 [6:59:54<33:01,  4.73s/it]loss_total_epoch 391.1780493222177
Training tokenizer:  95% 7629/8047 [6:59:59<33:02,  4.74s/it]loss_total_epoch 391.21520033478737
Training tokenizer:  95% 7630/8047 [7:00:04<33:06,  4.76s/it]loss_total_epoch 391.2597364485264
Training tokenizer:  95% 7631/8047 [7:00:09<32:46,  4.73s/it]loss_total_epoch 391.2995493412018
Training tokenizer:  95% 7632/8047 [7:00:13<32:39,  4.72s/it]loss_total_epoch 391.34055376425385
Training tokenizer:  95% 7633/8047 [7:00:18<32:37,  4.73s/it]loss_total_epoch 391.3821640871465
Training tokenizer:  95% 7634/8047 [7:00:23<32:29,  4.72s/it]loss_total_epoch 391.41998429596424
Training tokenizer:  95% 7635/8047 [7:00:28<32:28,  4.73s/it]loss_total_epoch 391.4604317843914
Training tokenizer:  95% 7636/8047 [7:00:32<32:27,  4.74s/it]loss_total_epoch 391.4967347532511
Training tokenizer:  95% 7637/8047 [7:00:37<32:27,  4.75s/it]loss_total_epoch 391.5314474180341
Training tokenizer:  95% 7638/8047 [7:00:42<32:08,  4.72s/it]loss_total_epoch 391.57100684568286
Training tokenizer:  95% 7639/8047 [7:00:47<32:08,  4.73s/it]loss_total_epoch 391.6113030984998
Training tokenizer:  95% 7640/8047 [7:00:51<32:08,  4.74s/it]loss_total_epoch 391.65627182275057
Training tokenizer:  95% 7641/8047 [7:00:56<32:12,  4.76s/it]loss_total_epoch 391.6895081996918
Training tokenizer:  95% 7642/8047 [7:01:01<32:07,  4.76s/it]loss_total_epoch 391.7276985757053
Training tokenizer:  95% 7643/8047 [7:01:06<32:03,  4.76s/it]loss_total_epoch 391.77079251408577
Training tokenizer:  95% 7644/8047 [7:01:10<32:04,  4.78s/it]loss_total_epoch 391.80878996104
Training tokenizer:  95% 7645/8047 [7:01:15<32:01,  4.78s/it]loss_total_epoch 391.83981997333467
Training tokenizer:  95% 7646/8047 [7:01:20<31:59,  4.79s/it]loss_total_epoch 391.8822036664933
Training tokenizer:  95% 7647/8047 [7:01:25<31:52,  4.78s/it]loss_total_epoch 391.9166056420654
Training tokenizer:  95% 7648/8047 [7:01:30<31:43,  4.77s/it]loss_total_epoch 391.94876449741423
Training tokenizer:  95% 7649/8047 [7:01:34<31:37,  4.77s/it]loss_total_epoch 391.98567119427025
Training tokenizer:  95% 7650/8047 [7:01:39<31:27,  4.76s/it]loss_total_epoch 392.03139231167734
Training tokenizer:  95% 7651/8047 [7:01:44<31:20,  4.75s/it]loss_total_epoch 392.07221907936037
Training tokenizer:  95% 7652/8047 [7:01:48<31:13,  4.74s/it]loss_total_epoch 392.1050872076303
Training tokenizer:  95% 7653/8047 [7:01:53<31:11,  4.75s/it]loss_total_epoch 392.1423296686262
Training tokenizer:  95% 7654/8047 [7:01:58<31:02,  4.74s/it]loss_total_epoch 392.1809507776052
Training tokenizer:  95% 7655/8047 [7:02:03<31:00,  4.75s/it]loss_total_epoch 392.2270047608763
Training tokenizer:  95% 7656/8047 [7:02:08<30:59,  4.76s/it]loss_total_epoch 392.262009518221
Training tokenizer:  95% 7657/8047 [7:02:12<30:52,  4.75s/it]loss_total_epoch 392.30457902513444
Training tokenizer:  95% 7658/8047 [7:02:17<30:50,  4.76s/it]loss_total_epoch 392.3406552542001
Training tokenizer:  95% 7659/8047 [7:02:22<30:44,  4.75s/it]loss_total_epoch 392.37778676860034
Training tokenizer:  95% 7660/8047 [7:02:27<30:42,  4.76s/it]loss_total_epoch 392.41689469851553
Training tokenizer:  95% 7661/8047 [7:02:31<30:40,  4.77s/it]loss_total_epoch 392.4632173758
Training tokenizer:  95% 7662/8047 [7:02:36<30:38,  4.77s/it]loss_total_epoch 392.50356695987284
Training tokenizer:  95% 7663/8047 [7:02:41<30:32,  4.77s/it]loss_total_epoch 392.54274722002447
Training tokenizer:  95% 7664/8047 [7:02:46<30:27,  4.77s/it]loss_total_epoch 392.57026729919016
Training tokenizer:  95% 7665/8047 [7:02:50<30:19,  4.76s/it]loss_total_epoch 392.6061921287328
Training tokenizer:  95% 7666/8047 [7:02:55<30:10,  4.75s/it]loss_total_epoch 392.6514578703791
Training tokenizer:  95% 7667/8047 [7:03:00<30:02,  4.74s/it]loss_total_epoch 392.69383732788265
Training tokenizer:  95% 7668/8047 [7:03:05<30:00,  4.75s/it]loss_total_epoch 392.7388664390892
Training tokenizer:  95% 7669/8047 [7:03:09<29:52,  4.74s/it]loss_total_epoch 392.78212199918926
Training tokenizer:  95% 7670/8047 [7:03:14<29:48,  4.74s/it]loss_total_epoch 392.8193041179329
Training tokenizer:  95% 7671/8047 [7:03:19<29:44,  4.74s/it]loss_total_epoch 392.8577664140612
Training tokenizer:  95% 7672/8047 [7:03:24<29:36,  4.74s/it]loss_total_epoch 392.89813537336886
Training tokenizer:  95% 7673/8047 [7:03:28<29:29,  4.73s/it]loss_total_epoch 392.94047637470067
Training tokenizer:  95% 7674/8047 [7:03:33<29:20,  4.72s/it]loss_total_epoch 392.9790091831237
Training tokenizer:  95% 7675/8047 [7:03:38<29:21,  4.73s/it]loss_total_epoch 393.01360916160047
Training tokenizer:  95% 7676/8047 [7:03:42<29:19,  4.74s/it]loss_total_epoch 393.0625786613673
Training tokenizer:  95% 7677/8047 [7:03:47<29:21,  4.76s/it]loss_total_epoch 393.10263962112367
Training tokenizer:  95% 7678/8047 [7:03:52<29:19,  4.77s/it]loss_total_epoch 393.1478321310133
Training tokenizer:  95% 7679/8047 [7:03:57<29:12,  4.76s/it]loss_total_epoch 393.1919984240085
Training tokenizer:  95% 7680/8047 [7:04:02<29:04,  4.75s/it]loss_total_epoch 393.2319117654115
Training tokenizer:  95% 7681/8047 [7:04:06<28:58,  4.75s/it]loss_total_epoch 393.26288419403136
Training tokenizer:  95% 7682/8047 [7:04:11<28:53,  4.75s/it]loss_total_epoch 393.2960882652551
Training tokenizer:  95% 7683/8047 [7:04:16<28:49,  4.75s/it]loss_total_epoch 393.331910783425
Training tokenizer:  95% 7684/8047 [7:04:21<28:44,  4.75s/it]loss_total_epoch 393.3742307368666
Training tokenizer:  96% 7685/8047 [7:04:25<28:42,  4.76s/it]loss_total_epoch 393.406651487574
Training tokenizer:  96% 7686/8047 [7:04:30<28:36,  4.75s/it]loss_total_epoch 393.44455749355257
Training tokenizer:  96% 7687/8047 [7:04:35<28:41,  4.78s/it]loss_total_epoch 393.48566539771855
Training tokenizer:  96% 7688/8047 [7:04:40<28:34,  4.78s/it]loss_total_epoch 393.5082416255027
Training tokenizer:  96% 7689/8047 [7:04:44<28:20,  4.75s/it]loss_total_epoch 393.5440009403974
Training tokenizer:  96% 7690/8047 [7:04:49<28:19,  4.76s/it]loss_total_epoch 393.57629255764186
Training tokenizer:  96% 7691/8047 [7:04:54<28:13,  4.76s/it]loss_total_epoch 393.61573682166636
Training tokenizer:  96% 7692/8047 [7:04:59<28:13,  4.77s/it]loss_total_epoch 393.6518849451095
Training tokenizer:  96% 7693/8047 [7:05:03<28:04,  4.76s/it]loss_total_epoch 393.69589917548
Training tokenizer:  96% 7694/8047 [7:05:08<27:56,  4.75s/it]loss_total_epoch 393.7355595100671
Training tokenizer:  96% 7695/8047 [7:05:13<27:50,  4.75s/it]loss_total_epoch 393.77659862302244
Training tokenizer:  96% 7696/8047 [7:05:18<27:48,  4.75s/it]loss_total_epoch 393.81593750976026
Training tokenizer:  96% 7697/8047 [7:05:22<27:50,  4.77s/it]loss_total_epoch 393.85775033198297
Training tokenizer:  96% 7698/8047 [7:05:27<27:46,  4.78s/it]loss_total_epoch 393.9006620403379
Training tokenizer:  96% 7699/8047 [7:05:32<27:49,  4.80s/it]loss_total_epoch 393.94054157845676
Training tokenizer:  96% 7700/8047 [7:05:37<27:39,  4.78s/it]loss_total_epoch 393.98164260201156
Training tokenizer:  96% 7701/8047 [7:05:42<27:32,  4.78s/it]loss_total_epoch 394.02103423886
Training tokenizer:  96% 7702/8047 [7:05:46<27:26,  4.77s/it]loss_total_epoch 394.0593175608665
Training tokenizer:  96% 7703/8047 [7:05:51<27:19,  4.77s/it]loss_total_epoch 394.1007069479674
Training tokenizer:  96% 7704/8047 [7:05:56<27:13,  4.76s/it]loss_total_epoch 394.13288415782154
Training tokenizer:  96% 7705/8047 [7:06:01<27:11,  4.77s/it]loss_total_epoch 394.17132467217743
Training tokenizer:  96% 7706/8047 [7:06:06<27:10,  4.78s/it]loss_total_epoch 394.2125069666654
Training tokenizer:  96% 7707/8047 [7:06:10<27:09,  4.79s/it]loss_total_epoch 394.2558505255729
Training tokenizer:  96% 7708/8047 [7:06:15<27:01,  4.78s/it]loss_total_epoch 394.29666956700385
Training tokenizer:  96% 7709/8047 [7:06:20<26:56,  4.78s/it]loss_total_epoch 394.33601905591786
Training tokenizer:  96% 7710/8047 [7:06:25<26:48,  4.77s/it]loss_total_epoch 394.37609080038965
Training tokenizer:  96% 7711/8047 [7:06:29<26:41,  4.77s/it]loss_total_epoch 394.41002201475203
Training tokenizer:  96% 7712/8047 [7:06:34<26:32,  4.75s/it]loss_total_epoch 394.4467187318951
Training tokenizer:  96% 7713/8047 [7:06:39<26:28,  4.76s/it]loss_total_epoch 394.48594295047224
Training tokenizer:  96% 7714/8047 [7:06:44<26:20,  4.75s/it]loss_total_epoch 394.5186812337488
Training tokenizer:  96% 7715/8047 [7:06:48<26:11,  4.73s/it]loss_total_epoch 394.5582138914615
Training tokenizer:  96% 7716/8047 [7:06:53<26:09,  4.74s/it]loss_total_epoch 394.6022641155869
Training tokenizer:  96% 7717/8047 [7:06:58<26:07,  4.75s/it]loss_total_epoch 394.64663531444967
Training tokenizer:  96% 7718/8047 [7:07:03<26:07,  4.76s/it]loss_total_epoch 394.69215851835907
Training tokenizer:  96% 7719/8047 [7:07:07<26:04,  4.77s/it]loss_total_epoch 394.7214007601142
Training tokenizer:  96% 7720/8047 [7:07:12<25:57,  4.76s/it]loss_total_epoch 394.7563864439726
Training tokenizer:  96% 7721/8047 [7:07:17<25:51,  4.76s/it]loss_total_epoch 394.78739011101425
Training tokenizer:  96% 7722/8047 [7:07:21<25:28,  4.70s/it]loss_total_epoch 394.83014075271785
Training tokenizer:  96% 7723/8047 [7:07:26<25:28,  4.72s/it]loss_total_epoch 394.87531252764165
Training tokenizer:  96% 7724/8047 [7:07:31<25:23,  4.72s/it]loss_total_epoch 394.9161118399352
Training tokenizer:  96% 7725/8047 [7:07:36<25:22,  4.73s/it]loss_total_epoch 394.9481267761439
Training tokenizer:  96% 7726/8047 [7:07:40<25:16,  4.72s/it]loss_total_epoch 394.9928062092513
Training tokenizer:  96% 7727/8047 [7:07:45<25:16,  4.74s/it]loss_total_epoch 395.0325813125819
Training tokenizer:  96% 7728/8047 [7:07:50<25:17,  4.76s/it]loss_total_epoch 395.06665096990764
Training tokenizer:  96% 7729/8047 [7:07:55<25:14,  4.76s/it]loss_total_epoch 395.09880813769996
Training tokenizer:  96% 7730/8047 [7:07:59<25:02,  4.74s/it]loss_total_epoch 395.14370585791767
Training tokenizer:  96% 7731/8047 [7:08:04<24:59,  4.75s/it]loss_total_epoch 395.1890859287232
Training tokenizer:  96% 7732/8047 [7:08:09<24:55,  4.75s/it]loss_total_epoch 395.2276854868978
Training tokenizer:  96% 7733/8047 [7:08:14<24:48,  4.74s/it]loss_total_epoch 395.26554573886096
Training tokenizer:  96% 7734/8047 [7:08:18<24:44,  4.74s/it]loss_total_epoch 395.30578985251486
Training tokenizer:  96% 7735/8047 [7:08:23<24:39,  4.74s/it]loss_total_epoch 395.34674535132945
Training tokenizer:  96% 7736/8047 [7:08:28<24:30,  4.73s/it]loss_total_epoch 395.39633842743933
Training tokenizer:  96% 7737/8047 [7:08:33<24:26,  4.73s/it]loss_total_epoch 395.43655941076577
Training tokenizer:  96% 7738/8047 [7:08:37<24:17,  4.72s/it]loss_total_epoch 395.4667024128139
Training tokenizer:  96% 7739/8047 [7:08:42<24:17,  4.73s/it]loss_total_epoch 395.50502281636
Training tokenizer:  96% 7740/8047 [7:08:47<24:12,  4.73s/it]loss_total_epoch 395.54413874819875
Training tokenizer:  96% 7741/8047 [7:08:52<24:11,  4.74s/it]loss_total_epoch 395.58300183340907
Training tokenizer:  96% 7742/8047 [7:08:56<24:08,  4.75s/it]loss_total_epoch 395.6245373748243
Training tokenizer:  96% 7743/8047 [7:09:01<23:57,  4.73s/it]loss_total_epoch 395.66202622279525
Training tokenizer:  96% 7744/8047 [7:09:06<23:55,  4.74s/it]loss_total_epoch 395.7028421163559
Training tokenizer:  96% 7745/8047 [7:09:10<23:51,  4.74s/it]loss_total_epoch 395.7455084249377
Training tokenizer:  96% 7746/8047 [7:09:15<23:48,  4.75s/it]loss_total_epoch 395.78708643466234
Training tokenizer:  96% 7747/8047 [7:09:20<23:41,  4.74s/it]loss_total_epoch 395.82039266452193
Training tokenizer:  96% 7748/8047 [7:09:25<23:42,  4.76s/it]loss_total_epoch 395.8669070415199
Training tokenizer:  96% 7749/8047 [7:09:30<23:40,  4.77s/it]loss_total_epoch 395.8991715051234
Training tokenizer:  96% 7750/8047 [7:09:34<23:30,  4.75s/it]loss_total_epoch 395.93897222727537
Training tokenizer:  96% 7751/8047 [7:09:39<23:24,  4.75s/it]loss_total_epoch 395.97527265548706
Training tokenizer:  96% 7752/8047 [7:09:44<23:19,  4.75s/it]loss_total_epoch 396.017089124769
Training tokenizer:  96% 7753/8047 [7:09:48<23:13,  4.74s/it]loss_total_epoch 396.0512917749584
Training tokenizer:  96% 7754/8047 [7:09:53<23:10,  4.75s/it]loss_total_epoch 396.09746101126075
Training tokenizer:  96% 7755/8047 [7:09:58<23:14,  4.78s/it]loss_total_epoch 396.13960095494986
Training tokenizer:  96% 7756/8047 [7:10:03<23:09,  4.78s/it]loss_total_epoch 396.177549097687
Training tokenizer:  96% 7757/8047 [7:10:08<23:05,  4.78s/it]loss_total_epoch 396.2174145244062
Training tokenizer:  96% 7758/8047 [7:10:12<22:59,  4.77s/it]loss_total_epoch 396.25220284610987
Training tokenizer:  96% 7759/8047 [7:10:17<22:57,  4.78s/it]loss_total_epoch 396.2933009825647
Training tokenizer:  96% 7760/8047 [7:10:22<22:53,  4.79s/it]loss_total_epoch 396.33832136914134
Training tokenizer:  96% 7761/8047 [7:10:27<22:48,  4.78s/it]loss_total_epoch 396.38753704354167
Training tokenizer:  96% 7762/8047 [7:10:31<22:37,  4.76s/it]loss_total_epoch 396.4240472391248
Training tokenizer:  96% 7763/8047 [7:10:36<22:30,  4.76s/it]loss_total_epoch 396.4736315943301
Training tokenizer:  96% 7764/8047 [7:10:41<22:22,  4.74s/it]loss_total_epoch 396.51295718178153
Training tokenizer:  96% 7765/8047 [7:10:46<22:23,  4.76s/it]loss_total_epoch 396.5545583628118
Training tokenizer:  97% 7766/8047 [7:10:50<22:16,  4.76s/it]loss_total_epoch 396.59180180728436
Training tokenizer:  97% 7767/8047 [7:10:55<22:10,  4.75s/it]loss_total_epoch 396.619436301291
Training tokenizer:  97% 7768/8047 [7:11:00<22:13,  4.78s/it]loss_total_epoch 396.65599198639393
Training tokenizer:  97% 7769/8047 [7:11:05<22:04,  4.77s/it]loss_total_epoch 396.6962547674775
Training tokenizer:  97% 7770/8047 [7:11:10<21:56,  4.75s/it]loss_total_epoch 396.730417534709
Training tokenizer:  97% 7771/8047 [7:11:14<21:50,  4.75s/it]loss_total_epoch 396.7615371607244
Training tokenizer:  97% 7772/8047 [7:11:19<21:46,  4.75s/it]loss_total_epoch 396.79806311428547
Training tokenizer:  97% 7773/8047 [7:11:24<21:39,  4.74s/it]loss_total_epoch 396.83443035930395
Training tokenizer:  97% 7774/8047 [7:11:29<21:38,  4.75s/it]loss_total_epoch 396.8802646882832
Training tokenizer:  97% 7775/8047 [7:11:33<21:29,  4.74s/it]loss_total_epoch 396.9138850606978
Training tokenizer:  97% 7776/8047 [7:11:38<21:27,  4.75s/it]loss_total_epoch 396.9583590440452
Training tokenizer:  97% 7777/8047 [7:11:43<21:22,  4.75s/it]loss_total_epoch 397.0021633543074
Training tokenizer:  97% 7778/8047 [7:11:48<21:18,  4.75s/it]loss_total_epoch 397.04433035477996
Training tokenizer:  97% 7779/8047 [7:11:52<21:18,  4.77s/it]loss_total_epoch 397.08598663285375
Training tokenizer:  97% 7780/8047 [7:11:57<21:12,  4.77s/it]loss_total_epoch 397.12401086837053
Training tokenizer:  97% 7781/8047 [7:12:02<21:07,  4.76s/it]loss_total_epoch 397.1570148356259
Training tokenizer:  97% 7782/8047 [7:12:07<20:59,  4.75s/it]loss_total_epoch 397.2006588540971
Training tokenizer:  97% 7783/8047 [7:12:11<20:53,  4.75s/it]loss_total_epoch 397.23929050937295
Training tokenizer:  97% 7784/8047 [7:12:16<20:47,  4.74s/it]loss_total_epoch 397.28114380314946
Training tokenizer:  97% 7785/8047 [7:12:21<20:41,  4.74s/it]loss_total_epoch 397.3165553249419
Training tokenizer:  97% 7786/8047 [7:12:26<20:39,  4.75s/it]loss_total_epoch 397.3563052378595
Training tokenizer:  97% 7787/8047 [7:12:30<20:36,  4.75s/it]loss_total_epoch 397.3899891600013
Training tokenizer:  97% 7788/8047 [7:12:35<20:29,  4.75s/it]loss_total_epoch 397.4335748627782
Training tokenizer:  97% 7789/8047 [7:12:40<20:21,  4.73s/it]loss_total_epoch 397.4666274189949
Training tokenizer:  97% 7790/8047 [7:12:45<20:21,  4.75s/it]loss_total_epoch 397.5004230439663
Training tokenizer:  97% 7791/8047 [7:12:49<20:16,  4.75s/it]loss_total_epoch 397.5352098569274
Training tokenizer:  97% 7792/8047 [7:12:54<20:07,  4.74s/it]loss_total_epoch 397.57866490632296
Training tokenizer:  97% 7793/8047 [7:12:59<19:56,  4.71s/it]loss_total_epoch 397.6220000386238
Training tokenizer:  97% 7794/8047 [7:13:03<19:56,  4.73s/it]loss_total_epoch 397.666770670563
Training tokenizer:  97% 7795/8047 [7:13:08<19:55,  4.74s/it]loss_total_epoch 397.7157750017941
Training tokenizer:  97% 7796/8047 [7:13:13<19:49,  4.74s/it]loss_total_epoch 397.75957380235195
Training tokenizer:  97% 7797/8047 [7:13:18<19:43,  4.73s/it]loss_total_epoch 397.7949276417494
Training tokenizer:  97% 7798/8047 [7:13:22<19:41,  4.74s/it]loss_total_epoch 397.82900070399046
Training tokenizer:  97% 7799/8047 [7:13:27<19:36,  4.74s/it]loss_total_epoch 397.86514921858907
Training tokenizer:  97% 7800/8047 [7:13:32<19:27,  4.73s/it]loss_total_epoch 397.8975262977183
Training tokenizer:  97% 7801/8047 [7:13:37<19:27,  4.75s/it]loss_total_epoch 397.9287594668567
Training tokenizer:  97% 7802/8047 [7:13:41<19:18,  4.73s/it]loss_total_epoch 397.9609596170485
Training tokenizer:  97% 7803/8047 [7:13:46<19:14,  4.73s/it]loss_total_epoch 398.0024621859193
Training tokenizer:  97% 7804/8047 [7:13:51<19:09,  4.73s/it]loss_total_epoch 398.04166696965694
Training tokenizer:  97% 7805/8047 [7:13:56<19:07,  4.74s/it]loss_total_epoch 398.0918739028275
Training tokenizer:  97% 7806/8047 [7:14:00<19:02,  4.74s/it]loss_total_epoch 398.12935787811875
Training tokenizer:  97% 7807/8047 [7:14:05<19:00,  4.75s/it]loss_total_epoch 398.16856878623366
Training tokenizer:  97% 7808/8047 [7:14:10<18:50,  4.73s/it]loss_total_epoch 398.2038308605552
Training tokenizer:  97% 7809/8047 [7:14:14<18:44,  4.73s/it]loss_total_epoch 398.2529377788305
Training tokenizer:  97% 7810/8047 [7:14:19<18:43,  4.74s/it]loss_total_epoch 398.2986238114536
Training tokenizer:  97% 7811/8047 [7:14:24<18:36,  4.73s/it]loss_total_epoch 398.3412452638149
Training tokenizer:  97% 7812/8047 [7:14:29<18:35,  4.75s/it]loss_total_epoch 398.3726877383888
Training tokenizer:  97% 7813/8047 [7:14:33<18:31,  4.75s/it]loss_total_epoch 398.41608114540577
Training tokenizer:  97% 7814/8047 [7:14:38<18:23,  4.74s/it]loss_total_epoch 398.4553356990218
Training tokenizer:  97% 7815/8047 [7:14:43<18:20,  4.74s/it]loss_total_epoch 398.49228074774146
Training tokenizer:  97% 7816/8047 [7:14:48<18:13,  4.73s/it]loss_total_epoch 398.5341100394726
Training tokenizer:  97% 7817/8047 [7:14:52<18:13,  4.75s/it]loss_total_epoch 398.57882184535265
Training tokenizer:  97% 7818/8047 [7:14:57<18:08,  4.75s/it]loss_total_epoch 398.6107782796025
Training tokenizer:  97% 7819/8047 [7:15:02<18:03,  4.75s/it]loss_total_epoch 398.66169852390885
Training tokenizer:  97% 7820/8047 [7:15:07<17:58,  4.75s/it]loss_total_epoch 398.70139906927943
Training tokenizer:  97% 7821/8047 [7:15:11<17:52,  4.74s/it]loss_total_epoch 398.7440724372864
Training tokenizer:  97% 7822/8047 [7:15:16<17:49,  4.75s/it]loss_total_epoch 398.78817017376423
Training tokenizer:  97% 7823/8047 [7:15:21<17:43,  4.75s/it]loss_total_epoch 398.8272498175502
Training tokenizer:  97% 7824/8047 [7:15:26<17:38,  4.75s/it]loss_total_epoch 398.8587077409029
Training tokenizer:  97% 7825/8047 [7:15:30<17:31,  4.74s/it]loss_total_epoch 398.89748526737094
Training tokenizer:  97% 7826/8047 [7:15:35<17:29,  4.75s/it]loss_total_epoch 398.9368594959378
Training tokenizer:  97% 7827/8047 [7:15:40<17:15,  4.71s/it]loss_total_epoch 398.9721667319536
Training tokenizer:  97% 7828/8047 [7:15:44<17:10,  4.70s/it]loss_total_epoch 399.00581995770335
Training tokenizer:  97% 7829/8047 [7:15:49<17:09,  4.72s/it]loss_total_epoch 399.04850237444043
Training tokenizer:  97% 7830/8047 [7:15:54<17:08,  4.74s/it]loss_total_epoch 399.0944677256048
Training tokenizer:  97% 7831/8047 [7:15:59<17:06,  4.75s/it]loss_total_epoch 399.13781018555164
Training tokenizer:  97% 7832/8047 [7:16:04<17:02,  4.75s/it]loss_total_epoch 399.18000088632107
Training tokenizer:  97% 7833/8047 [7:16:08<16:56,  4.75s/it]loss_total_epoch 399.21538626402617
Training tokenizer:  97% 7834/8047 [7:16:13<16:54,  4.76s/it]loss_total_epoch 399.24509826116264
Training tokenizer:  97% 7835/8047 [7:16:18<16:49,  4.76s/it]loss_total_epoch 399.27475501969457
Training tokenizer:  97% 7836/8047 [7:16:23<16:47,  4.78s/it]loss_total_epoch 399.3255225904286
Training tokenizer:  97% 7837/8047 [7:16:27<16:43,  4.78s/it]loss_total_epoch 399.36585810780525
Training tokenizer:  97% 7838/8047 [7:16:32<16:38,  4.78s/it]loss_total_epoch 399.40695283561945
Training tokenizer:  97% 7839/8047 [7:16:37<16:32,  4.77s/it]loss_total_epoch 399.4462259449065
Training tokenizer:  97% 7840/8047 [7:16:42<16:26,  4.77s/it]loss_total_epoch 399.48348177969456
Training tokenizer:  97% 7841/8047 [7:16:47<16:24,  4.78s/it]loss_total_epoch 399.5183936357498
Training tokenizer:  97% 7842/8047 [7:16:51<16:20,  4.78s/it]loss_total_epoch 399.5578994564712
Training tokenizer:  97% 7843/8047 [7:16:56<16:16,  4.79s/it]loss_total_epoch 399.59092547371984
Training tokenizer:  97% 7844/8047 [7:17:01<16:09,  4.78s/it]loss_total_epoch 399.6297364830971
Training tokenizer:  97% 7845/8047 [7:17:06<16:02,  4.76s/it]loss_total_epoch 399.67187190800905
Training tokenizer:  98% 7846/8047 [7:17:10<15:56,  4.76s/it]loss_total_epoch 399.7089043110609
Training tokenizer:  98% 7847/8047 [7:17:15<15:49,  4.75s/it]loss_total_epoch 399.75923382490873
Training tokenizer:  98% 7848/8047 [7:17:20<15:48,  4.76s/it]loss_total_epoch 399.80551900342107
Training tokenizer:  98% 7849/8047 [7:17:25<15:41,  4.75s/it]loss_total_epoch 399.84424586594105
Training tokenizer:  98% 7850/8047 [7:17:29<15:38,  4.76s/it]loss_total_epoch 399.8743700850755
Training tokenizer:  98% 7851/8047 [7:17:34<15:35,  4.78s/it]loss_total_epoch 399.9132962692529
Training tokenizer:  98% 7852/8047 [7:17:39<15:30,  4.77s/it]loss_total_epoch 399.9417762514204
Training tokenizer:  98% 7853/8047 [7:17:44<15:26,  4.77s/it]loss_total_epoch 399.9854685496539
Training tokenizer:  98% 7854/8047 [7:17:49<15:20,  4.77s/it]loss_total_epoch 400.02826459147036
Training tokenizer:  98% 7855/8047 [7:17:53<15:15,  4.77s/it]loss_total_epoch 400.0697378795594
Training tokenizer:  98% 7856/8047 [7:17:58<15:11,  4.77s/it]loss_total_epoch 400.11938580311835
Training tokenizer:  98% 7857/8047 [7:18:03<15:08,  4.78s/it]loss_total_epoch 400.16540520079434
Training tokenizer:  98% 7858/8047 [7:18:08<14:59,  4.76s/it]loss_total_epoch 400.19921069033444
Training tokenizer:  98% 7859/8047 [7:18:12<14:55,  4.76s/it]loss_total_epoch 400.2411604318768
Training tokenizer:  98% 7860/8047 [7:18:17<14:50,  4.76s/it]loss_total_epoch 400.2875311244279
Training tokenizer:  98% 7861/8047 [7:18:22<14:47,  4.77s/it]loss_total_epoch 400.3335120398551
Training tokenizer:  98% 7862/8047 [7:18:27<14:40,  4.76s/it]loss_total_epoch 400.36775684542954
Training tokenizer:  98% 7863/8047 [7:18:31<14:34,  4.75s/it]loss_total_epoch 400.40715560503304
Training tokenizer:  98% 7864/8047 [7:18:36<14:29,  4.75s/it]loss_total_epoch 400.4500487912446
Training tokenizer:  98% 7865/8047 [7:18:41<14:28,  4.77s/it]loss_total_epoch 400.4945938345045
Training tokenizer:  98% 7866/8047 [7:18:46<14:13,  4.71s/it]loss_total_epoch 400.5340812522918
Training tokenizer:  98% 7867/8047 [7:18:50<14:13,  4.74s/it]loss_total_epoch 400.5701143313199
Training tokenizer:  98% 7868/8047 [7:18:55<14:09,  4.74s/it]loss_total_epoch 400.6077560391277
Training tokenizer:  98% 7869/8047 [7:19:00<14:02,  4.73s/it]loss_total_epoch 400.6393942963332
Training tokenizer:  98% 7870/8047 [7:19:05<13:58,  4.74s/it]loss_total_epoch 400.67758842371404
Training tokenizer:  98% 7871/8047 [7:19:09<13:55,  4.75s/it]loss_total_epoch 400.7090592440218
Training tokenizer:  98% 7872/8047 [7:19:14<13:53,  4.76s/it]loss_total_epoch 400.7495277542621
Training tokenizer:  98% 7873/8047 [7:19:19<13:50,  4.77s/it]loss_total_epoch 400.7889650333673
Training tokenizer:  98% 7874/8047 [7:19:24<13:44,  4.76s/it]loss_total_epoch 400.8343940656632
Training tokenizer:  98% 7875/8047 [7:19:28<13:39,  4.76s/it]loss_total_epoch 400.88232951052487
Training tokenizer:  98% 7876/8047 [7:19:33<13:34,  4.76s/it]loss_total_epoch 400.92577046342194
Training tokenizer:  98% 7877/8047 [7:19:38<13:19,  4.70s/it]loss_total_epoch 400.9648936446756
Training tokenizer:  98% 7878/8047 [7:19:42<13:16,  4.72s/it]loss_total_epoch 401.00057414732873
Training tokenizer:  98% 7879/8047 [7:19:47<13:16,  4.74s/it]loss_total_epoch 401.0414909105748
Training tokenizer:  98% 7880/8047 [7:19:52<13:13,  4.75s/it]loss_total_epoch 401.0865222495049
Training tokenizer:  98% 7881/8047 [7:19:57<13:09,  4.76s/it]loss_total_epoch 401.13330646418035
Training tokenizer:  98% 7882/8047 [7:20:02<13:02,  4.74s/it]loss_total_epoch 401.1656651068479
Training tokenizer:  98% 7883/8047 [7:20:06<12:59,  4.75s/it]loss_total_epoch 401.2105809841305
Training tokenizer:  98% 7884/8047 [7:20:11<12:56,  4.76s/it]loss_total_epoch 401.24656728096306
Training tokenizer:  98% 7885/8047 [7:20:16<12:51,  4.76s/it]loss_total_epoch 401.2819560933858
Training tokenizer:  98% 7886/8047 [7:20:21<12:45,  4.76s/it]loss_total_epoch 401.32823658920825
Training tokenizer:  98% 7887/8047 [7:20:25<12:40,  4.75s/it]loss_total_epoch 401.36349689774215
Training tokenizer:  98% 7888/8047 [7:20:30<12:37,  4.77s/it]loss_total_epoch 401.40114677883685
Training tokenizer:  98% 7889/8047 [7:20:35<12:31,  4.76s/it]loss_total_epoch 401.4382101986557
Training tokenizer:  98% 7890/8047 [7:20:40<12:24,  4.74s/it]loss_total_epoch 401.47355371527374
Training tokenizer:  98% 7891/8047 [7:20:44<12:22,  4.76s/it]loss_total_epoch 401.5147383976728
Training tokenizer:  98% 7892/8047 [7:20:49<12:19,  4.77s/it]loss_total_epoch 401.5518284533173
Training tokenizer:  98% 7893/8047 [7:20:54<12:14,  4.77s/it]loss_total_epoch 401.59195931069553
Training tokenizer:  98% 7894/8047 [7:20:59<12:11,  4.78s/it]loss_total_epoch 401.6264401059598
Training tokenizer:  98% 7895/8047 [7:21:03<12:03,  4.76s/it]loss_total_epoch 401.6612337511033
Training tokenizer:  98% 7896/8047 [7:21:08<11:59,  4.77s/it]loss_total_epoch 401.6990902405232
Training tokenizer:  98% 7897/8047 [7:21:13<11:53,  4.76s/it]loss_total_epoch 401.74120471067727
Training tokenizer:  98% 7898/8047 [7:21:18<11:49,  4.76s/it]loss_total_epoch 401.7896898481995
Training tokenizer:  98% 7899/8047 [7:21:23<11:44,  4.76s/it]loss_total_epoch 401.82772961072624
Training tokenizer:  98% 7900/8047 [7:21:27<11:41,  4.77s/it]loss_total_epoch 401.87855562753975
Training tokenizer:  98% 7901/8047 [7:21:32<11:33,  4.75s/it]loss_total_epoch 401.91867905296385
Training tokenizer:  98% 7902/8047 [7:21:37<11:29,  4.75s/it]loss_total_epoch 401.9576933477074
Training tokenizer:  98% 7903/8047 [7:21:42<11:26,  4.77s/it]loss_total_epoch 402.0036686901003
Training tokenizer:  98% 7904/8047 [7:21:46<11:20,  4.76s/it]loss_total_epoch 402.03738826327026
Training tokenizer:  98% 7905/8047 [7:21:51<11:15,  4.76s/it]loss_total_epoch 402.07819514907897
Training tokenizer:  98% 7906/8047 [7:21:56<11:10,  4.76s/it]loss_total_epoch 402.1291747074574
Training tokenizer:  98% 7907/8047 [7:22:01<11:06,  4.76s/it]loss_total_epoch 402.1604935284704
Training tokenizer:  98% 7908/8047 [7:22:05<11:02,  4.77s/it]loss_total_epoch 402.1925859320909
Training tokenizer:  98% 7909/8047 [7:22:10<10:57,  4.76s/it]loss_total_epoch 402.2366829160601
Training tokenizer:  98% 7910/8047 [7:22:15<10:52,  4.76s/it]loss_total_epoch 402.27299359999597
Training tokenizer:  98% 7911/8047 [7:22:19<10:40,  4.71s/it]loss_total_epoch 402.30634502880275
Training tokenizer:  98% 7912/8047 [7:22:24<10:39,  4.73s/it]loss_total_epoch 402.3506116028875
Training tokenizer:  98% 7913/8047 [7:22:29<10:34,  4.74s/it]loss_total_epoch 402.38612679205835
Training tokenizer:  98% 7914/8047 [7:22:34<10:30,  4.74s/it]loss_total_epoch 402.4210727419704
Training tokenizer:  98% 7915/8047 [7:22:39<10:27,  4.75s/it]loss_total_epoch 402.46011183969676
Training tokenizer:  98% 7916/8047 [7:22:43<10:19,  4.73s/it]loss_total_epoch 402.49556710384786
Training tokenizer:  98% 7917/8047 [7:22:48<10:16,  4.74s/it]loss_total_epoch 402.52270388044417
Training tokenizer:  98% 7918/8047 [7:22:53<10:10,  4.73s/it]loss_total_epoch 402.56960193254054
Training tokenizer:  98% 7919/8047 [7:22:57<10:08,  4.75s/it]loss_total_epoch 402.60793491639197
Training tokenizer:  98% 7920/8047 [7:23:02<10:04,  4.76s/it]loss_total_epoch 402.6491670999676
Training tokenizer:  98% 7921/8047 [7:23:07<09:59,  4.76s/it]loss_total_epoch 402.70173738338053
Training tokenizer:  98% 7922/8047 [7:23:12<09:56,  4.77s/it]loss_total_epoch 402.7489142064005
Training tokenizer:  98% 7923/8047 [7:23:17<09:53,  4.79s/it]loss_total_epoch 402.7907862160355
Training tokenizer:  98% 7924/8047 [7:23:21<09:47,  4.78s/it]loss_total_epoch 402.81923030130565
Training tokenizer:  98% 7925/8047 [7:23:26<09:42,  4.78s/it]loss_total_epoch 402.85663378052413
Training tokenizer:  98% 7926/8047 [7:23:31<09:38,  4.78s/it]loss_total_epoch 402.8952696006745
Training tokenizer:  99% 7927/8047 [7:23:36<09:32,  4.77s/it]loss_total_epoch 402.93240381218493
Training tokenizer:  99% 7928/8047 [7:23:41<09:29,  4.79s/it]loss_total_epoch 402.97636908106506
Training tokenizer:  99% 7929/8047 [7:23:45<09:23,  4.78s/it]loss_total_epoch 403.01242510415614
Training tokenizer:  99% 7930/8047 [7:23:50<09:17,  4.77s/it]loss_total_epoch 403.05597244389355
Training tokenizer:  99% 7931/8047 [7:23:55<09:11,  4.75s/it]loss_total_epoch 403.0944933760911
Training tokenizer:  99% 7932/8047 [7:24:00<09:06,  4.75s/it]loss_total_epoch 403.1306317616254
Training tokenizer:  99% 7933/8047 [7:24:04<09:01,  4.75s/it]loss_total_epoch 403.15496603772044
Training tokenizer:  99% 7934/8047 [7:24:09<08:57,  4.76s/it]loss_total_epoch 403.197848983109
Training tokenizer:  99% 7935/8047 [7:24:14<08:54,  4.77s/it]loss_total_epoch 403.23945800960064
Training tokenizer:  99% 7936/8047 [7:24:19<08:50,  4.78s/it]loss_total_epoch 403.2806821092963
Training tokenizer:  99% 7937/8047 [7:24:23<08:45,  4.78s/it]loss_total_epoch 403.3211987018585
Training tokenizer:  99% 7938/8047 [7:24:28<08:40,  4.78s/it]loss_total_epoch 403.36116087436676
Training tokenizer:  99% 7939/8047 [7:24:33<08:35,  4.77s/it]loss_total_epoch 403.3974993303418
Training tokenizer:  99% 7940/8047 [7:24:38<08:30,  4.77s/it]loss_total_epoch 403.43474723771214
Training tokenizer:  99% 7941/8047 [7:24:42<08:24,  4.76s/it]loss_total_epoch 403.47737531363964
Training tokenizer:  99% 7942/8047 [7:24:47<08:21,  4.78s/it]loss_total_epoch 403.52346083149314
Training tokenizer:  99% 7943/8047 [7:24:52<08:17,  4.78s/it]loss_total_epoch 403.55611710622907
Training tokenizer:  99% 7944/8047 [7:24:57<08:12,  4.78s/it]loss_total_epoch 403.59902431443334
Training tokenizer:  99% 7945/8047 [7:25:02<08:06,  4.77s/it]loss_total_epoch 403.62926015630364
Training tokenizer:  99% 7946/8047 [7:25:06<08:02,  4.77s/it]loss_total_epoch 403.66448486223817
Training tokenizer:  99% 7947/8047 [7:25:11<07:57,  4.78s/it]loss_total_epoch 403.7032082043588
Training tokenizer:  99% 7948/8047 [7:25:16<07:53,  4.78s/it]loss_total_epoch 403.7507954314351
Training tokenizer:  99% 7949/8047 [7:25:21<07:49,  4.79s/it]loss_total_epoch 403.7899240627885
Training tokenizer:  99% 7950/8047 [7:25:26<07:44,  4.79s/it]loss_total_epoch 403.82873483002186
Training tokenizer:  99% 7951/8047 [7:25:30<07:37,  4.76s/it]loss_total_epoch 403.86776201426983
Training tokenizer:  99% 7952/8047 [7:25:35<07:31,  4.75s/it]loss_total_epoch 403.9082346521318
Training tokenizer:  99% 7953/8047 [7:25:40<07:26,  4.75s/it]loss_total_epoch 403.94571167603135
Training tokenizer:  99% 7954/8047 [7:25:44<07:20,  4.73s/it]loss_total_epoch 403.98252852633595
Training tokenizer:  99% 7955/8047 [7:25:49<07:16,  4.74s/it]loss_total_epoch 404.0220370143652
Training tokenizer:  99% 7956/8047 [7:25:54<07:12,  4.75s/it]loss_total_epoch 404.06186105683446
Training tokenizer:  99% 7957/8047 [7:25:59<07:02,  4.70s/it]loss_total_epoch 404.0965101867914
Training tokenizer:  99% 7958/8047 [7:26:03<07:00,  4.72s/it]loss_total_epoch 404.13606894016266
Training tokenizer:  99% 7959/8047 [7:26:08<06:54,  4.72s/it]loss_total_epoch 404.1802016571164
Training tokenizer:  99% 7960/8047 [7:26:13<06:51,  4.73s/it]loss_total_epoch 404.22994619235396
Training tokenizer:  99% 7961/8047 [7:26:18<06:47,  4.74s/it]loss_total_epoch 404.2747077047825
Training tokenizer:  99% 7962/8047 [7:26:22<06:43,  4.75s/it]loss_total_epoch 404.305779883638
Training tokenizer:  99% 7963/8047 [7:26:27<06:38,  4.75s/it]loss_total_epoch 404.3427033368498
Training tokenizer:  99% 7964/8047 [7:26:32<06:35,  4.76s/it]loss_total_epoch 404.38725007884204
Training tokenizer:  99% 7965/8047 [7:26:37<06:30,  4.76s/it]loss_total_epoch 404.42514359019697
Training tokenizer:  99% 7966/8047 [7:26:41<06:25,  4.75s/it]loss_total_epoch 404.46966109611094
Training tokenizer:  99% 7967/8047 [7:26:46<06:20,  4.75s/it]loss_total_epoch 404.5222101714462
Training tokenizer:  99% 7968/8047 [7:26:51<06:14,  4.74s/it]loss_total_epoch 404.57058574818075
Training tokenizer:  99% 7969/8047 [7:26:56<06:10,  4.75s/it]loss_total_epoch 404.61018709652126
Training tokenizer:  99% 7970/8047 [7:27:00<06:04,  4.74s/it]loss_total_epoch 404.6456011738628
Training tokenizer:  99% 7971/8047 [7:27:05<06:01,  4.75s/it]loss_total_epoch 404.6793913152069
Training tokenizer:  99% 7972/8047 [7:27:10<05:55,  4.75s/it]loss_total_epoch 404.7146687116474
Training tokenizer:  99% 7973/8047 [7:27:15<05:51,  4.75s/it]loss_total_epoch 404.7502521071583
Training tokenizer:  99% 7974/8047 [7:27:19<05:47,  4.76s/it]loss_total_epoch 404.786328015849
Training tokenizer:  99% 7975/8047 [7:27:24<05:43,  4.77s/it]loss_total_epoch 404.82495301775634
Training tokenizer:  99% 7976/8047 [7:27:29<05:39,  4.78s/it]loss_total_epoch 404.8626999501139
Training tokenizer:  99% 7977/8047 [7:27:34<05:34,  4.78s/it]loss_total_epoch 404.90430145151913
Training tokenizer:  99% 7978/8047 [7:27:39<05:30,  4.79s/it]loss_total_epoch 404.9422193635255
Training tokenizer:  99% 7979/8047 [7:27:43<05:24,  4.77s/it]loss_total_epoch 404.9772966410965
Training tokenizer:  99% 7980/8047 [7:27:48<05:20,  4.78s/it]loss_total_epoch 405.01996035687625
Training tokenizer:  99% 7981/8047 [7:27:53<05:15,  4.78s/it]loss_total_epoch 405.0578848775476
Training tokenizer:  99% 7982/8047 [7:27:58<05:10,  4.78s/it]loss_total_epoch 405.0965098235756
Training tokenizer:  99% 7983/8047 [7:28:02<05:04,  4.75s/it]loss_total_epoch 405.1297232378274
Training tokenizer:  99% 7984/8047 [7:28:07<05:01,  4.78s/it]loss_total_epoch 405.1680082436651
Training tokenizer:  99% 7985/8047 [7:28:12<04:55,  4.76s/it]loss_total_epoch 405.1999423522502
Training tokenizer:  99% 7986/8047 [7:28:17<04:50,  4.76s/it]loss_total_epoch 405.23952311463654
Training tokenizer:  99% 7987/8047 [7:28:21<04:45,  4.77s/it]loss_total_epoch 405.2813136894256
Training tokenizer:  99% 7988/8047 [7:28:26<04:41,  4.77s/it]loss_total_epoch 405.317648133263
Training tokenizer:  99% 7989/8047 [7:28:31<04:36,  4.78s/it]loss_total_epoch 405.3537230435759
Training tokenizer:  99% 7990/8047 [7:28:36<04:32,  4.78s/it]loss_total_epoch 405.3942715395242
Training tokenizer:  99% 7991/8047 [7:28:41<04:27,  4.78s/it]loss_total_epoch 405.43084350414574
Training tokenizer:  99% 7992/8047 [7:28:45<04:22,  4.78s/it]loss_total_epoch 405.47004880197346
Training tokenizer:  99% 7993/8047 [7:28:50<04:18,  4.79s/it]loss_total_epoch 405.5106860231608
Training tokenizer:  99% 7994/8047 [7:28:55<04:12,  4.77s/it]loss_total_epoch 405.5426267068833
Training tokenizer:  99% 7995/8047 [7:29:00<04:07,  4.76s/it]loss_total_epoch 405.5860641952604
Training tokenizer:  99% 7996/8047 [7:29:04<04:03,  4.77s/it]loss_total_epoch 405.62322193570435
Training tokenizer:  99% 7997/8047 [7:29:09<03:58,  4.76s/it]loss_total_epoch 405.66051167808473
Training tokenizer:  99% 7998/8047 [7:29:14<03:54,  4.78s/it]loss_total_epoch 405.6937611345202
Training tokenizer:  99% 7999/8047 [7:29:19<03:49,  4.79s/it]loss_total_epoch 405.73736436478794
Training tokenizer:  99% 8000/8047 [7:29:24<03:45,  4.79s/it]loss_total_epoch 405.7740104030818
Training tokenizer:  99% 8001/8047 [7:29:28<03:40,  4.79s/it]loss_total_epoch 405.8101033028215
Training tokenizer:  99% 8002/8047 [7:29:33<03:35,  4.79s/it]loss_total_epoch 405.84465291909873
Training tokenizer:  99% 8003/8047 [7:29:38<03:31,  4.80s/it]loss_total_epoch 405.8833735194057
Training tokenizer:  99% 8004/8047 [7:29:43<03:25,  4.78s/it]loss_total_epoch 405.9273695182055
Training tokenizer:  99% 8005/8047 [7:29:48<03:21,  4.79s/it]loss_total_epoch 405.965001327917
Training tokenizer:  99% 8006/8047 [7:29:52<03:15,  4.76s/it]loss_total_epoch 405.99756504036486
Training tokenizer: 100% 8007/8047 [7:29:57<03:10,  4.76s/it]loss_total_epoch 406.0432354528457
Training tokenizer: 100% 8008/8047 [7:30:02<03:06,  4.77s/it]loss_total_epoch 406.07451941631734
Training tokenizer: 100% 8009/8047 [7:30:06<03:00,  4.75s/it]loss_total_epoch 406.11251161806285
Training tokenizer: 100% 8010/8047 [7:30:11<02:55,  4.75s/it]loss_total_epoch 406.14914891682565
Training tokenizer: 100% 8011/8047 [7:30:16<02:51,  4.76s/it]loss_total_epoch 406.18486429937184
Training tokenizer: 100% 8012/8047 [7:30:21<02:47,  4.78s/it]loss_total_epoch 406.2248844560236
Training tokenizer: 100% 8013/8047 [7:30:26<02:42,  4.77s/it]loss_total_epoch 406.27439552359283
Training tokenizer: 100% 8014/8047 [7:30:30<02:37,  4.76s/it]loss_total_epoch 406.31146487407386
Training tokenizer: 100% 8015/8047 [7:30:35<02:31,  4.75s/it]loss_total_epoch 406.34198383800685
Training tokenizer: 100% 8016/8047 [7:30:40<02:27,  4.75s/it]loss_total_epoch 406.3800103124231
Training tokenizer: 100% 8017/8047 [7:30:45<02:22,  4.76s/it]loss_total_epoch 406.4138108100742
Training tokenizer: 100% 8018/8047 [7:30:49<02:18,  4.77s/it]loss_total_epoch 406.44562253169715
Training tokenizer: 100% 8019/8047 [7:30:54<02:13,  4.77s/it]loss_total_epoch 406.4852445181459
Training tokenizer: 100% 8020/8047 [7:30:59<02:09,  4.79s/it]loss_total_epoch 406.5205041933805
Training tokenizer: 100% 8021/8047 [7:31:04<02:04,  4.79s/it]loss_total_epoch 406.56018695421517
Training tokenizer: 100% 8022/8047 [7:31:09<01:59,  4.78s/it]loss_total_epoch 406.6051416490227
Training tokenizer: 100% 8023/8047 [7:31:13<01:54,  4.77s/it]loss_total_epoch 406.6493569109589
Training tokenizer: 100% 8024/8047 [7:31:18<01:49,  4.78s/it]loss_total_epoch 406.69034153036773
Training tokenizer: 100% 8025/8047 [7:31:23<01:44,  4.77s/it]loss_total_epoch 406.73007020168006
Training tokenizer: 100% 8026/8047 [7:31:28<01:39,  4.75s/it]loss_total_epoch 406.7709879409522
Training tokenizer: 100% 8027/8047 [7:31:32<01:35,  4.76s/it]loss_total_epoch 406.8100343924016
Training tokenizer: 100% 8028/8047 [7:31:37<01:30,  4.75s/it]loss_total_epoch 406.84835484810174
Training tokenizer: 100% 8029/8047 [7:31:42<01:25,  4.75s/it]loss_total_epoch 406.88427184708416
Training tokenizer: 100% 8030/8047 [7:31:46<01:20,  4.74s/it]loss_total_epoch 406.92245970480144
Training tokenizer: 100% 8031/8047 [7:31:51<01:16,  4.75s/it]loss_total_epoch 406.9678313974291
Training tokenizer: 100% 8032/8047 [7:31:56<01:11,  4.75s/it]loss_total_epoch 407.0039433930069
Training tokenizer: 100% 8033/8047 [7:32:01<01:06,  4.75s/it]loss_total_epoch 407.0345009509474
Training tokenizer: 100% 8034/8047 [7:32:06<01:01,  4.75s/it]loss_total_epoch 407.0736161340028
Training tokenizer: 100% 8035/8047 [7:32:10<00:57,  4.78s/it]loss_total_epoch 407.10198517888784
Training tokenizer: 100% 8036/8047 [7:32:15<00:52,  4.78s/it]loss_total_epoch 407.14648155495524
Training tokenizer: 100% 8037/8047 [7:32:20<00:47,  4.79s/it]loss_total_epoch 407.18215795606375
Training tokenizer: 100% 8038/8047 [7:32:25<00:42,  4.77s/it]loss_total_epoch 407.22209937125444
Training tokenizer: 100% 8039/8047 [7:32:29<00:38,  4.76s/it]loss_total_epoch 407.26829451695085
Training tokenizer: 100% 8040/8047 [7:32:34<00:33,  4.76s/it]loss_total_epoch 407.30585845187306
Training tokenizer: 100% 8041/8047 [7:32:39<00:28,  4.77s/it]loss_total_epoch 407.3452374152839
Training tokenizer: 100% 8042/8047 [7:32:44<00:23,  4.77s/it]loss_total_epoch 407.3862472064793
Training tokenizer: 100% 8043/8047 [7:32:49<00:19,  4.77s/it]loss_total_epoch 407.4244819432497
Training tokenizer: 100% 8044/8047 [7:32:53<00:14,  4.77s/it]loss_total_epoch 407.4610365629196
Training tokenizer: 100% 8045/8047 [7:32:58<00:09,  4.78s/it]loss_total_epoch 407.50768581032753
Training tokenizer: 100% 8046/8047 [7:33:03<00:04,  4.78s/it]loss_total_epoch 407.5506058782339
Training tokenizer: 100% 8047/8047 [7:33:08<00:00,  4.78s/it]Training tokenizer: 100% 8047/8047 [7:33:08<00:00,  3.38s/it]

Epoch 2 / 200

32183 3493 3560
Training tokenizer:   0% 0/8047 [00:00<?, ?it/s]loss_total_epoch 0.03057839162647724
Training tokenizer:   0% 1/8047 [00:04<10:42:16,  4.79s/it]loss_total_epoch 0.07456069253385067
Training tokenizer:   0% 2/8047 [00:09<10:31:48,  4.71s/it]loss_total_epoch 0.11446293629705906
Training tokenizer:   0% 3/8047 [00:14<10:36:09,  4.75s/it]loss_total_epoch 0.1482889298349619
Training tokenizer:   0% 4/8047 [00:19<10:39:49,  4.77s/it]loss_total_epoch 0.2000852469354868
Training tokenizer:   0% 5/8047 [00:23<10:38:57,  4.77s/it]loss_total_epoch 0.24038012139499187
Training tokenizer:   0% 6/8047 [00:28<10:49:37,  4.85s/it]loss_total_epoch 0.2751665059477091
Training tokenizer:   0% 7/8047 [00:33<10:56:10,  4.90s/it]loss_total_epoch 0.30916029401123524
Training tokenizer:   0% 8/8047 [00:39<11:10:04,  5.00s/it]loss_total_epoch 0.3545047249644995
Training tokenizer:   0% 9/8047 [00:43<10:59:42,  4.92s/it]loss_total_epoch 0.40518217347562313
Training tokenizer:   0% 10/8047 [00:48<10:55:06,  4.89s/it]loss_total_epoch 0.4473926369100809
Training tokenizer:   0% 11/8047 [00:53<10:51:09,  4.86s/it]loss_total_epoch 0.49064970202744007
Training tokenizer:   0% 12/8047 [00:58<10:48:37,  4.84s/it]loss_total_epoch 0.5349859949201345
Training tokenizer:   0% 13/8047 [01:03<10:47:29,  4.84s/it]loss_total_epoch 0.5720415133982897
Training tokenizer:   0% 14/8047 [01:07<10:45:06,  4.82s/it]loss_total_epoch 0.6115732844918966
Training tokenizer:   0% 15/8047 [01:12<10:47:37,  4.84s/it]loss_total_epoch 0.6482884455472231
Training tokenizer:   0% 16/8047 [01:17<10:45:01,  4.82s/it]loss_total_epoch 0.6928538400679827
Training tokenizer:   0% 17/8047 [01:22<10:42:20,  4.80s/it]loss_total_epoch 0.7270094584673643
Training tokenizer:   0% 18/8047 [01:26<10:39:53,  4.78s/it]loss_total_epoch 0.7623782809823751
Training tokenizer:   0% 19/8047 [01:31<10:41:55,  4.80s/it]loss_total_epoch 0.806754058226943
Training tokenizer:   0% 20/8047 [01:36<10:39:54,  4.78s/it]loss_total_epoch 0.8412796016782522
Training tokenizer:   0% 21/8047 [01:41<10:40:19,  4.79s/it]loss_total_epoch 0.8787518125027418
Training tokenizer:   0% 22/8047 [01:46<10:38:03,  4.77s/it]loss_total_epoch 0.917030917480588
Training tokenizer:   0% 23/8047 [01:50<10:36:47,  4.76s/it]loss_total_epoch 0.9551373291760683
Training tokenizer:   0% 24/8047 [01:55<10:29:12,  4.71s/it]loss_total_epoch 0.9906848538666964
Training tokenizer:   0% 25/8047 [02:00<10:30:37,  4.72s/it]loss_total_epoch 1.017109615728259
Training tokenizer:   0% 26/8047 [02:04<10:29:52,  4.71s/it]loss_total_epoch 1.0539598148316145
Training tokenizer:   0% 27/8047 [02:09<10:31:24,  4.72s/it]loss_total_epoch 1.0908246394246817
Training tokenizer:   0% 28/8047 [02:14<10:31:11,  4.72s/it]loss_total_epoch 1.1323418822139502
Training tokenizer:   0% 29/8047 [02:19<10:32:46,  4.74s/it]loss_total_epoch 1.1830487307161093
Training tokenizer:   0% 30/8047 [02:23<10:35:46,  4.76s/it]loss_total_epoch 1.2209020908921957
Training tokenizer:   0% 31/8047 [02:28<10:35:17,  4.76s/it]loss_total_epoch 1.2653871793299913
Training tokenizer:   0% 32/8047 [02:33<10:36:30,  4.76s/it]loss_total_epoch 1.3102938514202833
Training tokenizer:   0% 33/8047 [02:38<10:29:48,  4.72s/it]loss_total_epoch 1.3561092931777239
Training tokenizer:   0% 34/8047 [02:42<10:33:28,  4.74s/it]loss_total_epoch 1.3845053054392338
Training tokenizer:   0% 35/8047 [02:47<10:31:21,  4.73s/it]loss_total_epoch 1.4324486702680588
Training tokenizer:   0% 36/8047 [02:52<10:32:25,  4.74s/it]loss_total_epoch 1.4763413779437542
Training tokenizer:   0% 37/8047 [02:56<10:31:48,  4.73s/it]loss_total_epoch 1.5140056647360325
Training tokenizer:   0% 38/8047 [03:01<10:35:35,  4.76s/it]loss_total_epoch 1.5710038430988789
Training tokenizer:   0% 39/8047 [03:06<10:37:11,  4.77s/it]loss_total_epoch 1.6128853037953377
Training tokenizer:   0% 40/8047 [03:11<10:36:47,  4.77s/it]loss_total_epoch 1.6509371846914291
Training tokenizer:   1% 41/8047 [03:16<10:35:21,  4.76s/it]loss_total_epoch 1.6829922422766685
Training tokenizer:   1% 42/8047 [03:20<10:36:44,  4.77s/it]loss_total_epoch 1.7218472138047218
Training tokenizer:   1% 43/8047 [03:25<10:35:58,  4.77s/it]loss_total_epoch 1.7612896636128426
Training tokenizer:   1% 44/8047 [03:30<10:36:25,  4.77s/it]loss_total_epoch 1.8168082535266876
Training tokenizer:   1% 45/8047 [03:35<10:35:51,  4.77s/it]loss_total_epoch 1.8681244030594826
Training tokenizer:   1% 46/8047 [03:40<10:37:36,  4.78s/it]loss_total_epoch 1.9098343141376972
Training tokenizer:   1% 47/8047 [03:44<10:37:39,  4.78s/it]loss_total_epoch 1.9430394656956196
Training tokenizer:   1% 48/8047 [03:49<10:38:06,  4.79s/it]loss_total_epoch 1.9824818149209023
Training tokenizer:   1% 49/8047 [03:54<10:36:51,  4.78s/it]loss_total_epoch 2.0206496454775333
Training tokenizer:   1% 50/8047 [03:59<10:35:42,  4.77s/it]loss_total_epoch 2.0685360468924046
Training tokenizer:   1% 51/8047 [04:03<10:36:23,  4.78s/it]loss_total_epoch 2.1050626561045647
Training tokenizer:   1% 52/8047 [04:08<10:35:44,  4.77s/it]loss_total_epoch 2.142244003713131
Training tokenizer:   1% 53/8047 [04:13<10:36:18,  4.78s/it]loss_total_epoch 2.1911484859883785
Training tokenizer:   1% 54/8047 [04:18<10:38:42,  4.79s/it]loss_total_epoch 2.2340720742940903
Training tokenizer:   1% 55/8047 [04:23<10:35:14,  4.77s/it]loss_total_epoch 2.2831880897283554
Training tokenizer:   1% 56/8047 [04:27<10:36:29,  4.78s/it]loss_total_epoch 2.313660718500614
Training tokenizer:   1% 57/8047 [04:32<10:34:51,  4.77s/it]loss_total_epoch 2.3623389154672623
Training tokenizer:   1% 58/8047 [04:37<10:34:36,  4.77s/it]loss_total_epoch 2.4031959734857082
Training tokenizer:   1% 59/8047 [04:42<10:35:40,  4.77s/it]loss_total_epoch 2.4398209527134895
Training tokenizer:   1% 60/8047 [04:46<10:35:44,  4.78s/it]loss_total_epoch 2.488069050014019
Training tokenizer:   1% 61/8047 [04:51<10:36:10,  4.78s/it]loss_total_epoch 2.521196700632572
Training tokenizer:   1% 62/8047 [04:56<10:34:39,  4.77s/it]loss_total_epoch 2.567407038062811
Training tokenizer:   1% 63/8047 [05:01<10:35:06,  4.77s/it]loss_total_epoch 2.602287147194147
Training tokenizer:   1% 64/8047 [05:05<10:35:33,  4.78s/it]loss_total_epoch 2.6383116357028484
Training tokenizer:   1% 65/8047 [05:10<10:35:18,  4.78s/it]loss_total_epoch 2.6719077229499817
Training tokenizer:   1% 66/8047 [05:15<10:34:44,  4.77s/it]loss_total_epoch 2.6992367915809155
Training tokenizer:   1% 67/8047 [05:20<10:34:13,  4.77s/it]loss_total_epoch 2.7355472929775715
Training tokenizer:   1% 68/8047 [05:25<10:34:10,  4.77s/it]loss_total_epoch 2.7675027772784233
Training tokenizer:   1% 69/8047 [05:29<10:35:29,  4.78s/it]loss_total_epoch 2.8023661486804485
Training tokenizer:   1% 70/8047 [05:34<10:33:25,  4.76s/it]loss_total_epoch 2.846265748143196
Training tokenizer:   1% 71/8047 [05:39<10:31:58,  4.75s/it]loss_total_epoch 2.891567923128605
Training tokenizer:   1% 72/8047 [05:44<10:32:14,  4.76s/it]loss_total_epoch 2.9353062100708485
Training tokenizer:   1% 73/8047 [05:48<10:30:37,  4.75s/it]loss_total_epoch 2.9747230634093285
Training tokenizer:   1% 74/8047 [05:53<10:32:45,  4.76s/it]loss_total_epoch 3.0157752074301243
Training tokenizer:   1% 75/8047 [05:58<10:33:17,  4.77s/it]loss_total_epoch 3.055050015449524
Training tokenizer:   1% 76/8047 [06:03<10:33:29,  4.77s/it]loss_total_epoch 3.101798053830862
Training tokenizer:   1% 77/8047 [06:07<10:33:42,  4.77s/it]loss_total_epoch 3.1445922181010246
Training tokenizer:   1% 78/8047 [06:12<10:33:26,  4.77s/it]loss_total_epoch 3.19422185793519
Training tokenizer:   1% 79/8047 [06:17<10:32:44,  4.76s/it]loss_total_epoch 3.2298544608056545
Training tokenizer:   1% 80/8047 [06:22<10:34:15,  4.78s/it]loss_total_epoch 3.267198920249939
Training tokenizer:   1% 81/8047 [06:26<10:33:19,  4.77s/it]loss_total_epoch 3.298819452524185
Training tokenizer:   1% 82/8047 [06:31<10:32:52,  4.77s/it]loss_total_epoch 3.337875120341778
Training tokenizer:   1% 83/8047 [06:36<10:32:18,  4.76s/it]loss_total_epoch 3.3808188401162624
Training tokenizer:   1% 84/8047 [06:41<10:32:37,  4.77s/it]loss_total_epoch 3.4272390864789486
Training tokenizer:   1% 85/8047 [06:46<10:32:26,  4.77s/it]loss_total_epoch 3.466550078243017
Training tokenizer:   1% 86/8047 [06:50<10:30:37,  4.75s/it]loss_total_epoch 3.508068799972534
Training tokenizer:   1% 87/8047 [06:55<10:23:24,  4.70s/it]loss_total_epoch 3.542405091226101
Training tokenizer:   1% 88/8047 [07:00<10:25:43,  4.72s/it]loss_total_epoch 3.58477670699358
Training tokenizer:   1% 89/8047 [07:04<10:28:17,  4.74s/it]loss_total_epoch 3.625437878072262
Training tokenizer:   1% 90/8047 [07:09<10:27:49,  4.73s/it]loss_total_epoch 3.6620466485619545
Training tokenizer:   1% 91/8047 [07:14<10:29:00,  4.74s/it]loss_total_epoch 3.7027408480644226
Training tokenizer:   1% 92/8047 [07:19<10:30:47,  4.76s/it]loss_total_epoch 3.7426890432834625
Training tokenizer:   1% 93/8047 [07:23<10:29:21,  4.75s/it]loss_total_epoch 3.778153531253338
Training tokenizer:   1% 94/8047 [07:28<10:30:32,  4.76s/it]loss_total_epoch 3.8132523633539677
Training tokenizer:   1% 95/8047 [07:33<10:29:49,  4.75s/it]loss_total_epoch 3.853441182523966
Training tokenizer:   1% 96/8047 [07:38<10:24:13,  4.71s/it]loss_total_epoch 3.8833735063672066
Training tokenizer:   1% 97/8047 [07:42<10:26:13,  4.73s/it]loss_total_epoch 3.9235449880361557
Training tokenizer:   1% 98/8047 [07:47<10:28:26,  4.74s/it]loss_total_epoch 3.9670104570686817
Training tokenizer:   1% 99/8047 [07:52<10:27:46,  4.74s/it]loss_total_epoch 3.99240611307323
Training tokenizer:   1% 100/8047 [07:57<10:28:39,  4.75s/it]loss_total_epoch 4.035979410633445
Training tokenizer:   1% 101/8047 [08:01<10:29:22,  4.75s/it]loss_total_epoch 4.075407410040498
Training tokenizer:   1% 102/8047 [08:06<10:29:00,  4.75s/it]loss_total_epoch 4.116636695340276
Training tokenizer:   1% 103/8047 [08:11<10:28:25,  4.75s/it]loss_total_epoch 4.151329701766372
Training tokenizer:   1% 104/8047 [08:16<10:26:25,  4.73s/it]loss_total_epoch 4.194085335358977
Training tokenizer:   1% 105/8047 [08:20<10:27:51,  4.74s/it]loss_total_epoch 4.2305658888071775
Training tokenizer:   1% 106/8047 [08:25<10:27:17,  4.74s/it]loss_total_epoch 4.267570154741406
Training tokenizer:   1% 107/8047 [08:30<10:26:36,  4.74s/it]loss_total_epoch 4.319963255897164
Training tokenizer:   1% 108/8047 [08:34<10:26:35,  4.74s/it]loss_total_epoch 4.362868940457702
Training tokenizer:   1% 109/8047 [08:39<10:27:21,  4.74s/it]loss_total_epoch 4.3929431941360235
Training tokenizer:   1% 110/8047 [08:44<10:28:46,  4.75s/it]loss_total_epoch 4.4387154672294855
Training tokenizer:   1% 111/8047 [08:49<10:27:07,  4.74s/it]loss_total_epoch 4.479233061894774
Training tokenizer:   1% 112/8047 [08:53<10:27:50,  4.75s/it]loss_total_epoch 4.521304441615939
Training tokenizer:   1% 113/8047 [08:58<10:27:43,  4.75s/it]loss_total_epoch 4.566323911771178
Training tokenizer:   1% 114/8047 [09:03<10:28:10,  4.75s/it]loss_total_epoch 4.597724182531238
Training tokenizer:   1% 115/8047 [09:08<10:28:39,  4.76s/it]loss_total_epoch 4.633137898519635
Training tokenizer:   1% 116/8047 [09:13<10:28:39,  4.76s/it]loss_total_epoch 4.670820789411664
Training tokenizer:   1% 117/8047 [09:17<10:25:00,  4.73s/it]loss_total_epoch 4.718944480642676
Training tokenizer:   1% 118/8047 [09:22<10:26:05,  4.74s/it]loss_total_epoch 4.765384102240205
Training tokenizer:   1% 119/8047 [09:27<10:27:53,  4.75s/it]loss_total_epoch 4.803697751834989
Training tokenizer:   1% 120/8047 [09:31<10:25:36,  4.74s/it]loss_total_epoch 4.839821496978402
Training tokenizer:   2% 121/8047 [09:36<10:26:03,  4.74s/it]loss_total_epoch 4.875247674062848
Training tokenizer:   2% 122/8047 [09:41<10:27:40,  4.75s/it]loss_total_epoch 4.906133487820625
Training tokenizer:   2% 123/8047 [09:46<10:26:47,  4.75s/it]loss_total_epoch 4.94356095790863
Training tokenizer:   2% 124/8047 [09:50<10:25:57,  4.74s/it]loss_total_epoch 4.986707989126444
Training tokenizer:   2% 125/8047 [09:55<10:26:57,  4.75s/it]loss_total_epoch 5.022360626608133
Training tokenizer:   2% 126/8047 [10:00<10:29:33,  4.77s/it]loss_total_epoch 5.06029511615634
Training tokenizer:   2% 127/8047 [10:05<10:29:28,  4.77s/it]loss_total_epoch 5.090899031609297
Training tokenizer:   2% 128/8047 [10:10<10:28:44,  4.76s/it]loss_total_epoch 5.132920548319817
Training tokenizer:   2% 129/8047 [10:14<10:31:28,  4.79s/it]loss_total_epoch 5.1660117991268635
Training tokenizer:   2% 130/8047 [10:19<10:27:47,  4.76s/it]loss_total_epoch 5.202012322843075
Training tokenizer:   2% 131/8047 [10:24<10:29:26,  4.77s/it]loss_total_epoch 5.2450224459171295
Training tokenizer:   2% 132/8047 [10:29<10:27:06,  4.75s/it]loss_total_epoch 5.287404607981443
Training tokenizer:   2% 133/8047 [10:33<10:25:48,  4.74s/it]loss_total_epoch 5.322168376296759
Training tokenizer:   2% 134/8047 [10:38<10:26:12,  4.75s/it]loss_total_epoch 5.36028765514493
Training tokenizer:   2% 135/8047 [10:43<10:26:24,  4.75s/it]loss_total_epoch 5.3932077176868916
Training tokenizer:   2% 136/8047 [10:48<10:25:27,  4.74s/it]loss_total_epoch 5.43893887847662
Training tokenizer:   2% 137/8047 [10:52<10:26:40,  4.75s/it]loss_total_epoch 5.475425101816654
Training tokenizer:   2% 138/8047 [10:57<10:27:18,  4.76s/it]loss_total_epoch 5.512359920889139
Training tokenizer:   2% 139/8047 [11:02<10:25:01,  4.74s/it]loss_total_epoch 5.558774035423994
Training tokenizer:   2% 140/8047 [11:06<10:23:41,  4.73s/it]loss_total_epoch 5.5909782983362675
Training tokenizer:   2% 141/8047 [11:11<10:26:48,  4.76s/it]loss_total_epoch 5.630300905555487
Training tokenizer:   2% 142/8047 [11:16<10:26:47,  4.76s/it]loss_total_epoch 5.673183843493462
Training tokenizer:   2% 143/8047 [11:21<10:26:21,  4.75s/it]loss_total_epoch 5.719123981893063
Training tokenizer:   2% 144/8047 [11:26<10:25:09,  4.75s/it]loss_total_epoch 5.753713123500347
Training tokenizer:   2% 145/8047 [11:30<10:25:17,  4.75s/it]loss_total_epoch 5.790354669094086
Training tokenizer:   2% 146/8047 [11:35<10:24:43,  4.74s/it]loss_total_epoch 5.829940386116505
Training tokenizer:   2% 147/8047 [11:40<10:23:36,  4.74s/it]loss_total_epoch 5.868750751018524
Training tokenizer:   2% 148/8047 [11:45<10:25:26,  4.75s/it]loss_total_epoch 5.903271164745092
Training tokenizer:   2% 149/8047 [11:49<10:26:06,  4.76s/it]loss_total_epoch 5.937109507620335
Training tokenizer:   2% 150/8047 [11:54<10:26:29,  4.76s/it]loss_total_epoch 5.978355132043362
Training tokenizer:   2% 151/8047 [11:59<10:29:11,  4.78s/it]loss_total_epoch 6.02112528309226
Training tokenizer:   2% 152/8047 [12:04<10:27:13,  4.77s/it]loss_total_epoch 6.0623441226780415
Training tokenizer:   2% 153/8047 [12:08<10:25:56,  4.76s/it]loss_total_epoch 6.103874754160643
Training tokenizer:   2% 154/8047 [12:13<10:24:53,  4.75s/it]loss_total_epoch 6.13657384365797
Training tokenizer:   2% 155/8047 [12:18<10:25:29,  4.76s/it]loss_total_epoch 6.180350925773382
Training tokenizer:   2% 156/8047 [12:23<10:26:34,  4.76s/it]loss_total_epoch 6.213275197893381
Training tokenizer:   2% 157/8047 [12:27<10:27:23,  4.77s/it]loss_total_epoch 6.250879243016243
Training tokenizer:   2% 158/8047 [12:32<10:28:59,  4.78s/it]loss_total_epoch 6.292002569884062
Training tokenizer:   2% 159/8047 [12:37<10:29:02,  4.78s/it]loss_total_epoch 6.332943718880415
Training tokenizer:   2% 160/8047 [12:42<10:26:54,  4.77s/it]loss_total_epoch 6.374081775546074
Training tokenizer:   2% 161/8047 [12:47<10:26:46,  4.77s/it]loss_total_epoch 6.420053742825985
Training tokenizer:   2% 162/8047 [12:51<10:26:25,  4.77s/it]loss_total_epoch 6.455255150794983
Training tokenizer:   2% 163/8047 [12:56<10:23:47,  4.75s/it]loss_total_epoch 6.493409935384989
Training tokenizer:   2% 164/8047 [13:01<10:25:24,  4.76s/it]loss_total_epoch 6.524830732494593
Training tokenizer:   2% 165/8047 [13:06<10:25:04,  4.76s/it]loss_total_epoch 6.5692979618906975
Training tokenizer:   2% 166/8047 [13:10<10:27:02,  4.77s/it]loss_total_epoch 6.615142345428467
Training tokenizer:   2% 167/8047 [13:15<10:24:49,  4.76s/it]loss_total_epoch 6.649629823863506
Training tokenizer:   2% 168/8047 [13:20<10:26:15,  4.77s/it]loss_total_epoch 6.6950114741921425
Training tokenizer:   2% 169/8047 [13:25<10:26:08,  4.77s/it]loss_total_epoch 6.740682352334261
Training tokenizer:   2% 170/8047 [13:29<10:26:51,  4.77s/it]loss_total_epoch 6.785997468978167
Training tokenizer:   2% 171/8047 [13:34<10:26:41,  4.77s/it]loss_total_epoch 6.825952298939228
Training tokenizer:   2% 172/8047 [13:39<10:25:30,  4.77s/it]loss_total_epoch 6.865808486938477
Training tokenizer:   2% 173/8047 [13:44<10:24:52,  4.76s/it]loss_total_epoch 6.902311846613884
Training tokenizer:   2% 174/8047 [13:48<10:26:13,  4.77s/it]loss_total_epoch 6.935722101479769
Training tokenizer:   2% 175/8047 [13:53<10:25:35,  4.77s/it]loss_total_epoch 6.973883558064699
Training tokenizer:   2% 176/8047 [13:58<10:26:52,  4.78s/it]loss_total_epoch 7.010598607361317
Training tokenizer:   2% 177/8047 [14:03<10:25:03,  4.77s/it]loss_total_epoch 7.048367008566856
Training tokenizer:   2% 178/8047 [14:08<10:24:06,  4.76s/it]loss_total_epoch 7.083996322005987
Training tokenizer:   2% 179/8047 [14:12<10:23:37,  4.76s/it]loss_total_epoch 7.127674821764231
Training tokenizer:   2% 180/8047 [14:17<10:21:40,  4.74s/it]loss_total_epoch 7.165745582431555
Training tokenizer:   2% 181/8047 [14:22<10:23:18,  4.75s/it]loss_total_epoch 7.203091587871313
Training tokenizer:   2% 182/8047 [14:27<10:24:45,  4.77s/it]loss_total_epoch 7.24313984811306
Training tokenizer:   2% 183/8047 [14:31<10:24:31,  4.76s/it]loss_total_epoch 7.293267827481031
Training tokenizer:   2% 184/8047 [14:36<10:26:04,  4.78s/it]loss_total_epoch 7.324926510453224
Training tokenizer:   2% 185/8047 [14:41<10:24:34,  4.77s/it]loss_total_epoch 7.364390268921852
Training tokenizer:   2% 186/8047 [14:46<10:23:51,  4.76s/it]loss_total_epoch 7.400937229394913
Training tokenizer:   2% 187/8047 [14:50<10:22:34,  4.75s/it]loss_total_epoch 7.436112970113754
Training tokenizer:   2% 188/8047 [14:55<10:22:05,  4.75s/it]loss_total_epoch 7.474079728126526
Training tokenizer:   2% 189/8047 [15:00<10:21:55,  4.75s/it]loss_total_epoch 7.517524857074022
Training tokenizer:   2% 190/8047 [15:05<10:21:03,  4.74s/it]loss_total_epoch 7.54896168038249
Training tokenizer:   2% 191/8047 [15:09<10:22:10,  4.75s/it]loss_total_epoch 7.589605152606964
Training tokenizer:   2% 192/8047 [15:14<10:23:12,  4.76s/it]loss_total_epoch 7.630702264606953
Training tokenizer:   2% 193/8047 [15:19<10:24:17,  4.77s/it]loss_total_epoch 7.668320924043655
Training tokenizer:   2% 194/8047 [15:24<10:22:35,  4.76s/it]loss_total_epoch 7.698898648843169
Training tokenizer:   2% 195/8047 [15:28<10:22:11,  4.75s/it]loss_total_epoch 7.730728907510638
Training tokenizer:   2% 196/8047 [15:33<10:23:55,  4.77s/it]loss_total_epoch 7.771858753636479
Training tokenizer:   2% 197/8047 [15:38<10:23:32,  4.77s/it]loss_total_epoch 7.812571557238698
Training tokenizer:   2% 198/8047 [15:43<10:24:38,  4.77s/it]loss_total_epoch 7.851589614525437
Training tokenizer:   2% 199/8047 [15:48<10:24:14,  4.77s/it]loss_total_epoch 7.8838364239782095
Training tokenizer:   2% 200/8047 [15:52<10:17:52,  4.72s/it]loss_total_epoch 7.925077700987458
Training tokenizer:   2% 201/8047 [15:57<10:20:40,  4.75s/it]loss_total_epoch 7.965320071205497
Training tokenizer:   3% 202/8047 [16:02<10:21:11,  4.75s/it]loss_total_epoch 8.00615674071014
Training tokenizer:   3% 203/8047 [16:06<10:21:42,  4.76s/it]loss_total_epoch 8.047974428161979
Training tokenizer:   3% 204/8047 [16:11<10:21:57,  4.76s/it]loss_total_epoch 8.08484567143023
Training tokenizer:   3% 205/8047 [16:16<10:22:49,  4.77s/it]loss_total_epoch 8.12533582188189
Training tokenizer:   3% 206/8047 [16:21<10:23:36,  4.77s/it]loss_total_epoch 8.160134164616466
Training tokenizer:   3% 207/8047 [16:26<10:22:09,  4.76s/it]loss_total_epoch 8.19650780968368
Training tokenizer:   3% 208/8047 [16:30<10:20:58,  4.75s/it]loss_total_epoch 8.232508847489953
Training tokenizer:   3% 209/8047 [16:35<10:21:19,  4.76s/it]loss_total_epoch 8.264002388343215
Training tokenizer:   3% 210/8047 [16:40<10:21:36,  4.76s/it]loss_total_epoch 8.297096459195018
Training tokenizer:   3% 211/8047 [16:45<10:22:59,  4.77s/it]loss_total_epoch 8.328925954177976
Training tokenizer:   3% 212/8047 [16:49<10:21:39,  4.76s/it]loss_total_epoch 8.364662552252412
Training tokenizer:   3% 213/8047 [16:54<10:20:19,  4.75s/it]loss_total_epoch 8.403231451287866
Training tokenizer:   3% 214/8047 [16:59<10:17:56,  4.73s/it]loss_total_epoch 8.443900873884559
Training tokenizer:   3% 215/8047 [17:03<10:18:14,  4.74s/it]loss_total_epoch 8.482205277308822
Training tokenizer:   3% 216/8047 [17:08<10:18:10,  4.74s/it]loss_total_epoch 8.51512067951262
Training tokenizer:   3% 217/8047 [17:13<10:18:11,  4.74s/it]loss_total_epoch 8.543788831681013
Training tokenizer:   3% 218/8047 [17:18<10:18:21,  4.74s/it]loss_total_epoch 8.580628234893084
Training tokenizer:   3% 219/8047 [17:22<10:19:56,  4.75s/it]loss_total_epoch 8.62012803927064
Training tokenizer:   3% 220/8047 [17:27<10:18:25,  4.74s/it]loss_total_epoch 8.661942698061466
Training tokenizer:   3% 221/8047 [17:32<10:18:33,  4.74s/it]loss_total_epoch 8.701758306473494
Training tokenizer:   3% 222/8047 [17:37<10:20:49,  4.76s/it]loss_total_epoch 8.745558988302946
Training tokenizer:   3% 223/8047 [17:42<10:20:08,  4.76s/it]loss_total_epoch 8.789317585527897
Training tokenizer:   3% 224/8047 [17:46<10:22:16,  4.77s/it]loss_total_epoch 8.823751956224442
Training tokenizer:   3% 225/8047 [17:51<10:22:28,  4.77s/it]loss_total_epoch 8.868217442184687
Training tokenizer:   3% 226/8047 [17:56<10:21:51,  4.77s/it]loss_total_epoch 8.911360871046782
Training tokenizer:   3% 227/8047 [18:01<10:23:02,  4.78s/it]loss_total_epoch 8.94051137007773
Training tokenizer:   3% 228/8047 [18:05<10:22:30,  4.78s/it]loss_total_epoch 8.96869340352714
Training tokenizer:   3% 229/8047 [18:10<10:14:53,  4.72s/it]loss_total_epoch 9.008743604645133
Training tokenizer:   3% 230/8047 [18:15<10:15:05,  4.72s/it]loss_total_epoch 9.045630471780896
Training tokenizer:   3% 231/8047 [18:19<10:15:28,  4.72s/it]loss_total_epoch 9.086631974205375
Training tokenizer:   3% 232/8047 [18:24<10:18:00,  4.74s/it]loss_total_epoch 9.121848432347178
Training tokenizer:   3% 233/8047 [18:29<10:18:14,  4.75s/it]loss_total_epoch 9.16181960515678
Training tokenizer:   3% 234/8047 [18:34<10:18:07,  4.75s/it]loss_total_epoch 9.190040046349168
Training tokenizer:   3% 235/8047 [18:39<10:19:20,  4.76s/it]loss_total_epoch 9.231608344241977
Training tokenizer:   3% 236/8047 [18:43<10:22:02,  4.78s/it]loss_total_epoch 9.2641467358917
Training tokenizer:   3% 237/8047 [18:48<10:21:41,  4.78s/it]loss_total_epoch 9.30579042993486
Training tokenizer:   3% 238/8047 [18:53<10:21:22,  4.77s/it]loss_total_epoch 9.338811432942748
Training tokenizer:   3% 239/8047 [18:58<10:18:58,  4.76s/it]loss_total_epoch 9.377169532701373
Training tokenizer:   3% 240/8047 [19:02<10:17:54,  4.75s/it]loss_total_epoch 9.417781228199601
Training tokenizer:   3% 241/8047 [19:07<10:18:40,  4.76s/it]loss_total_epoch 9.459439815953374
Training tokenizer:   3% 242/8047 [19:12<10:20:13,  4.77s/it]loss_total_epoch 9.501688981428742
Training tokenizer:   3% 243/8047 [19:17<10:21:18,  4.78s/it]loss_total_epoch 9.54391766153276
Training tokenizer:   3% 244/8047 [19:21<10:19:28,  4.76s/it]loss_total_epoch 9.578031739220023
Training tokenizer:   3% 245/8047 [19:26<10:17:30,  4.75s/it]loss_total_epoch 9.615847734734416
Training tokenizer:   3% 246/8047 [19:31<10:18:50,  4.76s/it]loss_total_epoch 9.654802424833179
Training tokenizer:   3% 247/8047 [19:36<10:17:38,  4.75s/it]loss_total_epoch 9.695653157308698
Training tokenizer:   3% 248/8047 [19:40<10:16:58,  4.75s/it]loss_total_epoch 9.732085699215531
Training tokenizer:   3% 249/8047 [19:45<10:16:27,  4.74s/it]loss_total_epoch 9.768508771434426
Training tokenizer:   3% 250/8047 [19:50<10:16:44,  4.75s/it]loss_total_epoch 9.807054756209254
Training tokenizer:   3% 251/8047 [19:55<10:18:00,  4.76s/it]loss_total_epoch 9.842239646241069
Training tokenizer:   3% 252/8047 [19:59<10:16:31,  4.75s/it]loss_total_epoch 9.886133564636111
Training tokenizer:   3% 253/8047 [20:04<10:17:26,  4.75s/it]loss_total_epoch 9.930682005360723
Training tokenizer:   3% 254/8047 [20:09<10:18:00,  4.76s/it]loss_total_epoch 9.969220044091344
Training tokenizer:   3% 255/8047 [20:14<10:16:47,  4.75s/it]loss_total_epoch 10.011266687884927
Training tokenizer:   3% 256/8047 [20:18<10:15:42,  4.74s/it]loss_total_epoch 10.048359109088778
Training tokenizer:   3% 257/8047 [20:23<10:12:20,  4.72s/it]loss_total_epoch 10.08724587969482
Training tokenizer:   3% 258/8047 [20:28<10:12:59,  4.72s/it]loss_total_epoch 10.121322339400649
Training tokenizer:   3% 259/8047 [20:33<10:16:39,  4.75s/it]loss_total_epoch 10.162930263206363
Training tokenizer:   3% 260/8047 [20:37<10:17:07,  4.76s/it]loss_total_epoch 10.194087015464902
Training tokenizer:   3% 261/8047 [20:42<10:17:12,  4.76s/it]loss_total_epoch 10.22813187353313
Training tokenizer:   3% 262/8047 [20:47<10:17:42,  4.76s/it]loss_total_epoch 10.27260279096663
Training tokenizer:   3% 263/8047 [20:52<10:17:37,  4.76s/it]loss_total_epoch 10.311717951670289
Training tokenizer:   3% 264/8047 [20:56<10:18:05,  4.76s/it]loss_total_epoch 10.35205134563148
Training tokenizer:   3% 265/8047 [21:01<10:13:31,  4.73s/it]loss_total_epoch 10.392985241487622
Training tokenizer:   3% 266/8047 [21:06<10:17:09,  4.76s/it]loss_total_epoch 10.426519742235541
Training tokenizer:   3% 267/8047 [21:11<10:17:48,  4.76s/it]loss_total_epoch 10.4626859780401
Training tokenizer:   3% 268/8047 [21:15<10:17:04,  4.76s/it]loss_total_epoch 10.495662914589047
Training tokenizer:   3% 269/8047 [21:20<10:17:02,  4.76s/it]loss_total_epoch 10.540460107848048
Training tokenizer:   3% 270/8047 [21:25<10:13:09,  4.73s/it]loss_total_epoch 10.578576920554042
Training tokenizer:   3% 271/8047 [21:30<10:15:41,  4.75s/it]loss_total_epoch 10.621089896187186
Training tokenizer:   3% 272/8047 [21:34<10:16:32,  4.76s/it]loss_total_epoch 10.664458332583308
Training tokenizer:   3% 273/8047 [21:39<10:18:04,  4.77s/it]loss_total_epoch 10.697627121582627
Training tokenizer:   3% 274/8047 [21:44<10:20:40,  4.79s/it]loss_total_epoch 10.732448076829314
Training tokenizer:   3% 275/8047 [21:49<10:19:56,  4.79s/it]loss_total_epoch 10.76896726898849
Training tokenizer:   3% 276/8047 [21:54<10:19:02,  4.78s/it]loss_total_epoch 10.817729679867625
Training tokenizer:   3% 277/8047 [21:58<10:18:49,  4.78s/it]loss_total_epoch 10.85297896899283
Training tokenizer:   3% 278/8047 [22:03<10:19:36,  4.79s/it]loss_total_epoch 10.892418934032321
Training tokenizer:   3% 279/8047 [22:08<10:18:12,  4.78s/it]loss_total_epoch 10.935123601928353
Training tokenizer:   3% 280/8047 [22:13<10:17:06,  4.77s/it]loss_total_epoch 10.964202653616667
Training tokenizer:   3% 281/8047 [22:17<10:16:43,  4.76s/it]loss_total_epoch 11.006747528910637
Training tokenizer:   4% 282/8047 [22:22<10:17:18,  4.77s/it]loss_total_epoch 11.039580099284649
Training tokenizer:   4% 283/8047 [22:27<10:17:02,  4.77s/it]loss_total_epoch 11.074657689779997
Training tokenizer:   4% 284/8047 [22:32<10:16:54,  4.77s/it]loss_total_epoch 11.11694736033678
Training tokenizer:   4% 285/8047 [22:37<10:16:47,  4.77s/it]loss_total_epoch 11.157795056700706
Training tokenizer:   4% 286/8047 [22:41<10:18:12,  4.78s/it]loss_total_epoch 11.190392903983593
Training tokenizer:   4% 287/8047 [22:46<10:16:18,  4.77s/it]loss_total_epoch 11.233154762536287
Training tokenizer:   4% 288/8047 [22:51<10:16:50,  4.77s/it]loss_total_epoch 11.265482142567635
Training tokenizer:   4% 289/8047 [22:56<10:16:38,  4.77s/it]loss_total_epoch 11.303969141095877
Training tokenizer:   4% 290/8047 [23:00<10:16:40,  4.77s/it]loss_total_epoch 11.343089330941439
Training tokenizer:   4% 291/8047 [23:05<10:17:04,  4.77s/it]loss_total_epoch 11.377734206616879
Training tokenizer:   4% 292/8047 [23:10<10:15:29,  4.76s/it]loss_total_epoch 11.42285979911685
Training tokenizer:   4% 293/8047 [23:15<10:16:16,  4.77s/it]loss_total_epoch 11.469395868480206
Training tokenizer:   4% 294/8047 [23:19<10:12:51,  4.74s/it]loss_total_epoch 11.503496434539557
Training tokenizer:   4% 295/8047 [23:24<10:14:32,  4.76s/it]loss_total_epoch 11.532081646844745
Training tokenizer:   4% 296/8047 [23:29<10:14:33,  4.76s/it]loss_total_epoch 11.577244205400348
Training tokenizer:   4% 297/8047 [23:34<10:15:01,  4.76s/it]loss_total_epoch 11.620011882856488
Training tokenizer:   4% 298/8047 [23:38<10:14:01,  4.75s/it]loss_total_epoch 11.659032484516501
Training tokenizer:   4% 299/8047 [23:43<10:14:40,  4.76s/it]loss_total_epoch 11.700406970456243
Training tokenizer:   4% 300/8047 [23:48<10:13:18,  4.75s/it]loss_total_epoch 11.736837754026055
Training tokenizer:   4% 301/8047 [23:53<10:15:41,  4.77s/it]loss_total_epoch 11.768834928050637
Training tokenizer:   4% 302/8047 [23:57<10:13:13,  4.75s/it]loss_total_epoch 11.805870721116662
Training tokenizer:   4% 303/8047 [24:02<10:14:52,  4.76s/it]loss_total_epoch 11.839133208617568
Training tokenizer:   4% 304/8047 [24:07<10:14:23,  4.76s/it]loss_total_epoch 11.880367474630475
Training tokenizer:   4% 305/8047 [24:12<10:12:56,  4.75s/it]loss_total_epoch 11.910606978461146
Training tokenizer:   4% 306/8047 [24:16<10:13:14,  4.75s/it]loss_total_epoch 11.941795667633414
Training tokenizer:   4% 307/8047 [24:21<10:14:21,  4.76s/it]loss_total_epoch 11.989785460755229
Training tokenizer:   4% 308/8047 [24:26<10:14:19,  4.76s/it]loss_total_epoch 12.02893097512424
Training tokenizer:   4% 309/8047 [24:31<10:10:12,  4.73s/it]loss_total_epoch 12.07180149666965
Training tokenizer:   4% 310/8047 [24:35<10:12:56,  4.75s/it]loss_total_epoch 12.105852471664548
Training tokenizer:   4% 311/8047 [24:40<10:12:44,  4.75s/it]loss_total_epoch 12.138397121801972
Training tokenizer:   4% 312/8047 [24:45<10:14:32,  4.77s/it]loss_total_epoch 12.175262121483684
Training tokenizer:   4% 313/8047 [24:50<10:13:35,  4.76s/it]loss_total_epoch 12.212448449805379
Training tokenizer:   4% 314/8047 [24:54<10:11:06,  4.74s/it]loss_total_epoch 12.248150242492557
Training tokenizer:   4% 315/8047 [24:59<10:14:28,  4.77s/it]loss_total_epoch 12.293781047686934
Training tokenizer:   4% 316/8047 [25:04<10:17:13,  4.79s/it]loss_total_epoch 12.324688959866762
Training tokenizer:   4% 317/8047 [25:09<10:15:22,  4.78s/it]loss_total_epoch 12.365813553333282
Training tokenizer:   4% 318/8047 [25:14<10:13:50,  4.77s/it]loss_total_epoch 12.407377824187279
Training tokenizer:   4% 319/8047 [25:18<10:12:42,  4.76s/it]loss_total_epoch 12.448052395135164
Training tokenizer:   4% 320/8047 [25:23<10:11:45,  4.75s/it]loss_total_epoch 12.49212445691228
Training tokenizer:   4% 321/8047 [25:28<10:11:11,  4.75s/it]loss_total_epoch 12.527292300015688
Training tokenizer:   4% 322/8047 [25:33<10:12:32,  4.76s/it]loss_total_epoch 12.570507474243641
Training tokenizer:   4% 323/8047 [25:38<10:16:20,  4.79s/it]loss_total_epoch 12.601127464324236
Training tokenizer:   4% 324/8047 [25:42<10:06:33,  4.71s/it]loss_total_epoch 12.642339579761028
Training tokenizer:   4% 325/8047 [25:47<10:10:25,  4.74s/it]loss_total_epoch 12.681908432394266
Training tokenizer:   4% 326/8047 [25:52<10:11:06,  4.75s/it]loss_total_epoch 12.71860421076417
Training tokenizer:   4% 327/8047 [25:56<10:12:03,  4.76s/it]loss_total_epoch 12.758560847491026
Training tokenizer:   4% 328/8047 [26:01<10:11:24,  4.75s/it]loss_total_epoch 12.803486105054617
Training tokenizer:   4% 329/8047 [26:06<10:11:30,  4.75s/it]loss_total_epoch 12.844206240028143
Training tokenizer:   4% 330/8047 [26:11<10:09:33,  4.74s/it]loss_total_epoch 12.884210415184498
Training tokenizer:   4% 331/8047 [26:15<10:08:38,  4.73s/it]loss_total_epoch 12.918952565640211
Training tokenizer:   4% 332/8047 [26:20<10:01:48,  4.68s/it]loss_total_epoch 12.957479115575552
Training tokenizer:   4% 333/8047 [26:25<10:05:21,  4.71s/it]loss_total_epoch 12.990762431174517
Training tokenizer:   4% 334/8047 [26:29<10:07:04,  4.72s/it]loss_total_epoch 13.0232844799757
Training tokenizer:   4% 335/8047 [26:34<10:06:51,  4.72s/it]loss_total_epoch 13.062770277261734
Training tokenizer:   4% 336/8047 [26:39<10:06:44,  4.72s/it]loss_total_epoch 13.093951530754566
Training tokenizer:   4% 337/8047 [26:44<10:09:38,  4.74s/it]loss_total_epoch 13.132819321006536
Training tokenizer:   4% 338/8047 [26:48<10:09:14,  4.74s/it]loss_total_epoch 13.172251585870981
Training tokenizer:   4% 339/8047 [26:53<10:09:07,  4.74s/it]loss_total_epoch 13.211555685847998
Training tokenizer:   4% 340/8047 [26:58<10:09:34,  4.75s/it]loss_total_epoch 13.248793669044971
Training tokenizer:   4% 341/8047 [27:03<10:10:26,  4.75s/it]loss_total_epoch 13.28532587736845
Training tokenizer:   4% 342/8047 [27:07<10:12:53,  4.77s/it]loss_total_epoch 13.333423033356667
Training tokenizer:   4% 343/8047 [27:12<10:10:54,  4.76s/it]loss_total_epoch 13.376602474600077
Training tokenizer:   4% 344/8047 [27:17<10:11:02,  4.76s/it]loss_total_epoch 13.416626404970884
Training tokenizer:   4% 345/8047 [27:22<10:09:41,  4.75s/it]loss_total_epoch 13.452114287763834
Training tokenizer:   4% 346/8047 [27:26<10:04:02,  4.71s/it]loss_total_epoch 13.489660177379847
Training tokenizer:   4% 347/8047 [27:31<10:06:57,  4.73s/it]loss_total_epoch 13.529936727136374
Training tokenizer:   4% 348/8047 [27:36<10:08:31,  4.74s/it]loss_total_epoch 13.56821707636118
Training tokenizer:   4% 349/8047 [27:41<10:09:23,  4.75s/it]loss_total_epoch 13.603370420634747
Training tokenizer:   4% 350/8047 [27:45<10:07:25,  4.74s/it]loss_total_epoch 13.640120431780815
Training tokenizer:   4% 351/8047 [27:50<10:09:15,  4.75s/it]loss_total_epoch 13.667586410418153
Training tokenizer:   4% 352/8047 [27:55<10:08:55,  4.75s/it]loss_total_epoch 13.702190471813083
Training tokenizer:   4% 353/8047 [28:00<10:07:15,  4.74s/it]loss_total_epoch 13.740524841472507
Training tokenizer:   4% 354/8047 [28:04<10:05:43,  4.72s/it]loss_total_epoch 13.78097797371447
Training tokenizer:   4% 355/8047 [28:09<10:04:49,  4.72s/it]loss_total_epoch 13.819855147972703
Training tokenizer:   4% 356/8047 [28:14<10:05:36,  4.72s/it]loss_total_epoch 13.85013971850276
Training tokenizer:   4% 357/8047 [28:18<10:07:03,  4.74s/it]loss_total_epoch 13.8861183822155
Training tokenizer:   4% 358/8047 [28:23<10:07:40,  4.74s/it]loss_total_epoch 13.929111760109663
Training tokenizer:   4% 359/8047 [28:28<10:08:08,  4.75s/it]loss_total_epoch 13.964725710451603
Training tokenizer:   4% 360/8047 [28:33<10:08:28,  4.75s/it]loss_total_epoch 14.003861963748932
Training tokenizer:   4% 361/8047 [28:37<10:07:44,  4.74s/it]loss_total_epoch 14.04054332152009
Training tokenizer:   4% 362/8047 [28:42<10:09:45,  4.76s/it]loss_total_epoch 14.086578853428364
Training tokenizer:   5% 363/8047 [28:47<10:10:45,  4.77s/it]loss_total_epoch 14.122549694031477
Training tokenizer:   5% 364/8047 [28:52<10:09:43,  4.76s/it]loss_total_epoch 14.16051483899355
Training tokenizer:   5% 365/8047 [28:57<10:10:55,  4.77s/it]loss_total_epoch 14.196368735283613
Training tokenizer:   5% 366/8047 [29:01<10:07:10,  4.74s/it]loss_total_epoch 14.241016749292612
Training tokenizer:   5% 367/8047 [29:06<10:10:19,  4.77s/it]loss_total_epoch 14.27323367074132
Training tokenizer:   5% 368/8047 [29:11<10:10:56,  4.77s/it]loss_total_epoch 14.314627930521965
Training tokenizer:   5% 369/8047 [29:16<10:08:17,  4.75s/it]loss_total_epoch 14.338857680559158
Training tokenizer:   5% 370/8047 [29:20<10:08:39,  4.76s/it]loss_total_epoch 14.371654391288757
Training tokenizer:   5% 371/8047 [29:25<10:08:19,  4.75s/it]loss_total_epoch 14.40800952911377
Training tokenizer:   5% 372/8047 [29:30<10:08:38,  4.76s/it]loss_total_epoch 14.44529914483428
Training tokenizer:   5% 373/8047 [29:35<10:09:19,  4.76s/it]loss_total_epoch 14.482624724507332
Training tokenizer:   5% 374/8047 [29:39<10:13:25,  4.80s/it]loss_total_epoch 14.519861698150635
Training tokenizer:   5% 375/8047 [29:44<10:10:09,  4.77s/it]loss_total_epoch 14.556625060737133
Training tokenizer:   5% 376/8047 [29:49<10:13:23,  4.80s/it]loss_total_epoch 14.602434579282999
Training tokenizer:   5% 377/8047 [29:54<10:11:02,  4.78s/it]loss_total_epoch 14.646427921950817
Training tokenizer:   5% 378/8047 [29:59<10:10:38,  4.78s/it]loss_total_epoch 14.685963682830334
Training tokenizer:   5% 379/8047 [30:03<10:10:18,  4.78s/it]loss_total_epoch 14.721027933061123
Training tokenizer:   5% 380/8047 [30:08<10:10:32,  4.78s/it]loss_total_epoch 14.76501139253378
Training tokenizer:   5% 381/8047 [30:13<10:10:05,  4.78s/it]loss_total_epoch 14.805247973650694
Training tokenizer:   5% 382/8047 [30:18<10:10:09,  4.78s/it]loss_total_epoch 14.843909393996
Training tokenizer:   5% 383/8047 [30:22<10:11:17,  4.79s/it]loss_total_epoch 14.884545464068651
Training tokenizer:   5% 384/8047 [30:27<10:10:59,  4.78s/it]loss_total_epoch 14.919288039207458
Training tokenizer:   5% 385/8047 [30:32<10:09:22,  4.77s/it]loss_total_epoch 14.961046572774649
Training tokenizer:   5% 386/8047 [30:37<10:07:58,  4.76s/it]loss_total_epoch 14.996606804430485
Training tokenizer:   5% 387/8047 [30:41<10:07:16,  4.76s/it]loss_total_epoch 15.042419377714396
Training tokenizer:   5% 388/8047 [30:46<10:06:00,  4.75s/it]loss_total_epoch 15.079492632299662
Training tokenizer:   5% 389/8047 [30:51<10:06:36,  4.75s/it]loss_total_epoch 15.120857041329145
Training tokenizer:   5% 390/8047 [30:56<10:06:51,  4.76s/it]loss_total_epoch 15.163114383816719
Training tokenizer:   5% 391/8047 [31:01<10:10:01,  4.78s/it]loss_total_epoch 15.198962684720755
Training tokenizer:   5% 392/8047 [31:05<10:09:26,  4.78s/it]loss_total_epoch 15.246194522827864
Training tokenizer:   5% 393/8047 [31:10<10:04:08,  4.74s/it]loss_total_epoch 15.294877212494612
Training tokenizer:   5% 394/8047 [31:15<10:02:45,  4.73s/it]loss_total_epoch 15.339165512472391
Training tokenizer:   5% 395/8047 [31:19<10:03:45,  4.73s/it]loss_total_epoch 15.372730635106564
Training tokenizer:   5% 396/8047 [31:24<10:06:56,  4.76s/it]loss_total_epoch 15.410612888634205
Training tokenizer:   5% 397/8047 [31:29<10:07:12,  4.76s/it]loss_total_epoch 15.44470977038145
Training tokenizer:   5% 398/8047 [31:34<10:08:08,  4.77s/it]loss_total_epoch 15.473226396366954
Training tokenizer:   5% 399/8047 [31:39<10:09:34,  4.78s/it]loss_total_epoch 15.51139466278255
Training tokenizer:   5% 400/8047 [31:43<10:09:15,  4.78s/it]loss_total_epoch 15.54671466909349
Training tokenizer:   5% 401/8047 [31:48<10:07:55,  4.77s/it]loss_total_epoch 15.584525039419532
Training tokenizer:   5% 402/8047 [31:53<10:06:22,  4.76s/it]loss_total_epoch 15.627129489555955
Training tokenizer:   5% 403/8047 [31:58<10:05:48,  4.76s/it]loss_total_epoch 15.670722240582108
Training tokenizer:   5% 404/8047 [32:02<10:08:58,  4.78s/it]loss_total_epoch 15.69971931539476
Training tokenizer:   5% 405/8047 [32:07<10:06:05,  4.76s/it]loss_total_epoch 15.73602264560759
Training tokenizer:   5% 406/8047 [32:12<10:04:29,  4.75s/it]loss_total_epoch 15.767241606488824
Training tokenizer:   5% 407/8047 [32:17<10:03:40,  4.74s/it]loss_total_epoch 15.798770459368825
Training tokenizer:   5% 408/8047 [32:21<10:04:23,  4.75s/it]loss_total_epoch 15.833806613460183
Training tokenizer:   5% 409/8047 [32:26<10:05:59,  4.76s/it]loss_total_epoch 15.8770423065871
Training tokenizer:   5% 410/8047 [32:31<10:07:25,  4.77s/it]loss_total_epoch 15.904494879767299
Training tokenizer:   5% 411/8047 [32:36<10:04:31,  4.75s/it]loss_total_epoch 15.947116246446967
Training tokenizer:   5% 412/8047 [32:40<10:03:21,  4.74s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-8d5qq6t6'
loss_total_epoch 15.978433476760983
Training tokenizer:   5% 413/8047 [32:45<10:02:18,  4.73s/it]loss_total_epoch 16.00990573130548
Training tokenizer:   5% 414/8047 [32:50<10:04:00,  4.75s/it]loss_total_epoch 16.046389093622565
Training tokenizer:   5% 415/8047 [32:55<10:06:14,  4.77s/it]loss_total_epoch 16.0833521168679
Training tokenizer:   5% 416/8047 [32:59<10:04:59,  4.76s/it]loss_total_epoch 16.122822428122163
Training tokenizer:   5% 417/8047 [33:04<10:05:30,  4.76s/it]loss_total_epoch 16.15127058699727
Training tokenizer:   5% 418/8047 [33:09<10:05:00,  4.76s/it]loss_total_epoch 16.19152932614088
Training tokenizer:   5% 419/8047 [33:14<10:05:59,  4.77s/it]loss_total_epoch 16.223414689302444
Training tokenizer:   5% 420/8047 [33:19<10:08:02,  4.78s/it]loss_total_epoch 16.252070242539048
Training tokenizer:   5% 421/8047 [33:23<10:07:51,  4.78s/it]loss_total_epoch 16.287179393693805
Training tokenizer:   5% 422/8047 [33:28<10:05:58,  4.77s/it]loss_total_epoch 16.319798236712813
Training tokenizer:   5% 423/8047 [33:33<10:06:57,  4.78s/it]loss_total_epoch 16.355320626869798
Training tokenizer:   5% 424/8047 [33:38<10:06:23,  4.77s/it]loss_total_epoch 16.401967549696565
Training tokenizer:   5% 425/8047 [33:42<10:05:26,  4.77s/it]loss_total_epoch 16.446622664108872
Training tokenizer:   5% 426/8047 [33:47<10:06:41,  4.78s/it]loss_total_epoch 16.483098393306136
Training tokenizer:   5% 427/8047 [33:52<10:06:15,  4.77s/it]loss_total_epoch 16.51384738832712
Training tokenizer:   5% 428/8047 [33:57<10:06:19,  4.77s/it]loss_total_epoch 16.549643900245428
Training tokenizer:   5% 429/8047 [34:02<10:06:13,  4.77s/it]loss_total_epoch 16.584630977362394
Training tokenizer:   5% 430/8047 [34:06<10:05:58,  4.77s/it]loss_total_epoch 16.61966773867607
Training tokenizer:   5% 431/8047 [34:11<10:06:20,  4.78s/it]loss_total_epoch 16.651431117206812
Training tokenizer:   5% 432/8047 [34:16<10:03:41,  4.76s/it]loss_total_epoch 16.683779250830412
Training tokenizer:   5% 433/8047 [34:20<10:01:30,  4.74s/it]loss_total_epoch 16.723239731043577
Training tokenizer:   5% 434/8047 [34:25<10:02:36,  4.75s/it]loss_total_epoch 16.753245254978538
Training tokenizer:   5% 435/8047 [34:30<10:04:11,  4.76s/it]loss_total_epoch 16.787190759554505
Training tokenizer:   5% 436/8047 [34:35<10:04:06,  4.76s/it]loss_total_epoch 16.823446532711387
Training tokenizer:   5% 437/8047 [34:40<10:05:50,  4.78s/it]loss_total_epoch 16.862708067521453
Training tokenizer:   5% 438/8047 [34:44<10:03:11,  4.76s/it]loss_total_epoch 16.90361146442592
Training tokenizer:   5% 439/8047 [34:49<10:03:03,  4.76s/it]loss_total_epoch 16.933461220934987
Training tokenizer:   5% 440/8047 [34:54<10:03:19,  4.76s/it]loss_total_epoch 16.97277665324509
Training tokenizer:   5% 441/8047 [34:59<10:02:38,  4.75s/it]loss_total_epoch 17.012250931933522
Training tokenizer:   5% 442/8047 [35:03<10:01:22,  4.74s/it]loss_total_epoch 17.051846178248525
Training tokenizer:   6% 443/8047 [35:08<10:02:59,  4.76s/it]loss_total_epoch 17.086930425837636
Training tokenizer:   6% 444/8047 [35:13<10:03:42,  4.76s/it]loss_total_epoch 17.125123469159007
Training tokenizer:   6% 445/8047 [35:18<10:04:13,  4.77s/it]loss_total_epoch 17.176183665171266
Training tokenizer:   6% 446/8047 [35:22<10:04:49,  4.77s/it]loss_total_epoch 17.212817957624793
Training tokenizer:   6% 447/8047 [35:27<10:06:25,  4.79s/it]loss_total_epoch 17.24611863307655
Training tokenizer:   6% 448/8047 [35:32<10:00:52,  4.74s/it]loss_total_epoch 17.28648112900555
Training tokenizer:   6% 449/8047 [35:37<10:01:30,  4.75s/it]loss_total_epoch 17.328478692099452
Training tokenizer:   6% 450/8047 [35:41<10:01:45,  4.75s/it]loss_total_epoch 17.36801782809198
Training tokenizer:   6% 451/8047 [35:46<10:02:23,  4.76s/it]loss_total_epoch 17.40705326013267
Training tokenizer:   6% 452/8047 [35:51<10:03:21,  4.77s/it]loss_total_epoch 17.434629233554006
Training tokenizer:   6% 453/8047 [35:56<10:03:30,  4.77s/it]loss_total_epoch 17.472804183140397
Training tokenizer:   6% 454/8047 [36:00<10:01:14,  4.75s/it]loss_total_epoch 17.511029867455363
Training tokenizer:   6% 455/8047 [36:05<10:01:45,  4.76s/it]loss_total_epoch 17.54981448315084
Training tokenizer:   6% 456/8047 [36:10<10:00:46,  4.75s/it]loss_total_epoch 17.597204910591245
Training tokenizer:   6% 457/8047 [36:15<10:01:36,  4.76s/it]loss_total_epoch 17.645086178556085
Training tokenizer:   6% 458/8047 [36:19<10:01:20,  4.75s/it]loss_total_epoch 17.684189500287175
Training tokenizer:   6% 459/8047 [36:24<10:02:28,  4.76s/it]loss_total_epoch 17.72459496371448
Training tokenizer:   6% 460/8047 [36:29<10:02:24,  4.76s/it]loss_total_epoch 17.76256579346955
Training tokenizer:   6% 461/8047 [36:34<10:02:16,  4.76s/it]loss_total_epoch 17.808139437809587
Training tokenizer:   6% 462/8047 [36:39<10:02:28,  4.77s/it]loss_total_epoch 17.84180529229343
Training tokenizer:   6% 463/8047 [36:43<10:01:19,  4.76s/it]loss_total_epoch 17.879563568159938
Training tokenizer:   6% 464/8047 [36:48<10:00:28,  4.75s/it]loss_total_epoch 17.908374899998307
Training tokenizer:   6% 465/8047 [36:53<9:59:41,  4.75s/it] loss_total_epoch 17.93795674480498
Training tokenizer:   6% 466/8047 [36:58<10:04:32,  4.78s/it]loss_total_epoch 17.974991152063012
Training tokenizer:   6% 467/8047 [37:02<10:00:59,  4.76s/it]loss_total_epoch 18.02087310142815
Training tokenizer:   6% 468/8047 [37:07<9:59:45,  4.75s/it] loss_total_epoch 18.052648523822427
Training tokenizer:   6% 469/8047 [37:12<10:00:39,  4.76s/it]loss_total_epoch 18.08608172647655
Training tokenizer:   6% 470/8047 [37:17<9:58:32,  4.74s/it] loss_total_epoch 18.122447403147817
Training tokenizer:   6% 471/8047 [37:21<10:00:35,  4.76s/it]loss_total_epoch 18.16237640567124
Training tokenizer:   6% 472/8047 [37:26<10:01:15,  4.76s/it]loss_total_epoch 18.198690691962838
Training tokenizer:   6% 473/8047 [37:31<10:01:32,  4.77s/it]loss_total_epoch 18.237837044522166
Training tokenizer:   6% 474/8047 [37:36<10:00:41,  4.76s/it]loss_total_epoch 18.27505179308355
Training tokenizer:   6% 475/8047 [37:40<9:59:29,  4.75s/it] loss_total_epoch 18.31286875717342
Training tokenizer:   6% 476/8047 [37:45<10:00:14,  4.76s/it]loss_total_epoch 18.35468307323754
Training tokenizer:   6% 477/8047 [37:50<10:01:00,  4.76s/it]loss_total_epoch 18.39759543351829
Training tokenizer:   6% 478/8047 [37:55<10:00:37,  4.76s/it]loss_total_epoch 18.42544748261571
Training tokenizer:   6% 479/8047 [37:59<10:01:50,  4.77s/it]loss_total_epoch 18.463744036853313
Training tokenizer:   6% 480/8047 [38:04<10:02:14,  4.78s/it]loss_total_epoch 18.501566793769598
Training tokenizer:   6% 481/8047 [38:09<10:01:42,  4.77s/it]loss_total_epoch 18.537851758301258
Training tokenizer:   6% 482/8047 [38:14<9:58:20,  4.75s/it] loss_total_epoch 18.56873345375061
Training tokenizer:   6% 483/8047 [38:18<9:55:53,  4.73s/it]loss_total_epoch 18.59970714710653
Training tokenizer:   6% 484/8047 [38:23<9:56:08,  4.73s/it]loss_total_epoch 18.635784236714244
Training tokenizer:   6% 485/8047 [38:28<9:58:55,  4.75s/it]loss_total_epoch 18.67722632922232
Training tokenizer:   6% 486/8047 [38:33<9:57:09,  4.74s/it]loss_total_epoch 18.71511618234217
Training tokenizer:   6% 487/8047 [38:37<9:59:01,  4.75s/it]loss_total_epoch 18.752493465319276
Training tokenizer:   6% 488/8047 [38:42<9:59:46,  4.76s/it]loss_total_epoch 18.78126229904592
Training tokenizer:   6% 489/8047 [38:47<9:58:50,  4.75s/it]loss_total_epoch 18.815631614997983
Training tokenizer:   6% 490/8047 [38:52<9:58:25,  4.75s/it]loss_total_epoch 18.842605279758573
Training tokenizer:   6% 491/8047 [38:56<9:59:49,  4.76s/it]loss_total_epoch 18.882463889196515
Training tokenizer:   6% 492/8047 [39:01<10:00:34,  4.77s/it]loss_total_epoch 18.928851818665862
Training tokenizer:   6% 493/8047 [39:06<10:03:45,  4.80s/it]loss_total_epoch 18.974668415263295
Training tokenizer:   6% 494/8047 [39:11<10:08:04,  4.83s/it]loss_total_epoch 19.01972541026771
Training tokenizer:   6% 495/8047 [39:16<10:05:47,  4.81s/it]loss_total_epoch 19.051998445764184
Training tokenizer:   6% 496/8047 [39:21<10:05:19,  4.81s/it]loss_total_epoch 19.087528137490153
Training tokenizer:   6% 497/8047 [39:25<10:04:04,  4.80s/it]loss_total_epoch 19.13320436514914
Training tokenizer:   6% 498/8047 [39:30<10:02:57,  4.79s/it]loss_total_epoch 19.171975249424577
Training tokenizer:   6% 499/8047 [39:35<10:02:46,  4.79s/it]loss_total_epoch 19.214679026976228
Training tokenizer:   6% 500/8047 [39:40<10:01:12,  4.78s/it]loss_total_epoch 19.26489573903382
Training tokenizer:   6% 501/8047 [39:44<10:00:21,  4.77s/it]loss_total_epoch 19.30240481905639
Training tokenizer:   6% 502/8047 [39:49<9:59:24,  4.77s/it] loss_total_epoch 19.339805165305734
Training tokenizer:   6% 503/8047 [39:54<9:59:09,  4.77s/it]loss_total_epoch 19.37797055207193
Training tokenizer:   6% 504/8047 [39:59<10:00:17,  4.77s/it]loss_total_epoch 19.411781487986445
Training tokenizer:   6% 505/8047 [40:04<10:01:02,  4.78s/it]loss_total_epoch 19.452682370319963
Training tokenizer:   6% 506/8047 [40:08<10:02:51,  4.80s/it]loss_total_epoch 19.487273318693042
Training tokenizer:   6% 507/8047 [40:13<10:02:00,  4.79s/it]loss_total_epoch 19.51709128729999
Training tokenizer:   6% 508/8047 [40:18<10:00:53,  4.78s/it]loss_total_epoch 19.55822574906051
Training tokenizer:   6% 509/8047 [40:23<9:58:12,  4.76s/it] loss_total_epoch 19.589143035933375
Training tokenizer:   6% 510/8047 [40:27<9:56:42,  4.75s/it]loss_total_epoch 19.62823404558003
Training tokenizer:   6% 511/8047 [40:32<9:59:03,  4.77s/it]loss_total_epoch 19.659914193674922
Training tokenizer:   6% 512/8047 [40:37<9:58:56,  4.77s/it]loss_total_epoch 19.69868622161448
Training tokenizer:   6% 513/8047 [40:42<9:57:42,  4.76s/it]loss_total_epoch 19.734028918668628
Training tokenizer:   6% 514/8047 [40:47<9:59:06,  4.77s/it]loss_total_epoch 19.77501585148275
Training tokenizer:   6% 515/8047 [40:51<9:59:33,  4.78s/it]loss_total_epoch 19.814893318340182
Training tokenizer:   6% 516/8047 [40:56<10:03:26,  4.81s/it]loss_total_epoch 19.843595758080482
Training tokenizer:   6% 517/8047 [41:01<10:02:06,  4.80s/it]loss_total_epoch 19.87949712201953
Training tokenizer:   6% 518/8047 [41:06<10:00:58,  4.79s/it]loss_total_epoch 19.92339549958706
Training tokenizer:   6% 519/8047 [41:11<10:01:05,  4.79s/it]loss_total_epoch 19.966260328888893
Training tokenizer:   6% 520/8047 [41:15<9:59:55,  4.78s/it] loss_total_epoch 19.992374608293176
Training tokenizer:   6% 521/8047 [41:20<9:58:15,  4.77s/it]loss_total_epoch 20.03991654329002
Training tokenizer:   6% 522/8047 [41:25<9:58:12,  4.77s/it]loss_total_epoch 20.08641098625958
Training tokenizer:   6% 523/8047 [41:30<9:57:52,  4.77s/it]loss_total_epoch 20.121276075020432
Training tokenizer:   7% 524/8047 [41:34<9:58:34,  4.77s/it]loss_total_epoch 20.165686832740903
Training tokenizer:   7% 525/8047 [41:39<10:00:27,  4.79s/it]loss_total_epoch 20.211912589147687
Training tokenizer:   7% 526/8047 [41:44<9:56:30,  4.76s/it] loss_total_epoch 20.250584261491895
Training tokenizer:   7% 527/8047 [41:49<9:57:57,  4.77s/it]loss_total_epoch 20.283464627340436
Training tokenizer:   7% 528/8047 [41:53<9:57:20,  4.77s/it]loss_total_epoch 20.31607194803655
Training tokenizer:   7% 529/8047 [41:58<9:55:22,  4.75s/it]loss_total_epoch 20.35686676390469
Training tokenizer:   7% 530/8047 [42:03<9:54:19,  4.74s/it]loss_total_epoch 20.39070233143866
Training tokenizer:   7% 531/8047 [42:08<9:55:59,  4.76s/it]loss_total_epoch 20.415474941954017
Training tokenizer:   7% 532/8047 [42:12<9:56:41,  4.76s/it]loss_total_epoch 20.458889240399003
Training tokenizer:   7% 533/8047 [42:17<9:57:19,  4.77s/it]loss_total_epoch 20.491286305710673
Training tokenizer:   7% 534/8047 [42:22<9:57:29,  4.77s/it]loss_total_epoch 20.52935684286058
Training tokenizer:   7% 535/8047 [42:27<9:57:30,  4.77s/it]loss_total_epoch 20.562786301597953
Training tokenizer:   7% 536/8047 [42:32<9:58:58,  4.78s/it]loss_total_epoch 20.598445842042565
Training tokenizer:   7% 537/8047 [42:36<9:57:11,  4.77s/it]loss_total_epoch 20.64167576469481
Training tokenizer:   7% 538/8047 [42:41<9:55:56,  4.76s/it]loss_total_epoch 20.686962297186255
Training tokenizer:   7% 539/8047 [42:46<9:56:22,  4.77s/it]loss_total_epoch 20.73520972393453
Training tokenizer:   7% 540/8047 [42:51<9:54:28,  4.75s/it]loss_total_epoch 20.775485644116998
Training tokenizer:   7% 541/8047 [42:55<9:53:44,  4.75s/it]loss_total_epoch 20.81426571495831
Training tokenizer:   7% 542/8047 [43:00<9:52:38,  4.74s/it]loss_total_epoch 20.86349847353995
Training tokenizer:   7% 543/8047 [43:05<9:53:39,  4.75s/it]loss_total_epoch 20.901140773668885
Training tokenizer:   7% 544/8047 [43:09<9:50:36,  4.72s/it]loss_total_epoch 20.94293079711497
Training tokenizer:   7% 545/8047 [43:14<9:53:02,  4.74s/it]loss_total_epoch 20.97613105736673
Training tokenizer:   7% 546/8047 [43:19<9:55:31,  4.76s/it]loss_total_epoch 21.019013671204448
Training tokenizer:   7% 547/8047 [43:24<9:53:38,  4.75s/it]loss_total_epoch 21.056954594329
Training tokenizer:   7% 548/8047 [43:29<9:55:34,  4.77s/it]loss_total_epoch 21.08757739327848
Training tokenizer:   7% 549/8047 [43:33<9:54:52,  4.76s/it]loss_total_epoch 21.126577833667397
Training tokenizer:   7% 550/8047 [43:38<9:55:00,  4.76s/it]loss_total_epoch 21.16252595745027
Training tokenizer:   7% 551/8047 [43:43<9:54:38,  4.76s/it]loss_total_epoch 21.198973750695586
Training tokenizer:   7% 552/8047 [43:48<9:55:58,  4.77s/it]loss_total_epoch 21.243877669796348
Training tokenizer:   7% 553/8047 [43:52<9:55:20,  4.77s/it]loss_total_epoch 21.287887489423156
Training tokenizer:   7% 554/8047 [43:57<9:53:57,  4.76s/it]loss_total_epoch 21.32730002515018
Training tokenizer:   7% 555/8047 [44:02<9:53:42,  4.75s/it]loss_total_epoch 21.357337076216936
Training tokenizer:   7% 556/8047 [44:07<9:54:02,  4.76s/it]loss_total_epoch 21.394554995000362
Training tokenizer:   7% 557/8047 [44:11<9:54:55,  4.77s/it]loss_total_epoch 21.433534786105156
Training tokenizer:   7% 558/8047 [44:16<9:57:47,  4.79s/it]loss_total_epoch 21.47153541445732
Training tokenizer:   7% 559/8047 [44:21<9:58:55,  4.80s/it]loss_total_epoch 21.511239264160395
Training tokenizer:   7% 560/8047 [44:26<9:57:40,  4.79s/it]loss_total_epoch 21.543195143342018
Training tokenizer:   7% 561/8047 [44:31<9:58:11,  4.79s/it]loss_total_epoch 21.584808945655823
Training tokenizer:   7% 562/8047 [44:35<9:56:19,  4.78s/it]loss_total_epoch 21.621580060571432
Training tokenizer:   7% 563/8047 [44:40<9:53:36,  4.76s/it]loss_total_epoch 21.66309268400073
Training tokenizer:   7% 564/8047 [44:45<9:53:26,  4.76s/it]loss_total_epoch 21.699600603431463
Training tokenizer:   7% 565/8047 [44:50<9:55:12,  4.77s/it]loss_total_epoch 21.73789655044675
Training tokenizer:   7% 566/8047 [44:54<9:53:58,  4.76s/it]loss_total_epoch 21.777975641191006
Training tokenizer:   7% 567/8047 [44:59<9:54:13,  4.77s/it]loss_total_epoch 21.812609646469355
Training tokenizer:   7% 568/8047 [45:04<9:54:45,  4.77s/it]loss_total_epoch 21.855215344578028
Training tokenizer:   7% 569/8047 [45:09<9:52:51,  4.76s/it]loss_total_epoch 21.894835896790028
Training tokenizer:   7% 570/8047 [45:13<9:51:16,  4.74s/it]loss_total_epoch 21.928493436425924
Training tokenizer:   7% 571/8047 [45:18<9:50:43,  4.74s/it]loss_total_epoch 21.964215453714132
Training tokenizer:   7% 572/8047 [45:23<9:51:24,  4.75s/it]loss_total_epoch 22.004924178123474
Training tokenizer:   7% 573/8047 [45:28<9:49:05,  4.73s/it]loss_total_epoch 22.050509244203568
Training tokenizer:   7% 574/8047 [45:32<9:49:47,  4.74s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-sstq6_5c'
loss_total_epoch 22.09346206486225
Training tokenizer:   7% 575/8047 [45:37<9:51:32,  4.75s/it]loss_total_epoch 22.126087799668312
Training tokenizer:   7% 576/8047 [45:42<9:51:50,  4.75s/it]loss_total_epoch 22.165737383067608
Training tokenizer:   7% 577/8047 [45:47<9:52:02,  4.76s/it]loss_total_epoch 22.204693906009197
Training tokenizer:   7% 578/8047 [45:51<9:51:05,  4.75s/it]loss_total_epoch 22.236037701368332
Training tokenizer:   7% 579/8047 [45:56<9:49:59,  4.74s/it]loss_total_epoch 22.272201493382454
Training tokenizer:   7% 580/8047 [46:01<9:50:29,  4.74s/it]loss_total_epoch 22.309556290507317
Training tokenizer:   7% 581/8047 [46:06<9:49:36,  4.74s/it]loss_total_epoch 22.34255913645029
Training tokenizer:   7% 582/8047 [46:10<9:50:12,  4.74s/it]loss_total_epoch 22.376707337796688
Training tokenizer:   7% 583/8047 [46:15<9:51:11,  4.75s/it]loss_total_epoch 22.41474924236536
Training tokenizer:   7% 584/8047 [46:20<9:52:32,  4.76s/it]loss_total_epoch 22.455370046198368
Training tokenizer:   7% 585/8047 [46:25<9:52:34,  4.76s/it]loss_total_epoch 22.494133688509464
Training tokenizer:   7% 586/8047 [46:29<9:53:26,  4.77s/it]loss_total_epoch 22.53159225359559
Training tokenizer:   7% 587/8047 [46:34<9:51:54,  4.76s/it]loss_total_epoch 22.573358613997698
Training tokenizer:   7% 588/8047 [46:39<9:52:35,  4.77s/it]loss_total_epoch 22.60892228409648
Training tokenizer:   7% 589/8047 [46:44<9:51:15,  4.76s/it]loss_total_epoch 22.64606250077486
Training tokenizer:   7% 590/8047 [46:48<9:51:45,  4.76s/it]loss_total_epoch 22.678980857133865
Training tokenizer:   7% 591/8047 [46:53<9:52:53,  4.77s/it]loss_total_epoch 22.721606332808733
Training tokenizer:   7% 592/8047 [46:58<9:51:17,  4.76s/it]loss_total_epoch 22.761422034353018
Training tokenizer:   7% 593/8047 [47:03<9:49:38,  4.75s/it]loss_total_epoch 22.793253764510155
Training tokenizer:   7% 594/8047 [47:07<9:48:56,  4.74s/it]loss_total_epoch 22.83087833970785
Training tokenizer:   7% 595/8047 [47:12<9:51:03,  4.76s/it]loss_total_epoch 22.86325052380562
Training tokenizer:   7% 596/8047 [47:17<9:50:34,  4.76s/it]loss_total_epoch 22.902483489364386
Training tokenizer:   7% 597/8047 [47:22<9:50:00,  4.75s/it]loss_total_epoch 22.946062840521336
Training tokenizer:   7% 598/8047 [47:27<9:53:57,  4.78s/it]loss_total_epoch 22.96981068328023
Training tokenizer:   7% 599/8047 [47:31<9:55:07,  4.79s/it]loss_total_epoch 22.998752249404788
Training tokenizer:   7% 600/8047 [47:36<9:54:10,  4.79s/it]loss_total_epoch 23.0406108032912
Training tokenizer:   7% 601/8047 [47:41<9:54:25,  4.79s/it]loss_total_epoch 23.083331217989326
Training tokenizer:   7% 602/8047 [47:46<9:54:18,  4.79s/it]loss_total_epoch 23.116164630278945
Training tokenizer:   7% 603/8047 [47:51<9:54:07,  4.79s/it]loss_total_epoch 23.15444472618401
Training tokenizer:   8% 604/8047 [47:55<9:54:16,  4.79s/it]loss_total_epoch 23.195892272517085
Training tokenizer:   8% 605/8047 [48:00<9:52:03,  4.77s/it]loss_total_epoch 23.22831610403955
Training tokenizer:   8% 606/8047 [48:05<9:51:06,  4.77s/it]loss_total_epoch 23.26638305000961
Training tokenizer:   8% 607/8047 [48:10<9:52:28,  4.78s/it]loss_total_epoch 23.308635314926505
Training tokenizer:   8% 608/8047 [48:14<9:51:43,  4.77s/it]loss_total_epoch 23.341128131374717
Training tokenizer:   8% 609/8047 [48:19<9:52:02,  4.78s/it]loss_total_epoch 23.38224101625383
Training tokenizer:   8% 610/8047 [48:24<9:51:56,  4.78s/it]loss_total_epoch 23.4286686796695
Training tokenizer:   8% 611/8047 [48:29<9:51:15,  4.77s/it]loss_total_epoch 23.473381591960788
Training tokenizer:   8% 612/8047 [48:33<9:51:13,  4.77s/it]loss_total_epoch 23.52076786197722
Training tokenizer:   8% 613/8047 [48:38<9:49:13,  4.76s/it]loss_total_epoch 23.563420886173844
Training tokenizer:   8% 614/8047 [48:43<9:51:21,  4.77s/it]loss_total_epoch 23.606468314304948
Training tokenizer:   8% 615/8047 [48:48<9:51:04,  4.77s/it]loss_total_epoch 23.639686001464725
Training tokenizer:   8% 616/8047 [48:53<9:52:44,  4.79s/it]loss_total_epoch 23.6767979580909
Training tokenizer:   8% 617/8047 [48:57<9:52:32,  4.78s/it]loss_total_epoch 23.71491743810475
Training tokenizer:   8% 618/8047 [49:02<9:51:47,  4.78s/it]loss_total_epoch 23.755959590896964
Training tokenizer:   8% 619/8047 [49:07<9:48:43,  4.76s/it]loss_total_epoch 23.794446988031268
Training tokenizer:   8% 620/8047 [49:12<9:50:08,  4.77s/it]loss_total_epoch 23.835175456479192
Training tokenizer:   8% 621/8047 [49:16<9:51:19,  4.78s/it]loss_total_epoch 23.876196948811412
Training tokenizer:   8% 622/8047 [49:21<9:50:18,  4.77s/it]loss_total_epoch 23.902083411812782
Training tokenizer:   8% 623/8047 [49:26<9:51:58,  4.78s/it]loss_total_epoch 23.935276370495558
Training tokenizer:   8% 624/8047 [49:31<9:51:43,  4.78s/it]loss_total_epoch 23.9736888371408
Training tokenizer:   8% 625/8047 [49:36<9:50:03,  4.77s/it]loss_total_epoch 24.020558800548315
Training tokenizer:   8% 626/8047 [49:40<9:51:51,  4.79s/it]loss_total_epoch 24.059122260659933
Training tokenizer:   8% 627/8047 [49:45<9:50:44,  4.78s/it]loss_total_epoch 24.10122376680374
Training tokenizer:   8% 628/8047 [49:50<9:52:28,  4.79s/it]loss_total_epoch 24.1421976685524
Training tokenizer:   8% 629/8047 [49:55<9:51:00,  4.78s/it]loss_total_epoch 24.17616529017687
Training tokenizer:   8% 630/8047 [49:59<9:50:10,  4.77s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-a8vthm3_'
loss_total_epoch 24.20546056330204
Training tokenizer:   8% 631/8047 [50:04<9:52:47,  4.80s/it]loss_total_epoch 24.245797749608755
Training tokenizer:   8% 632/8047 [50:09<9:50:30,  4.78s/it]loss_total_epoch 24.28228133544326
Training tokenizer:   8% 633/8047 [50:14<9:53:32,  4.80s/it]loss_total_epoch 24.326318092644215
Training tokenizer:   8% 634/8047 [50:19<9:52:03,  4.79s/it]loss_total_epoch 24.36083097010851
Training tokenizer:   8% 635/8047 [50:23<9:51:24,  4.79s/it]loss_total_epoch 24.403814412653446
Training tokenizer:   8% 636/8047 [50:28<9:51:27,  4.79s/it]loss_total_epoch 24.44766966998577
Training tokenizer:   8% 637/8047 [50:33<9:50:20,  4.78s/it]loss_total_epoch 24.48544503748417
Training tokenizer:   8% 638/8047 [50:38<9:46:25,  4.75s/it]loss_total_epoch 24.530854985117912
Training tokenizer:   8% 639/8047 [50:42<9:41:43,  4.71s/it]loss_total_epoch 24.57295210659504
Training tokenizer:   8% 640/8047 [50:47<9:40:09,  4.70s/it]loss_total_epoch 24.608242947608232
Training tokenizer:   8% 641/8047 [50:52<9:43:37,  4.73s/it]loss_total_epoch 24.645230017602444
Training tokenizer:   8% 642/8047 [50:57<9:44:22,  4.73s/it]loss_total_epoch 24.696161542087793
Training tokenizer:   8% 643/8047 [51:01<9:44:49,  4.74s/it]loss_total_epoch 24.73394975438714
Training tokenizer:   8% 644/8047 [51:06<9:47:04,  4.76s/it]loss_total_epoch 24.77413995936513
Training tokenizer:   8% 645/8047 [51:11<9:48:38,  4.77s/it]loss_total_epoch 24.800721615552902
Training tokenizer:   8% 646/8047 [51:16<9:47:39,  4.76s/it]loss_total_epoch 24.846081238240004
Training tokenizer:   8% 647/8047 [51:20<9:49:47,  4.78s/it]loss_total_epoch 24.88167979195714
Training tokenizer:   8% 648/8047 [51:25<9:49:55,  4.78s/it]loss_total_epoch 24.927979692816734
Training tokenizer:   8% 649/8047 [51:30<9:49:42,  4.78s/it]loss_total_epoch 24.965259734541178
Training tokenizer:   8% 650/8047 [51:35<9:48:56,  4.78s/it]loss_total_epoch 25.008874278515577
Training tokenizer:   8% 651/8047 [51:40<9:48:58,  4.78s/it]loss_total_epoch 25.04912482202053
Training tokenizer:   8% 652/8047 [51:44<9:47:20,  4.77s/it]loss_total_epoch 25.080140501260757
Training tokenizer:   8% 653/8047 [51:49<9:48:13,  4.77s/it]loss_total_epoch 25.119090855121613
Training tokenizer:   8% 654/8047 [51:54<9:46:14,  4.76s/it]loss_total_epoch 25.166033420711756
Training tokenizer:   8% 655/8047 [51:59<9:48:09,  4.77s/it]loss_total_epoch 25.20593649148941
Training tokenizer:   8% 656/8047 [52:03<9:46:30,  4.76s/it]loss_total_epoch 25.240417081862688
Training tokenizer:   8% 657/8047 [52:08<9:44:55,  4.75s/it]loss_total_epoch 25.267123317345977
Training tokenizer:   8% 658/8047 [52:13<9:47:26,  4.77s/it]loss_total_epoch 25.30555414222181
Training tokenizer:   8% 659/8047 [52:18<9:46:59,  4.77s/it]loss_total_epoch 25.334029717370868
Training tokenizer:   8% 660/8047 [52:22<9:48:08,  4.78s/it]loss_total_epoch 25.373217595741153
Training tokenizer:   8% 661/8047 [52:27<9:47:51,  4.78s/it]loss_total_epoch 25.404949279502034
Training tokenizer:   8% 662/8047 [52:32<9:48:46,  4.78s/it]loss_total_epoch 25.440108699724078
Training tokenizer:   8% 663/8047 [52:37<9:49:32,  4.79s/it]loss_total_epoch 25.48400149680674
Training tokenizer:   8% 664/8047 [52:42<9:50:02,  4.80s/it]loss_total_epoch 25.522613974288106
Training tokenizer:   8% 665/8047 [52:46<9:49:37,  4.79s/it]loss_total_epoch 25.555160773918033
Training tokenizer:   8% 666/8047 [52:51<9:51:38,  4.81s/it]loss_total_epoch 25.58271013945341
Training tokenizer:   8% 667/8047 [52:56<9:51:11,  4.81s/it]loss_total_epoch 25.614118449389935
Training tokenizer:   8% 668/8047 [53:01<9:50:03,  4.80s/it]loss_total_epoch 25.657397776842117
Training tokenizer:   8% 669/8047 [53:06<9:49:30,  4.79s/it]loss_total_epoch 25.696752902120352
Training tokenizer:   8% 670/8047 [53:10<9:49:23,  4.79s/it]loss_total_epoch 25.73310538753867
Training tokenizer:   8% 671/8047 [53:15<9:48:53,  4.79s/it]loss_total_epoch 25.762142745777965
Training tokenizer:   8% 672/8047 [53:20<9:48:33,  4.79s/it]loss_total_epoch 25.797287510707974
Training tokenizer:   8% 673/8047 [53:25<9:47:16,  4.78s/it]loss_total_epoch 25.838071489706635
Training tokenizer:   8% 674/8047 [53:29<9:46:27,  4.77s/it]loss_total_epoch 25.87898770906031
Training tokenizer:   8% 675/8047 [53:34<9:44:10,  4.75s/it]loss_total_epoch 25.910531098023057
Training tokenizer:   8% 676/8047 [53:39<9:45:43,  4.77s/it]loss_total_epoch 25.947752403095365
Training tokenizer:   8% 677/8047 [53:44<9:46:31,  4.77s/it]loss_total_epoch 25.975178819149733
Training tokenizer:   8% 678/8047 [53:49<9:44:59,  4.76s/it]loss_total_epoch 26.00884737074375
Training tokenizer:   8% 679/8047 [53:53<9:45:25,  4.77s/it]loss_total_epoch 26.039406709372997
Training tokenizer:   8% 680/8047 [53:58<9:45:45,  4.77s/it]loss_total_epoch 26.079748518764973
Training tokenizer:   8% 681/8047 [54:03<9:48:03,  4.79s/it]loss_total_epoch 26.117004316300154
Training tokenizer:   8% 682/8047 [54:08<9:46:59,  4.78s/it]loss_total_epoch 26.161785639822483
Training tokenizer:   8% 683/8047 [54:12<9:46:44,  4.78s/it]loss_total_epoch 26.203152738511562
Training tokenizer:   9% 684/8047 [54:17<9:46:13,  4.78s/it]loss_total_epoch 26.245478462427855
Training tokenizer:   9% 685/8047 [54:22<9:45:09,  4.77s/it]loss_total_epoch 26.290229208767414
Training tokenizer:   9% 686/8047 [54:27<9:44:13,  4.76s/it]loss_total_epoch 26.32715095579624
Training tokenizer:   9% 687/8047 [54:32<9:46:52,  4.78s/it]loss_total_epoch 26.35950441658497
Training tokenizer:   9% 688/8047 [54:36<9:46:28,  4.78s/it]loss_total_epoch 26.394757945090532
Training tokenizer:   9% 689/8047 [54:41<9:46:19,  4.78s/it]loss_total_epoch 26.435984801501036
Training tokenizer:   9% 690/8047 [54:46<9:50:04,  4.81s/it]loss_total_epoch 26.463961511850357
Training tokenizer:   9% 691/8047 [54:51<9:46:53,  4.79s/it]loss_total_epoch 26.50479406863451
Training tokenizer:   9% 692/8047 [54:55<9:45:29,  4.78s/it]loss_total_epoch 26.545048989355564
Training tokenizer:   9% 693/8047 [55:00<9:45:22,  4.78s/it]loss_total_epoch 26.582643691450357
Training tokenizer:   9% 694/8047 [55:05<9:44:42,  4.77s/it]loss_total_epoch 26.623666174709797
Training tokenizer:   9% 695/8047 [55:10<9:41:45,  4.75s/it]loss_total_epoch 26.650286186486483
Training tokenizer:   9% 696/8047 [55:14<9:42:10,  4.75s/it]loss_total_epoch 26.6839771643281
Training tokenizer:   9% 697/8047 [55:19<9:43:58,  4.77s/it]loss_total_epoch 26.724108662456274
Training tokenizer:   9% 698/8047 [55:24<9:45:25,  4.78s/it]loss_total_epoch 26.76862847432494
Training tokenizer:   9% 699/8047 [55:29<9:45:50,  4.78s/it]loss_total_epoch 26.807035598903894
Training tokenizer:   9% 700/8047 [55:34<9:44:28,  4.77s/it]loss_total_epoch 26.849523682147264
Training tokenizer:   9% 701/8047 [55:38<9:42:43,  4.76s/it]loss_total_epoch 26.88839103281498
Training tokenizer:   9% 702/8047 [55:43<9:44:50,  4.78s/it]loss_total_epoch 26.928268384188414
Training tokenizer:   9% 703/8047 [55:48<9:44:16,  4.77s/it]loss_total_epoch 26.964384868741035
Training tokenizer:   9% 704/8047 [55:53<9:44:25,  4.78s/it]loss_total_epoch 27.00069509446621
Training tokenizer:   9% 705/8047 [55:57<9:44:27,  4.78s/it]loss_total_epoch 27.034875627607107
Training tokenizer:   9% 706/8047 [56:02<9:44:25,  4.78s/it]loss_total_epoch 27.077736422419548
Training tokenizer:   9% 707/8047 [56:07<9:46:05,  4.79s/it]loss_total_epoch 27.115579813718796
Training tokenizer:   9% 708/8047 [56:12<9:43:33,  4.77s/it]loss_total_epoch 27.154169529676437
Training tokenizer:   9% 709/8047 [56:17<9:43:58,  4.77s/it]loss_total_epoch 27.18739202618599
Training tokenizer:   9% 710/8047 [56:21<9:43:29,  4.77s/it]loss_total_epoch 27.221132595092058
Training tokenizer:   9% 711/8047 [56:26<9:43:04,  4.77s/it]loss_total_epoch 27.250925924628973
Training tokenizer:   9% 712/8047 [56:31<9:44:45,  4.78s/it]loss_total_epoch 27.28572103753686
Training tokenizer:   9% 713/8047 [56:36<9:43:14,  4.77s/it]loss_total_epoch 27.320705123245716
Training tokenizer:   9% 714/8047 [56:40<9:43:34,  4.77s/it]loss_total_epoch 27.34957590326667
Training tokenizer:   9% 715/8047 [56:45<9:44:53,  4.79s/it]loss_total_epoch 27.378895793110132
Training tokenizer:   9% 716/8047 [56:50<9:44:18,  4.78s/it]loss_total_epoch 27.405998315662146
Training tokenizer:   9% 717/8047 [56:55<9:44:08,  4.78s/it]loss_total_epoch 27.439210921525955
Training tokenizer:   9% 718/8047 [57:00<9:43:46,  4.78s/it]loss_total_epoch 27.481255501508713
Training tokenizer:   9% 719/8047 [57:04<9:41:37,  4.76s/it]loss_total_epoch 27.516942020505667
Training tokenizer:   9% 720/8047 [57:09<9:42:30,  4.77s/it]loss_total_epoch 27.55050139501691
Training tokenizer:   9% 721/8047 [57:14<9:39:12,  4.74s/it]loss_total_epoch 27.58537795767188
Training tokenizer:   9% 722/8047 [57:18<9:34:07,  4.70s/it]loss_total_epoch 27.623650547116995
Training tokenizer:   9% 723/8047 [57:23<9:39:14,  4.75s/it]loss_total_epoch 27.661455392837524
Training tokenizer:   9% 724/8047 [57:28<9:41:33,  4.76s/it]loss_total_epoch 27.706030409783125
Training tokenizer:   9% 725/8047 [57:33<9:42:20,  4.77s/it]loss_total_epoch 27.73987740650773
Training tokenizer:   9% 726/8047 [57:38<9:42:08,  4.77s/it]loss_total_epoch 27.785189285874367
Training tokenizer:   9% 727/8047 [57:42<9:41:43,  4.77s/it]loss_total_epoch 27.829274207353592
Training tokenizer:   9% 728/8047 [57:47<9:42:08,  4.77s/it]loss_total_epoch 27.864578790962696
Training tokenizer:   9% 729/8047 [57:52<9:42:53,  4.78s/it]loss_total_epoch 27.890160016715527
Training tokenizer:   9% 730/8047 [57:57<9:41:14,  4.77s/it]loss_total_epoch 27.92730449140072
Training tokenizer:   9% 731/8047 [58:01<9:40:18,  4.76s/it]loss_total_epoch 27.96369255706668
Training tokenizer:   9% 732/8047 [58:06<9:39:58,  4.76s/it]loss_total_epoch 28.007726833224297
Training tokenizer:   9% 733/8047 [58:11<9:38:28,  4.75s/it]loss_total_epoch 28.04097381234169
Training tokenizer:   9% 734/8047 [58:16<9:40:00,  4.76s/it]loss_total_epoch 28.075618237257004
Training tokenizer:   9% 735/8047 [58:20<9:39:43,  4.76s/it]loss_total_epoch 28.113932128995657
Training tokenizer:   9% 736/8047 [58:25<9:38:46,  4.75s/it]loss_total_epoch 28.139291727915406
Training tokenizer:   9% 737/8047 [58:30<9:39:03,  4.75s/it]loss_total_epoch 28.174252847209573
Training tokenizer:   9% 738/8047 [58:35<9:37:27,  4.74s/it]loss_total_epoch 28.218800211325288
Training tokenizer:   9% 739/8047 [58:40<9:41:46,  4.78s/it]loss_total_epoch 28.25284525565803
Training tokenizer:   9% 740/8047 [58:44<9:41:50,  4.78s/it]loss_total_epoch 28.290808523073792
Training tokenizer:   9% 741/8047 [58:49<9:38:53,  4.75s/it]loss_total_epoch 28.32584716938436
Training tokenizer:   9% 742/8047 [58:54<9:40:20,  4.77s/it]loss_total_epoch 28.358932895585895
Training tokenizer:   9% 743/8047 [58:59<9:40:10,  4.77s/it]loss_total_epoch 28.4078227262944
Training tokenizer:   9% 744/8047 [59:03<9:40:17,  4.77s/it]loss_total_epoch 28.43354761786759
Training tokenizer:   9% 745/8047 [59:08<9:40:46,  4.77s/it]loss_total_epoch 28.462619703263044
Training tokenizer:   9% 746/8047 [59:13<9:40:37,  4.77s/it]loss_total_epoch 28.493507964536548
Training tokenizer:   9% 747/8047 [59:18<9:40:33,  4.77s/it]loss_total_epoch 28.531563187018037
Training tokenizer:   9% 748/8047 [59:22<9:38:22,  4.75s/it]loss_total_epoch 28.559682177379727
Training tokenizer:   9% 749/8047 [59:27<9:39:38,  4.77s/it]loss_total_epoch 28.58606314845383
Training tokenizer:   9% 750/8047 [59:32<9:40:28,  4.77s/it]loss_total_epoch 28.622286716476083
Training tokenizer:   9% 751/8047 [59:37<9:37:09,  4.75s/it]loss_total_epoch 28.66347892023623
Training tokenizer:   9% 752/8047 [59:41<9:38:07,  4.76s/it]loss_total_epoch 28.700360910966992
Training tokenizer:   9% 753/8047 [59:46<9:37:12,  4.75s/it]loss_total_epoch 28.743538798764348
Training tokenizer:   9% 754/8047 [59:51<9:37:20,  4.75s/it]loss_total_epoch 28.781259244307876
Training tokenizer:   9% 755/8047 [59:56<9:39:35,  4.77s/it]loss_total_epoch 28.814188396558166
Training tokenizer:   9% 756/8047 [1:00:00<9:39:21,  4.77s/it]loss_total_epoch 28.85459833405912
Training tokenizer:   9% 757/8047 [1:00:05<9:38:39,  4.76s/it]loss_total_epoch 28.894673028960824
Training tokenizer:   9% 758/8047 [1:00:10<9:39:23,  4.77s/it]loss_total_epoch 28.930394204333425
Training tokenizer:   9% 759/8047 [1:00:15<9:38:44,  4.76s/it]loss_total_epoch 28.968904366716743
Training tokenizer:   9% 760/8047 [1:00:20<9:46:49,  4.83s/it]loss_total_epoch 29.008042516186833
Training tokenizer:   9% 761/8047 [1:00:25<9:44:43,  4.82s/it]loss_total_epoch 29.045447366312146
Training tokenizer:   9% 762/8047 [1:00:29<9:43:59,  4.81s/it]loss_total_epoch 29.086211314424872
Training tokenizer:   9% 763/8047 [1:00:34<9:45:14,  4.82s/it]loss_total_epoch 29.12326486222446
Training tokenizer:   9% 764/8047 [1:00:39<9:44:19,  4.81s/it]loss_total_epoch 29.162691360339522
Training tokenizer:  10% 765/8047 [1:00:44<9:41:58,  4.80s/it]loss_total_epoch 29.199697313830256
Training tokenizer:  10% 766/8047 [1:00:48<9:40:40,  4.79s/it]loss_total_epoch 29.241683235391974
Training tokenizer:  10% 767/8047 [1:00:53<9:41:45,  4.79s/it]loss_total_epoch 29.2736461404711
Training tokenizer:  10% 768/8047 [1:00:58<9:40:28,  4.78s/it]loss_total_epoch 29.31592016480863
Training tokenizer:  10% 769/8047 [1:01:03<9:40:37,  4.79s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-9_bax63y'
loss_total_epoch 29.356170101091266
Training tokenizer:  10% 770/8047 [1:01:08<9:39:01,  4.77s/it]loss_total_epoch 29.39026994071901
Training tokenizer:  10% 771/8047 [1:01:12<9:36:59,  4.76s/it]loss_total_epoch 29.428157361224294
Training tokenizer:  10% 772/8047 [1:01:17<9:36:39,  4.76s/it]loss_total_epoch 29.464018726721406
Training tokenizer:  10% 773/8047 [1:01:22<9:34:20,  4.74s/it]loss_total_epoch 29.49269206263125
Training tokenizer:  10% 774/8047 [1:01:27<9:34:06,  4.74s/it]loss_total_epoch 29.53291125781834
Training tokenizer:  10% 775/8047 [1:01:31<9:35:28,  4.75s/it]loss_total_epoch 29.568507386371493
Training tokenizer:  10% 776/8047 [1:01:36<9:34:54,  4.74s/it]loss_total_epoch 29.60767068900168
Training tokenizer:  10% 777/8047 [1:01:41<9:34:10,  4.74s/it]loss_total_epoch 29.65626191906631
Training tokenizer:  10% 778/8047 [1:01:46<9:35:25,  4.75s/it]loss_total_epoch 29.694723362103105
Training tokenizer:  10% 779/8047 [1:01:50<9:36:34,  4.76s/it]loss_total_epoch 29.73541439138353
Training tokenizer:  10% 780/8047 [1:01:55<9:34:16,  4.74s/it]loss_total_epoch 29.77831311710179
Training tokenizer:  10% 781/8047 [1:02:00<9:41:31,  4.80s/it]loss_total_epoch 29.810552397742867
Training tokenizer:  10% 782/8047 [1:02:05<9:39:31,  4.79s/it]loss_total_epoch 29.843421818688512
Training tokenizer:  10% 783/8047 [1:02:09<9:38:07,  4.78s/it]loss_total_epoch 29.87947073020041
Training tokenizer:  10% 784/8047 [1:02:14<9:38:17,  4.78s/it]loss_total_epoch 29.916947411373258
Training tokenizer:  10% 785/8047 [1:02:19<9:37:32,  4.77s/it]loss_total_epoch 29.951055573299527
Training tokenizer:  10% 786/8047 [1:02:24<9:38:40,  4.78s/it]loss_total_epoch 29.98587146960199
Training tokenizer:  10% 787/8047 [1:02:29<9:38:45,  4.78s/it]loss_total_epoch 30.012530056759715
Training tokenizer:  10% 788/8047 [1:02:33<9:38:57,  4.79s/it]loss_total_epoch 30.04989149980247
Training tokenizer:  10% 789/8047 [1:02:38<9:38:46,  4.78s/it]loss_total_epoch 30.08363608829677
Training tokenizer:  10% 790/8047 [1:02:43<9:37:46,  4.78s/it]loss_total_epoch 30.122082287445664
Training tokenizer:  10% 791/8047 [1:02:48<9:38:10,  4.78s/it]loss_total_epoch 30.161166122183204
Training tokenizer:  10% 792/8047 [1:02:53<9:42:42,  4.82s/it]loss_total_epoch 30.2040574233979
Training tokenizer:  10% 793/8047 [1:02:57<9:39:57,  4.80s/it]loss_total_epoch 30.241215268149972
Training tokenizer:  10% 794/8047 [1:03:02<9:36:46,  4.77s/it]loss_total_epoch 30.275685327127576
Training tokenizer:  10% 795/8047 [1:03:07<9:37:11,  4.78s/it]loss_total_epoch 30.30961491726339
Training tokenizer:  10% 796/8047 [1:03:12<9:36:47,  4.77s/it]loss_total_epoch 30.338088044896722
Training tokenizer:  10% 797/8047 [1:03:16<9:36:30,  4.77s/it]loss_total_epoch 30.373161213472486
Training tokenizer:  10% 798/8047 [1:03:21<9:37:54,  4.78s/it]loss_total_epoch 30.41520526446402
Training tokenizer:  10% 799/8047 [1:03:26<9:38:39,  4.79s/it]loss_total_epoch 30.461104789748788
Training tokenizer:  10% 800/8047 [1:03:31<9:38:49,  4.79s/it]loss_total_epoch 30.496000299230218
Training tokenizer:  10% 801/8047 [1:03:36<9:37:06,  4.78s/it]loss_total_epoch 30.538331834599376
Training tokenizer:  10% 802/8047 [1:03:40<9:36:50,  4.78s/it]loss_total_epoch 30.568799521774054
Training tokenizer:  10% 803/8047 [1:03:45<9:35:41,  4.77s/it]loss_total_epoch 30.61262822151184
Training tokenizer:  10% 804/8047 [1:03:50<9:35:38,  4.77s/it]loss_total_epoch 30.65175648033619
Training tokenizer:  10% 805/8047 [1:03:55<9:35:59,  4.77s/it]loss_total_epoch 30.692281533032656
Training tokenizer:  10% 806/8047 [1:03:59<9:37:00,  4.78s/it]loss_total_epoch 30.726509150117636
Training tokenizer:  10% 807/8047 [1:04:04<9:34:46,  4.76s/it]loss_total_epoch 30.755127642303705
Training tokenizer:  10% 808/8047 [1:04:09<9:33:30,  4.75s/it]loss_total_epoch 30.79335630312562
Training tokenizer:  10% 809/8047 [1:04:14<9:32:20,  4.74s/it]loss_total_epoch 30.825877126306295
Training tokenizer:  10% 810/8047 [1:04:18<9:31:40,  4.74s/it]loss_total_epoch 30.859122022986412
Training tokenizer:  10% 811/8047 [1:04:23<9:30:49,  4.73s/it]loss_total_epoch 30.89351077005267
Training tokenizer:  10% 812/8047 [1:04:28<9:32:53,  4.75s/it]loss_total_epoch 30.940419547259808
Training tokenizer:  10% 813/8047 [1:04:33<9:35:31,  4.77s/it]loss_total_epoch 30.971248392015696
Training tokenizer:  10% 814/8047 [1:04:37<9:34:40,  4.77s/it]loss_total_epoch 31.013225875794888
Training tokenizer:  10% 815/8047 [1:04:42<9:34:28,  4.77s/it]loss_total_epoch 31.049231614917517
Training tokenizer:  10% 816/8047 [1:04:47<9:34:43,  4.77s/it]loss_total_epoch 31.09593454375863
Training tokenizer:  10% 817/8047 [1:04:52<9:35:20,  4.77s/it]loss_total_epoch 31.13131618872285
Training tokenizer:  10% 818/8047 [1:04:56<9:34:09,  4.77s/it]loss_total_epoch 31.156478110700846
Training tokenizer:  10% 819/8047 [1:05:01<9:35:26,  4.78s/it]loss_total_epoch 31.195174265652895
Training tokenizer:  10% 820/8047 [1:05:06<9:35:08,  4.77s/it]loss_total_epoch 31.235244557261467
Training tokenizer:  10% 821/8047 [1:05:11<9:34:34,  4.77s/it]loss_total_epoch 31.27554766088724
Training tokenizer:  10% 822/8047 [1:05:16<9:34:28,  4.77s/it]loss_total_epoch 31.30895210057497
Training tokenizer:  10% 823/8047 [1:05:20<9:33:30,  4.76s/it]loss_total_epoch 31.353963412344456
Training tokenizer:  10% 824/8047 [1:05:25<9:35:10,  4.78s/it]loss_total_epoch 31.39209819212556
Training tokenizer:  10% 825/8047 [1:05:30<9:35:20,  4.78s/it]loss_total_epoch 31.426503863185644
Training tokenizer:  10% 826/8047 [1:05:35<9:35:47,  4.78s/it]loss_total_epoch 31.45370964333415
Training tokenizer:  10% 827/8047 [1:05:39<9:34:55,  4.78s/it]loss_total_epoch 31.493688851594925
Training tokenizer:  10% 828/8047 [1:05:44<9:36:49,  4.79s/it]loss_total_epoch 31.533060897141695
Training tokenizer:  10% 829/8047 [1:05:49<9:37:29,  4.80s/it]loss_total_epoch 31.570818107575178
Training tokenizer:  10% 830/8047 [1:05:54<9:36:06,  4.79s/it]loss_total_epoch 31.613586474210024
Training tokenizer:  10% 831/8047 [1:05:59<9:35:07,  4.78s/it]loss_total_epoch 31.653942186385393
Training tokenizer:  10% 832/8047 [1:06:03<9:35:21,  4.78s/it]loss_total_epoch 31.69226600602269
Training tokenizer:  10% 833/8047 [1:06:08<9:34:54,  4.78s/it]loss_total_epoch 31.728467114269733
Training tokenizer:  10% 834/8047 [1:06:13<9:33:37,  4.77s/it]loss_total_epoch 31.773245565593243
Training tokenizer:  10% 835/8047 [1:06:18<9:31:26,  4.75s/it]loss_total_epoch 31.82000556960702
Training tokenizer:  10% 836/8047 [1:06:23<9:33:51,  4.77s/it]loss_total_epoch 31.857824444770813
Training tokenizer:  10% 837/8047 [1:06:27<9:33:23,  4.77s/it]loss_total_epoch 31.890516564249992
Training tokenizer:  10% 838/8047 [1:06:32<9:32:50,  4.77s/it]loss_total_epoch 31.92684929072857
Training tokenizer:  10% 839/8047 [1:06:37<9:35:36,  4.79s/it]loss_total_epoch 31.96193888410926
Training tokenizer:  10% 840/8047 [1:06:42<9:33:23,  4.77s/it]loss_total_epoch 31.99620718881488
Training tokenizer:  10% 841/8047 [1:06:46<9:34:06,  4.78s/it]loss_total_epoch 32.036589685827494
Training tokenizer:  10% 842/8047 [1:06:51<9:34:54,  4.79s/it]loss_total_epoch 32.069252230226994
Training tokenizer:  10% 843/8047 [1:06:56<9:37:19,  4.81s/it]loss_total_epoch 32.096036246046424
Training tokenizer:  10% 844/8047 [1:07:01<9:36:42,  4.80s/it]loss_total_epoch 32.13465307466686
Training tokenizer:  11% 845/8047 [1:07:06<9:35:11,  4.79s/it]loss_total_epoch 32.17393711023033
Training tokenizer:  11% 846/8047 [1:07:10<9:28:51,  4.74s/it]loss_total_epoch 32.21142732538283
Training tokenizer:  11% 847/8047 [1:07:15<9:29:21,  4.74s/it]loss_total_epoch 32.24728553928435
Training tokenizer:  11% 848/8047 [1:07:20<9:26:32,  4.72s/it]loss_total_epoch 32.290402764454484
Training tokenizer:  11% 849/8047 [1:07:24<9:28:14,  4.74s/it]loss_total_epoch 32.32399493269622
Training tokenizer:  11% 850/8047 [1:07:29<9:24:48,  4.71s/it]loss_total_epoch 32.360926831141114
Training tokenizer:  11% 851/8047 [1:07:34<9:28:15,  4.74s/it]loss_total_epoch 32.404746590182185
Training tokenizer:  11% 852/8047 [1:07:39<9:28:30,  4.74s/it]loss_total_epoch 32.44455671869218
Training tokenizer:  11% 853/8047 [1:07:43<9:28:50,  4.74s/it]loss_total_epoch 32.4879053439945
Training tokenizer:  11% 854/8047 [1:07:48<9:27:36,  4.73s/it]loss_total_epoch 32.519487565383315
Training tokenizer:  11% 855/8047 [1:07:53<9:27:02,  4.73s/it]loss_total_epoch 32.55647932551801
Training tokenizer:  11% 856/8047 [1:07:58<9:27:55,  4.74s/it]loss_total_epoch 32.59049622155726
Training tokenizer:  11% 857/8047 [1:08:02<9:29:06,  4.75s/it]loss_total_epoch 32.62295090965927
Training tokenizer:  11% 858/8047 [1:08:07<9:28:48,  4.75s/it]loss_total_epoch 32.657625099644065
Training tokenizer:  11% 859/8047 [1:08:12<9:30:24,  4.76s/it]loss_total_epoch 32.6947264354676
Training tokenizer:  11% 860/8047 [1:08:17<9:29:09,  4.75s/it]loss_total_epoch 32.73629764281213
Training tokenizer:  11% 861/8047 [1:08:22<9:39:00,  4.83s/it]loss_total_epoch 32.770492432639
Training tokenizer:  11% 862/8047 [1:08:26<9:35:57,  4.81s/it]loss_total_epoch 32.815124629065394
Training tokenizer:  11% 863/8047 [1:08:31<9:34:12,  4.80s/it]loss_total_epoch 32.85940651409328
Training tokenizer:  11% 864/8047 [1:08:36<9:31:32,  4.77s/it]loss_total_epoch 32.89938513748348
Training tokenizer:  11% 865/8047 [1:08:41<9:32:35,  4.78s/it]loss_total_epoch 32.932002952322364
Training tokenizer:  11% 866/8047 [1:08:45<9:32:03,  4.78s/it]loss_total_epoch 32.967857195064425
Training tokenizer:  11% 867/8047 [1:08:50<9:31:10,  4.77s/it]loss_total_epoch 33.01558249257505
Training tokenizer:  11% 868/8047 [1:08:55<9:31:08,  4.77s/it]loss_total_epoch 33.06037157587707
Training tokenizer:  11% 869/8047 [1:09:00<9:31:43,  4.78s/it]loss_total_epoch 33.09183636121452
Training tokenizer:  11% 870/8047 [1:09:05<9:31:14,  4.78s/it]loss_total_epoch 33.12363207899034
Training tokenizer:  11% 871/8047 [1:09:09<9:30:34,  4.77s/it]loss_total_epoch 33.162143060937524
Training tokenizer:  11% 872/8047 [1:09:14<9:30:01,  4.77s/it]loss_total_epoch 33.19944362156093
Training tokenizer:  11% 873/8047 [1:09:19<9:29:28,  4.76s/it]loss_total_epoch 33.24330121092498
Training tokenizer:  11% 874/8047 [1:09:24<9:29:42,  4.77s/it]loss_total_epoch 33.27596555836499
Training tokenizer:  11% 875/8047 [1:09:29<9:40:38,  4.86s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-hgu_awd4'
loss_total_epoch 33.31263383664191
Training tokenizer:  11% 876/8047 [1:09:33<9:35:58,  4.82s/it]loss_total_epoch 33.34640519507229
Training tokenizer:  11% 877/8047 [1:09:38<9:34:47,  4.81s/it]loss_total_epoch 33.381075171753764
Training tokenizer:  11% 878/8047 [1:09:43<9:34:42,  4.81s/it]loss_total_epoch 33.42606902308762
Training tokenizer:  11% 879/8047 [1:09:48<9:31:01,  4.78s/it]loss_total_epoch 33.46700646542013
Training tokenizer:  11% 880/8047 [1:09:52<9:29:56,  4.77s/it]loss_total_epoch 33.51098017953336
Training tokenizer:  11% 881/8047 [1:09:57<9:30:27,  4.78s/it]loss_total_epoch 33.554077396169305
Training tokenizer:  11% 882/8047 [1:10:02<9:29:51,  4.77s/it]loss_total_epoch 33.59106700308621
Training tokenizer:  11% 883/8047 [1:10:07<9:30:21,  4.78s/it]loss_total_epoch 33.62671243958175
Training tokenizer:  11% 884/8047 [1:10:12<9:31:02,  4.78s/it]loss_total_epoch 33.67050560750067
Training tokenizer:  11% 885/8047 [1:10:16<9:30:01,  4.78s/it]loss_total_epoch 33.7043097037822
Training tokenizer:  11% 886/8047 [1:10:21<9:30:28,  4.78s/it]loss_total_epoch 33.741283940151334
Training tokenizer:  11% 887/8047 [1:10:26<9:30:02,  4.78s/it]loss_total_epoch 33.778451999649405
Training tokenizer:  11% 888/8047 [1:10:31<9:29:34,  4.77s/it]loss_total_epoch 33.8136433865875
Training tokenizer:  11% 889/8047 [1:10:35<9:29:37,  4.77s/it]loss_total_epoch 33.84608351998031
Training tokenizer:  11% 890/8047 [1:10:40<9:29:11,  4.77s/it]loss_total_epoch 33.87911041267216
Training tokenizer:  11% 891/8047 [1:10:45<9:28:01,  4.76s/it]loss_total_epoch 33.91882042773068
Training tokenizer:  11% 892/8047 [1:10:50<9:27:37,  4.76s/it]loss_total_epoch 33.94099038653076
Training tokenizer:  11% 893/8047 [1:10:55<9:30:11,  4.78s/it]loss_total_epoch 33.98156054876745
Training tokenizer:  11% 894/8047 [1:10:59<9:25:48,  4.75s/it]loss_total_epoch 34.0201231520623
Training tokenizer:  11% 895/8047 [1:11:04<9:27:06,  4.76s/it]loss_total_epoch 34.0456648748368
Training tokenizer:  11% 896/8047 [1:11:09<9:26:59,  4.76s/it]loss_total_epoch 34.075753999873996
Training tokenizer:  11% 897/8047 [1:11:13<9:26:02,  4.75s/it]loss_total_epoch 34.11619850061834
Training tokenizer:  11% 898/8047 [1:11:18<9:28:21,  4.77s/it]loss_total_epoch 34.1462155431509
Training tokenizer:  11% 899/8047 [1:11:23<9:28:45,  4.77s/it]loss_total_epoch 34.17846343293786
Training tokenizer:  11% 900/8047 [1:11:28<9:28:06,  4.77s/it]loss_total_epoch 34.215825110673904
Training tokenizer:  11% 901/8047 [1:11:33<9:27:01,  4.76s/it]loss_total_epoch 34.25645211711526
Training tokenizer:  11% 902/8047 [1:11:37<9:28:11,  4.77s/it]loss_total_epoch 34.29382536932826
Training tokenizer:  11% 903/8047 [1:11:42<9:28:14,  4.77s/it]loss_total_epoch 34.33161723986268
Training tokenizer:  11% 904/8047 [1:11:47<9:30:32,  4.79s/it]loss_total_epoch 34.374798495322466
Training tokenizer:  11% 905/8047 [1:11:52<9:27:58,  4.77s/it]loss_total_epoch 34.40880436822772
Training tokenizer:  11% 906/8047 [1:11:56<9:27:13,  4.77s/it]loss_total_epoch 34.45210852101445
Training tokenizer:  11% 907/8047 [1:12:01<9:28:53,  4.78s/it]loss_total_epoch 34.48938949406147
Training tokenizer:  11% 908/8047 [1:12:06<9:28:01,  4.77s/it]loss_total_epoch 34.52514807134867
Training tokenizer:  11% 909/8047 [1:12:11<9:26:55,  4.77s/it]loss_total_epoch 34.55482819490135
Training tokenizer:  11% 910/8047 [1:12:16<9:27:59,  4.78s/it]loss_total_epoch 34.59169272892177
Training tokenizer:  11% 911/8047 [1:12:20<9:30:40,  4.80s/it]loss_total_epoch 34.62687467224896
Training tokenizer:  11% 912/8047 [1:12:25<9:30:28,  4.80s/it]loss_total_epoch 34.67054763995111
Training tokenizer:  11% 913/8047 [1:12:30<9:30:00,  4.79s/it]loss_total_epoch 34.70679328776896
Training tokenizer:  11% 914/8047 [1:12:35<9:29:47,  4.79s/it]loss_total_epoch 34.75290266983211
Training tokenizer:  11% 915/8047 [1:12:40<9:30:04,  4.80s/it]loss_total_epoch 34.78664727322757
Training tokenizer:  11% 916/8047 [1:12:44<9:28:56,  4.79s/it]loss_total_epoch 34.81963471509516
Training tokenizer:  11% 917/8047 [1:12:49<9:31:19,  4.81s/it]loss_total_epoch 34.85199670307338
Training tokenizer:  11% 918/8047 [1:12:54<9:28:20,  4.78s/it]loss_total_epoch 34.88699804805219
Training tokenizer:  11% 919/8047 [1:12:59<9:26:23,  4.77s/it]loss_total_epoch 34.925172025337815
Training tokenizer:  11% 920/8047 [1:13:03<9:25:57,  4.76s/it]loss_total_epoch 34.96264190785587
Training tokenizer:  11% 921/8047 [1:13:08<9:26:05,  4.77s/it]loss_total_epoch 35.00059418566525
Training tokenizer:  11% 922/8047 [1:13:13<9:28:32,  4.79s/it]loss_total_epoch 35.04565523006022
Training tokenizer:  11% 923/8047 [1:13:18<9:26:55,  4.77s/it]loss_total_epoch 35.082149216905236
Training tokenizer:  11% 924/8047 [1:13:23<9:26:26,  4.77s/it]loss_total_epoch 35.122229965403676
Training tokenizer:  11% 925/8047 [1:13:27<9:27:33,  4.78s/it]loss_total_epoch 35.15957817994058
Training tokenizer:  12% 926/8047 [1:13:32<9:29:19,  4.80s/it]loss_total_epoch 35.20373966731131
Training tokenizer:  12% 927/8047 [1:13:37<9:26:59,  4.78s/it]loss_total_epoch 35.23335983790457
Training tokenizer:  12% 928/8047 [1:13:42<9:27:48,  4.79s/it]loss_total_epoch 35.27180987410247
Training tokenizer:  12% 929/8047 [1:13:47<9:28:39,  4.79s/it]loss_total_epoch 35.3060905802995
Training tokenizer:  12% 930/8047 [1:13:51<9:29:16,  4.80s/it]loss_total_epoch 35.35290784575045
Training tokenizer:  12% 931/8047 [1:13:56<9:28:17,  4.79s/it]loss_total_epoch 35.392722917720675
Training tokenizer:  12% 932/8047 [1:14:01<9:24:40,  4.76s/it]loss_total_epoch 35.42730959691107
Training tokenizer:  12% 933/8047 [1:14:06<9:26:27,  4.78s/it]loss_total_epoch 35.45511561073363
Training tokenizer:  12% 934/8047 [1:14:10<9:23:56,  4.76s/it]loss_total_epoch 35.494140500202775
Training tokenizer:  12% 935/8047 [1:14:15<9:23:42,  4.76s/it]loss_total_epoch 35.52639741264284
Training tokenizer:  12% 936/8047 [1:14:20<9:23:19,  4.75s/it]loss_total_epoch 35.57470268942416
Training tokenizer:  12% 937/8047 [1:14:25<9:22:31,  4.75s/it]loss_total_epoch 35.61754051409662
Training tokenizer:  12% 938/8047 [1:14:29<9:18:08,  4.71s/it]loss_total_epoch 35.660793559625745
Training tokenizer:  12% 939/8047 [1:14:34<9:22:04,  4.74s/it]loss_total_epoch 35.69643169827759
Training tokenizer:  12% 940/8047 [1:14:39<9:23:42,  4.76s/it]loss_total_epoch 35.72903553210199
Training tokenizer:  12% 941/8047 [1:14:44<9:23:19,  4.76s/it]loss_total_epoch 35.75837633013725
Training tokenizer:  12% 942/8047 [1:14:48<9:26:02,  4.78s/it]loss_total_epoch 35.78916819393635
Training tokenizer:  12% 943/8047 [1:14:53<9:24:45,  4.77s/it]loss_total_epoch 35.81479392386973
Training tokenizer:  12% 944/8047 [1:14:58<9:26:54,  4.79s/it]loss_total_epoch 35.84617309458554
Training tokenizer:  12% 945/8047 [1:15:03<9:26:51,  4.79s/it]loss_total_epoch 35.88554526679218
Training tokenizer:  12% 946/8047 [1:15:08<9:26:13,  4.78s/it]loss_total_epoch 35.922459876164794
Training tokenizer:  12% 947/8047 [1:15:12<9:25:59,  4.78s/it]loss_total_epoch 35.958720644935966
Training tokenizer:  12% 948/8047 [1:15:17<9:26:15,  4.79s/it]loss_total_epoch 35.991443229839206
Training tokenizer:  12% 949/8047 [1:15:22<9:25:24,  4.78s/it]loss_total_epoch 36.02985875867307
Training tokenizer:  12% 950/8047 [1:15:27<9:25:15,  4.78s/it]loss_total_epoch 36.06703850440681
Training tokenizer:  12% 951/8047 [1:15:31<9:23:48,  4.77s/it]loss_total_epoch 36.10843134112656
Training tokenizer:  12% 952/8047 [1:15:36<9:22:12,  4.75s/it]loss_total_epoch 36.150638142600656
Training tokenizer:  12% 953/8047 [1:15:41<9:30:02,  4.82s/it]loss_total_epoch 36.19245524518192
Training tokenizer:  12% 954/8047 [1:15:46<9:28:06,  4.81s/it]loss_total_epoch 36.233930418267846
Training tokenizer:  12% 955/8047 [1:15:51<9:24:16,  4.77s/it]loss_total_epoch 36.269265139475465
Training tokenizer:  12% 956/8047 [1:15:55<9:22:55,  4.76s/it]loss_total_epoch 36.300606643781066
Training tokenizer:  12% 957/8047 [1:16:00<9:23:16,  4.77s/it]loss_total_epoch 36.34187341295183
Training tokenizer:  12% 958/8047 [1:16:05<9:23:52,  4.77s/it]loss_total_epoch 36.381905822083354
Training tokenizer:  12% 959/8047 [1:16:10<9:24:22,  4.78s/it]loss_total_epoch 36.41646868549287
Training tokenizer:  12% 960/8047 [1:16:14<9:24:54,  4.78s/it]loss_total_epoch 36.459190698340535
Training tokenizer:  12% 961/8047 [1:16:19<9:25:11,  4.79s/it]loss_total_epoch 36.50284324400127
Training tokenizer:  12% 962/8047 [1:16:24<9:27:19,  4.80s/it]loss_total_epoch 36.54261978901923
Training tokenizer:  12% 963/8047 [1:16:29<9:26:18,  4.80s/it]loss_total_epoch 36.591063337400556
Training tokenizer:  12% 964/8047 [1:16:34<9:27:02,  4.80s/it]loss_total_epoch 36.625685503706336
Training tokenizer:  12% 965/8047 [1:16:38<9:24:43,  4.78s/it]loss_total_epoch 36.66212445311248
Training tokenizer:  12% 966/8047 [1:16:43<9:25:15,  4.79s/it]loss_total_epoch 36.70766225270927
Training tokenizer:  12% 967/8047 [1:16:48<9:23:52,  4.78s/it]loss_total_epoch 36.7430098708719
Training tokenizer:  12% 968/8047 [1:16:53<9:22:31,  4.77s/it]loss_total_epoch 36.779732605442405
Training tokenizer:  12% 969/8047 [1:16:57<9:20:19,  4.75s/it]loss_total_epoch 36.812301153317094
Training tokenizer:  12% 970/8047 [1:17:02<9:22:25,  4.77s/it]loss_total_epoch 36.843400586396456
Training tokenizer:  12% 971/8047 [1:17:07<9:21:57,  4.77s/it]loss_total_epoch 36.88051611557603
Training tokenizer:  12% 972/8047 [1:17:12<9:20:51,  4.76s/it]loss_total_epoch 36.923076409846544
Training tokenizer:  12% 973/8047 [1:17:17<9:22:06,  4.77s/it]loss_total_epoch 36.96199204027653
Training tokenizer:  12% 974/8047 [1:17:21<9:21:08,  4.76s/it]loss_total_epoch 37.00458086282015
Training tokenizer:  12% 975/8047 [1:17:26<9:20:59,  4.76s/it]loss_total_epoch 37.04532443359494
Training tokenizer:  12% 976/8047 [1:17:31<9:21:28,  4.76s/it]loss_total_epoch 37.085245452821255
Training tokenizer:  12% 977/8047 [1:17:36<9:22:12,  4.77s/it]loss_total_epoch 37.118117701262236
Training tokenizer:  12% 978/8047 [1:17:40<9:23:17,  4.78s/it]loss_total_epoch 37.1577122323215
Training tokenizer:  12% 979/8047 [1:17:45<9:21:58,  4.77s/it]loss_total_epoch 37.189023178070784
Training tokenizer:  12% 980/8047 [1:17:50<9:21:18,  4.77s/it]loss_total_epoch 37.22876987606287
Training tokenizer:  12% 981/8047 [1:17:55<9:21:06,  4.76s/it]loss_total_epoch 37.264221493154764
Training tokenizer:  12% 982/8047 [1:17:59<9:17:58,  4.74s/it]loss_total_epoch 37.305014841258526
Training tokenizer:  12% 983/8047 [1:18:04<9:19:30,  4.75s/it]loss_total_epoch 37.338038612157106
Training tokenizer:  12% 984/8047 [1:18:09<9:21:02,  4.77s/it]loss_total_epoch 37.37754575163126
Training tokenizer:  12% 985/8047 [1:18:14<9:22:39,  4.78s/it]loss_total_epoch 37.41530165448785
Training tokenizer:  12% 986/8047 [1:18:19<9:22:18,  4.78s/it]loss_total_epoch 37.46161010861397
Training tokenizer:  12% 987/8047 [1:18:23<9:22:00,  4.78s/it]loss_total_epoch 37.49042052030563
Training tokenizer:  12% 988/8047 [1:18:28<9:20:26,  4.76s/it]loss_total_epoch 37.52437711134553
Training tokenizer:  12% 989/8047 [1:18:33<9:21:37,  4.77s/it]loss_total_epoch 37.570015739649534
Training tokenizer:  12% 990/8047 [1:18:38<9:20:29,  4.77s/it]loss_total_epoch 37.60805142298341
Training tokenizer:  12% 991/8047 [1:18:42<9:20:48,  4.77s/it]loss_total_epoch 37.642995085567236
Training tokenizer:  12% 992/8047 [1:18:47<9:21:32,  4.78s/it]loss_total_epoch 37.677414298057556
Training tokenizer:  12% 993/8047 [1:18:52<9:20:46,  4.77s/it]loss_total_epoch 37.718524646013975
Training tokenizer:  12% 994/8047 [1:18:57<9:21:10,  4.77s/it]loss_total_epoch 37.752393160015345
Training tokenizer:  12% 995/8047 [1:19:01<9:21:19,  4.78s/it]loss_total_epoch 37.79471017792821
Training tokenizer:  12% 996/8047 [1:19:06<9:22:17,  4.78s/it]loss_total_epoch 37.83668049424887
Training tokenizer:  12% 997/8047 [1:19:11<9:21:07,  4.78s/it]loss_total_epoch 37.875777918845415
Training tokenizer:  12% 998/8047 [1:19:16<9:19:04,  4.76s/it]loss_total_epoch 37.9166389927268
Training tokenizer:  12% 999/8047 [1:19:21<9:22:19,  4.79s/it]loss_total_epoch 37.949619214981794
Training tokenizer:  12% 1000/8047 [1:19:25<9:20:18,  4.77s/it]loss_total_epoch 37.97712111286819
Training tokenizer:  12% 1001/8047 [1:19:30<9:20:38,  4.77s/it]loss_total_epoch 38.00938104279339
Training tokenizer:  12% 1002/8047 [1:19:35<9:15:52,  4.73s/it]loss_total_epoch 38.05038410611451
Training tokenizer:  12% 1003/8047 [1:19:39<9:15:56,  4.74s/it]loss_total_epoch 38.08841700665653
Training tokenizer:  12% 1004/8047 [1:19:44<9:15:48,  4.73s/it]loss_total_epoch 38.11956415325403
Training tokenizer:  12% 1005/8047 [1:19:49<9:15:15,  4.73s/it]loss_total_epoch 38.16339370980859
Training tokenizer:  13% 1006/8047 [1:19:54<9:17:35,  4.75s/it]loss_total_epoch 38.193619476631284
Training tokenizer:  13% 1007/8047 [1:19:58<9:16:07,  4.74s/it]loss_total_epoch 38.23115544207394
Training tokenizer:  13% 1008/8047 [1:20:03<9:18:13,  4.76s/it]loss_total_epoch 38.271596094593406
Training tokenizer:  13% 1009/8047 [1:20:08<9:20:29,  4.78s/it]loss_total_epoch 38.3130377586931
Training tokenizer:  13% 1010/8047 [1:20:13<9:18:50,  4.76s/it]loss_total_epoch 38.34910688363016
Training tokenizer:  13% 1011/8047 [1:20:18<9:19:09,  4.77s/it]loss_total_epoch 38.38462713547051
Training tokenizer:  13% 1012/8047 [1:20:22<9:18:42,  4.77s/it]loss_total_epoch 38.41625135578215
Training tokenizer:  13% 1013/8047 [1:20:27<9:19:20,  4.77s/it]loss_total_epoch 38.45344834588468
Training tokenizer:  13% 1014/8047 [1:20:32<9:19:41,  4.77s/it]loss_total_epoch 38.481640024110675
Training tokenizer:  13% 1015/8047 [1:20:37<9:17:52,  4.76s/it]loss_total_epoch 38.522436609491706
Training tokenizer:  13% 1016/8047 [1:20:41<9:18:04,  4.76s/it]loss_total_epoch 38.553621316328645
Training tokenizer:  13% 1017/8047 [1:20:46<9:18:24,  4.77s/it]loss_total_epoch 38.59475609846413
Training tokenizer:  13% 1018/8047 [1:20:51<9:17:33,  4.76s/it]loss_total_epoch 38.63468222878873
Training tokenizer:  13% 1019/8047 [1:20:56<9:17:25,  4.76s/it]loss_total_epoch 38.66342330724001
Training tokenizer:  13% 1020/8047 [1:21:00<9:16:15,  4.75s/it]loss_total_epoch 38.696204364299774
Training tokenizer:  13% 1021/8047 [1:21:05<9:17:34,  4.76s/it]loss_total_epoch 38.72690669260919
Training tokenizer:  13% 1022/8047 [1:21:10<9:18:42,  4.77s/it]loss_total_epoch 38.76403762958944
Training tokenizer:  13% 1023/8047 [1:21:15<9:17:30,  4.76s/it]loss_total_epoch 38.812366826459765
Training tokenizer:  13% 1024/8047 [1:21:19<9:09:57,  4.70s/it]loss_total_epoch 38.84186593256891
Training tokenizer:  13% 1025/8047 [1:21:24<9:12:55,  4.72s/it]loss_total_epoch 38.878440694883466
Training tokenizer:  13% 1026/8047 [1:21:29<9:12:33,  4.72s/it]loss_total_epoch 38.916764510795474
Training tokenizer:  13% 1027/8047 [1:21:34<9:13:08,  4.73s/it]loss_total_epoch 38.94637947715819
Training tokenizer:  13% 1028/8047 [1:21:38<9:16:14,  4.75s/it]loss_total_epoch 38.98706081695855
Training tokenizer:  13% 1029/8047 [1:21:43<9:17:02,  4.76s/it]loss_total_epoch 39.022438110783696
Training tokenizer:  13% 1030/8047 [1:21:48<9:17:28,  4.77s/it]loss_total_epoch 39.06230611540377
Training tokenizer:  13% 1031/8047 [1:21:53<9:18:09,  4.77s/it]loss_total_epoch 39.09338370338082
Training tokenizer:  13% 1032/8047 [1:21:57<9:17:51,  4.77s/it]loss_total_epoch 39.123369976878166
Training tokenizer:  13% 1033/8047 [1:22:02<9:16:50,  4.76s/it]loss_total_epoch 39.160309728235006
Training tokenizer:  13% 1034/8047 [1:22:07<9:18:17,  4.78s/it]loss_total_epoch 39.19744563475251
Training tokenizer:  13% 1035/8047 [1:22:12<9:17:59,  4.77s/it]loss_total_epoch 39.2290217615664
Training tokenizer:  13% 1036/8047 [1:22:17<9:16:27,  4.76s/it]loss_total_epoch 39.2685286141932
Training tokenizer:  13% 1037/8047 [1:22:21<9:16:10,  4.76s/it]loss_total_epoch 39.299565602093935
Training tokenizer:  13% 1038/8047 [1:22:26<9:14:22,  4.75s/it]loss_total_epoch 39.34308775141835
Training tokenizer:  13% 1039/8047 [1:22:31<9:14:10,  4.74s/it]loss_total_epoch 39.38835080340505
Training tokenizer:  13% 1040/8047 [1:22:35<9:14:30,  4.75s/it]loss_total_epoch 39.42323139682412
Training tokenizer:  13% 1041/8047 [1:22:40<9:12:55,  4.74s/it]loss_total_epoch 39.45620270073414
Training tokenizer:  13% 1042/8047 [1:22:45<9:14:33,  4.75s/it]loss_total_epoch 39.49304174259305
Training tokenizer:  13% 1043/8047 [1:22:50<9:14:47,  4.75s/it]loss_total_epoch 39.53392381593585
Training tokenizer:  13% 1044/8047 [1:22:54<9:13:38,  4.74s/it]loss_total_epoch 39.567036252468824
Training tokenizer:  13% 1045/8047 [1:22:59<9:13:17,  4.74s/it]loss_total_epoch 39.60723089799285
Training tokenizer:  13% 1046/8047 [1:23:04<9:13:18,  4.74s/it]loss_total_epoch 39.64495037123561
Training tokenizer:  13% 1047/8047 [1:23:09<9:15:18,  4.76s/it]loss_total_epoch 39.67667004093528
Training tokenizer:  13% 1048/8047 [1:23:14<9:16:44,  4.77s/it]loss_total_epoch 39.718478336930275
Training tokenizer:  13% 1049/8047 [1:23:18<9:16:56,  4.78s/it]loss_total_epoch 39.758551098406315
Training tokenizer:  13% 1050/8047 [1:23:23<9:16:30,  4.77s/it]loss_total_epoch 39.788491044193506
Training tokenizer:  13% 1051/8047 [1:23:28<9:17:45,  4.78s/it]loss_total_epoch 39.832483533769846
Training tokenizer:  13% 1052/8047 [1:23:33<9:19:14,  4.80s/it]loss_total_epoch 39.86152705177665
Training tokenizer:  13% 1053/8047 [1:23:38<9:18:48,  4.79s/it]loss_total_epoch 39.89524035900831
Training tokenizer:  13% 1054/8047 [1:23:42<9:19:38,  4.80s/it]loss_total_epoch 39.928217779845
Training tokenizer:  13% 1055/8047 [1:23:47<9:18:10,  4.79s/it]loss_total_epoch 39.96772527322173
Training tokenizer:  13% 1056/8047 [1:23:52<9:17:57,  4.79s/it]loss_total_epoch 40.01119572669268
Training tokenizer:  13% 1057/8047 [1:23:57<9:17:54,  4.79s/it]loss_total_epoch 40.05235796794295
Training tokenizer:  13% 1058/8047 [1:24:01<9:16:34,  4.78s/it]loss_total_epoch 40.09011946618557
Training tokenizer:  13% 1059/8047 [1:24:06<9:16:34,  4.78s/it]loss_total_epoch 40.12492457032204
Training tokenizer:  13% 1060/8047 [1:24:11<9:17:12,  4.78s/it]loss_total_epoch 40.15202770009637
Training tokenizer:  13% 1061/8047 [1:24:16<9:16:26,  4.78s/it]loss_total_epoch 40.19203166663647
Training tokenizer:  13% 1062/8047 [1:24:21<9:17:07,  4.79s/it]loss_total_epoch 40.22796382009983
Training tokenizer:  13% 1063/8047 [1:24:25<9:13:57,  4.76s/it]loss_total_epoch 40.26582120358944
Training tokenizer:  13% 1064/8047 [1:24:30<9:12:52,  4.75s/it]loss_total_epoch 40.301564540714025
Training tokenizer:  13% 1065/8047 [1:24:35<9:14:55,  4.77s/it]loss_total_epoch 40.34032138064504
Training tokenizer:  13% 1066/8047 [1:24:40<9:15:24,  4.77s/it]loss_total_epoch 40.3750996440649
Training tokenizer:  13% 1067/8047 [1:24:44<9:14:24,  4.77s/it]loss_total_epoch 40.40578071773052
Training tokenizer:  13% 1068/8047 [1:24:49<9:14:19,  4.77s/it]loss_total_epoch 40.43997676670551
Training tokenizer:  13% 1069/8047 [1:24:54<9:14:33,  4.77s/it]loss_total_epoch 40.470942536368966
Training tokenizer:  13% 1070/8047 [1:24:59<9:15:16,  4.78s/it]loss_total_epoch 40.518381444737315
Training tokenizer:  13% 1071/8047 [1:25:03<9:13:56,  4.76s/it]loss_total_epoch 40.553543196991086
Training tokenizer:  13% 1072/8047 [1:25:08<9:15:01,  4.77s/it]loss_total_epoch 40.5830798689276
Training tokenizer:  13% 1073/8047 [1:25:13<9:14:35,  4.77s/it]loss_total_epoch 40.617112865671515
Training tokenizer:  13% 1074/8047 [1:25:18<9:13:52,  4.77s/it]loss_total_epoch 40.661940364167094
Training tokenizer:  13% 1075/8047 [1:25:22<9:11:58,  4.75s/it]loss_total_epoch 40.69859862886369
Training tokenizer:  13% 1076/8047 [1:25:27<9:12:12,  4.75s/it]loss_total_epoch 40.73195552267134
Training tokenizer:  13% 1077/8047 [1:25:32<9:11:34,  4.75s/it]loss_total_epoch 40.76975831948221
Training tokenizer:  13% 1078/8047 [1:25:37<9:11:52,  4.75s/it]loss_total_epoch 40.8088685143739
Training tokenizer:  13% 1079/8047 [1:25:41<9:10:53,  4.74s/it]loss_total_epoch 40.851500334218144
Training tokenizer:  13% 1080/8047 [1:25:46<9:10:41,  4.74s/it]loss_total_epoch 40.89639280177653
Training tokenizer:  13% 1081/8047 [1:25:51<9:12:07,  4.76s/it]loss_total_epoch 40.93035087175667
Training tokenizer:  13% 1082/8047 [1:25:56<9:11:36,  4.75s/it]loss_total_epoch 40.96146186068654
Training tokenizer:  13% 1083/8047 [1:26:00<9:12:04,  4.76s/it]loss_total_epoch 40.99949859455228
Training tokenizer:  13% 1084/8047 [1:26:05<9:13:15,  4.77s/it]loss_total_epoch 41.03080825507641
Training tokenizer:  13% 1085/8047 [1:26:10<9:14:11,  4.78s/it]loss_total_epoch 41.07027402892709
Training tokenizer:  13% 1086/8047 [1:26:15<9:13:31,  4.77s/it]loss_total_epoch 41.102520279586315
Training tokenizer:  14% 1087/8047 [1:26:20<9:13:40,  4.77s/it]loss_total_epoch 41.13539623096585
Training tokenizer:  14% 1088/8047 [1:26:24<9:13:22,  4.77s/it]loss_total_epoch 41.17473344132304
Training tokenizer:  14% 1089/8047 [1:26:29<9:14:21,  4.78s/it]loss_total_epoch 41.20946563780308
Training tokenizer:  14% 1090/8047 [1:26:34<9:14:59,  4.79s/it]loss_total_epoch 41.246918093413115
Training tokenizer:  14% 1091/8047 [1:26:39<9:17:03,  4.81s/it]loss_total_epoch 41.28444852307439
Training tokenizer:  14% 1092/8047 [1:26:44<9:16:01,  4.80s/it]loss_total_epoch 41.32440385594964
Training tokenizer:  14% 1093/8047 [1:26:48<9:14:26,  4.78s/it]loss_total_epoch 41.35958041623235
Training tokenizer:  14% 1094/8047 [1:26:53<9:13:40,  4.78s/it]loss_total_epoch 41.39534189924598
Training tokenizer:  14% 1095/8047 [1:26:58<9:12:58,  4.77s/it]loss_total_epoch 41.431644301861525
Training tokenizer:  14% 1096/8047 [1:27:03<9:11:45,  4.76s/it]loss_total_epoch 41.469160694628954
Training tokenizer:  14% 1097/8047 [1:27:07<9:09:45,  4.75s/it]loss_total_epoch 41.50832674279809
Training tokenizer:  14% 1098/8047 [1:27:12<9:10:25,  4.75s/it]loss_total_epoch 41.53533042781055
Training tokenizer:  14% 1099/8047 [1:27:17<9:12:22,  4.77s/it]loss_total_epoch 41.5776360463351
Training tokenizer:  14% 1100/8047 [1:27:22<9:12:37,  4.77s/it]loss_total_epoch 41.62933243252337
Training tokenizer:  14% 1101/8047 [1:27:26<9:12:11,  4.77s/it]loss_total_epoch 41.660393729805946
Training tokenizer:  14% 1102/8047 [1:27:31<9:12:27,  4.77s/it]loss_total_epoch 41.696657463908195
Training tokenizer:  14% 1103/8047 [1:27:36<9:13:13,  4.78s/it]loss_total_epoch 41.732604805380106
Training tokenizer:  14% 1104/8047 [1:27:41<9:11:57,  4.77s/it]loss_total_epoch 41.771486449986696
Training tokenizer:  14% 1105/8047 [1:27:46<9:12:19,  4.77s/it]loss_total_epoch 41.80490739643574
Training tokenizer:  14% 1106/8047 [1:27:50<9:12:47,  4.78s/it]loss_total_epoch 41.837062396109104
Training tokenizer:  14% 1107/8047 [1:27:55<9:13:22,  4.78s/it]loss_total_epoch 41.87156022340059
Training tokenizer:  14% 1108/8047 [1:28:00<9:12:54,  4.78s/it]loss_total_epoch 41.9101064838469
Training tokenizer:  14% 1109/8047 [1:28:05<9:08:00,  4.74s/it]loss_total_epoch 41.95316994562745
Training tokenizer:  14% 1110/8047 [1:28:09<9:07:15,  4.73s/it]loss_total_epoch 41.988359060138464
Training tokenizer:  14% 1111/8047 [1:28:14<9:09:08,  4.75s/it]loss_total_epoch 42.019617296755314
Training tokenizer:  14% 1112/8047 [1:28:19<9:10:45,  4.76s/it]loss_total_epoch 42.05300509557128
Training tokenizer:  14% 1113/8047 [1:28:24<9:09:11,  4.75s/it]loss_total_epoch 42.087897434830666
Training tokenizer:  14% 1114/8047 [1:28:28<9:08:05,  4.74s/it]loss_total_epoch 42.126463420689106
Training tokenizer:  14% 1115/8047 [1:28:33<9:07:28,  4.74s/it]loss_total_epoch 42.161706898361444
Training tokenizer:  14% 1116/8047 [1:28:38<9:08:31,  4.75s/it]loss_total_epoch 42.18881416320801
Training tokenizer:  14% 1117/8047 [1:28:43<9:08:41,  4.75s/it]loss_total_epoch 42.227450385689735
Training tokenizer:  14% 1118/8047 [1:28:47<9:09:25,  4.76s/it]loss_total_epoch 42.26242299377918
Training tokenizer:  14% 1119/8047 [1:28:52<9:09:35,  4.76s/it]loss_total_epoch 42.307394467294216
Training tokenizer:  14% 1120/8047 [1:28:57<9:10:58,  4.77s/it]loss_total_epoch 42.348127476871014
Training tokenizer:  14% 1121/8047 [1:29:02<9:11:36,  4.78s/it]loss_total_epoch 42.38696753978729
Training tokenizer:  14% 1122/8047 [1:29:06<9:09:57,  4.76s/it]loss_total_epoch 42.423367604613304
Training tokenizer:  14% 1123/8047 [1:29:11<9:09:50,  4.76s/it]loss_total_epoch 42.45922829210758
Training tokenizer:  14% 1124/8047 [1:29:16<9:09:56,  4.77s/it]loss_total_epoch 42.50026120617986
Training tokenizer:  14% 1125/8047 [1:29:21<9:04:09,  4.72s/it]loss_total_epoch 42.534412171691656
Training tokenizer:  14% 1126/8047 [1:29:25<9:07:24,  4.75s/it]loss_total_epoch 42.58391731977463
Training tokenizer:  14% 1127/8047 [1:29:30<9:04:47,  4.72s/it]loss_total_epoch 42.62042102590203
Training tokenizer:  14% 1128/8047 [1:29:35<9:06:35,  4.74s/it]loss_total_epoch 42.66148417070508
Training tokenizer:  14% 1129/8047 [1:29:40<9:06:13,  4.74s/it]loss_total_epoch 42.69495712965727
Training tokenizer:  14% 1130/8047 [1:29:44<9:05:08,  4.73s/it]loss_total_epoch 42.735622473061085
Training tokenizer:  14% 1131/8047 [1:29:49<9:07:12,  4.75s/it]loss_total_epoch 42.77799067273736
Training tokenizer:  14% 1132/8047 [1:29:54<9:05:42,  4.73s/it]loss_total_epoch 42.80559026449919
Training tokenizer:  14% 1133/8047 [1:29:59<9:08:58,  4.76s/it]loss_total_epoch 42.83581321500242
Training tokenizer:  14% 1134/8047 [1:30:03<9:07:25,  4.75s/it]loss_total_epoch 42.872919438406825
Training tokenizer:  14% 1135/8047 [1:30:08<9:06:25,  4.74s/it]loss_total_epoch 42.903043892234564
Training tokenizer:  14% 1136/8047 [1:30:13<9:07:15,  4.75s/it]loss_total_epoch 42.93688840046525
Training tokenizer:  14% 1137/8047 [1:30:18<9:08:06,  4.76s/it]loss_total_epoch 42.971348740160465
Training tokenizer:  14% 1138/8047 [1:30:22<9:10:17,  4.78s/it]loss_total_epoch 43.00835097208619
Training tokenizer:  14% 1139/8047 [1:30:27<9:08:39,  4.77s/it]loss_total_epoch 43.04997968673706
Training tokenizer:  14% 1140/8047 [1:30:32<9:08:46,  4.77s/it]loss_total_epoch 43.087060175836086
Training tokenizer:  14% 1141/8047 [1:30:37<9:09:31,  4.77s/it]loss_total_epoch 43.12332176417112
Training tokenizer:  14% 1142/8047 [1:30:41<9:06:15,  4.75s/it]loss_total_epoch 43.16715893894434
Training tokenizer:  14% 1143/8047 [1:30:46<9:00:20,  4.70s/it]loss_total_epoch 43.19733198918402
Training tokenizer:  14% 1144/8047 [1:30:51<9:03:44,  4.73s/it]loss_total_epoch 43.23753163404763
Training tokenizer:  14% 1145/8047 [1:30:56<9:06:53,  4.75s/it]loss_total_epoch 43.27512831054628
Training tokenizer:  14% 1146/8047 [1:31:00<9:07:48,  4.76s/it]loss_total_epoch 43.31242856569588
Training tokenizer:  14% 1147/8047 [1:31:05<9:07:06,  4.76s/it]loss_total_epoch 43.339855590835214
Training tokenizer:  14% 1148/8047 [1:31:10<9:05:23,  4.74s/it]loss_total_epoch 43.38210345990956
Training tokenizer:  14% 1149/8047 [1:31:15<9:08:11,  4.77s/it]loss_total_epoch 43.41617070324719
Training tokenizer:  14% 1150/8047 [1:31:19<9:07:24,  4.76s/it]loss_total_epoch 43.467878265306354
Training tokenizer:  14% 1151/8047 [1:31:24<9:07:04,  4.76s/it]loss_total_epoch 43.505541594699025
Training tokenizer:  14% 1152/8047 [1:31:29<9:05:39,  4.75s/it]loss_total_epoch 43.54448148049414
Training tokenizer:  14% 1153/8047 [1:31:34<9:05:45,  4.75s/it]loss_total_epoch 43.57451710663736
Training tokenizer:  14% 1154/8047 [1:31:38<9:04:34,  4.74s/it]loss_total_epoch 43.600656010210514
Training tokenizer:  14% 1155/8047 [1:31:43<9:05:02,  4.75s/it]loss_total_epoch 43.63865082338452
Training tokenizer:  14% 1156/8047 [1:31:48<9:04:48,  4.74s/it]loss_total_epoch 43.671263452619314
Training tokenizer:  14% 1157/8047 [1:31:53<9:03:56,  4.74s/it]loss_total_epoch 43.72244795039296
Training tokenizer:  14% 1158/8047 [1:31:57<9:06:09,  4.76s/it]loss_total_epoch 43.761639304459095
Training tokenizer:  14% 1159/8047 [1:32:02<9:09:37,  4.79s/it]loss_total_epoch 43.80390003696084
Training tokenizer:  14% 1160/8047 [1:32:07<9:09:35,  4.79s/it]loss_total_epoch 43.84489934518933
Training tokenizer:  14% 1161/8047 [1:32:12<9:08:37,  4.78s/it]loss_total_epoch 43.87981440126896
Training tokenizer:  14% 1162/8047 [1:32:17<9:07:49,  4.77s/it]loss_total_epoch 43.90805651061237
Training tokenizer:  14% 1163/8047 [1:32:21<9:09:29,  4.79s/it]loss_total_epoch 43.947030974552035
Training tokenizer:  14% 1164/8047 [1:32:26<9:11:01,  4.80s/it]loss_total_epoch 43.981226189062
Training tokenizer:  14% 1165/8047 [1:32:31<9:08:22,  4.78s/it]loss_total_epoch 44.01227046735585
Training tokenizer:  14% 1166/8047 [1:32:36<9:06:09,  4.76s/it]loss_total_epoch 44.04436010681093
Training tokenizer:  15% 1167/8047 [1:32:40<9:06:16,  4.76s/it]loss_total_epoch 44.07811046950519
Training tokenizer:  15% 1168/8047 [1:32:45<9:06:01,  4.76s/it]loss_total_epoch 44.1175675932318
Training tokenizer:  15% 1169/8047 [1:32:50<9:07:13,  4.77s/it]loss_total_epoch 44.16278977878392
Training tokenizer:  15% 1170/8047 [1:32:55<9:05:54,  4.76s/it]loss_total_epoch 44.19885760731995
Training tokenizer:  15% 1171/8047 [1:32:59<9:06:00,  4.76s/it]loss_total_epoch 44.235920855775476
Training tokenizer:  15% 1172/8047 [1:33:04<9:08:00,  4.78s/it]loss_total_epoch 44.274133970960975
Training tokenizer:  15% 1173/8047 [1:33:09<9:07:23,  4.78s/it]loss_total_epoch 44.31712910346687
Training tokenizer:  15% 1174/8047 [1:33:14<9:07:14,  4.78s/it]loss_total_epoch 44.356233732774854
Training tokenizer:  15% 1175/8047 [1:33:19<9:08:42,  4.79s/it]loss_total_epoch 44.3966257404536
Training tokenizer:  15% 1176/8047 [1:33:23<9:07:54,  4.78s/it]loss_total_epoch 44.43698406778276
Training tokenizer:  15% 1177/8047 [1:33:28<9:09:01,  4.79s/it]loss_total_epoch 44.47347514145076
Training tokenizer:  15% 1178/8047 [1:33:33<9:08:05,  4.79s/it]loss_total_epoch 44.51791244186461
Training tokenizer:  15% 1179/8047 [1:33:38<9:06:31,  4.77s/it]loss_total_epoch 44.55142410285771
Training tokenizer:  15% 1180/8047 [1:33:42<9:04:48,  4.76s/it]loss_total_epoch 44.58419934846461
Training tokenizer:  15% 1181/8047 [1:33:47<9:04:15,  4.76s/it]loss_total_epoch 44.617774890735745
Training tokenizer:  15% 1182/8047 [1:33:52<9:04:58,  4.76s/it]loss_total_epoch 44.64982029981911
Training tokenizer:  15% 1183/8047 [1:33:57<9:05:08,  4.77s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-avfmf3ms'
loss_total_epoch 44.68200553022325
Training tokenizer:  15% 1184/8047 [1:34:02<9:05:03,  4.77s/it]loss_total_epoch 44.71819907985628
Training tokenizer:  15% 1185/8047 [1:34:06<9:04:17,  4.76s/it]loss_total_epoch 44.74744936451316
Training tokenizer:  15% 1186/8047 [1:34:11<9:04:37,  4.76s/it]loss_total_epoch 44.78431557863951
Training tokenizer:  15% 1187/8047 [1:34:16<9:04:37,  4.76s/it]loss_total_epoch 44.82737649604678
Training tokenizer:  15% 1188/8047 [1:34:21<9:03:38,  4.76s/it]loss_total_epoch 44.87144296616316
Training tokenizer:  15% 1189/8047 [1:34:25<9:01:29,  4.74s/it]loss_total_epoch 44.91602727025747
Training tokenizer:  15% 1190/8047 [1:34:30<9:03:28,  4.76s/it]loss_total_epoch 44.95749940723181
Training tokenizer:  15% 1191/8047 [1:34:35<9:03:04,  4.75s/it]loss_total_epoch 44.99625448510051
Training tokenizer:  15% 1192/8047 [1:34:40<9:03:23,  4.76s/it]loss_total_epoch 45.03108585253358
Training tokenizer:  15% 1193/8047 [1:34:44<9:04:54,  4.77s/it]loss_total_epoch 45.071468476206064
Training tokenizer:  15% 1194/8047 [1:34:49<9:04:07,  4.76s/it]loss_total_epoch 45.10601716116071
Training tokenizer:  15% 1195/8047 [1:34:54<9:03:08,  4.76s/it]loss_total_epoch 45.138681281358004
Training tokenizer:  15% 1196/8047 [1:34:59<9:04:57,  4.77s/it]loss_total_epoch 45.17177199572325
Training tokenizer:  15% 1197/8047 [1:35:03<9:06:00,  4.78s/it]loss_total_epoch 45.19744656607509
Training tokenizer:  15% 1198/8047 [1:35:08<9:06:39,  4.79s/it]loss_total_epoch 45.23542248830199
Training tokenizer:  15% 1199/8047 [1:35:13<9:06:16,  4.79s/it]loss_total_epoch 45.27546260878444
Training tokenizer:  15% 1200/8047 [1:35:18<9:05:35,  4.78s/it]loss_total_epoch 45.31527479737997
Training tokenizer:  15% 1201/8047 [1:35:23<9:06:16,  4.79s/it]loss_total_epoch 45.35449928417802
Training tokenizer:  15% 1202/8047 [1:35:27<9:04:06,  4.77s/it]loss_total_epoch 45.39082797244191
Training tokenizer:  15% 1203/8047 [1:35:32<9:04:22,  4.77s/it]loss_total_epoch 45.428236205130816
Training tokenizer:  15% 1204/8047 [1:35:37<9:04:09,  4.77s/it]loss_total_epoch 45.45857260003686
Training tokenizer:  15% 1205/8047 [1:35:42<9:04:52,  4.78s/it]loss_total_epoch 45.498043563216925
Training tokenizer:  15% 1206/8047 [1:35:46<9:02:58,  4.76s/it]loss_total_epoch 45.53998842090368
Training tokenizer:  15% 1207/8047 [1:35:51<9:03:49,  4.77s/it]loss_total_epoch 45.57388123124838
Training tokenizer:  15% 1208/8047 [1:35:56<9:09:43,  4.82s/it]loss_total_epoch 45.61395536735654
Training tokenizer:  15% 1209/8047 [1:36:01<9:06:55,  4.80s/it]loss_total_epoch 45.65109005197883
Training tokenizer:  15% 1210/8047 [1:36:06<9:04:54,  4.78s/it]loss_total_epoch 45.68292282894254
Training tokenizer:  15% 1211/8047 [1:36:10<9:05:05,  4.78s/it]loss_total_epoch 45.71717271581292
Training tokenizer:  15% 1212/8047 [1:36:15<9:04:40,  4.78s/it]loss_total_epoch 45.755112413316965
Training tokenizer:  15% 1213/8047 [1:36:20<9:03:57,  4.78s/it]loss_total_epoch 45.7969659678638
Training tokenizer:  15% 1214/8047 [1:36:25<9:03:13,  4.77s/it]loss_total_epoch 45.835857678204775
Training tokenizer:  15% 1215/8047 [1:36:29<9:01:48,  4.76s/it]loss_total_epoch 45.874121487140656
Training tokenizer:  15% 1216/8047 [1:36:34<9:03:45,  4.78s/it]loss_total_epoch 45.906985338777304
Training tokenizer:  15% 1217/8047 [1:36:39<9:02:02,  4.76s/it]loss_total_epoch 45.93749437481165
Training tokenizer:  15% 1218/8047 [1:36:44<9:01:41,  4.76s/it]loss_total_epoch 45.97892989218235
Training tokenizer:  15% 1219/8047 [1:36:49<9:03:35,  4.78s/it]loss_total_epoch 46.01666260883212
Training tokenizer:  15% 1220/8047 [1:36:53<9:01:57,  4.76s/it]loss_total_epoch 46.04870320856571
Training tokenizer:  15% 1221/8047 [1:36:58<9:00:08,  4.75s/it]loss_total_epoch 46.09310757368803
Training tokenizer:  15% 1222/8047 [1:37:03<8:59:38,  4.74s/it]loss_total_epoch 46.13839698955417
Training tokenizer:  15% 1223/8047 [1:37:08<9:01:23,  4.76s/it]loss_total_epoch 46.176934607326984
Training tokenizer:  15% 1224/8047 [1:37:12<9:00:01,  4.75s/it]loss_total_epoch 46.21474411338568
Training tokenizer:  15% 1225/8047 [1:37:17<9:00:31,  4.75s/it]loss_total_epoch 46.24877925589681
Training tokenizer:  15% 1226/8047 [1:37:22<9:02:24,  4.77s/it]loss_total_epoch 46.27871826849878
Training tokenizer:  15% 1227/8047 [1:37:27<9:03:53,  4.78s/it]loss_total_epoch 46.31503594852984
Training tokenizer:  15% 1228/8047 [1:37:31<9:02:37,  4.77s/it]loss_total_epoch 46.35389150492847
Training tokenizer:  15% 1229/8047 [1:37:36<9:03:20,  4.78s/it]loss_total_epoch 46.40241654776037
Training tokenizer:  15% 1230/8047 [1:37:41<8:57:15,  4.73s/it]loss_total_epoch 46.438105626031756
Training tokenizer:  15% 1231/8047 [1:37:46<8:57:46,  4.73s/it]loss_total_epoch 46.467965388670564
Training tokenizer:  15% 1232/8047 [1:37:50<8:57:25,  4.73s/it]loss_total_epoch 46.496281092986465
Training tokenizer:  15% 1233/8047 [1:37:55<8:58:58,  4.75s/it]loss_total_epoch 46.53495308943093
Training tokenizer:  15% 1234/8047 [1:38:00<8:58:18,  4.74s/it]loss_total_epoch 46.57851135917008
Training tokenizer:  15% 1235/8047 [1:38:05<8:59:12,  4.75s/it]loss_total_epoch 46.61053883843124
Training tokenizer:  15% 1236/8047 [1:38:09<9:01:22,  4.77s/it]loss_total_epoch 46.653957607224584
Training tokenizer:  15% 1237/8047 [1:38:14<9:01:00,  4.77s/it]loss_total_epoch 46.68779586814344
Training tokenizer:  15% 1238/8047 [1:38:19<9:00:12,  4.76s/it]loss_total_epoch 46.724805699661374
Training tokenizer:  15% 1239/8047 [1:38:24<8:58:49,  4.75s/it]loss_total_epoch 46.76776428334415
Training tokenizer:  15% 1240/8047 [1:38:28<8:58:57,  4.75s/it]loss_total_epoch 46.79955737479031
Training tokenizer:  15% 1241/8047 [1:38:33<9:00:36,  4.77s/it]loss_total_epoch 46.84483961947262
Training tokenizer:  15% 1242/8047 [1:38:38<9:00:31,  4.77s/it]loss_total_epoch 46.886164559051394
Training tokenizer:  15% 1243/8047 [1:38:43<9:02:07,  4.78s/it]loss_total_epoch 46.91929972730577
Training tokenizer:  15% 1244/8047 [1:38:48<9:01:57,  4.78s/it]loss_total_epoch 46.953155962750316
Training tokenizer:  15% 1245/8047 [1:38:52<9:04:02,  4.80s/it]loss_total_epoch 46.98897290416062
Training tokenizer:  15% 1246/8047 [1:38:57<9:01:09,  4.77s/it]loss_total_epoch 47.023292848840356
Training tokenizer:  15% 1247/8047 [1:39:02<9:02:23,  4.79s/it]loss_total_epoch 47.05272792652249
Training tokenizer:  16% 1248/8047 [1:39:07<9:02:06,  4.78s/it]loss_total_epoch 47.09328327700496
Training tokenizer:  16% 1249/8047 [1:39:11<9:01:24,  4.78s/it]loss_total_epoch 47.132597375661135
Training tokenizer:  16% 1250/8047 [1:39:16<9:00:59,  4.78s/it]loss_total_epoch 47.16664654016495
Training tokenizer:  16% 1251/8047 [1:39:21<9:01:56,  4.78s/it]loss_total_epoch 47.20125938951969
Training tokenizer:  16% 1252/8047 [1:39:26<9:03:46,  4.80s/it]loss_total_epoch 47.24524925649166
Training tokenizer:  16% 1253/8047 [1:39:31<9:01:20,  4.78s/it]loss_total_epoch 47.287317149341106
Training tokenizer:  16% 1254/8047 [1:39:35<9:02:49,  4.79s/it]loss_total_epoch 47.33688402175903
Training tokenizer:  16% 1255/8047 [1:39:40<9:01:08,  4.78s/it]loss_total_epoch 47.37447663769126
Training tokenizer:  16% 1256/8047 [1:39:45<9:00:43,  4.78s/it]loss_total_epoch 47.40116162598133
Training tokenizer:  16% 1257/8047 [1:39:50<9:01:00,  4.78s/it]loss_total_epoch 47.44167462736368
Training tokenizer:  16% 1258/8047 [1:39:55<9:01:06,  4.78s/it]loss_total_epoch 47.484449185431004
Training tokenizer:  16% 1259/8047 [1:39:59<9:01:32,  4.79s/it]loss_total_epoch 47.523300278931856
Training tokenizer:  16% 1260/8047 [1:40:04<9:04:21,  4.81s/it]loss_total_epoch 47.55350583046675
Training tokenizer:  16% 1261/8047 [1:40:09<9:03:13,  4.80s/it]loss_total_epoch 47.59053032100201
Training tokenizer:  16% 1262/8047 [1:40:14<9:02:04,  4.79s/it]loss_total_epoch 47.62737576663494
Training tokenizer:  16% 1263/8047 [1:40:18<8:59:40,  4.77s/it]loss_total_epoch 47.66787553578615
Training tokenizer:  16% 1264/8047 [1:40:23<8:58:05,  4.76s/it]loss_total_epoch 47.70256954059005
Training tokenizer:  16% 1265/8047 [1:40:28<8:57:53,  4.76s/it]loss_total_epoch 47.73038639687002
Training tokenizer:  16% 1266/8047 [1:40:33<8:57:57,  4.76s/it]loss_total_epoch 47.76223341934383
Training tokenizer:  16% 1267/8047 [1:40:37<8:58:57,  4.77s/it]loss_total_epoch 47.80113552697003
Training tokenizer:  16% 1268/8047 [1:40:42<8:59:23,  4.77s/it]loss_total_epoch 47.8359761107713
Training tokenizer:  16% 1269/8047 [1:40:47<8:57:26,  4.76s/it]loss_total_epoch 47.86930632404983
Training tokenizer:  16% 1270/8047 [1:40:52<9:00:57,  4.79s/it]loss_total_epoch 47.9058893378824
Training tokenizer:  16% 1271/8047 [1:40:57<9:00:45,  4.79s/it]loss_total_epoch 47.94879457168281
Training tokenizer:  16% 1272/8047 [1:41:02<9:03:57,  4.82s/it]loss_total_epoch 47.97912160307169
Training tokenizer:  16% 1273/8047 [1:41:06<9:03:25,  4.81s/it]loss_total_epoch 48.01613246649504
Training tokenizer:  16% 1274/8047 [1:41:11<9:03:15,  4.81s/it]loss_total_epoch 48.046883162111044
Training tokenizer:  16% 1275/8047 [1:41:16<9:01:21,  4.80s/it]loss_total_epoch 48.0794611312449
Training tokenizer:  16% 1276/8047 [1:41:21<9:01:57,  4.80s/it]loss_total_epoch 48.12265649065375
Training tokenizer:  16% 1277/8047 [1:41:26<9:02:02,  4.80s/it]loss_total_epoch 48.15309216827154
Training tokenizer:  16% 1278/8047 [1:41:30<8:59:46,  4.78s/it]loss_total_epoch 48.18411615677178
Training tokenizer:  16% 1279/8047 [1:41:35<8:58:32,  4.77s/it]loss_total_epoch 48.21983123011887
Training tokenizer:  16% 1280/8047 [1:41:40<8:56:41,  4.76s/it]loss_total_epoch 48.258281299844384
Training tokenizer:  16% 1281/8047 [1:41:45<8:56:58,  4.76s/it]loss_total_epoch 48.29738678224385
Training tokenizer:  16% 1282/8047 [1:41:49<8:56:59,  4.76s/it]loss_total_epoch 48.33476317860186
Training tokenizer:  16% 1283/8047 [1:41:54<8:54:43,  4.74s/it]loss_total_epoch 48.36657792143524
Training tokenizer:  16% 1284/8047 [1:41:59<8:55:07,  4.75s/it]loss_total_epoch 48.414693208411336
Training tokenizer:  16% 1285/8047 [1:42:04<8:56:20,  4.76s/it]loss_total_epoch 48.44895510561764
Training tokenizer:  16% 1286/8047 [1:42:08<8:54:46,  4.75s/it]loss_total_epoch 48.49015579558909
Training tokenizer:  16% 1287/8047 [1:42:13<8:54:39,  4.75s/it]loss_total_epoch 48.536342887207866
Training tokenizer:  16% 1288/8047 [1:42:18<8:55:41,  4.76s/it]loss_total_epoch 48.57226298563182
Training tokenizer:  16% 1289/8047 [1:42:23<8:57:21,  4.77s/it]loss_total_epoch 48.60321735776961
Training tokenizer:  16% 1290/8047 [1:42:27<8:56:37,  4.77s/it]loss_total_epoch 48.64108276925981
Training tokenizer:  16% 1291/8047 [1:42:32<8:55:14,  4.75s/it]loss_total_epoch 48.679357102140784
Training tokenizer:  16% 1292/8047 [1:42:37<8:53:36,  4.74s/it]loss_total_epoch 48.71420434676111
Training tokenizer:  16% 1293/8047 [1:42:42<8:54:49,  4.75s/it]loss_total_epoch 48.74998343922198
Training tokenizer:  16% 1294/8047 [1:42:46<8:55:38,  4.76s/it]loss_total_epoch 48.79029887728393
Training tokenizer:  16% 1295/8047 [1:42:51<8:55:47,  4.76s/it]loss_total_epoch 48.83244794048369
Training tokenizer:  16% 1296/8047 [1:42:56<8:56:18,  4.77s/it]loss_total_epoch 48.871923165395856
Training tokenizer:  16% 1297/8047 [1:43:01<8:57:58,  4.78s/it]loss_total_epoch 48.914864426478744
Training tokenizer:  16% 1298/8047 [1:43:05<8:58:22,  4.79s/it]loss_total_epoch 48.951396802440286
Training tokenizer:  16% 1299/8047 [1:43:10<8:58:12,  4.79s/it]loss_total_epoch 48.98399116285145
Training tokenizer:  16% 1300/8047 [1:43:15<8:57:24,  4.78s/it]loss_total_epoch 49.021702038124204
Training tokenizer:  16% 1301/8047 [1:43:20<8:56:26,  4.77s/it]loss_total_epoch 49.063655311241746
Training tokenizer:  16% 1302/8047 [1:43:25<8:56:45,  4.77s/it]loss_total_epoch 49.106818491593
Training tokenizer:  16% 1303/8047 [1:43:29<8:55:26,  4.76s/it]loss_total_epoch 49.139621993526816
Training tokenizer:  16% 1304/8047 [1:43:34<8:55:05,  4.76s/it]loss_total_epoch 49.178439708426595
Training tokenizer:  16% 1305/8047 [1:43:39<8:55:30,  4.77s/it]loss_total_epoch 49.217294527217746
Training tokenizer:  16% 1306/8047 [1:43:44<8:55:25,  4.77s/it]loss_total_epoch 49.25600870512426
Training tokenizer:  16% 1307/8047 [1:43:48<8:53:44,  4.75s/it]loss_total_epoch 49.29174858517945
Training tokenizer:  16% 1308/8047 [1:43:53<8:53:51,  4.75s/it]loss_total_epoch 49.32494062744081
Training tokenizer:  16% 1309/8047 [1:43:58<8:52:06,  4.74s/it]loss_total_epoch 49.35751139186323
Training tokenizer:  16% 1310/8047 [1:44:02<8:50:53,  4.73s/it]loss_total_epoch 49.40377317555249
Training tokenizer:  16% 1311/8047 [1:44:07<8:52:13,  4.74s/it]loss_total_epoch 49.446626434102654
Training tokenizer:  16% 1312/8047 [1:44:12<8:47:00,  4.69s/it]loss_total_epoch 49.48797889985144
Training tokenizer:  16% 1313/8047 [1:44:17<8:49:08,  4.71s/it]loss_total_epoch 49.52794064767659
Training tokenizer:  16% 1314/8047 [1:44:21<8:50:50,  4.73s/it]loss_total_epoch 49.56791460700333
Training tokenizer:  16% 1315/8047 [1:44:26<8:53:45,  4.76s/it]loss_total_epoch 49.60516309924424
Training tokenizer:  16% 1316/8047 [1:44:31<8:52:15,  4.74s/it]loss_total_epoch 49.638511361554265
Training tokenizer:  16% 1317/8047 [1:44:36<8:54:39,  4.77s/it]loss_total_epoch 49.672230115160346
Training tokenizer:  16% 1318/8047 [1:44:40<8:54:10,  4.76s/it]loss_total_epoch 49.70956760086119
Training tokenizer:  16% 1319/8047 [1:44:45<8:54:55,  4.77s/it]loss_total_epoch 49.74575692228973
Training tokenizer:  16% 1320/8047 [1:44:50<8:55:11,  4.77s/it]loss_total_epoch 49.784186182543635
Training tokenizer:  16% 1321/8047 [1:44:55<8:55:33,  4.78s/it]loss_total_epoch 49.819196900352836
Training tokenizer:  16% 1322/8047 [1:45:00<8:55:01,  4.77s/it]loss_total_epoch 49.85679894499481
Training tokenizer:  16% 1323/8047 [1:45:04<8:52:35,  4.75s/it]loss_total_epoch 49.89757207222283
Training tokenizer:  16% 1324/8047 [1:45:09<8:51:08,  4.74s/it]loss_total_epoch 49.93997380323708
Training tokenizer:  16% 1325/8047 [1:45:14<8:51:02,  4.74s/it]loss_total_epoch 49.97334550507367
Training tokenizer:  16% 1326/8047 [1:45:18<8:48:11,  4.72s/it]loss_total_epoch 50.015108743682504
Training tokenizer:  16% 1327/8047 [1:45:23<8:50:36,  4.74s/it]loss_total_epoch 50.0524176415056
Training tokenizer:  17% 1328/8047 [1:45:28<8:52:14,  4.75s/it]loss_total_epoch 50.094599997624755
Training tokenizer:  17% 1329/8047 [1:45:33<8:49:36,  4.73s/it]loss_total_epoch 50.12742850743234
Training tokenizer:  17% 1330/8047 [1:45:37<8:52:17,  4.75s/it]loss_total_epoch 50.16084867157042
Training tokenizer:  17% 1331/8047 [1:45:42<8:53:47,  4.77s/it]loss_total_epoch 50.20076129771769
Training tokenizer:  17% 1332/8047 [1:45:47<8:52:38,  4.76s/it]loss_total_epoch 50.23598997481167
Training tokenizer:  17% 1333/8047 [1:45:52<8:53:25,  4.77s/it]loss_total_epoch 50.272354589775205
Training tokenizer:  17% 1334/8047 [1:45:57<8:53:13,  4.77s/it]loss_total_epoch 50.30832009203732
Training tokenizer:  17% 1335/8047 [1:46:01<8:52:37,  4.76s/it]loss_total_epoch 50.35514277406037
Training tokenizer:  17% 1336/8047 [1:46:06<8:52:30,  4.76s/it]loss_total_epoch 50.3953714761883
Training tokenizer:  17% 1337/8047 [1:46:11<8:52:47,  4.76s/it]loss_total_epoch 50.432697078213096
Training tokenizer:  17% 1338/8047 [1:46:16<8:51:06,  4.75s/it]loss_total_epoch 50.462471537292004
Training tokenizer:  17% 1339/8047 [1:46:20<8:51:18,  4.75s/it]loss_total_epoch 50.49652500078082
Training tokenizer:  17% 1340/8047 [1:46:25<8:52:23,  4.76s/it]loss_total_epoch 50.52940537407994
Training tokenizer:  17% 1341/8047 [1:46:30<8:50:09,  4.74s/it]loss_total_epoch 50.56052923947573
Training tokenizer:  17% 1342/8047 [1:46:35<8:51:02,  4.75s/it]loss_total_epoch 50.59590841084719
Training tokenizer:  17% 1343/8047 [1:46:39<8:52:50,  4.77s/it]loss_total_epoch 50.63049750775099
Training tokenizer:  17% 1344/8047 [1:46:44<8:53:15,  4.77s/it]loss_total_epoch 50.6691573895514
Training tokenizer:  17% 1345/8047 [1:46:49<8:53:24,  4.78s/it]loss_total_epoch 50.71318507194519
Training tokenizer:  17% 1346/8047 [1:46:54<8:52:24,  4.77s/it]loss_total_epoch 50.750486589968204
Training tokenizer:  17% 1347/8047 [1:46:58<8:52:15,  4.77s/it]loss_total_epoch 50.79340138286352
Training tokenizer:  17% 1348/8047 [1:47:03<8:53:02,  4.77s/it]loss_total_epoch 50.83107581362128
Training tokenizer:  17% 1349/8047 [1:47:08<8:53:17,  4.78s/it]loss_total_epoch 50.87412886694074
Training tokenizer:  17% 1350/8047 [1:47:13<8:51:57,  4.77s/it]loss_total_epoch 50.9125584512949
Training tokenizer:  17% 1351/8047 [1:47:18<8:51:09,  4.76s/it]loss_total_epoch 50.95165907219052
Training tokenizer:  17% 1352/8047 [1:47:22<8:50:31,  4.75s/it]loss_total_epoch 50.99536770582199
Training tokenizer:  17% 1353/8047 [1:47:27<8:51:04,  4.76s/it]loss_total_epoch 51.04042325541377
Training tokenizer:  17% 1354/8047 [1:47:32<8:51:00,  4.76s/it]loss_total_epoch 51.07021558098495
Training tokenizer:  17% 1355/8047 [1:47:37<8:52:42,  4.78s/it]loss_total_epoch 51.10414216481149
Training tokenizer:  17% 1356/8047 [1:47:41<8:50:58,  4.76s/it]loss_total_epoch 51.141006691381335
Training tokenizer:  17% 1357/8047 [1:47:46<8:52:13,  4.77s/it]loss_total_epoch 51.18304137699306
Training tokenizer:  17% 1358/8047 [1:47:51<8:50:47,  4.76s/it]loss_total_epoch 51.21973195485771
Training tokenizer:  17% 1359/8047 [1:47:56<8:52:05,  4.77s/it]loss_total_epoch 51.255569284781814
Training tokenizer:  17% 1360/8047 [1:48:00<8:51:39,  4.77s/it]loss_total_epoch 51.282055573537946
Training tokenizer:  17% 1361/8047 [1:48:05<8:50:32,  4.76s/it]loss_total_epoch 51.311291901394725
Training tokenizer:  17% 1362/8047 [1:48:10<8:51:59,  4.77s/it]loss_total_epoch 51.35098739527166
Training tokenizer:  17% 1363/8047 [1:48:15<8:50:34,  4.76s/it]loss_total_epoch 51.38918982259929
Training tokenizer:  17% 1364/8047 [1:48:20<8:51:57,  4.78s/it]loss_total_epoch 51.43057799898088
Training tokenizer:  17% 1365/8047 [1:48:24<8:51:03,  4.77s/it]loss_total_epoch 51.46309754438698
Training tokenizer:  17% 1366/8047 [1:48:29<8:50:43,  4.77s/it]loss_total_epoch 51.50604181177914
Training tokenizer:  17% 1367/8047 [1:48:34<8:51:37,  4.78s/it]loss_total_epoch 51.54423368908465
Training tokenizer:  17% 1368/8047 [1:48:39<8:50:04,  4.76s/it]loss_total_epoch 51.58481283672154
Training tokenizer:  17% 1369/8047 [1:48:43<8:50:46,  4.77s/it]loss_total_epoch 51.618127377703786
Training tokenizer:  17% 1370/8047 [1:48:48<8:49:20,  4.76s/it]loss_total_epoch 51.65695749782026
Training tokenizer:  17% 1371/8047 [1:48:53<8:48:54,  4.75s/it]loss_total_epoch 51.70241569168866
Training tokenizer:  17% 1372/8047 [1:48:58<8:47:37,  4.74s/it]loss_total_epoch 51.736006336286664
Training tokenizer:  17% 1373/8047 [1:49:02<8:47:57,  4.75s/it]loss_total_epoch 51.776393761858344
Training tokenizer:  17% 1374/8047 [1:49:07<8:45:16,  4.72s/it]loss_total_epoch 51.80929677374661
Training tokenizer:  17% 1375/8047 [1:49:12<8:42:52,  4.70s/it]loss_total_epoch 51.844896061345935
Training tokenizer:  17% 1376/8047 [1:49:16<8:45:29,  4.73s/it]loss_total_epoch 51.88598248176277
Training tokenizer:  17% 1377/8047 [1:49:21<8:46:31,  4.74s/it]loss_total_epoch 51.9166285675019
Training tokenizer:  17% 1378/8047 [1:49:26<8:48:02,  4.75s/it]loss_total_epoch 51.9542402792722
Training tokenizer:  17% 1379/8047 [1:49:31<8:49:47,  4.77s/it]loss_total_epoch 51.995093109086156
Training tokenizer:  17% 1380/8047 [1:49:36<8:49:54,  4.77s/it]loss_total_epoch 52.03002862073481
Training tokenizer:  17% 1381/8047 [1:49:40<8:49:38,  4.77s/it]loss_total_epoch 52.063974460586905
Training tokenizer:  17% 1382/8047 [1:49:45<8:47:39,  4.75s/it]loss_total_epoch 52.1066284943372
Training tokenizer:  17% 1383/8047 [1:49:50<8:48:08,  4.76s/it]loss_total_epoch 52.146796910092235
Training tokenizer:  17% 1384/8047 [1:49:55<8:50:03,  4.77s/it]loss_total_epoch 52.18832301534712
Training tokenizer:  17% 1385/8047 [1:49:59<8:50:48,  4.78s/it]loss_total_epoch 52.2274563703686
Training tokenizer:  17% 1386/8047 [1:50:04<8:50:52,  4.78s/it]loss_total_epoch 52.25960344262421
Training tokenizer:  17% 1387/8047 [1:50:09<8:47:45,  4.75s/it]loss_total_epoch 52.301699278876185
Training tokenizer:  17% 1388/8047 [1:50:14<8:49:01,  4.77s/it]loss_total_epoch 52.34216736070812
Training tokenizer:  17% 1389/8047 [1:50:18<8:50:17,  4.78s/it]loss_total_epoch 52.382029270753264
Training tokenizer:  17% 1390/8047 [1:50:23<8:49:14,  4.77s/it]loss_total_epoch 52.412188509479165
Training tokenizer:  17% 1391/8047 [1:50:28<8:50:46,  4.78s/it]loss_total_epoch 52.44950751028955
Training tokenizer:  17% 1392/8047 [1:50:33<8:50:58,  4.79s/it]loss_total_epoch 52.48650456406176
Training tokenizer:  17% 1393/8047 [1:50:38<8:49:14,  4.77s/it]loss_total_epoch 52.533901209011674
Training tokenizer:  17% 1394/8047 [1:50:42<8:50:00,  4.78s/it]loss_total_epoch 52.571239264681935
Training tokenizer:  17% 1395/8047 [1:50:47<8:48:51,  4.77s/it]loss_total_epoch 52.611610325053334
Training tokenizer:  17% 1396/8047 [1:50:52<8:47:54,  4.76s/it]loss_total_epoch 52.643111201003194
Training tokenizer:  17% 1397/8047 [1:50:57<8:47:19,  4.76s/it]loss_total_epoch 52.685814714059234
Training tokenizer:  17% 1398/8047 [1:51:01<8:47:10,  4.76s/it]loss_total_epoch 52.725143847987056
Training tokenizer:  17% 1399/8047 [1:51:06<8:49:00,  4.77s/it]loss_total_epoch 52.76312247850001
Training tokenizer:  17% 1400/8047 [1:51:11<8:49:34,  4.78s/it]Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 740, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/shutil.py", line 738, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-kb9smn3z'
loss_total_epoch 52.797970274463296
Training tokenizer:  17% 1401/8047 [1:51:16<8:49:04,  4.78s/it]loss_total_epoch 52.83727792091668
Training tokenizer:  17% 1402/8047 [1:51:20<8:48:53,  4.78s/it]loss_total_epoch 52.87892314605415
Training tokenizer:  17% 1403/8047 [1:51:25<8:47:38,  4.77s/it]loss_total_epoch 52.91727310232818
Training tokenizer:  17% 1404/8047 [1:51:30<8:46:05,  4.75s/it]loss_total_epoch 52.94545236043632
Training tokenizer:  17% 1405/8047 [1:51:35<8:47:16,  4.76s/it]loss_total_epoch 52.984388971701264
Training tokenizer:  17% 1406/8047 [1:51:39<8:46:55,  4.76s/it]loss_total_epoch 53.01822126470506
Training tokenizer:  17% 1407/8047 [1:51:44<8:46:25,  4.76s/it]loss_total_epoch 53.0545129198581
Training tokenizer:  17% 1408/8047 [1:51:49<8:44:48,  4.74s/it]loss_total_epoch 53.086241686716676
Training tokenizer:  18% 1409/8047 [1:51:54<8:43:04,  4.73s/it]loss_total_epoch 53.122337790206075
Training tokenizer:  18% 1410/8047 [1:51:58<8:43:07,  4.73s/it]loss_total_epoch 53.16630461253226
Training tokenizer:  18% 1411/8047 [1:52:03<8:45:30,  4.75s/it]loss_total_epoch 53.19736419990659
Training tokenizer:  18% 1412/8047 [1:52:08<8:43:54,  4.74s/it]loss_total_epoch 53.24149640277028
Training tokenizer:  18% 1413/8047 [1:52:13<8:46:38,  4.76s/it]loss_total_epoch 53.272395296022296
Training tokenizer:  18% 1414/8047 [1:52:17<8:47:02,  4.77s/it]loss_total_epoch 53.31190017424524
Training tokenizer:  18% 1415/8047 [1:52:22<8:47:47,  4.77s/it]loss_total_epoch 53.3544234726578
Training tokenizer:  18% 1416/8047 [1:52:27<8:47:25,  4.77s/it]loss_total_epoch 53.39132650755346
Training tokenizer:  18% 1417/8047 [1:52:32<8:46:20,  4.76s/it]loss_total_epoch 53.426185404881835
Training tokenizer:  18% 1418/8047 [1:52:37<8:45:29,  4.76s/it]loss_total_epoch 53.45903686620295
Training tokenizer:  18% 1419/8047 [1:52:41<8:44:31,  4.75s/it]loss_total_epoch 53.49420274235308
Training tokenizer:  18% 1420/8047 [1:52:46<8:45:37,  4.76s/it]loss_total_epoch 53.532055063173175
Training tokenizer:  18% 1421/8047 [1:52:51<8:45:30,  4.76s/it]loss_total_epoch 53.569114638492465
Training tokenizer:  18% 1422/8047 [1:52:56<8:44:08,  4.75s/it]loss_total_epoch 53.60344183631241
Training tokenizer:  18% 1423/8047 [1:53:00<8:43:16,  4.74s/it]loss_total_epoch 53.64564366824925
Training tokenizer:  18% 1424/8047 [1:53:05<8:44:11,  4.75s/it]loss_total_epoch 53.68216744996607
Training tokenizer:  18% 1425/8047 [1:53:10<8:45:46,  4.76s/it]loss_total_epoch 53.71293439716101
Training tokenizer:  18% 1426/8047 [1:53:15<8:46:49,  4.77s/it]loss_total_epoch 53.74722059071064
Training tokenizer:  18% 1427/8047 [1:53:19<8:46:05,  4.77s/it]loss_total_epoch 53.79310853034258
Training tokenizer:  18% 1428/8047 [1:53:24<8:40:41,  4.72s/it]loss_total_epoch 53.82382730767131
Training tokenizer:  18% 1429/8047 [1:53:29<8:38:36,  4.70s/it]loss_total_epoch 53.85989937186241
Training tokenizer:  18% 1430/8047 [1:53:33<8:39:09,  4.71s/it]loss_total_epoch 53.900812197476625
Training tokenizer:  18% 1431/8047 [1:53:38<8:41:29,  4.73s/it]loss_total_epoch 53.93408288806677
Training tokenizer:  18% 1432/8047 [1:53:43<8:43:00,  4.74s/it]loss_total_epoch 53.97329553216696
Training tokenizer:  18% 1433/8047 [1:53:48<8:42:36,  4.74s/it]loss_total_epoch 54.00749873369932
Training tokenizer:  18% 1434/8047 [1:53:52<8:43:12,  4.75s/it]loss_total_epoch 54.04246775433421
Training tokenizer:  18% 1435/8047 [1:53:57<8:43:20,  4.75s/it]loss_total_epoch 54.087891813367605
Training tokenizer:  18% 1436/8047 [1:54:02<8:43:58,  4.76s/it]loss_total_epoch 54.13214375823736
Training tokenizer:  18% 1437/8047 [1:54:07<8:43:29,  4.75s/it]loss_total_epoch 54.164754755795
Training tokenizer:  18% 1438/8047 [1:54:11<8:44:26,  4.76s/it]loss_total_epoch 54.208530467003584
Training tokenizer:  18% 1439/8047 [1:54:16<8:46:16,  4.78s/it]loss_total_epoch 54.240002900362015
Training tokenizer:  18% 1440/8047 [1:54:21<8:45:50,  4.78s/it]loss_total_epoch 54.27208387106657
Training tokenizer:  18% 1441/8047 [1:54:26<8:45:24,  4.77s/it]loss_total_epoch 54.31541679799557
Training tokenizer:  18% 1442/8047 [1:54:31<8:43:39,  4.76s/it]loss_total_epoch 54.349400509148836
Training tokenizer:  18% 1443/8047 [1:54:35<8:43:16,  4.75s/it]loss_total_epoch 54.39362979307771
Training tokenizer:  18% 1444/8047 [1:54:40<8:41:56,  4.74s/it]loss_total_epoch 54.4320856295526
Training tokenizer:  18% 1445/8047 [1:54:45<8:43:30,  4.76s/it]loss_total_epoch 54.47145199403167
Training tokenizer:  18% 1446/8047 [1:54:50<8:44:43,  4.77s/it]loss_total_epoch 54.50487761199474
Training tokenizer:  18% 1447/8047 [1:54:54<8:43:32,  4.76s/it]loss_total_epoch 54.54301732778549
Training tokenizer:  18% 1448/8047 [1:54:59<8:42:03,  4.75s/it]loss_total_epoch 54.57781057804823
Training tokenizer:  18% 1449/8047 [1:55:04<8:41:55,  4.75s/it]loss_total_epoch 54.620717607438564
Training tokenizer:  18% 1450/8047 [1:55:09<8:42:10,  4.75s/it]loss_total_epoch 54.66019310429692
Training tokenizer:  18% 1451/8047 [1:55:13<8:41:56,  4.75s/it]loss_total_epoch 54.68768319673836
Training tokenizer:  18% 1452/8047 [1:55:18<8:42:11,  4.75s/it]loss_total_epoch 54.71768699772656
Training tokenizer:  18% 1453/8047 [1:55:23<8:42:05,  4.75s/it]loss_total_epoch 54.76076070405543
Training tokenizer:  18% 1454/8047 [1:55:28<8:41:49,  4.75s/it]loss_total_epoch 54.801472725346684
Training tokenizer:  18% 1455/8047 [1:55:32<8:42:46,  4.76s/it]loss_total_epoch 54.845211463049054
Training tokenizer:  18% 1456/8047 [1:55:37<8:42:45,  4.76s/it]loss_total_epoch 54.88775992579758
Training tokenizer:  18% 1457/8047 [1:55:42<8:43:18,  4.76s/it]loss_total_epoch 54.921818720176816
Training tokenizer:  18% 1458/8047 [1:55:47<8:41:12,  4.75s/it]loss_total_epoch 54.953494692221284
Training tokenizer:  18% 1459/8047 [1:55:51<8:43:28,  4.77s/it]loss_total_epoch 54.99729444645345
Training tokenizer:  18% 1460/8047 [1:55:56<8:43:56,  4.77s/it]loss_total_epoch 55.04174815304577
Training tokenizer:  18% 1461/8047 [1:56:01<8:43:25,  4.77s/it]loss_total_epoch 55.08125865645707
Training tokenizer:  18% 1462/8047 [1:56:06<8:42:46,  4.76s/it]loss_total_epoch 55.11614258773625
Training tokenizer:  18% 1463/8047 [1:56:10<8:43:06,  4.77s/it]loss_total_epoch 55.1546996999532
Training tokenizer:  18% 1464/8047 [1:56:15<8:43:28,  4.77s/it]loss_total_epoch 55.19173593260348
Training tokenizer:  18% 1465/8047 [1:56:20<8:44:16,  4.78s/it]loss_total_epoch 55.2296214196831
Training tokenizer:  18% 1466/8047 [1:56:25<8:43:28,  4.77s/it]loss_total_epoch 55.266882518306375
Training tokenizer:  18% 1467/8047 [1:56:30<8:43:19,  4.77s/it]loss_total_epoch 55.30789569579065
Training tokenizer:  18% 1468/8047 [1:56:34<8:42:24,  4.76s/it]loss_total_epoch 55.351693948730826
Training tokenizer:  18% 1469/8047 [1:56:39<8:41:41,  4.76s/it]loss_total_epoch 55.38969957642257
Training tokenizer:  18% 1470/8047 [1:56:44<8:43:21,  4.77s/it]loss_total_epoch 55.42900826968253
Training tokenizer:  18% 1471/8047 [1:56:49<8:42:05,  4.76s/it]loss_total_epoch 55.4606758300215
Training tokenizer:  18% 1472/8047 [1:56:53<8:40:45,  4.75s/it]loss_total_epoch 55.50011087395251
Training tokenizer:  18% 1473/8047 [1:56:58<8:40:59,  4.76s/it]loss_total_epoch 55.53361271508038
Training tokenizer:  18% 1474/8047 [1:57:03<8:41:03,  4.76s/it]loss_total_epoch 55.568698627874255
Training tokenizer:  18% 1475/8047 [1:57:08<8:40:34,  4.75s/it]loss_total_epoch 55.61153128556907
Training tokenizer:  18% 1476/8047 [1:57:12<8:40:10,  4.75s/it]loss_total_epoch 55.64419628493488
Training tokenizer:  18% 1477/8047 [1:57:17<8:40:50,  4.76s/it]loss_total_epoch 55.691103430464864
Training tokenizer:  18% 1478/8047 [1:57:22<8:38:40,  4.74s/it]loss_total_epoch 55.73294467292726
Training tokenizer:  18% 1479/8047 [1:57:27<8:39:21,  4.74s/it]loss_total_epoch 55.76440149731934
Training tokenizer:  18% 1480/8047 [1:57:31<8:39:58,  4.75s/it]loss_total_epoch 55.80859977938235
Training tokenizer:  18% 1481/8047 [1:57:36<8:41:02,  4.76s/it]loss_total_epoch 55.848913656547666
Training tokenizer:  18% 1482/8047 [1:57:41<8:41:15,  4.76s/it]loss_total_epoch 55.876297967508435
Training tokenizer:  18% 1483/8047 [1:57:46<8:41:58,  4.77s/it]loss_total_epoch 55.91069031320512
Training tokenizer:  18% 1484/8047 [1:57:50<8:41:32,  4.77s/it]loss_total_epoch 55.9531533960253
Training tokenizer:  18% 1485/8047 [1:57:55<8:41:13,  4.77s/it]loss_total_epoch 55.9938291888684
Training tokenizer:  18% 1486/8047 [1:58:00<8:40:13,  4.76s/it]loss_total_epoch 56.03683617897332
Training tokenizer:  18% 1487/8047 [1:58:05<8:40:10,  4.76s/it]loss_total_epoch 56.084431586787105
Training tokenizer:  18% 1488/8047 [1:58:09<8:41:04,  4.77s/it]loss_total_epoch 56.12165307812393
Training tokenizer:  19% 1489/8047 [1:58:14<8:41:22,  4.77s/it]loss_total_epoch 56.15999354980886
Training tokenizer:  19% 1490/8047 [1:58:19<8:41:24,  4.77s/it]loss_total_epoch 56.20521145872772
Training tokenizer:  19% 1491/8047 [1:58:24<8:41:51,  4.78s/it]loss_total_epoch 56.24012648127973
Training tokenizer:  19% 1492/8047 [1:58:28<8:38:55,  4.75s/it]loss_total_epoch 56.28757639415562
Training tokenizer:  19% 1493/8047 [1:58:33<8:38:46,  4.75s/it]loss_total_epoch 56.325353341177106
Training tokenizer:  19% 1494/8047 [1:58:38<8:39:47,  4.76s/it]loss_total_epoch 56.36596784554422
Training tokenizer:  19% 1495/8047 [1:58:43<8:39:48,  4.76s/it]loss_total_epoch 56.40167851932347
Training tokenizer:  19% 1496/8047 [1:58:48<8:38:41,  4.75s/it]loss_total_epoch 56.44245590828359
Training tokenizer:  19% 1497/8047 [1:58:52<8:37:06,  4.74s/it]loss_total_epoch 56.47614222206175
Training tokenizer:  19% 1498/8047 [1:58:57<8:36:24,  4.73s/it]loss_total_epoch 56.50973000563681
Training tokenizer:  19% 1499/8047 [1:59:02<8:37:04,  4.74s/it]loss_total_epoch 56.54790926165879
Training tokenizer:  19% 1500/8047 [1:59:06<8:37:42,  4.74s/it]loss_total_epoch 56.577281007543206
Training tokenizer:  19% 1501/8047 [1:59:11<8:36:36,  4.74s/it]loss_total_epoch 56.604416850954294
Training tokenizer:  19% 1502/8047 [1:59:16<8:36:47,  4.74s/it]loss_total_epoch 56.65113051608205
Training tokenizer:  19% 1503/8047 [1:59:21<8:34:44,  4.72s/it]loss_total_epoch 56.69076802954078
Training tokenizer:  19% 1504/8047 [1:59:25<8:35:59,  4.73s/it]loss_total_epoch 56.729486498981714
Training tokenizer:  19% 1505/8047 [1:59:30<8:37:44,  4.75s/it]loss_total_epoch 56.77069678902626
Training tokenizer:  19% 1506/8047 [1:59:35<8:38:16,  4.75s/it]loss_total_epoch 56.813540797680616
Training tokenizer:  19% 1507/8047 [1:59:40<8:38:21,  4.76s/it]loss_total_epoch 56.849163468927145
Training tokenizer:  19% 1508/8047 [1:59:44<8:40:12,  4.77s/it]loss_total_epoch 56.88843409344554
Training tokenizer:  19% 1509/8047 [1:59:49<8:38:17,  4.76s/it]loss_total_epoch 56.927119709551334
Training tokenizer:  19% 1510/8047 [1:59:54<8:38:07,  4.76s/it]loss_total_epoch 56.95436636172235
Training tokenizer:  19% 1511/8047 [1:59:59<8:37:24,  4.75s/it]loss_total_epoch 56.98594003729522
Training tokenizer:  19% 1512/8047 [2:00:03<8:39:17,  4.77s/it]loss_total_epoch 57.028581535443664
Training tokenizer:  19% 1513/8047 [2:00:08<8:39:41,  4.77s/it]loss_total_epoch 57.07292710803449
Training tokenizer:  19% 1514/8047 [2:00:13<8:39:12,  4.77s/it]loss_total_epoch 57.10362129099667
Training tokenizer:  19% 1515/8047 [2:00:18<8:39:43,  4.77s/it]loss_total_epoch 57.14376023598015
Training tokenizer:  19% 1516/8047 [2:00:23<8:39:23,  4.77s/it]loss_total_epoch 57.17923845909536
Training tokenizer:  19% 1517/8047 [2:00:27<8:39:11,  4.77s/it]loss_total_epoch 57.21274226717651
Training tokenizer:  19% 1518/8047 [2:00:32<8:38:49,  4.77s/it]loss_total_epoch 57.24743411131203
Training tokenizer:  19% 1519/8047 [2:00:37<8:38:01,  4.76s/it]loss_total_epoch 57.28267537243664
Training tokenizer:  19% 1520/8047 [2:00:42<8:37:53,  4.76s/it]loss_total_epoch 57.32216230966151
Training tokenizer:  19% 1521/8047 [2:00:46<8:36:25,  4.75s/it]loss_total_epoch 57.352875888347626
Training tokenizer:  19% 1522/8047 [2:00:51<8:35:03,  4.74s/it]loss_total_epoch 57.38688812032342
Training tokenizer:  19% 1523/8047 [2:00:56<8:34:59,  4.74s/it]loss_total_epoch 57.42654538527131
Training tokenizer:  19% 1524/8047 [2:01:01<8:35:48,  4.74s/it]loss_total_epoch 57.457123421132565
Training tokenizer:  19% 1525/8047 [2:01:05<8:35:58,  4.75s/it]loss_total_epoch 57.486666763201356
Training tokenizer:  19% 1526/8047 [2:01:10<8:38:09,  4.77s/it]loss_total_epoch 57.51617363281548
Training tokenizer:  19% 1527/8047 [2:01:15<8:39:43,  4.78s/it]loss_total_epoch 57.55012101493776
Training tokenizer:  19% 1528/8047 [2:01:20<8:38:28,  4.77s/it]loss_total_epoch 57.5872960370034
Training tokenizer:  19% 1529/8047 [2:01:24<8:37:49,  4.77s/it]loss_total_epoch 57.62121148221195
Training tokenizer:  19% 1530/8047 [2:01:29<8:37:38,  4.77s/it]loss_total_epoch 57.65718681924045
Training tokenizer:  19% 1531/8047 [2:01:34<8:39:10,  4.78s/it]loss_total_epoch 57.69133326970041
Training tokenizer:  19% 1532/8047 [2:01:39<8:38:46,  4.78s/it]loss_total_epoch 57.731619169935584
Training tokenizer:  19% 1533/8047 [2:01:44<8:37:19,  4.77s/it]loss_total_epoch 57.76599310152233
Training tokenizer:  19% 1534/8047 [2:01:48<8:37:09,  4.76s/it]loss_total_epoch 57.80301537923515
Training tokenizer:  19% 1535/8047 [2:01:53<8:36:38,  4.76s/it]loss_total_epoch 57.841562205925584
Training tokenizer:  19% 1536/8047 [2:01:58<8:36:37,  4.76s/it]loss_total_epoch 57.87762880139053
Training tokenizer:  19% 1537/8047 [2:02:03<8:36:50,  4.76s/it]loss_total_epoch 57.91403849609196
Training tokenizer:  19% 1538/8047 [2:02:07<8:35:37,  4.75s/it]loss_total_epoch 57.946828154847026
Training tokenizer:  19% 1539/8047 [2:02:12<8:35:07,  4.75s/it]loss_total_epoch 57.983057329431176
Training tokenizer:  19% 1540/8047 [2:02:17<8:33:55,  4.74s/it]loss_total_epoch 58.03009164147079
Training tokenizer:  19% 1541/8047 [2:02:22<8:36:15,  4.76s/it]loss_total_epoch 58.06706297211349
Training tokenizer:  19% 1542/8047 [2:02:26<8:36:58,  4.77s/it]loss_total_epoch 58.10677413828671
Training tokenizer:  19% 1543/8047 [2:02:31<8:35:42,  4.76s/it]loss_total_epoch 58.15269343368709
Training tokenizer:  19% 1544/8047 [2:02:36<8:35:16,  4.75s/it]loss_total_epoch 58.187447084113955
Training tokenizer:  19% 1545/8047 [2:02:41<8:35:33,  4.76s/it]loss_total_epoch 58.225881258025765
Training tokenizer:  19% 1546/8047 [2:02:45<8:35:30,  4.76s/it]loss_total_epoch 58.269229816272855
Training tokenizer:  19% 1547/8047 [2:02:50<8:36:18,  4.77s/it]loss_total_epoch 58.299930253997445
Training tokenizer:  19% 1548/8047 [2:02:55<8:35:51,  4.76s/it]loss_total_epoch 58.34034952707589
Training tokenizer:  19% 1549/8047 [2:03:00<8:36:46,  4.77s/it]loss_total_epoch 58.37360002659261
Training tokenizer:  19% 1550/8047 [2:03:04<8:33:42,  4.74s/it]loss_total_epoch 58.4116961825639
Training tokenizer:  19% 1551/8047 [2:03:09<8:35:31,  4.76s/it]loss_total_epoch 58.44871595688164
Training tokenizer:  19% 1552/8047 [2:03:14<8:34:13,  4.75s/it]loss_total_epoch 58.47203893214464
Training tokenizer:  19% 1553/8047 [2:03:19<8:34:50,  4.76s/it]loss_total_epoch 58.505703650414944
Training tokenizer:  19% 1554/8047 [2:03:23<8:33:41,  4.75s/it]loss_total_epoch 58.53111585229635
Training tokenizer:  19% 1555/8047 [2:03:28<8:34:54,  4.76s/it]loss_total_epoch 58.56169875897467
Training tokenizer:  19% 1556/8047 [2:03:33<8:34:50,  4.76s/it]loss_total_epoch 58.6036427076906
Training tokenizer:  19% 1557/8047 [2:03:38<8:34:29,  4.76s/it]loss_total_epoch 58.6325389649719
Training tokenizer:  19% 1558/8047 [2:03:42<8:33:29,  4.75s/it]loss_total_epoch 58.67575434781611
Training tokenizer:  19% 1559/8047 [2:03:47<8:34:24,  4.76s/it]loss_total_epoch 58.71323843486607
Training tokenizer:  19% 1560/8047 [2:03:52<8:35:23,  4.77s/it]loss_total_epoch 58.75704355351627
Training tokenizer:  19% 1561/8047 [2:03:57<8:36:07,  4.77s/it]loss_total_epoch 58.79923992417753
Training tokenizer:  19% 1562/8047 [2:04:02<8:36:44,  4.78s/it]loss_total_epoch 58.829528680071235
Training tokenizer:  19% 1563/8047 [2:04:06<8:34:43,  4.76s/it]loss_total_epoch 58.8687455561012
Training tokenizer:  19% 1564/8047 [2:04:11<8:35:24,  4.77s/it]loss_total_epoch 58.9133863914758
Training tokenizer:  19% 1565/8047 [2:04:16<8:31:32,  4.74s/it]loss_total_epoch 58.95310705713928
Training tokenizer:  19% 1566/8047 [2:04:20<8:32:32,  4.75s/it]loss_total_epoch 58.98449763841927
Training tokenizer:  19% 1567/8047 [2:04:25<8:33:06,  4.75s/it]loss_total_epoch 59.02380167506635
Training tokenizer:  19% 1568/8047 [2:04:30<8:34:10,  4.76s/it]loss_total_epoch 59.065095445141196
Training tokenizer:  19% 1569/8047 [2:04:35<8:34:53,  4.77s/it]loss_total_epoch 59.09800733812153
Training tokenizer:  20% 1570/8047 [2:04:40<8:34:59,  4.77s/it]loss_total_epoch 59.13052283041179
Training tokenizer:  20% 1571/8047 [2:04:44<8:36:34,  4.79s/it]loss_total_epoch 59.167106656357646
Training tokenizer:  20% 1572/8047 [2:04:49<8:37:20,  4.79s/it]loss_total_epoch 59.204516684636474
Training tokenizer:  20% 1573/8047 [2:04:54<8:30:10,  4.73s/it]loss_total_epoch 59.22964773885906
Training tokenizer:  20% 1574/8047 [2:04:59<8:32:20,  4.75s/it]loss_total_epoch 59.26033768244088
Training tokenizer:  20% 1575/8047 [2:05:03<8:32:44,  4.75s/it]loss_total_epoch 59.29521025530994
Training tokenizer:  20% 1576/8047 [2:05:08<8:33:41,  4.76s/it]loss_total_epoch 59.33423098735511
Training tokenizer:  20% 1577/8047 [2:05:13<8:34:05,  4.77s/it]loss_total_epoch 59.369083343073726
Training tokenizer:  20% 1578/8047 [2:05:18<8:33:34,  4.76s/it]loss_total_epoch 59.401419719681144
Training tokenizer:  20% 1579/8047 [2:05:22<8:34:38,  4.77s/it]loss_total_epoch 59.43691519089043
Training tokenizer:  20% 1580/8047 [2:05:27<8:34:20,  4.77s/it]loss_total_epoch 59.47412863560021
Training tokenizer:  20% 1581/8047 [2:05:32<8:33:50,  4.77s/it]loss_total_epoch 59.5039111468941
Training tokenizer:  20% 1582/8047 [2:05:37<8:32:58,  4.76s/it]loss_total_epoch 59.547083811834455
Training tokenizer:  20% 1583/8047 [2:05:42<8:32:46,  4.76s/it]loss_total_epoch 59.58496386371553
Training tokenizer:  20% 1584/8047 [2:05:46<8:32:12,  4.76s/it]loss_total_epoch 59.623339833691716
Training tokenizer:  20% 1585/8047 [2:05:51<8:31:58,  4.75s/it]loss_total_epoch 59.66242451407015
Training tokenizer:  20% 1586/8047 [2:05:56<8:27:41,  4.71s/it]loss_total_epoch 59.69617437757552
Training tokenizer:  20% 1587/8047 [2:06:00<8:28:37,  4.72s/it]loss_total_epoch 59.731545152142644
Training tokenizer:  20% 1588/8047 [2:06:05<8:31:05,  4.75s/it]loss_total_epoch 59.75744107365608
Training tokenizer:  20% 1589/8047 [2:06:10<8:31:36,  4.75s/it]loss_total_epoch 59.791625048965216
Training tokenizer:  20% 1590/8047 [2:06:15<8:32:10,  4.76s/it]loss_total_epoch 59.820625368505716
Training tokenizer:  20% 1591/8047 [2:06:19<8:31:42,  4.76s/it]loss_total_epoch 59.85615849122405
Training tokenizer:  20% 1592/8047 [2:06:24<8:31:44,  4.76s/it]loss_total_epoch 59.88529068976641
Training tokenizer:  20% 1593/8047 [2:06:29<8:32:15,  4.76s/it]loss_total_epoch 59.924091421067715
Training tokenizer:  20% 1594/8047 [2:06:34<8:31:40,  4.76s/it]loss_total_epoch 59.96226066723466
Training tokenizer:  20% 1595/8047 [2:06:39<8:31:54,  4.76s/it]loss_total_epoch 59.9969277381897
Training tokenizer:  20% 1596/8047 [2:06:43<8:30:27,  4.75s/it]loss_total_epoch 60.03900517895818
Training tokenizer:  20% 1597/8047 [2:06:48<8:25:44,  4.70s/it]loss_total_epoch 60.07062258198857
Training tokenizer:  20% 1598/8047 [2:06:53<8:26:38,  4.71s/it]loss_total_epoch 60.10633982345462
Training tokenizer:  20% 1599/8047 [2:06:57<8:28:43,  4.73s/it]loss_total_epoch 60.1447778865695
Training tokenizer:  20% 1600/8047 [2:07:02<8:28:53,  4.74s/it]loss_total_epoch 60.18663442134857
Training tokenizer:  20% 1601/8047 [2:07:07<8:28:12,  4.73s/it]loss_total_epoch 60.22375822812319
Training tokenizer:  20% 1602/8047 [2:07:11<8:26:39,  4.72s/it]loss_total_epoch 60.26182983070612
Training tokenizer:  20% 1603/8047 [2:07:16<8:28:06,  4.73s/it]loss_total_epoch 60.3069457821548
Training tokenizer:  20% 1604/8047 [2:07:21<8:30:39,  4.76s/it]loss_total_epoch 60.34868572279811
Training tokenizer:  20% 1605/8047 [2:07:26<8:30:41,  4.76s/it]loss_total_epoch 60.39108708128333
Training tokenizer:  20% 1606/8047 [2:07:31<8:29:50,  4.75s/it]loss_total_epoch 60.44292006269097
Training tokenizer:  20% 1607/8047 [2:07:35<8:29:59,  4.75s/it]loss_total_epoch 60.48365671560168
Training tokenizer:  20% 1608/8047 [2:07:40<8:30:07,  4.75s/it]loss_total_epoch 60.51307889260352
Training tokenizer:  20% 1609/8047 [2:07:45<8:29:31,  4.75s/it]loss_total_epoch 60.54721541143954
Training tokenizer:  20% 1610/8047 [2:07:50<8:29:30,  4.75s/it]loss_total_epoch 60.57603679411113
Training tokenizer:  20% 1611/8047 [2:07:54<8:28:50,  4.74s/it]loss_total_epoch 60.608848279342055
Training tokenizer:  20% 1612/8047 [2:07:59<8:28:38,  4.74s/it]loss_total_epoch 60.63933381438255
Training tokenizer:  20% 1613/8047 [2:08:04<8:29:35,  4.75s/it]loss_total_epoch 60.67782333120704
Training tokenizer:  20% 1614/8047 [2:08:09<8:30:14,  4.76s/it]loss_total_epoch 60.715016808360815
Training tokenizer:  20% 1615/8047 [2:08:13<8:34:31,  4.80s/it]loss_total_epoch 60.743179094046354
Training tokenizer:  20% 1616/8047 [2:08:18<8:32:52,  4.79s/it]loss_total_epoch 60.77339017577469
Training tokenizer:  20% 1617/8047 [2:08:23<8:33:57,  4.80s/it]loss_total_epoch 60.81299722753465
Training tokenizer:  20% 1618/8047 [2:08:28<8:34:53,  4.81s/it]loss_total_epoch 60.849345149472356
Training tokenizer:  20% 1619/8047 [2:08:33<8:32:47,  4.79s/it]loss_total_epoch 60.886734222993255
Training tokenizer:  20% 1620/8047 [2:08:37<8:30:18,  4.76s/it]loss_total_epoch 60.91850887797773
Training tokenizer:  20% 1621/8047 [2:08:42<8:28:29,  4.75s/it]loss_total_epoch 60.95975930057466
Training tokenizer:  20% 1622/8047 [2:08:47<8:26:35,  4.73s/it]loss_total_epoch 60.99073983728886
Training tokenizer:  20% 1623/8047 [2:08:52<8:28:26,  4.75s/it]loss_total_epoch 61.023985255509615
Training tokenizer:  20% 1624/8047 [2:08:56<8:27:45,  4.74s/it]loss_total_epoch 61.05428755655885
Training tokenizer:  20% 1625/8047 [2:09:01<8:29:03,  4.76s/it]/space/zboucher/anaconda3/envs/project_env/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
Pysteps configuration file found at: /space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/pysteps/pystepsrc

wandb: Currently logged in as: zeina (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.12.7
wandb: Syncing run iconic-butterfly-76
wandb:  View project at https://wandb.ai/zeina/iris
wandb:  View run at https://wandb.ai/zeina/iris/runs/1wbcnbjv
wandb: Run data is saved locally in /space/zboucher/iris/outputs/2023-09-25/16-58-37/wandb/run-20230925_165838-1wbcnbjv
wandb: Run `wandb offline` to turn off syncing.
A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)
[Powered by Stella]
/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)

Tokenizer : shape of latent is (512, 4, 4).
91642241 parameters in agent.tokenizer
20358656 parameters in agent.world_model

Epoch 1 / 200

32183 3493 3560
Training tokenizer:   0% 0/2013 [00:00<?, ?it/s]loss_total_epoch 1.7402417361736298
Training tokenizer:   0% 1/2013 [00:04<2:37:13,  4.69s/it]loss_total_epoch 3.1574997901916504
Training tokenizer:   0% 2/2013 [00:08<2:27:25,  4.40s/it]loss_total_epoch 4.070024743676186
Training tokenizer:   0% 3/2013 [00:13<2:24:55,  4.33s/it]loss_total_epoch 4.92124705016613
Training tokenizer:   0% 4/2013 [00:17<2:23:32,  4.29s/it]loss_total_epoch 5.795354798436165
Training tokenizer:   0% 5/2013 [00:21<2:22:51,  4.27s/it]loss_total_epoch 6.539969101548195
Training tokenizer:   0% 6/2013 [00:25<2:22:20,  4.26s/it]loss_total_epoch 7.2098728865385056
Training tokenizer:   0% 7/2013 [00:30<2:22:08,  4.25s/it]loss_total_epoch 7.894738033413887
Training tokenizer:   0% 8/2013 [00:34<2:22:01,  4.25s/it]loss_total_epoch 8.558931082487106
Training tokenizer:   0% 9/2013 [00:38<2:21:54,  4.25s/it]loss_total_epoch 9.213632494211197
Training tokenizer:   0% 10/2013 [00:42<2:21:53,  4.25s/it]loss_total_epoch 9.793548971414566
Training tokenizer:   1% 11/2013 [00:47<2:22:00,  4.26s/it]loss_total_epoch 10.367791622877121
Training tokenizer:   1% 12/2013 [00:51<2:22:01,  4.26s/it]loss_total_epoch 10.900106772780418
Training tokenizer:   1% 13/2013 [00:55<2:22:02,  4.26s/it]loss_total_epoch 11.444485142827034
Training tokenizer:   1% 14/2013 [00:59<2:22:15,  4.27s/it]loss_total_epoch 11.962931618094444
Training tokenizer:   1% 15/2013 [01:04<2:22:21,  4.27s/it]loss_total_epoch 12.471296653151512
Training tokenizer:   1% 16/2013 [01:08<2:22:26,  4.28s/it]loss_total_epoch 12.96697910130024
Training tokenizer:   1% 17/2013 [01:12<2:22:26,  4.28s/it]loss_total_epoch 13.437959462404251
Training tokenizer:   1% 18/2013 [01:17<2:22:34,  4.29s/it]loss_total_epoch 13.915255464613438
Training tokenizer:   1% 19/2013 [01:21<2:22:49,  4.30s/it]loss_total_epoch 14.365220554172993
Training tokenizer:   1% 20/2013 [01:25<2:22:48,  4.30s/it]loss_total_epoch 14.786691226065159
Training tokenizer:   1% 21/2013 [01:30<2:23:00,  4.31s/it]loss_total_epoch 15.228324227035046
Training tokenizer:   1% 22/2013 [01:34<2:23:18,  4.32s/it]loss_total_epoch 15.6607735902071
Training tokenizer:   1% 23/2013 [01:38<2:23:18,  4.32s/it]loss_total_epoch 16.096870750188828
Training tokenizer:   1% 24/2013 [01:43<2:23:29,  4.33s/it]loss_total_epoch 16.553473711013794
Training tokenizer:   1% 25/2013 [01:47<2:23:25,  4.33s/it]loss_total_epoch 16.980185568332672
Training tokenizer:   1% 26/2013 [01:51<2:23:35,  4.34s/it]loss_total_epoch 17.39334674179554
Training tokenizer:   1% 27/2013 [01:56<2:23:33,  4.34s/it]loss_total_epoch 17.81430061161518
Training tokenizer:   1% 28/2013 [02:00<2:23:27,  4.34s/it]loss_total_epoch 18.210688836872578
Training tokenizer:   1% 29/2013 [02:04<2:23:30,  4.34s/it]loss_total_epoch 18.634637624025345
Training tokenizer:   1% 30/2013 [02:09<2:23:39,  4.35s/it]loss_total_epoch 19.066073462367058
Training tokenizer:   2% 31/2013 [02:13<2:23:40,  4.35s/it]loss_total_epoch 19.484323501586914
Training tokenizer:   2% 32/2013 [02:17<2:23:40,  4.35s/it]loss_total_epoch 19.87735029309988
Training tokenizer:   2% 33/2013 [02:22<2:23:44,  4.36s/it]loss_total_epoch 20.270424596965313
Training tokenizer:   2% 34/2013 [02:26<2:23:43,  4.36s/it]loss_total_epoch 20.652764841914177
Training tokenizer:   2% 35/2013 [02:30<2:23:46,  4.36s/it]loss_total_epoch 21.04792034626007
Training tokenizer:   2% 36/2013 [02:35<2:23:40,  4.36s/it]loss_total_epoch 21.453371711075306
Training tokenizer:   2% 37/2013 [02:39<2:23:43,  4.36s/it]loss_total_epoch 21.80316385626793
Training tokenizer:   2% 38/2013 [02:44<2:23:45,  4.37s/it]loss_total_epoch 22.198792211711407
Training tokenizer:   2% 39/2013 [02:48<2:23:49,  4.37s/it]loss_total_epoch 22.56695793569088
Training tokenizer:   2% 40/2013 [02:52<2:23:56,  4.38s/it]loss_total_epoch 22.960971176624298
Training tokenizer:   2% 41/2013 [02:57<2:23:55,  4.38s/it]loss_total_epoch 23.33179797232151
Training tokenizer:   2% 42/2013 [03:01<2:23:49,  4.38s/it]loss_total_epoch 23.696084387600422
Training tokenizer:   2% 43/2013 [03:05<2:23:51,  4.38s/it]loss_total_epoch 24.03983174264431
Training tokenizer:   2% 44/2013 [03:10<2:23:56,  4.39s/it]loss_total_epoch 24.395809710025787
Training tokenizer:   2% 45/2013 [03:14<2:24:06,  4.39s/it]loss_total_epoch 24.77448994666338
Training tokenizer:   2% 46/2013 [03:19<2:24:03,  4.39s/it]loss_total_epoch 25.138403959572315
Training tokenizer:   2% 47/2013 [03:23<2:24:02,  4.40s/it]loss_total_epoch 25.528165891766548
Training tokenizer:   2% 48/2013 [03:27<2:24:05,  4.40s/it]loss_total_epoch 25.891432255506516
Training tokenizer:   2% 49/2013 [03:32<2:24:12,  4.41s/it]loss_total_epoch 26.237250857055187
Training tokenizer:   2% 50/2013 [03:36<2:24:23,  4.41s/it]loss_total_epoch 26.59647463262081
Training tokenizer:   3% 51/2013 [03:41<2:24:29,  4.42s/it]loss_total_epoch 26.96601176261902
Training tokenizer:   3% 52/2013 [03:45<2:24:16,  4.41s/it]loss_total_epoch 27.32374119758606
Training tokenizer:   3% 53/2013 [03:50<2:24:12,  4.41s/it]loss_total_epoch 27.672355070710182
Training tokenizer:   3% 54/2013 [03:54<2:24:05,  4.41s/it]loss_total_epoch 28.01933528482914
Training tokenizer:   3% 55/2013 [03:58<2:23:56,  4.41s/it]loss_total_epoch 28.340994708240032
Training tokenizer:   3% 56/2013 [04:03<2:23:56,  4.41s/it]loss_total_epoch 28.65255691856146
Training tokenizer:   3% 57/2013 [04:07<2:24:00,  4.42s/it]loss_total_epoch 28.970766216516495
Training tokenizer:   3% 58/2013 [04:12<2:24:10,  4.42s/it]loss_total_epoch 29.287317216396332
Training tokenizer:   3% 59/2013 [04:16<2:24:00,  4.42s/it]loss_total_epoch 29.624656066298485
Training tokenizer:   3% 60/2013 [04:20<2:23:56,  4.42s/it]loss_total_epoch 29.919478699564934
Training tokenizer:   3% 61/2013 [04:25<2:23:57,  4.43s/it]loss_total_epoch 30.206084430217743
Training tokenizer:   3% 62/2013 [04:29<2:23:53,  4.43s/it]loss_total_epoch 30.530318915843964
Training tokenizer:   3% 63/2013 [04:34<2:24:02,  4.43s/it]loss_total_epoch 30.833729803562164
Training tokenizer:   3% 64/2013 [04:38<2:25:08,  4.47s/it]loss_total_epoch 31.112846668809652
Training tokenizer:   3% 65/2013 [04:43<2:24:50,  4.46s/it]loss_total_epoch 31.464783284813166
Training tokenizer:   3% 66/2013 [04:47<2:24:34,  4.46s/it]loss_total_epoch 31.775775261223316
Training tokenizer:   3% 67/2013 [04:52<2:24:28,  4.45s/it]loss_total_epoch 32.06415420770645
Training tokenizer:   3% 68/2013 [04:56<2:24:25,  4.46s/it]loss_total_epoch 32.3986289575696
Training tokenizer:   3% 69/2013 [05:01<2:25:50,  4.50s/it]loss_total_epoch 32.70557340979576
Training tokenizer:   3% 70/2013 [05:05<2:25:18,  4.49s/it]loss_total_epoch 33.003125201910734
Training tokenizer:   4% 71/2013 [05:10<2:25:09,  4.48s/it]loss_total_epoch 33.28372498601675
Training tokenizer:   4% 72/2013 [05:14<2:24:38,  4.47s/it]loss_total_epoch 33.58310858160257
Training tokenizer:   4% 73/2013 [05:19<2:24:28,  4.47s/it]loss_total_epoch 33.915478236973286
Training tokenizer:   4% 74/2013 [05:23<2:24:19,  4.47s/it]loss_total_epoch 34.24993122369051
Training tokenizer:   4% 75/2013 [05:28<2:24:09,  4.46s/it]loss_total_epoch 34.549343049526215
Training tokenizer:   4% 76/2013 [05:32<2:23:57,  4.46s/it]loss_total_epoch 34.86279734969139
Training tokenizer:   4% 77/2013 [05:36<2:23:59,  4.46s/it]loss_total_epoch 35.15556342154741
Training tokenizer:   4% 78/2013 [05:41<2:24:15,  4.47s/it]loss_total_epoch 35.45442195981741
Training tokenizer:   4% 79/2013 [05:45<2:24:22,  4.48s/it]loss_total_epoch 35.7360830232501
Training tokenizer:   4% 80/2013 [05:50<2:24:06,  4.47s/it]loss_total_epoch 36.066687144339085
Training tokenizer:   4% 81/2013 [05:54<2:24:15,  4.48s/it]loss_total_epoch 36.39098457247019
Training tokenizer:   4% 82/2013 [05:59<2:24:10,  4.48s/it]loss_total_epoch 36.708866856992245
Training tokenizer:   4% 83/2013 [06:03<2:24:18,  4.49s/it]loss_total_epoch 37.00478007644415
Training tokenizer:   4% 84/2013 [06:08<2:24:25,  4.49s/it]loss_total_epoch 37.302144788205624
Training tokenizer:   4% 85/2013 [06:12<2:24:23,  4.49s/it]loss_total_epoch 37.58516736328602
Training tokenizer:   4% 86/2013 [06:17<2:24:36,  4.50s/it]loss_total_epoch 37.880166597664356
Training tokenizer:   4% 87/2013 [06:21<2:24:21,  4.50s/it]loss_total_epoch 38.21040863543749
Training tokenizer:   4% 88/2013 [06:26<2:24:28,  4.50s/it]loss_total_epoch 38.51038183271885
Training tokenizer:   4% 89/2013 [06:30<2:24:20,  4.50s/it]loss_total_epoch 38.801166012883186
Training tokenizer:   4% 90/2013 [06:35<2:24:22,  4.50s/it]loss_total_epoch 39.09313593059778
Training tokenizer:   5% 91/2013 [06:39<2:24:33,  4.51s/it]loss_total_epoch 39.3736497759819
Training tokenizer:   5% 92/2013 [06:44<2:24:42,  4.52s/it]loss_total_epoch 39.63276267424226
Training tokenizer:   5% 93/2013 [06:48<2:24:38,  4.52s/it]loss_total_epoch 39.89225699752569
Training tokenizer:   5% 94/2013 [06:53<2:24:41,  4.52s/it]loss_total_epoch 40.18974303454161
Training tokenizer:   5% 95/2013 [06:58<2:24:29,  4.52s/it]loss_total_epoch 40.46179870516062
Training tokenizer:   5% 96/2013 [07:02<2:24:28,  4.52s/it]loss_total_epoch 40.7497630789876
Training tokenizer:   5% 97/2013 [07:07<2:24:15,  4.52s/it]loss_total_epoch 41.02306041121483
Training tokenizer:   5% 98/2013 [07:11<2:24:08,  4.52s/it]loss_total_epoch 41.318314366042614
Training tokenizer:   5% 99/2013 [07:16<2:24:13,  4.52s/it]loss_total_epoch 41.58920245245099
Training tokenizer:   5% 100/2013 [07:20<2:24:11,  4.52s/it]loss_total_epoch 41.841602884233
Training tokenizer:   5% 101/2013 [07:25<2:24:04,  4.52s/it]loss_total_epoch 42.13817869126797
Training tokenizer:   5% 102/2013 [07:29<2:24:04,  4.52s/it]loss_total_epoch 42.44750223308802
Training tokenizer:   5% 103/2013 [07:34<2:24:16,  4.53s/it]loss_total_epoch 42.75353863090277
Training tokenizer:   5% 104/2013 [07:38<2:24:07,  4.53s/it]loss_total_epoch 43.048499122262
Training tokenizer:   5% 105/2013 [07:43<2:24:08,  4.53s/it]loss_total_epoch 43.31126792728901
Training tokenizer:   5% 106/2013 [07:47<2:24:00,  4.53s/it]loss_total_epoch 43.601976335048676
Training tokenizer:   5% 107/2013 [07:52<2:24:00,  4.53s/it]loss_total_epoch 43.890181481838226
Training tokenizer:   5% 108/2013 [07:56<2:24:05,  4.54s/it]loss_total_epoch 44.187240943312645
Training tokenizer:   5% 109/2013 [08:01<2:24:11,  4.54s/it]loss_total_epoch 44.46299409121275
Training tokenizer:   5% 110/2013 [08:06<2:24:11,  4.55s/it]loss_total_epoch 44.77151204645634
Training tokenizer:   6% 111/2013 [08:10<2:24:17,  4.55s/it]loss_total_epoch 45.08188283443451
Training tokenizer:   6% 112/2013 [08:15<2:24:21,  4.56s/it]loss_total_epoch 45.335229221731424
Training tokenizer:   6% 113/2013 [08:19<2:24:15,  4.56s/it]loss_total_epoch 45.60836496576667
Training tokenizer:   6% 114/2013 [08:24<2:24:18,  4.56s/it]loss_total_epoch 45.941111993044615
Training tokenizer:   6% 115/2013 [08:28<2:23:57,  4.55s/it]loss_total_epoch 46.18978061154485
Training tokenizer:   6% 116/2013 [08:33<2:23:49,  4.55s/it]loss_total_epoch 46.45276855677366
Training tokenizer:   6% 117/2013 [08:37<2:23:40,  4.55s/it]loss_total_epoch 46.68672336265445
Training tokenizer:   6% 118/2013 [08:42<2:23:44,  4.55s/it]loss_total_epoch 46.936059430241585
Training tokenizer:   6% 119/2013 [08:46<2:23:38,  4.55s/it]loss_total_epoch 47.22912012040615
Training tokenizer:   6% 120/2013 [08:51<2:23:34,  4.55s/it]loss_total_epoch 47.4960977807641
Training tokenizer:   6% 121/2013 [08:56<2:23:35,  4.55s/it]loss_total_epoch 47.78424076363444
Training tokenizer:   6% 122/2013 [09:00<2:23:24,  4.55s/it]loss_total_epoch 48.05134683474898
Training tokenizer:   6% 123/2013 [09:05<2:23:53,  4.57s/it]loss_total_epoch 48.33730347827077
Training tokenizer:   6% 124/2013 [09:09<2:23:54,  4.57s/it]loss_total_epoch 48.634734746068716
Training tokenizer:   6% 125/2013 [09:14<2:23:50,  4.57s/it]loss_total_epoch 48.9347790107131
Training tokenizer:   6% 126/2013 [09:18<2:23:39,  4.57s/it]loss_total_epoch 49.222229443490505
Training tokenizer:   6% 127/2013 [09:23<2:23:42,  4.57s/it]loss_total_epoch 49.51746718212962
Training tokenizer:   6% 128/2013 [09:28<2:23:39,  4.57s/it]loss_total_epoch 49.783784594386816
Training tokenizer:   6% 129/2013 [09:32<2:23:38,  4.57s/it]loss_total_epoch 50.11213308945298
Training tokenizer:   6% 130/2013 [09:37<2:23:53,  4.58s/it]loss_total_epoch 50.4330566264689
Training tokenizer:   7% 131/2013 [09:41<2:23:52,  4.59s/it]loss_total_epoch 50.71274568513036
Training tokenizer:   7% 132/2013 [09:46<2:23:45,  4.59s/it]loss_total_epoch 50.998282458633184
Training tokenizer:   7% 133/2013 [09:51<2:23:38,  4.58s/it]loss_total_epoch 51.25895683467388
Training tokenizer:   7% 134/2013 [09:55<2:24:06,  4.60s/it]loss_total_epoch 51.53852790594101
Training tokenizer:   7% 135/2013 [10:00<2:23:50,  4.60s/it]loss_total_epoch 51.83516947180033
Training tokenizer:   7% 136/2013 [10:04<2:23:38,  4.59s/it]loss_total_epoch 52.126701816916466
Training tokenizer:   7% 137/2013 [10:09<2:23:35,  4.59s/it]loss_total_epoch 52.39841495081782
Training tokenizer:   7% 138/2013 [10:14<2:23:42,  4.60s/it]loss_total_epoch 52.697430584579706
Training tokenizer:   7% 139/2013 [10:18<2:23:54,  4.61s/it]loss_total_epoch 52.97281949967146
Training tokenizer:   7% 140/2013 [10:23<2:23:55,  4.61s/it]loss_total_epoch 53.246108911931515
Training tokenizer:   7% 141/2013 [10:27<2:23:37,  4.60s/it]loss_total_epoch 53.53294259309769
Training tokenizer:   7% 142/2013 [10:32<2:23:43,  4.61s/it]loss_total_epoch 53.785457260906696
Training tokenizer:   7% 143/2013 [10:37<2:23:25,  4.60s/it]loss_total_epoch 54.05525344610214
Training tokenizer:   7% 144/2013 [10:41<2:23:16,  4.60s/it]loss_total_epoch 54.294990096241236
Training tokenizer:   7% 145/2013 [10:46<2:23:32,  4.61s/it]loss_total_epoch 54.53824935853481
Training tokenizer:   7% 146/2013 [10:50<2:23:27,  4.61s/it]loss_total_epoch 54.81449932977557
Training tokenizer:   7% 147/2013 [10:55<2:23:20,  4.61s/it]loss_total_epoch 55.09957642480731
Training tokenizer:   7% 148/2013 [11:00<2:23:20,  4.61s/it]loss_total_epoch 55.33667175099254
Training tokenizer:   7% 149/2013 [11:04<2:23:20,  4.61s/it]loss_total_epoch 55.629648540169
Training tokenizer:   7% 150/2013 [11:09<2:23:42,  4.63s/it]loss_total_epoch 55.899018466472626
Training tokenizer:   8% 151/2013 [11:14<2:23:33,  4.63s/it]loss_total_epoch 56.18429538607597
Training tokenizer:   8% 152/2013 [11:18<2:23:33,  4.63s/it]loss_total_epoch 56.4871542006731
Training tokenizer:   8% 153/2013 [11:23<2:23:24,  4.63s/it]loss_total_epoch 56.744579162448645
Training tokenizer:   8% 154/2013 [11:27<2:23:31,  4.63s/it]loss_total_epoch 57.01726812124252
Training tokenizer:   8% 155/2013 [11:32<2:23:25,  4.63s/it]loss_total_epoch 57.31053137779236
Training tokenizer:   8% 156/2013 [11:37<2:23:21,  4.63s/it]loss_total_epoch 57.58091274648905
Training tokenizer:   8% 157/2013 [11:41<2:23:20,  4.63s/it]loss_total_epoch 57.85118084400892
Training tokenizer:   8% 158/2013 [11:46<2:23:41,  4.65s/it]loss_total_epoch 58.12171535566449
Training tokenizer:   8% 159/2013 [11:51<2:23:38,  4.65s/it]loss_total_epoch 58.379359588027
Training tokenizer:   8% 160/2013 [11:55<2:23:20,  4.64s/it]loss_total_epoch 58.65166747942567
Training tokenizer:   8% 161/2013 [12:00<2:23:14,  4.64s/it]loss_total_epoch 58.93713727220893
Training tokenizer:   8% 162/2013 [12:05<2:23:22,  4.65s/it]loss_total_epoch 59.24127580597997
Training tokenizer:   8% 163/2013 [12:09<2:23:14,  4.65s/it]loss_total_epoch 59.49595236033201
Training tokenizer:   8% 164/2013 [12:14<2:23:07,  4.64s/it]loss_total_epoch 59.76652133092284
Training tokenizer:   8% 165/2013 [12:19<2:23:13,  4.65s/it]loss_total_epoch 60.04672774299979
Training tokenizer:   8% 166/2013 [12:23<2:23:31,  4.66s/it]loss_total_epoch 60.36069529131055
Training tokenizer:   8% 167/2013 [12:28<2:23:17,  4.66s/it]loss_total_epoch 60.642164800316095
Training tokenizer:   8% 168/2013 [12:33<2:23:22,  4.66s/it]loss_total_epoch 60.951774064451456
Training tokenizer:   8% 169/2013 [12:37<2:23:19,  4.66s/it]loss_total_epoch 61.204967103898525
Training tokenizer:   8% 170/2013 [12:42<2:23:18,  4.67s/it]loss_total_epoch 61.46524949744344
Training tokenizer:   8% 171/2013 [12:47<2:23:31,  4.68s/it]loss_total_epoch 61.73620245605707
Training tokenizer:   9% 172/2013 [12:51<2:23:37,  4.68s/it]loss_total_epoch 62.035034865140915
Training tokenizer:   9% 173/2013 [12:56<2:23:12,  4.67s/it]loss_total_epoch 62.29086894541979
Training tokenizer:   9% 174/2013 [13:01<2:22:59,  4.67s/it]loss_total_epoch 62.55587838590145
Training tokenizer:   9% 175/2013 [13:05<2:23:07,  4.67s/it]loss_total_epoch 62.838814839720726
Training tokenizer:   9% 176/2013 [13:10<2:22:52,  4.67s/it]loss_total_epoch 63.11339070647955
Training tokenizer:   9% 177/2013 [13:15<2:22:54,  4.67s/it]loss_total_epoch 63.39101130142808
Training tokenizer:   9% 178/2013 [13:19<2:22:48,  4.67s/it]loss_total_epoch 63.6813848875463
Training tokenizer:   9% 179/2013 [13:24<2:22:58,  4.68s/it]loss_total_epoch 64.01427586004138
Training tokenizer:   9% 180/2013 [13:29<2:23:00,  4.68s/it]loss_total_epoch 64.3009308539331
Training tokenizer:   9% 181/2013 [13:33<2:22:52,  4.68s/it]loss_total_epoch 64.5852120667696
Training tokenizer:   9% 182/2013 [13:38<2:22:57,  4.68s/it]loss_total_epoch 64.86040814965963
Training tokenizer:   9% 183/2013 [13:43<2:23:11,  4.69s/it]loss_total_epoch 65.14485004171729
Training tokenizer:   9% 184/2013 [13:47<2:22:55,  4.69s/it]loss_total_epoch 65.40817454084754
Training tokenizer:   9% 185/2013 [13:52<2:22:52,  4.69s/it]loss_total_epoch 65.68412028625607
Training tokenizer:   9% 186/2013 [13:57<2:22:45,  4.69s/it]loss_total_epoch 65.95046996697783
Training tokenizer:   9% 187/2013 [14:02<2:22:42,  4.69s/it]loss_total_epoch 66.1937411390245
Training tokenizer:   9% 188/2013 [14:06<2:22:30,  4.69s/it]loss_total_epoch 66.47628547623754
Training tokenizer:   9% 189/2013 [14:11<2:22:42,  4.69s/it]loss_total_epoch 66.74124690890312
Training tokenizer:   9% 190/2013 [14:16<2:22:31,  4.69s/it]loss_total_epoch 67.03966398537159
Training tokenizer:   9% 191/2013 [14:20<2:22:33,  4.69s/it]loss_total_epoch 67.31933727487922
Training tokenizer:  10% 192/2013 [14:25<2:22:30,  4.70s/it]loss_total_epoch 67.58095632866025
Training tokenizer:  10% 193/2013 [14:30<2:22:57,  4.71s/it]loss_total_epoch 67.8259063102305
Training tokenizer:  10% 194/2013 [14:35<2:23:12,  4.72s/it]loss_total_epoch 68.09660215675831
Training tokenizer:  10% 195/2013 [14:39<2:24:05,  4.76s/it]loss_total_epoch 68.37630678340793
Training tokenizer:  10% 196/2013 [14:44<2:23:30,  4.74s/it]loss_total_epoch 68.64828334003687
Training tokenizer:  10% 197/2013 [14:49<2:23:26,  4.74s/it]loss_total_epoch 68.90718786045909
Training tokenizer:  10% 198/2013 [14:54<2:23:12,  4.73s/it]loss_total_epoch 69.19006647542119
Training tokenizer:  10% 199/2013 [14:58<2:23:16,  4.74s/it]loss_total_epoch 69.45069876685739
Training tokenizer:  10% 200/2013 [15:03<2:23:01,  4.73s/it]loss_total_epoch 69.72208466008306
Training tokenizer:  10% 201/2013 [15:08<2:22:38,  4.72s/it]loss_total_epoch 69.99091167002916
Training tokenizer:  10% 202/2013 [15:12<2:22:45,  4.73s/it]loss_total_epoch 70.26230175793171
Training tokenizer:  10% 203/2013 [15:17<2:22:39,  4.73s/it]loss_total_epoch 70.52130390331149
Training tokenizer:  10% 204/2013 [15:22<2:22:48,  4.74s/it]loss_total_epoch 70.76609306409955
Training tokenizer:  10% 205/2013 [15:27<2:22:40,  4.73s/it]loss_total_epoch 71.0409023500979
Training tokenizer:  10% 206/2013 [15:31<2:22:33,  4.73s/it]loss_total_epoch 71.30400223657489
Training tokenizer:  10% 207/2013 [15:36<2:22:42,  4.74s/it]loss_total_epoch 71.56407880410552
Training tokenizer:  10% 208/2013 [15:41<2:22:24,  4.73s/it]loss_total_epoch 71.83456420525908
Training tokenizer:  10% 209/2013 [15:46<2:22:11,  4.73s/it]loss_total_epoch 72.09219593554735
Training tokenizer:  10% 210/2013 [15:50<2:22:20,  4.74s/it]loss_total_epoch 72.36777741461992
Training tokenizer:  10% 211/2013 [15:55<2:22:43,  4.75s/it]loss_total_epoch 72.61882667988539
Training tokenizer:  11% 212/2013 [16:00<2:22:35,  4.75s/it]loss_total_epoch 72.87889108806849
Training tokenizer:  11% 213/2013 [16:05<2:22:30,  4.75s/it]loss_total_epoch 73.14904943853617
Training tokenizer:  11% 214/2013 [16:09<2:22:12,  4.74s/it]loss_total_epoch 73.42871917039156
Training tokenizer:  11% 215/2013 [16:14<2:22:13,  4.75s/it]loss_total_epoch 73.69966246932745
Training tokenizer:  11% 216/2013 [16:19<2:22:23,  4.75s/it]loss_total_epoch 73.97187434136868
Training tokenizer:  11% 217/2013 [16:24<2:22:12,  4.75s/it]loss_total_epoch 74.23076862841845
Training tokenizer:  11% 218/2013 [16:28<2:22:08,  4.75s/it]loss_total_epoch 74.49410493671894
Training tokenizer:  11% 219/2013 [16:33<2:22:25,  4.76s/it]loss_total_epoch 74.75905044004321
Training tokenizer:  11% 220/2013 [16:38<2:22:07,  4.76s/it]loss_total_epoch 75.01627239957452
Training tokenizer:  11% 221/2013 [16:43<2:22:15,  4.76s/it]loss_total_epoch 75.28673420846462
Training tokenizer:  11% 222/2013 [16:47<2:22:18,  4.77s/it]loss_total_epoch 75.57329946756363
Training tokenizer:  11% 223/2013 [16:52<2:22:05,  4.76s/it]loss_total_epoch 75.83988568931818
Training tokenizer:  11% 224/2013 [16:57<2:21:46,  4.75s/it]loss_total_epoch 76.15699238330126
Training tokenizer:  11% 225/2013 [17:02<2:21:54,  4.76s/it]loss_total_epoch 76.444206379354
Training tokenizer:  11% 226/2013 [17:06<2:21:39,  4.76s/it]loss_total_epoch 76.71564186364412
Training tokenizer:  11% 227/2013 [17:11<2:21:46,  4.76s/it]loss_total_epoch 76.99132579937577
Training tokenizer:  11% 228/2013 [17:16<2:21:36,  4.76s/it]loss_total_epoch 77.25007618963718
Training tokenizer:  11% 229/2013 [17:21<2:21:30,  4.76s/it]loss_total_epoch 77.5159389898181
Training tokenizer:  11% 230/2013 [17:26<2:21:47,  4.77s/it]loss_total_epoch 77.78491623327136
Training tokenizer:  11% 231/2013 [17:30<2:21:50,  4.78s/it]loss_total_epoch 78.11031994596124
Training tokenizer:  12% 232/2013 [17:35<2:21:33,  4.77s/it]loss_total_epoch 78.34843568503857
Training tokenizer:  12% 233/2013 [17:40<2:21:26,  4.77s/it]loss_total_epoch 78.61392718926072
Training tokenizer:  12% 234/2013 [17:45<2:21:29,  4.77s/it]loss_total_epoch 78.8858972787857
Training tokenizer:  12% 235/2013 [17:49<2:21:40,  4.78s/it]loss_total_epoch 79.18597214668989
Training tokenizer:  12% 236/2013 [17:54<2:21:47,  4.79s/it]loss_total_epoch 79.46187404170632
Training tokenizer:  12% 237/2013 [17:59<2:21:32,  4.78s/it]loss_total_epoch 79.74368004873395
Training tokenizer:  12% 238/2013 [18:04<2:21:36,  4.79s/it]loss_total_epoch 80.03364209458232
Training tokenizer:  12% 239/2013 [18:09<2:21:47,  4.80s/it]loss_total_epoch 80.3154859803617
Training tokenizer:  12% 240/2013 [18:13<2:21:42,  4.80s/it]loss_total_epoch 80.58018040657043
Training tokenizer:  12% 241/2013 [18:18<2:21:27,  4.79s/it]loss_total_epoch 80.8156359679997
Training tokenizer:  12% 242/2013 [18:23<2:21:31,  4.79s/it]loss_total_epoch 81.10871343687177
Training tokenizer:  12% 243/2013 [18:28<2:21:47,  4.81s/it]loss_total_epoch 81.42804677411914
Training tokenizer:  12% 244/2013 [18:33<2:21:37,  4.80s/it]loss_total_epoch 81.72358019277453
Training tokenizer:  12% 245/2013 [18:37<2:21:30,  4.80s/it]loss_total_epoch 82.00965815410018
Training tokenizer:  12% 246/2013 [18:42<2:21:26,  4.80s/it]loss_total_epoch 82.26574255153537
Training tokenizer:  12% 247/2013 [18:47<2:21:17,  4.80s/it]loss_total_epoch 82.5661401040852
Training tokenizer:  12% 248/2013 [18:52<2:21:12,  4.80s/it]loss_total_epoch 82.85800513252616
Training tokenizer:  12% 249/2013 [18:57<2:21:11,  4.80s/it]loss_total_epoch 83.13948491960764
Training tokenizer:  12% 250/2013 [19:01<2:20:57,  4.80s/it]loss_total_epoch 83.44747520983219
Training tokenizer:  12% 251/2013 [19:06<2:20:51,  4.80s/it]loss_total_epoch 83.73341245949268
Training tokenizer:  13% 252/2013 [19:11<2:20:47,  4.80s/it]loss_total_epoch 83.99748822301626
Training tokenizer:  13% 253/2013 [19:16<2:20:41,  4.80s/it]loss_total_epoch 84.2821374386549
Training tokenizer:  13% 254/2013 [19:21<2:20:48,  4.80s/it]loss_total_epoch 84.56451712548733
Training tokenizer:  13% 255/2013 [19:25<2:20:52,  4.81s/it]loss_total_epoch 84.83622531592846
Training tokenizer:  13% 256/2013 [19:30<2:20:43,  4.81s/it]loss_total_epoch 85.1123715788126
Training tokenizer:  13% 257/2013 [19:35<2:20:35,  4.80s/it]loss_total_epoch 85.37441382184625
Training tokenizer:  13% 258/2013 [19:40<2:20:38,  4.81s/it]loss_total_epoch 85.63851978629827
Training tokenizer:  13% 259/2013 [19:45<2:20:46,  4.82s/it]loss_total_epoch 85.91852477192879
Training tokenizer:  13% 260/2013 [19:50<2:20:41,  4.82s/it]loss_total_epoch 86.18931310623884
Training tokenizer:  13% 261/2013 [19:54<2:20:48,  4.82s/it]loss_total_epoch 86.47018484771252
Training tokenizer:  13% 262/2013 [19:59<2:21:06,  4.84s/it]loss_total_epoch 86.74722772836685
Training tokenizer:  13% 263/2013 [20:04<2:20:47,  4.83s/it]loss_total_epoch 87.04819133877754
Training tokenizer:  13% 264/2013 [20:09<2:21:02,  4.84s/it]loss_total_epoch 87.33552435785532
Training tokenizer:  13% 265/2013 [20:14<2:20:40,  4.83s/it]loss_total_epoch 87.61297450214624
Training tokenizer:  13% 266/2013 [20:19<2:20:47,  4.84s/it]loss_total_epoch 87.87927174568176
Training tokenizer:  13% 267/2013 [20:23<2:20:50,  4.84s/it]loss_total_epoch 88.14839332923293
Training tokenizer:  13% 268/2013 [20:28<2:20:55,  4.85s/it]loss_total_epoch 88.41886987537146
Training tokenizer:  13% 269/2013 [20:33<2:20:51,  4.85s/it]loss_total_epoch 88.70002075657248
Training tokenizer:  13% 270/2013 [20:38<2:20:48,  4.85s/it]loss_total_epoch 88.95924594253302
Training tokenizer:  13% 271/2013 [20:43<2:20:34,  4.84s/it]loss_total_epoch 89.23575695976615
Training tokenizer:  14% 272/2013 [20:48<2:20:36,  4.85s/it]loss_total_epoch 89.4845179580152
Training tokenizer:  14% 273/2013 [20:52<2:20:26,  4.84s/it]loss_total_epoch 89.76576131209731
Training tokenizer:  14% 274/2013 [20:57<2:20:27,  4.85s/it]loss_total_epoch 90.0386229865253
Training tokenizer:  14% 275/2013 [21:02<2:20:34,  4.85s/it]loss_total_epoch 90.3308303207159
Training tokenizer:  14% 276/2013 [21:07<2:20:41,  4.86s/it]loss_total_epoch 90.62165071070194
Training tokenizer:  14% 277/2013 [21:12<2:20:30,  4.86s/it]loss_total_epoch 90.89239083603024
Training tokenizer:  14% 278/2013 [21:17<2:20:39,  4.86s/it]loss_total_epoch 91.17833762988448
Training tokenizer:  14% 279/2013 [21:22<2:20:42,  4.87s/it]loss_total_epoch 91.46215346083045
Training tokenizer:  14% 280/2013 [21:27<2:20:30,  4.86s/it]loss_total_epoch 91.72873244434595
Training tokenizer:  14% 281/2013 [21:31<2:20:15,  4.86s/it]loss_total_epoch 91.98980871960521
Training tokenizer:  14% 282/2013 [21:36<2:20:16,  4.86s/it]loss_total_epoch 92.2236636057496
Training tokenizer:  14% 283/2013 [21:41<2:20:01,  4.86s/it]loss_total_epoch 92.51056356728077
Training tokenizer:  14% 284/2013 [21:46<2:19:57,  4.86s/it]loss_total_epoch 92.77761712670326
Training tokenizer:  14% 285/2013 [21:51<2:20:01,  4.86s/it]loss_total_epoch 93.06341244280338
Training tokenizer:  14% 286/2013 [21:56<2:20:07,  4.87s/it]loss_total_epoch 93.28180715814233
Training tokenizer:  14% 287/2013 [22:01<2:19:56,  4.86s/it]loss_total_epoch 93.56313469260931
Training tokenizer:  14% 288/2013 [22:05<2:20:02,  4.87s/it]loss_total_epoch 93.84788980334997
Training tokenizer:  14% 289/2013 [22:10<2:20:00,  4.87s/it]loss_total_epoch 94.12169325351715
Training tokenizer:  14% 290/2013 [22:15<2:20:08,  4.88s/it]loss_total_epoch 94.37775334343314
Training tokenizer:  14% 291/2013 [22:20<2:20:15,  4.89s/it]loss_total_epoch 94.64275258779526
Training tokenizer:  15% 292/2013 [22:25<2:19:53,  4.88s/it]loss_total_epoch 94.89199562743306
Training tokenizer:  15% 293/2013 [22:30<2:20:05,  4.89s/it]loss_total_epoch 95.13981983438134
Training tokenizer:  15% 294/2013 [22:35<2:19:53,  4.88s/it]loss_total_epoch 95.43271185085177
Training tokenizer:  15% 295/2013 [22:40<2:19:39,  4.88s/it]loss_total_epoch 95.71333338320255
Training tokenizer:  15% 296/2013 [22:45<2:19:47,  4.88s/it]loss_total_epoch 96.01127929985523
Training tokenizer:  15% 297/2013 [22:49<2:19:48,  4.89s/it]loss_total_epoch 96.29518797248602
Training tokenizer:  15% 298/2013 [22:54<2:19:44,  4.89s/it]loss_total_epoch 96.57491267472506
Training tokenizer:  15% 299/2013 [22:59<2:19:34,  4.89s/it]loss_total_epoch 96.86477469280362
Training tokenizer:  15% 300/2013 [23:04<2:20:12,  4.91s/it]loss_total_epoch 97.13267955556512
Training tokenizer:  15% 301/2013 [23:09<2:20:14,  4.91s/it]loss_total_epoch 97.41163378581405
Training tokenizer:  15% 302/2013 [23:14<2:20:13,  4.92s/it]loss_total_epoch 97.69909413531423
Training tokenizer:  15% 303/2013 [23:19<2:20:13,  4.92s/it]loss_total_epoch 97.94859502464533
Training tokenizer:  15% 304/2013 [23:24<2:20:02,  4.92s/it]loss_total_epoch 98.22464185208082
Training tokenizer:  15% 305/2013 [23:29<2:19:47,  4.91s/it]loss_total_epoch 98.49261755496264
Training tokenizer:  15% 306/2013 [23:34<2:20:00,  4.92s/it]loss_total_epoch 98.75742982327938
Training tokenizer:  15% 307/2013 [23:39<2:19:54,  4.92s/it]loss_total_epoch 98.97531841695309
Training tokenizer:  15% 308/2013 [23:43<2:19:29,  4.91s/it]loss_total_epoch 99.23131630942225
Training tokenizer:  15% 309/2013 [23:48<2:19:47,  4.92s/it]loss_total_epoch 99.51116383820772
Training tokenizer:  15% 310/2013 [23:53<2:19:59,  4.93s/it]loss_total_epoch 99.78835685178638
Training tokenizer:  15% 311/2013 [23:58<2:19:43,  4.93s/it]loss_total_epoch 100.05713172256947
Training tokenizer:  15% 312/2013 [24:03<2:19:46,  4.93s/it]loss_total_epoch 100.32515953481197
Training tokenizer:  16% 313/2013 [24:08<2:19:45,  4.93s/it]loss_total_epoch 100.62264906615019
Training tokenizer:  16% 314/2013 [24:13<2:19:49,  4.94s/it]loss_total_epoch 100.8976300060749
Training tokenizer:  16% 315/2013 [24:18<2:19:40,  4.94s/it]loss_total_epoch 101.16366748139262
Training tokenizer:  16% 316/2013 [24:23<2:19:36,  4.94s/it]loss_total_epoch 101.46838045492768
Training tokenizer:  16% 317/2013 [24:28<2:19:16,  4.93s/it]loss_total_epoch 101.72664311528206
Training tokenizer:  16% 318/2013 [24:33<2:19:25,  4.94s/it]loss_total_epoch 101.99654731526971
Training tokenizer:  16% 319/2013 [24:38<2:19:10,  4.93s/it]loss_total_epoch 102.28095326200128
Training tokenizer:  16% 320/2013 [24:43<2:18:56,  4.92s/it]loss_total_epoch 102.58107091858983
Training tokenizer:  16% 321/2013 [24:48<2:18:55,  4.93s/it]loss_total_epoch 102.8416398987174
Training tokenizer:  16% 322/2013 [24:53<2:18:48,  4.93s/it]loss_total_epoch 103.12371150776744
Training tokenizer:  16% 323/2013 [24:58<2:19:06,  4.94s/it]loss_total_epoch 103.37858217954636
Training tokenizer:  16% 324/2013 [25:02<2:19:13,  4.95s/it]loss_total_epoch 103.64745919406414
Training tokenizer:  16% 325/2013 [25:07<2:19:07,  4.95s/it]loss_total_epoch 103.91194289550185
Training tokenizer:  16% 326/2013 [25:12<2:19:15,  4.95s/it]loss_total_epoch 104.19631370157003
Training tokenizer:  16% 327/2013 [25:17<2:19:21,  4.96s/it]loss_total_epoch 104.48454956710339
Training tokenizer:  16% 328/2013 [25:22<2:19:23,  4.96s/it]loss_total_epoch 104.73645539954305
Training tokenizer:  16% 329/2013 [25:27<2:19:18,  4.96s/it]loss_total_epoch 104.98436756059527
Training tokenizer:  16% 330/2013 [25:32<2:19:05,  4.96s/it]loss_total_epoch 105.238451551646
Training tokenizer:  16% 331/2013 [25:37<2:19:09,  4.96s/it]loss_total_epoch 105.4898933134973
Training tokenizer:  16% 332/2013 [25:42<2:18:53,  4.96s/it]loss_total_epoch 105.76794756576419
Training tokenizer:  17% 333/2013 [25:47<2:19:08,  4.97s/it]loss_total_epoch 106.01127972081304
Training tokenizer:  17% 334/2013 [25:52<2:19:07,  4.97s/it]loss_total_epoch 106.30787747725844
Training tokenizer:  17% 335/2013 [25:57<2:19:03,  4.97s/it]loss_total_epoch 106.56197568029165
Training tokenizer:  17% 336/2013 [26:02<2:19:11,  4.98s/it]loss_total_epoch 106.83172650635242
Training tokenizer:  17% 337/2013 [26:07<2:19:03,  4.98s/it]loss_total_epoch 107.1004267334938
Training tokenizer:  17% 338/2013 [26:12<2:19:10,  4.99s/it]loss_total_epoch 107.3871668279171
Training tokenizer:  17% 339/2013 [26:17<2:18:48,  4.98s/it]loss_total_epoch 107.64480815827847
Training tokenizer:  17% 340/2013 [26:22<2:18:54,  4.98s/it]loss_total_epoch 107.88806901872158
Training tokenizer:  17% 341/2013 [26:27<2:18:37,  4.97s/it]loss_total_epoch 108.16089405491948
Training tokenizer:  17% 342/2013 [26:32<2:18:44,  4.98s/it]loss_total_epoch 108.44869377091527
Training tokenizer:  17% 343/2013 [26:37<2:18:45,  4.99s/it]loss_total_epoch 108.72321453690529
Training tokenizer:  17% 344/2013 [26:42<2:18:56,  5.00s/it]loss_total_epoch 108.99129390716553
Training tokenizer:  17% 345/2013 [26:47<2:19:02,  5.00s/it]loss_total_epoch 109.28979667276144
Training tokenizer:  17% 346/2013 [26:52<2:18:52,  5.00s/it]loss_total_epoch 109.54219077900052
Training tokenizer:  17% 347/2013 [26:57<2:18:50,  5.00s/it]loss_total_epoch 109.78491605445743
Training tokenizer:  17% 348/2013 [27:02<2:18:50,  5.00s/it]loss_total_epoch 110.07044868171215
Training tokenizer:  17% 349/2013 [27:07<2:18:52,  5.01s/it]loss_total_epoch 110.32300450280309
Training tokenizer:  17% 350/2013 [27:12<2:18:56,  5.01s/it]loss_total_epoch 110.59152900055051
Training tokenizer:  17% 351/2013 [27:17<2:18:55,  5.02s/it]loss_total_epoch 110.83933531492949
Training tokenizer:  17% 352/2013 [27:22<2:18:50,  5.02s/it]loss_total_epoch 111.13153064996004
Training tokenizer:  18% 353/2013 [27:27<2:18:56,  5.02s/it]loss_total_epoch 111.3852281793952
Training tokenizer:  18% 354/2013 [27:32<2:18:52,  5.02s/it]loss_total_epoch 111.64340170100331
Training tokenizer:  18% 355/2013 [27:37<2:18:23,  5.01s/it]loss_total_epoch 111.9184255413711
Training tokenizer:  18% 356/2013 [27:42<2:17:58,  5.00s/it]loss_total_epoch 112.1706731915474
Training tokenizer:  18% 357/2013 [27:47<2:17:50,  4.99s/it]loss_total_epoch 112.422826372087
Training tokenizer:  18% 358/2013 [27:52<2:18:06,  5.01s/it]loss_total_epoch 112.67817119508982
Training tokenizer:  18% 359/2013 [27:57<2:18:04,  5.01s/it]loss_total_epoch 112.91905001550913
Training tokenizer:  18% 360/2013 [28:02<2:18:13,  5.02s/it]loss_total_epoch 113.19547859206796
Training tokenizer:  18% 361/2013 [28:07<2:18:16,  5.02s/it]loss_total_epoch 113.43902388960123
Training tokenizer:  18% 362/2013 [28:12<2:18:02,  5.02s/it]loss_total_epoch 113.72415837273002
Training tokenizer:  18% 363/2013 [28:17<2:18:04,  5.02s/it]loss_total_epoch 114.00208865106106
Training tokenizer:  18% 364/2013 [28:22<2:18:28,  5.04s/it]loss_total_epoch 114.27391626685858
Training tokenizer:  18% 365/2013 [28:27<2:18:15,  5.03s/it]loss_total_epoch 114.5586052685976
Training tokenizer:  18% 366/2013 [28:32<2:18:02,  5.03s/it]loss_total_epoch 114.83353845775127
Training tokenizer:  18% 367/2013 [28:37<2:17:59,  5.03s/it]loss_total_epoch 115.12283916026354
Training tokenizer:  18% 368/2013 [28:42<2:17:55,  5.03s/it]loss_total_epoch 115.43386326730251
Training tokenizer:  18% 369/2013 [28:48<2:18:14,  5.05s/it]loss_total_epoch 115.731051646173
Training tokenizer:  18% 370/2013 [28:53<2:17:54,  5.04s/it]loss_total_epoch 116.05960354208946
Training tokenizer:  18% 371/2013 [28:58<2:18:04,  5.05s/it]loss_total_epoch 116.35592259466648
Training tokenizer:  18% 372/2013 [29:03<2:18:04,  5.05s/it]loss_total_epoch 116.66024467349052
Training tokenizer:  19% 373/2013 [29:08<2:18:00,  5.05s/it]loss_total_epoch 116.93078964203596
Training tokenizer:  19% 374/2013 [29:13<2:17:59,  5.05s/it]loss_total_epoch 117.19787333533168
Training tokenizer:  19% 375/2013 [29:18<2:17:59,  5.05s/it]loss_total_epoch 117.46027459576726
Training tokenizer:  19% 376/2013 [29:23<2:17:45,  5.05s/it]loss_total_epoch 117.72112902253866
Training tokenizer:  19% 377/2013 [29:28<2:17:35,  5.05s/it]loss_total_epoch 117.99067479372025
Training tokenizer:  19% 378/2013 [29:33<2:17:27,  5.04s/it]loss_total_epoch 118.26530294492841
Training tokenizer:  19% 379/2013 [29:38<2:17:14,  5.04s/it]loss_total_epoch 118.57161671295762
Training tokenizer:  19% 380/2013 [29:43<2:17:01,  5.03s/it]loss_total_epoch 118.8427028618753
Training tokenizer:  19% 381/2013 [29:48<2:17:01,  5.04s/it]loss_total_epoch 119.14465166255832
Training tokenizer:  19% 382/2013 [29:53<2:17:08,  5.04s/it]loss_total_epoch 119.43069052323699
Training tokenizer:  19% 383/2013 [29:58<2:17:14,  5.05s/it]loss_total_epoch 119.70038916543126
Training tokenizer:  19% 384/2013 [30:03<2:17:28,  5.06s/it]loss_total_epoch 119.94273917376995
Training tokenizer:  19% 385/2013 [30:08<2:17:27,  5.07s/it]loss_total_epoch 120.19900747388601
Training tokenizer:  19% 386/2013 [30:13<2:17:26,  5.07s/it]loss_total_epoch 120.46577234566212
Training tokenizer:  19% 387/2013 [30:19<2:17:35,  5.08s/it]loss_total_epoch 120.7527004852891
Training tokenizer:  19% 388/2013 [30:24<2:17:22,  5.07s/it]loss_total_epoch 121.01559849828482
Training tokenizer:  19% 389/2013 [30:29<2:17:29,  5.08s/it]loss_total_epoch 121.29438921809196
Training tokenizer:  19% 390/2013 [30:34<2:17:28,  5.08s/it]loss_total_epoch 121.56671044230461
Training tokenizer:  19% 391/2013 [30:39<2:17:30,  5.09s/it]loss_total_epoch 121.83593599125743
Training tokenizer:  19% 392/2013 [30:44<2:17:24,  5.09s/it]loss_total_epoch 122.14051257446408
Training tokenizer:  20% 393/2013 [30:49<2:17:12,  5.08s/it]loss_total_epoch 122.39996626228094
Training tokenizer:  20% 394/2013 [30:54<2:17:00,  5.08s/it]loss_total_epoch 122.66885048151016
Training tokenizer:  20% 395/2013 [30:59<2:16:38,  5.07s/it]loss_total_epoch 122.9516830444336
Training tokenizer:  20% 396/2013 [31:04<2:16:29,  5.06s/it]loss_total_epoch 123.23235924169421
Training tokenizer:  20% 397/2013 [31:09<2:16:55,  5.08s/it]loss_total_epoch 123.50825707986951
Training tokenizer:  20% 398/2013 [31:14<2:17:08,  5.10s/it]loss_total_epoch 123.77805399894714
Training tokenizer:  20% 399/2013 [31:20<2:17:06,  5.10s/it]loss_total_epoch 124.07357105240226
Training tokenizer:  20% 400/2013 [31:25<2:16:52,  5.09s/it]loss_total_epoch 124.31712809205055
Training tokenizer:  20% 401/2013 [31:30<2:16:43,  5.09s/it]loss_total_epoch 124.57313175499439
Training tokenizer:  20% 402/2013 [31:35<2:16:35,  5.09s/it]loss_total_epoch 124.84638554602861
Training tokenizer:  20% 403/2013 [31:40<2:16:36,  5.09s/it]loss_total_epoch 125.09326650947332
Training tokenizer:  20% 404/2013 [31:45<2:16:49,  5.10s/it]loss_total_epoch 125.38705889880657
Training tokenizer:  20% 405/2013 [31:50<2:17:02,  5.11s/it]loss_total_epoch 125.63650266453624
Training tokenizer:  20% 406/2013 [31:55<2:16:51,  5.11s/it]loss_total_epoch 125.90375539660454
Training tokenizer:  20% 407/2013 [32:00<2:16:40,  5.11s/it]loss_total_epoch 126.16005463153124
Training tokenizer:  20% 408/2013 [32:05<2:16:45,  5.11s/it]loss_total_epoch 126.46186446398497
Training tokenizer:  20% 409/2013 [32:11<2:16:46,  5.12s/it]loss_total_epoch 126.73016825318336
Training tokenizer:  20% 410/2013 [32:16<2:16:36,  5.11s/it]loss_total_epoch 127.0096485465765
Training tokenizer:  20% 411/2013 [32:21<2:16:42,  5.12s/it]loss_total_epoch 127.28106897324324
Training tokenizer:  20% 412/2013 [32:26<2:16:32,  5.12s/it]loss_total_epoch 127.55180402845144
Training tokenizer:  21% 413/2013 [32:31<2:19:15,  5.22s/it]loss_total_epoch 127.82511571794748
Training tokenizer:  21% 414/2013 [32:37<2:18:27,  5.20s/it]loss_total_epoch 128.08584474027157
Training tokenizer:  21% 415/2013 [32:42<2:18:12,  5.19s/it]loss_total_epoch 128.34921475872397
Training tokenizer:  21% 416/2013 [32:47<2:17:40,  5.17s/it]loss_total_epoch 128.58680547773838
Training tokenizer:  21% 417/2013 [32:52<2:17:10,  5.16s/it]loss_total_epoch 128.85781925916672
Training tokenizer:  21% 418/2013 [32:57<2:16:48,  5.15s/it]loss_total_epoch 129.1093213558197
Training tokenizer:  21% 419/2013 [33:02<2:16:18,  5.13s/it]loss_total_epoch 129.38934015855193
Training tokenizer:  21% 420/2013 [33:07<2:16:46,  5.15s/it]loss_total_epoch 129.67962865903974
Training tokenizer:  21% 421/2013 [33:13<2:16:37,  5.15s/it]loss_total_epoch 129.96288783848286
Training tokenizer:  21% 422/2013 [33:18<2:16:26,  5.15s/it]loss_total_epoch 130.21270187944174
Training tokenizer:  21% 423/2013 [33:23<2:15:59,  5.13s/it]loss_total_epoch 130.47647109255195
Training tokenizer:  21% 424/2013 [33:28<2:16:08,  5.14s/it]loss_total_epoch 130.73528084531426
Training tokenizer:  21% 425/2013 [33:33<2:16:10,  5.15s/it]loss_total_epoch 131.01696395501494
Training tokenizer:  21% 426/2013 [33:38<2:16:04,  5.14s/it]loss_total_epoch 131.26062425225973
Training tokenizer:  21% 427/2013 [33:44<2:18:10,  5.23s/it]loss_total_epoch 131.54179133474827
Training tokenizer:  21% 428/2013 [33:49<2:17:15,  5.20s/it]loss_total_epoch 131.80457258597016
Training tokenizer:  21% 429/2013 [33:54<2:16:34,  5.17s/it]loss_total_epoch 132.07634329423308
Training tokenizer:  21% 430/2013 [33:59<2:16:02,  5.16s/it]loss_total_epoch 132.3179721608758
Training tokenizer:  21% 431/2013 [34:04<2:15:49,  5.15s/it]loss_total_epoch 132.60901406407356
Training tokenizer:  21% 432/2013 [34:09<2:16:01,  5.16s/it]loss_total_epoch 132.87563045322895
Training tokenizer:  22% 433/2013 [34:15<2:17:47,  5.23s/it]loss_total_epoch 133.1629715114832
Training tokenizer:  22% 434/2013 [34:20<2:17:22,  5.22s/it]loss_total_epoch 133.4295721873641
Training tokenizer:  22% 435/2013 [34:25<2:16:34,  5.19s/it]loss_total_epoch 133.69968182966113
Training tokenizer:  22% 436/2013 [34:30<2:16:29,  5.19s/it]loss_total_epoch 133.97539687529206
Training tokenizer:  22% 437/2013 [34:35<2:15:51,  5.17s/it]loss_total_epoch 134.23789754137397
Training tokenizer:  22% 438/2013 [34:41<2:15:53,  5.18s/it]loss_total_epoch 134.4916231893003
Training tokenizer:  22% 439/2013 [34:46<2:15:47,  5.18s/it]loss_total_epoch 134.72946149110794
Training tokenizer:  22% 440/2013 [34:51<2:15:58,  5.19s/it]loss_total_epoch 135.03493679314852
Training tokenizer:  22% 441/2013 [34:56<2:15:26,  5.17s/it]loss_total_epoch 135.3236806690693
Training tokenizer:  22% 442/2013 [35:01<2:15:15,  5.17s/it]loss_total_epoch 135.60380487889051
Training tokenizer:  22% 443/2013 [35:06<2:15:13,  5.17s/it]loss_total_epoch 135.84839997068048
Training tokenizer:  22% 444/2013 [35:12<2:14:54,  5.16s/it]loss_total_epoch 136.11848205700517
Training tokenizer:  22% 445/2013 [35:17<2:15:13,  5.17s/it]loss_total_epoch 136.39513629302382
Training tokenizer:  22% 446/2013 [35:22<2:15:22,  5.18s/it]loss_total_epoch 136.68250399455428
Training tokenizer:  22% 447/2013 [35:27<2:15:10,  5.18s/it]loss_total_epoch 136.95576559007168
Training tokenizer:  22% 448/2013 [35:32<2:15:00,  5.18s/it]loss_total_epoch 137.20620189979672
Training tokenizer:  22% 449/2013 [35:38<2:16:36,  5.24s/it]loss_total_epoch 137.49771155044436
Training tokenizer:  22% 450/2013 [35:43<2:16:24,  5.24s/it]loss_total_epoch 137.76064838469028
Training tokenizer:  22% 451/2013 [35:48<2:15:46,  5.22s/it]loss_total_epoch 138.02701830118895
Training tokenizer:  22% 452/2013 [35:53<2:15:37,  5.21s/it]loss_total_epoch 138.31181202083826
Training tokenizer:  23% 453/2013 [35:59<2:15:50,  5.22s/it]loss_total_epoch 138.56557174772024
Training tokenizer:  23% 454/2013 [36:04<2:15:27,  5.21s/it]loss_total_epoch 138.8485541678965
Training tokenizer:  23% 455/2013 [36:09<2:15:13,  5.21s/it]loss_total_epoch 139.15217019245028
Training tokenizer:  23% 456/2013 [36:14<2:15:17,  5.21s/it]loss_total_epoch 139.41793873906136
Training tokenizer:  23% 457/2013 [36:19<2:15:08,  5.21s/it]loss_total_epoch 139.67661509662867
Training tokenizer:  23% 458/2013 [36:25<2:15:10,  5.22s/it]loss_total_epoch 139.92874171957374
Training tokenizer:  23% 459/2013 [36:30<2:15:17,  5.22s/it]loss_total_epoch 140.184751752764
Training tokenizer:  23% 460/2013 [36:35<2:15:09,  5.22s/it]loss_total_epoch 140.48658439144492
Training tokenizer:  23% 461/2013 [36:40<2:15:10,  5.23s/it]loss_total_epoch 140.75949419289827
Training tokenizer:  23% 462/2013 [36:46<2:15:16,  5.23s/it]loss_total_epoch 141.02254156768322
Training tokenizer:  23% 463/2013 [36:51<2:15:00,  5.23s/it]loss_total_epoch 141.30641485005617
Training tokenizer:  23% 464/2013 [36:56<2:14:53,  5.22s/it]loss_total_epoch 141.5846707895398
Training tokenizer:  23% 465/2013 [37:01<2:14:44,  5.22s/it]loss_total_epoch 141.87502214312553
Training tokenizer:  23% 466/2013 [37:06<2:14:28,  5.22s/it]loss_total_epoch 142.15138382464647
Training tokenizer:  23% 467/2013 [37:12<2:14:47,  5.23s/it]loss_total_epoch 142.44114790856838
Training tokenizer:  23% 468/2013 [37:17<2:14:51,  5.24s/it]loss_total_epoch 142.69711973890662
Training tokenizer:  23% 469/2013 [37:22<2:14:54,  5.24s/it]loss_total_epoch 142.9787819944322
Training tokenizer:  23% 470/2013 [37:27<2:14:52,  5.24s/it]loss_total_epoch 143.22236320748925
Training tokenizer:  23% 471/2013 [37:33<2:14:48,  5.25s/it]loss_total_epoch 143.50661225244403
Training tokenizer:  23% 472/2013 [37:38<2:14:28,  5.24s/it]loss_total_epoch 143.74448762461543
Training tokenizer:  23% 473/2013 [37:43<2:14:35,  5.24s/it]loss_total_epoch 144.0118915811181
Training tokenizer:  24% 474/2013 [37:48<2:14:47,  5.25s/it]loss_total_epoch 144.28397193178535
Training tokenizer:  24% 475/2013 [37:54<2:14:44,  5.26s/it]loss_total_epoch 144.5575850084424
Training tokenizer:  24% 476/2013 [37:59<2:14:31,  5.25s/it]loss_total_epoch 144.82660030201077
Training tokenizer:  24% 477/2013 [38:04<2:14:18,  5.25s/it]loss_total_epoch 145.0735871680081
Training tokenizer:  24% 478/2013 [38:09<2:14:05,  5.24s/it]loss_total_epoch 145.34920893236995
Training tokenizer:  24% 479/2013 [38:15<2:13:49,  5.23s/it]loss_total_epoch 145.61589713022113
Training tokenizer:  24% 480/2013 [38:20<2:13:50,  5.24s/it]loss_total_epoch 145.90723511204123
Training tokenizer:  24% 481/2013 [38:25<2:14:14,  5.26s/it]loss_total_epoch 146.15659011900425
Training tokenizer:  24% 482/2013 [38:30<2:13:39,  5.24s/it]loss_total_epoch 146.39814902096987
Training tokenizer:  24% 483/2013 [38:36<2:13:51,  5.25s/it]loss_total_epoch 146.6634830981493
Training tokenizer:  24% 484/2013 [38:41<2:13:50,  5.25s/it]loss_total_epoch 146.93595491349697
Training tokenizer:  24% 485/2013 [38:46<2:13:39,  5.25s/it]loss_total_epoch 147.1939461529255
Training tokenizer:  24% 486/2013 [38:51<2:13:40,  5.25s/it]loss_total_epoch 147.46920513361692
Training tokenizer:  24% 487/2013 [38:57<2:13:19,  5.24s/it]loss_total_epoch 147.73826647922397
Training tokenizer:  24% 488/2013 [39:02<2:13:24,  5.25s/it]loss_total_epoch 147.97833364084363
Training tokenizer:  24% 489/2013 [39:07<2:13:21,  5.25s/it]loss_total_epoch 148.23519295081496
Training tokenizer:  24% 490/2013 [39:12<2:13:11,  5.25s/it]loss_total_epoch 148.5199039913714
Training tokenizer:  24% 491/2013 [39:18<2:13:28,  5.26s/it]loss_total_epoch 148.7625189088285
Training tokenizer:  24% 492/2013 [39:23<2:13:15,  5.26s/it]loss_total_epoch 149.01966985687613
Training tokenizer:  24% 493/2013 [39:28<2:13:24,  5.27s/it]loss_total_epoch 149.29597280547023
Training tokenizer:  25% 494/2013 [39:33<2:13:35,  5.28s/it]loss_total_epoch 149.55886264890432
Training tokenizer:  25% 495/2013 [39:39<2:13:42,  5.28s/it]loss_total_epoch 149.81839125230908
Training tokenizer:  25% 496/2013 [39:44<2:13:50,  5.29s/it]loss_total_epoch 150.08444630727172
Training tokenizer:  25% 497/2013 [39:49<2:13:22,  5.28s/it]loss_total_epoch 150.36321276798844
Training tokenizer:  25% 498/2013 [39:55<2:12:57,  5.27s/it]loss_total_epoch 150.64343562349677
Training tokenizer:  25% 499/2013 [40:00<2:13:03,  5.27s/it]loss_total_epoch 150.89063587784767
Training tokenizer:  25% 500/2013 [40:05<2:12:51,  5.27s/it]loss_total_epoch 151.14714906737208
Training tokenizer:  25% 501/2013 [40:10<2:13:01,  5.28s/it]loss_total_epoch 151.41671887412667
Training tokenizer:  25% 502/2013 [40:16<2:12:25,  5.26s/it]loss_total_epoch 151.69263846054673
Training tokenizer:  25% 503/2013 [40:21<2:12:45,  5.28s/it]loss_total_epoch 151.95770862698555
Training tokenizer:  25% 504/2013 [40:26<2:12:50,  5.28s/it]loss_total_epoch 152.22800812497735
Training tokenizer:  25% 505/2013 [40:32<2:13:05,  5.30s/it]loss_total_epoch 152.47570656985044
Training tokenizer:  25% 506/2013 [40:37<2:13:15,  5.31s/it]loss_total_epoch 152.76108137145638
Training tokenizer:  25% 507/2013 [40:42<2:13:03,  5.30s/it]loss_total_epoch 153.01155043765903
Training tokenizer:  25% 508/2013 [40:47<2:12:43,  5.29s/it]loss_total_epoch 153.2917831465602
Training tokenizer:  25% 509/2013 [40:53<2:12:46,  5.30s/it]loss_total_epoch 153.56044942885637
Training tokenizer:  25% 510/2013 [40:58<2:13:25,  5.33s/it]loss_total_epoch 153.8536513671279
Training tokenizer:  25% 511/2013 [41:03<2:13:13,  5.32s/it]loss_total_epoch 154.0999519713223
Training tokenizer:  25% 512/2013 [41:09<2:13:01,  5.32s/it]loss_total_epoch 154.38143308088183
Training tokenizer:  25% 513/2013 [41:14<2:13:29,  5.34s/it]loss_total_epoch 154.64852260798216
Training tokenizer:  26% 514/2013 [41:20<2:13:23,  5.34s/it]loss_total_epoch 154.9086406864226
Training tokenizer:  26% 515/2013 [41:25<2:13:24,  5.34s/it]loss_total_epoch 155.17739813029766
Training tokenizer:  26% 516/2013 [41:30<2:12:57,  5.33s/it]loss_total_epoch 155.45258204638958
Training tokenizer:  26% 517/2013 [41:35<2:12:55,  5.33s/it]loss_total_epoch 155.72980764508247
Training tokenizer:  26% 518/2013 [41:41<2:12:56,  5.34s/it]loss_total_epoch 155.98875926062465
Training tokenizer:  26% 519/2013 [41:46<2:12:55,  5.34s/it]loss_total_epoch 156.26547892391682
Training tokenizer:  26% 520/2013 [41:51<2:12:34,  5.33s/it]loss_total_epoch 156.53011387586594
Training tokenizer:  26% 521/2013 [41:57<2:12:19,  5.32s/it]loss_total_epoch 156.8031643331051
Training tokenizer:  26% 522/2013 [42:02<2:11:58,  5.31s/it]loss_total_epoch 157.0531672053039
Training tokenizer:  26% 523/2013 [42:07<2:12:22,  5.33s/it]loss_total_epoch 157.3026484809816
Training tokenizer:  26% 524/2013 [42:13<2:12:02,  5.32s/it]loss_total_epoch 157.56095975264907
Training tokenizer:  26% 525/2013 [42:18<2:11:57,  5.32s/it]loss_total_epoch 157.81865979358554
Training tokenizer:  26% 526/2013 [42:23<2:12:10,  5.33s/it]loss_total_epoch 158.1021872125566
Training tokenizer:  26% 527/2013 [42:29<2:12:00,  5.33s/it]loss_total_epoch 158.3629068993032
Training tokenizer:  26% 528/2013 [42:34<2:11:48,  5.33s/it]loss_total_epoch 158.6459902524948
Training tokenizer:  26% 529/2013 [42:39<2:11:51,  5.33s/it]loss_total_epoch 158.91350408643484
Training tokenizer:  26% 530/2013 [42:45<2:12:07,  5.35s/it]loss_total_epoch 159.15766111761332
Training tokenizer:  26% 531/2013 [42:50<2:12:01,  5.35s/it]loss_total_epoch 159.41191846877337
Training tokenizer:  26% 532/2013 [42:55<2:11:59,  5.35s/it]loss_total_epoch 159.67404225468636
Training tokenizer:  26% 533/2013 [43:01<2:11:59,  5.35s/it]loss_total_epoch 159.98265846073627
Training tokenizer:  27% 534/2013 [43:06<2:11:50,  5.35s/it]loss_total_epoch 160.25797466933727
Training tokenizer:  27% 535/2013 [43:12<2:11:32,  5.34s/it]loss_total_epoch 160.52961434051394
Training tokenizer:  27% 536/2013 [43:17<2:11:21,  5.34s/it]loss_total_epoch 160.79090125858784
Training tokenizer:  27% 537/2013 [43:22<2:11:32,  5.35s/it]loss_total_epoch 161.06552125886083
Training tokenizer:  27% 538/2013 [43:28<2:11:15,  5.34s/it]loss_total_epoch 161.34761817753315
Training tokenizer:  27% 539/2013 [43:33<2:11:11,  5.34s/it]loss_total_epoch 161.6117013655603
Training tokenizer:  27% 540/2013 [43:38<2:10:56,  5.33s/it]loss_total_epoch 161.87372970208526
Training tokenizer:  27% 541/2013 [43:44<2:11:20,  5.35s/it]loss_total_epoch 162.13189072161913
Training tokenizer:  27% 542/2013 [43:49<2:11:25,  5.36s/it]loss_total_epoch 162.39829970896244
Training tokenizer:  27% 543/2013 [43:54<2:11:28,  5.37s/it]loss_total_epoch 162.66105576232076
Training tokenizer:  27% 544/2013 [44:00<2:11:18,  5.36s/it]loss_total_epoch 162.8927828706801
Training tokenizer:  27% 545/2013 [44:05<2:11:27,  5.37s/it]loss_total_epoch 163.14523227140307
Training tokenizer:  27% 546/2013 [44:10<2:11:16,  5.37s/it]loss_total_epoch 163.42803958803415
Training tokenizer:  27% 547/2013 [44:16<2:10:57,  5.36s/it]loss_total_epoch 163.6697210110724
Training tokenizer:  27% 548/2013 [44:21<2:11:00,  5.37s/it]loss_total_epoch 163.9091845676303
Training tokenizer:  27% 549/2013 [44:27<2:10:51,  5.36s/it]loss_total_epoch 164.17424938082695
Training tokenizer:  27% 550/2013 [44:32<2:10:51,  5.37s/it]loss_total_epoch 164.39118008688092
Training tokenizer:  27% 551/2013 [44:37<2:10:45,  5.37s/it]loss_total_epoch 164.63885704055429
Training tokenizer:  27% 552/2013 [44:43<2:10:55,  5.38s/it]loss_total_epoch 164.9194721095264
Training tokenizer:  27% 553/2013 [44:48<2:10:59,  5.38s/it]loss_total_epoch 165.20735673233867
Training tokenizer:  28% 554/2013 [44:53<2:10:48,  5.38s/it]loss_total_epoch 165.4650734886527
Training tokenizer:  28% 555/2013 [44:59<2:10:41,  5.38s/it]loss_total_epoch 165.70752745866776
Training tokenizer:  28% 556/2013 [45:04<2:11:02,  5.40s/it]loss_total_epoch 165.98904675617814
Training tokenizer:  28% 557/2013 [45:10<2:10:52,  5.39s/it]loss_total_epoch 166.25140729919076
Training tokenizer:  28% 558/2013 [45:15<2:10:51,  5.40s/it]loss_total_epoch 166.51517233625054
Training tokenizer:  28% 559/2013 [45:21<2:11:34,  5.43s/it]loss_total_epoch 166.8123274333775
Training tokenizer:  28% 560/2013 [45:26<2:11:13,  5.42s/it]loss_total_epoch 167.0707775913179
Training tokenizer:  28% 561/2013 [45:31<2:10:45,  5.40s/it]loss_total_epoch 167.34428456798196
Training tokenizer:  28% 562/2013 [45:37<2:10:45,  5.41s/it]loss_total_epoch 167.63073579594493
Training tokenizer:  28% 563/2013 [45:42<2:09:55,  5.38s/it]loss_total_epoch 167.90122592821717
Training tokenizer:  28% 564/2013 [45:47<2:10:06,  5.39s/it]loss_total_epoch 168.15627828612924
Training tokenizer:  28% 565/2013 [45:53<2:10:17,  5.40s/it]loss_total_epoch 168.45243648067117
Training tokenizer:  28% 566/2013 [45:58<2:09:59,  5.39s/it]loss_total_epoch 168.7258563451469
Training tokenizer:  28% 567/2013 [46:04<2:09:51,  5.39s/it]loss_total_epoch 168.98773046955466
Training tokenizer:  28% 568/2013 [46:09<2:10:06,  5.40s/it]loss_total_epoch 169.2582302391529
Training tokenizer:  28% 569/2013 [46:15<2:10:20,  5.42s/it]loss_total_epoch 169.52111665531993
Training tokenizer:  28% 570/2013 [46:20<2:10:05,  5.41s/it]loss_total_epoch 169.78683211281896
Training tokenizer:  28% 571/2013 [46:25<2:09:43,  5.40s/it]loss_total_epoch 170.04510025680065
Training tokenizer:  28% 572/2013 [46:31<2:09:40,  5.40s/it]loss_total_epoch 170.32415889948606
Training tokenizer:  28% 573/2013 [46:36<2:10:15,  5.43s/it]loss_total_epoch 170.59217455238104
Training tokenizer:  29% 574/2013 [46:42<2:09:40,  5.41s/it]loss_total_epoch 170.85877079889178
Training tokenizer:  29% 575/2013 [46:47<2:09:24,  5.40s/it]loss_total_epoch 171.16405876353383
Training tokenizer:  29% 576/2013 [46:52<2:09:38,  5.41s/it]loss_total_epoch 171.40487910062075
Training tokenizer:  29% 577/2013 [46:58<2:09:53,  5.43s/it]loss_total_epoch 171.68016623705626
Training tokenizer:  29% 578/2013 [47:03<2:09:34,  5.42s/it]loss_total_epoch 171.9099551215768
Training tokenizer:  29% 579/2013 [47:09<2:09:17,  5.41s/it]loss_total_epoch 172.18928788602352
Training tokenizer:  29% 580/2013 [47:14<2:09:07,  5.41s/it]loss_total_epoch 172.46402472257614
Training tokenizer:  29% 581/2013 [47:19<2:09:32,  5.43s/it]loss_total_epoch 172.72041303291917
Training tokenizer:  29% 582/2013 [47:25<2:09:32,  5.43s/it]loss_total_epoch 172.97510876134038
Training tokenizer:  29% 583/2013 [47:30<2:09:31,  5.43s/it]loss_total_epoch 173.25524412468076
Training tokenizer:  29% 584/2013 [47:36<2:09:40,  5.44s/it]loss_total_epoch 173.5510761551559
Training tokenizer:  29% 585/2013 [47:41<2:09:23,  5.44s/it]loss_total_epoch 173.8063304387033
Training tokenizer:  29% 586/2013 [47:47<2:09:25,  5.44s/it]loss_total_epoch 174.09444208070636
Training tokenizer:  29% 587/2013 [47:52<2:09:08,  5.43s/it]loss_total_epoch 174.35778841003776
Training tokenizer:  29% 588/2013 [47:58<2:09:22,  5.45s/it]loss_total_epoch 174.64365449920297
Training tokenizer:  29% 589/2013 [48:03<2:09:38,  5.46s/it]loss_total_epoch 174.89118986204267
Training tokenizer:  29% 590/2013 [48:09<2:09:08,  5.45s/it]loss_total_epoch 175.1635184586048
Training tokenizer:  29% 591/2013 [48:14<2:09:13,  5.45s/it]loss_total_epoch 175.43899884447455
Training tokenizer:  29% 592/2013 [48:19<2:09:18,  5.46s/it]loss_total_epoch 175.70879289135337
Training tokenizer:  29% 593/2013 [48:25<2:08:59,  5.45s/it]loss_total_epoch 175.95277302712202
Training tokenizer:  30% 594/2013 [48:30<2:09:02,  5.46s/it]loss_total_epoch 176.23567540198565
Training tokenizer:  30% 595/2013 [48:36<2:08:58,  5.46s/it]loss_total_epoch 176.4843797273934
Training tokenizer:  30% 596/2013 [48:41<2:08:59,  5.46s/it]loss_total_epoch 176.76868491247296
Training tokenizer:  30% 597/2013 [48:47<2:08:56,  5.46s/it]loss_total_epoch 177.0458396859467
Training tokenizer:  30% 598/2013 [48:52<2:08:39,  5.46s/it]loss_total_epoch 177.29561598226428
Training tokenizer:  30% 599/2013 [48:58<2:08:34,  5.46s/it]loss_total_epoch 177.57513071224093
Training tokenizer:  30% 600/2013 [49:03<2:08:34,  5.46s/it]loss_total_epoch 177.84043380245566
Training tokenizer:  30% 601/2013 [49:09<2:08:45,  5.47s/it]loss_total_epoch 178.114053491503
Training tokenizer:  30% 602/2013 [49:14<2:08:49,  5.48s/it]loss_total_epoch 178.38395745679736
Training tokenizer:  30% 603/2013 [49:20<2:08:39,  5.48s/it]loss_total_epoch 178.6466974169016
Training tokenizer:  30% 604/2013 [49:25<2:08:37,  5.48s/it]loss_total_epoch 178.9315832555294
Training tokenizer:  30% 605/2013 [49:31<2:08:48,  5.49s/it]loss_total_epoch 179.1993569880724
Training tokenizer:  30% 606/2013 [49:36<2:09:02,  5.50s/it]loss_total_epoch 179.49510683864355
Training tokenizer:  30% 607/2013 [49:42<2:09:02,  5.51s/it]loss_total_epoch 179.75400663539767
Training tokenizer:  30% 608/2013 [49:47<2:08:42,  5.50s/it]loss_total_epoch 180.04419810697436
Training tokenizer:  30% 609/2013 [49:53<2:08:17,  5.48s/it]loss_total_epoch 180.2867860943079
Training tokenizer:  30% 610/2013 [49:58<2:08:15,  5.49s/it]loss_total_epoch 180.54343743622303
Training tokenizer:  30% 611/2013 [50:04<2:08:14,  5.49s/it]loss_total_epoch 180.79465682432055
Training tokenizer:  30% 612/2013 [50:09<2:08:32,  5.50s/it]loss_total_epoch 181.0569255053997
Training tokenizer:  30% 613/2013 [50:15<2:08:04,  5.49s/it]loss_total_epoch 181.31698138266802
Training tokenizer:  31% 614/2013 [50:20<2:08:08,  5.50s/it]loss_total_epoch 181.58297253772616
Training tokenizer:  31% 615/2013 [50:26<2:07:54,  5.49s/it]loss_total_epoch 181.84471214190125
Training tokenizer:  31% 616/2013 [50:31<2:07:54,  5.49s/it]loss_total_epoch 182.11100812256336
Training tokenizer:  31% 617/2013 [50:36<2:07:42,  5.49s/it]loss_total_epoch 182.3746104799211
Training tokenizer:  31% 618/2013 [50:42<2:07:50,  5.50s/it]loss_total_epoch 182.64219906553626
Training tokenizer:  31% 619/2013 [50:48<2:08:10,  5.52s/it]loss_total_epoch 182.89374765753746
Training tokenizer:  31% 620/2013 [50:53<2:08:12,  5.52s/it]loss_total_epoch 183.1522796191275
Training tokenizer:  31% 621/2013 [50:59<2:07:59,  5.52s/it]loss_total_epoch 183.41625513881445
Training tokenizer:  31% 622/2013 [51:04<2:07:50,  5.51s/it]loss_total_epoch 183.65347066149116
Training tokenizer:  31% 623/2013 [51:10<2:07:29,  5.50s/it]loss_total_epoch 183.9125610664487
Training tokenizer:  31% 624/2013 [51:15<2:07:36,  5.51s/it]loss_total_epoch 184.16846526786685
Training tokenizer:  31% 625/2013 [51:21<2:07:40,  5.52s/it]loss_total_epoch 184.41996206343174
Training tokenizer:  31% 626/2013 [51:26<2:07:38,  5.52s/it]loss_total_epoch 184.68437721580267
Training tokenizer:  31% 627/2013 [51:32<2:07:31,  5.52s/it]loss_total_epoch 184.95403742045164
Training tokenizer:  31% 628/2013 [51:37<2:07:33,  5.53s/it]loss_total_epoch 185.23830768838525
Training tokenizer:  31% 629/2013 [51:43<2:07:20,  5.52s/it]loss_total_epoch 185.50481211021543
Training tokenizer:  31% 630/2013 [51:48<2:07:01,  5.51s/it]loss_total_epoch 185.77748264744878
Training tokenizer:  31% 631/2013 [51:54<2:07:07,  5.52s/it]loss_total_epoch 186.04661429300904
Training tokenizer:  31% 632/2013 [51:59<2:06:56,  5.52s/it]loss_total_epoch 186.29961621388793
Training tokenizer:  31% 633/2013 [52:05<2:06:56,  5.52s/it]loss_total_epoch 186.5651868097484
Training tokenizer:  31% 634/2013 [52:10<2:07:08,  5.53s/it]loss_total_epoch 186.8165379539132
Training tokenizer:  32% 635/2013 [52:16<2:06:53,  5.53s/it]loss_total_epoch 187.0931013673544
Training tokenizer:  32% 636/2013 [52:21<2:07:16,  5.55s/it]loss_total_epoch 187.34937455505133
Training tokenizer:  32% 637/2013 [52:27<2:06:58,  5.54s/it]loss_total_epoch 187.62049240246415
Training tokenizer:  32% 638/2013 [52:33<2:07:01,  5.54s/it]loss_total_epoch 187.88725832104683
Training tokenizer:  32% 639/2013 [52:38<2:06:53,  5.54s/it]loss_total_epoch 188.18352527916431
Training tokenizer:  32% 640/2013 [52:44<2:07:10,  5.56s/it]loss_total_epoch 188.4577685892582
Training tokenizer:  32% 641/2013 [52:49<2:07:15,  5.57s/it]loss_total_epoch 188.72286804765463
Training tokenizer:  32% 642/2013 [52:55<2:06:58,  5.56s/it]loss_total_epoch 188.9616304114461
Training tokenizer:  32% 643/2013 [53:00<2:06:59,  5.56s/it]loss_total_epoch 189.21619279682636
Training tokenizer:  32% 644/2013 [53:06<2:06:54,  5.56s/it]loss_total_epoch 189.42932615056634
Training tokenizer:  32% 645/2013 [53:12<2:06:45,  5.56s/it]loss_total_epoch 189.7039117179811
Training tokenizer:  32% 646/2013 [53:17<2:06:38,  5.56s/it]loss_total_epoch 189.96795058622956
Training tokenizer:  32% 647/2013 [53:23<2:06:45,  5.57s/it]loss_total_epoch 190.23293646797538
Training tokenizer:  32% 648/2013 [53:28<2:06:45,  5.57s/it]loss_total_epoch 190.52267281338573
Training tokenizer:  32% 649/2013 [53:34<2:07:02,  5.59s/it]loss_total_epoch 190.80394216254354
Training tokenizer:  32% 650/2013 [53:39<2:06:50,  5.58s/it]loss_total_epoch 191.08736823126674
Training tokenizer:  32% 651/2013 [53:45<2:06:29,  5.57s/it]loss_total_epoch 191.35496467351913
Training tokenizer:  32% 652/2013 [53:51<2:06:43,  5.59s/it]loss_total_epoch 191.6157330274582
Training tokenizer:  32% 653/2013 [53:56<2:06:56,  5.60s/it]loss_total_epoch 191.8695330657065
Training tokenizer:  32% 654/2013 [54:02<2:06:31,  5.59s/it]loss_total_epoch 192.10479114577174
Training tokenizer:  33% 655/2013 [54:07<2:06:23,  5.58s/it]loss_total_epoch 192.36628448590636
Training tokenizer:  33% 656/2013 [54:13<2:06:13,  5.58s/it]loss_total_epoch 192.61480005830526
Training tokenizer:  33% 657/2013 [54:19<2:06:16,  5.59s/it]loss_total_epoch 192.84852646663785
Training tokenizer:  33% 658/2013 [54:24<2:06:09,  5.59s/it]loss_total_epoch 193.1202008612454
Training tokenizer:  33% 659/2013 [54:30<2:06:06,  5.59s/it]loss_total_epoch 193.38565543666482
Training tokenizer:  33% 660/2013 [54:35<2:05:59,  5.59s/it]loss_total_epoch 193.64877872914076
Training tokenizer:  33% 661/2013 [54:41<2:06:02,  5.59s/it]loss_total_epoch 193.89091438427567
Training tokenizer:  33% 662/2013 [54:47<2:06:05,  5.60s/it]loss_total_epoch 194.13909105211496
Training tokenizer:  33% 663/2013 [54:52<2:05:59,  5.60s/it]loss_total_epoch 194.41483801975846
Training tokenizer:  33% 664/2013 [54:58<2:05:32,  5.58s/it]loss_total_epoch 194.671691916883
Training tokenizer:  33% 665/2013 [55:03<2:05:28,  5.58s/it]loss_total_epoch 194.9470128789544
Training tokenizer:  33% 666/2013 [55:09<2:05:11,  5.58s/it]loss_total_epoch 195.19457636028528
Training tokenizer:  33% 667/2013 [55:14<2:05:10,  5.58s/it]loss_total_epoch 195.44081829860806
Training tokenizer:  33% 668/2013 [55:20<2:05:25,  5.60s/it]loss_total_epoch 195.68869715556502
Training tokenizer:  33% 669/2013 [55:26<2:05:22,  5.60s/it]loss_total_epoch 195.94334411993623
Training tokenizer:  33% 670/2013 [55:31<2:05:17,  5.60s/it]loss_total_epoch 196.20538752526045
Training tokenizer:  33% 671/2013 [55:37<2:05:04,  5.59s/it]loss_total_epoch 196.47578110173345
Training tokenizer:  33% 672/2013 [55:42<2:05:13,  5.60s/it]loss_total_epoch 196.74166895449162
Training tokenizer:  33% 673/2013 [55:48<2:05:00,  5.60s/it]loss_total_epoch 197.04205624759197
Training tokenizer:  33% 674/2013 [55:54<2:04:51,  5.59s/it]loss_total_epoch 197.27531833201647
Training tokenizer:  34% 675/2013 [55:59<2:04:29,  5.58s/it]loss_total_epoch 197.52995896339417
Training tokenizer:  34% 676/2013 [56:05<2:04:24,  5.58s/it]loss_total_epoch 197.82191707193851
Training tokenizer:  34% 677/2013 [56:10<2:04:26,  5.59s/it]loss_total_epoch 198.09833212941885
Training tokenizer:  34% 678/2013 [56:16<2:04:36,  5.60s/it]loss_total_epoch 198.34942439943552
Training tokenizer:  34% 679/2013 [56:22<2:04:38,  5.61s/it]loss_total_epoch 198.62594671547413
Training tokenizer:  34% 680/2013 [56:27<2:04:36,  5.61s/it]loss_total_epoch 198.89599254727364
Training tokenizer:  34% 681/2013 [56:33<2:04:38,  5.61s/it]loss_total_epoch 199.1811013892293
Training tokenizer:  34% 682/2013 [56:39<2:04:59,  5.63s/it]loss_total_epoch 199.44816922396421
Training tokenizer:  34% 683/2013 [56:44<2:04:52,  5.63s/it]loss_total_epoch 199.71884545311332
Training tokenizer:  34% 684/2013 [56:50<2:05:00,  5.64s/it]loss_total_epoch 200.00030929595232
Training tokenizer:  34% 685/2013 [56:55<2:04:43,  5.64s/it]loss_total_epoch 200.24562741070986
Training tokenizer:  34% 686/2013 [57:01<2:04:36,  5.63s/it]loss_total_epoch 200.5280865430832
Training tokenizer:  34% 687/2013 [57:07<2:04:19,  5.63s/it]loss_total_epoch 200.80097774416208
Training tokenizer:  34% 688/2013 [57:12<2:04:08,  5.62s/it]loss_total_epoch 201.07933831959963
Training tokenizer:  34% 689/2013 [57:18<2:04:19,  5.63s/it]loss_total_epoch 201.3580449745059
Training tokenizer:  34% 690/2013 [57:24<2:04:11,  5.63s/it]loss_total_epoch 201.61429024115205
Training tokenizer:  34% 691/2013 [57:29<2:04:12,  5.64s/it]loss_total_epoch 201.90543568506837
Training tokenizer:  34% 692/2013 [57:35<2:04:10,  5.64s/it]loss_total_epoch 202.19788705185056
Training tokenizer:  34% 693/2013 [57:40<2:03:38,  5.62s/it]loss_total_epoch 202.47826686874032
Training tokenizer:  34% 694/2013 [57:46<2:03:28,  5.62s/it]loss_total_epoch 202.74839479103684
Training tokenizer:  35% 695/2013 [57:52<2:03:23,  5.62s/it]loss_total_epoch 203.0272814296186
Training tokenizer:  35% 696/2013 [57:57<2:03:33,  5.63s/it]loss_total_epoch 203.28819209337234
Training tokenizer:  35% 697/2013 [58:03<2:03:50,  5.65s/it]loss_total_epoch 203.56053413823247
Training tokenizer:  35% 698/2013 [58:09<2:03:38,  5.64s/it]loss_total_epoch 203.8502659536898
Training tokenizer:  35% 699/2013 [58:14<2:03:26,  5.64s/it]loss_total_epoch 204.1054826155305
Training tokenizer:  35% 700/2013 [58:20<2:03:22,  5.64s/it]loss_total_epoch 204.3757525421679
Training tokenizer:  35% 701/2013 [58:26<2:03:19,  5.64s/it]loss_total_epoch 204.64505728706717
Training tokenizer:  35% 702/2013 [58:31<2:03:30,  5.65s/it]loss_total_epoch 204.8937469087541
Training tokenizer:  35% 703/2013 [58:37<2:03:33,  5.66s/it]loss_total_epoch 205.16096615046263
Training tokenizer:  35% 704/2013 [58:43<2:03:20,  5.65s/it]loss_total_epoch 205.42869751900434
Training tokenizer:  35% 705/2013 [58:48<2:03:20,  5.66s/it]loss_total_epoch 205.70327312499285
Training tokenizer:  35% 706/2013 [58:54<2:03:14,  5.66s/it]loss_total_epoch 205.96967478469014
Training tokenizer:  35% 707/2013 [59:00<2:03:14,  5.66s/it]loss_total_epoch 206.23847733065486
Training tokenizer:  35% 708/2013 [59:05<2:03:06,  5.66s/it]loss_total_epoch 206.50175266340375
Training tokenizer:  35% 709/2013 [59:11<2:03:06,  5.66s/it]loss_total_epoch 206.7760550007224
Training tokenizer:  35% 710/2013 [59:17<2:03:00,  5.66s/it]loss_total_epoch 207.0372174717486
Training tokenizer:  35% 711/2013 [59:22<2:02:57,  5.67s/it]loss_total_epoch 207.300952591002
Training tokenizer:  35% 712/2013 [59:28<2:03:03,  5.68s/it]loss_total_epoch 207.6013810634613
Training tokenizer:  35% 713/2013 [59:34<2:02:57,  5.68s/it]loss_total_epoch 207.8817806020379
Training tokenizer:  35% 714/2013 [59:39<2:02:41,  5.67s/it]loss_total_epoch 208.13504068553448
Training tokenizer:  36% 715/2013 [59:45<2:03:04,  5.69s/it]loss_total_epoch 208.39148741215467
Training tokenizer:  36% 716/2013 [59:51<2:02:57,  5.69s/it]loss_total_epoch 208.6243465654552
Training tokenizer:  36% 717/2013 [59:56<2:03:04,  5.70s/it]loss_total_epoch 208.88951040804386
Training tokenizer:  36% 718/2013 [1:00:02<2:03:04,  5.70s/it]loss_total_epoch 209.12776774540544
Training tokenizer:  36% 719/2013 [1:00:08<2:03:02,  5.70s/it]loss_total_epoch 209.37520863488317
Training tokenizer:  36% 720/2013 [1:00:13<2:02:48,  5.70s/it]loss_total_epoch 209.61367506161332
Training tokenizer:  36% 721/2013 [1:00:19<2:02:31,  5.69s/it]loss_total_epoch 209.87140498682857
Training tokenizer:  36% 722/2013 [1:00:25<2:02:15,  5.68s/it]loss_total_epoch 210.1505260169506
Training tokenizer:  36% 723/2013 [1:00:30<2:02:03,  5.68s/it]loss_total_epoch 210.41301830112934
Training tokenizer:  36% 724/2013 [1:00:36<2:02:09,  5.69s/it]loss_total_epoch 210.68488258123398
Training tokenizer:  36% 725/2013 [1:00:42<2:02:18,  5.70s/it]loss_total_epoch 210.95086701586843
Training tokenizer:  36% 726/2013 [1:00:48<2:01:57,  5.69s/it]loss_total_epoch 211.17185678705573
Training tokenizer:  36% 727/2013 [1:00:53<2:02:11,  5.70s/it]loss_total_epoch 211.4177754893899
Training tokenizer:  36% 728/2013 [1:00:59<2:02:01,  5.70s/it]loss_total_epoch 211.69526217132807
Training tokenizer:  36% 729/2013 [1:01:05<2:01:48,  5.69s/it]loss_total_epoch 211.9749636873603
Training tokenizer:  36% 730/2013 [1:01:10<2:01:54,  5.70s/it]loss_total_epoch 212.21314204484224
Training tokenizer:  36% 731/2013 [1:01:16<2:01:55,  5.71s/it]loss_total_epoch 212.45112271234393
Training tokenizer:  36% 732/2013 [1:01:22<2:01:32,  5.69s/it]loss_total_epoch 212.69434218108654
Training tokenizer:  36% 733/2013 [1:01:27<2:01:25,  5.69s/it]loss_total_epoch 212.92668714374304
Training tokenizer:  36% 734/2013 [1:01:33<2:01:10,  5.68s/it]loss_total_epoch 213.17842890322208
Training tokenizer:  37% 735/2013 [1:01:39<2:01:25,  5.70s/it]loss_total_epoch 213.43326863273978
Training tokenizer:  37% 736/2013 [1:01:45<2:01:30,  5.71s/it]loss_total_epoch 213.66875391080976
Training tokenizer:  37% 737/2013 [1:01:50<2:01:36,  5.72s/it]loss_total_epoch 213.90429394319654
Training tokenizer:  37% 738/2013 [1:01:56<2:01:12,  5.70s/it]loss_total_epoch 214.14685178920627
Training tokenizer:  37% 739/2013 [1:02:02<2:00:56,  5.70s/it]loss_total_epoch 214.37909742444754
Training tokenizer:  37% 740/2013 [1:02:07<2:00:42,  5.69s/it]loss_total_epoch 214.62565610930324
Training tokenizer:  37% 741/2013 [1:02:13<2:01:02,  5.71s/it]loss_total_epoch 214.85486065223813
Training tokenizer:  37% 742/2013 [1:02:19<2:00:44,  5.70s/it]loss_total_epoch 215.1108112409711
Training tokenizer:  37% 743/2013 [1:02:25<2:00:39,  5.70s/it]loss_total_epoch 215.3734201565385
Training tokenizer:  37% 744/2013 [1:02:30<2:00:42,  5.71s/it]loss_total_epoch 215.64889361709356
Training tokenizer:  37% 745/2013 [1:02:36<2:00:52,  5.72s/it]loss_total_epoch 215.88736250251532
Training tokenizer:  37% 746/2013 [1:02:42<2:00:46,  5.72s/it]loss_total_epoch 216.12389661371708
Training tokenizer:  37% 747/2013 [1:02:47<2:00:39,  5.72s/it]loss_total_epoch 216.3990362510085
Training tokenizer:  37% 748/2013 [1:02:53<2:00:42,  5.73s/it]loss_total_epoch 216.6353653408587
Training tokenizer:  37% 749/2013 [1:02:59<2:00:42,  5.73s/it]loss_total_epoch 216.89721943438053
Training tokenizer:  37% 750/2013 [1:03:05<2:00:42,  5.73s/it]loss_total_epoch 217.17634028941393
Training tokenizer:  37% 751/2013 [1:03:10<2:00:37,  5.73s/it]loss_total_epoch 217.4405061006546
Training tokenizer:  37% 752/2013 [1:03:16<2:00:40,  5.74s/it]loss_total_epoch 217.6873646825552
Training tokenizer:  37% 753/2013 [1:03:22<2:00:22,  5.73s/it]loss_total_epoch 217.97667152434587
Training tokenizer:  37% 754/2013 [1:03:28<2:00:33,  5.75s/it]loss_total_epoch 218.24130626767874
Training tokenizer:  38% 755/2013 [1:03:33<2:00:33,  5.75s/it]loss_total_epoch 218.50886727496982
Training tokenizer:  38% 756/2013 [1:03:39<2:00:30,  5.75s/it]loss_total_epoch 218.74124700948596
Training tokenizer:  38% 757/2013 [1:03:45<2:00:42,  5.77s/it]loss_total_epoch 219.00620495900512
Training tokenizer:  38% 758/2013 [1:03:51<2:00:14,  5.75s/it]loss_total_epoch 219.23246300965548
Training tokenizer:  38% 759/2013 [1:03:56<2:00:17,  5.76s/it]loss_total_epoch 219.47200172394514
Training tokenizer:  38% 760/2013 [1:04:02<2:00:27,  5.77s/it]loss_total_epoch 219.7152673713863
Training tokenizer:  38% 761/2013 [1:04:08<2:00:17,  5.76s/it]loss_total_epoch 219.96732645854354
Training tokenizer:  38% 762/2013 [1:04:14<2:00:25,  5.78s/it]loss_total_epoch 220.19771580398083
Training tokenizer:  38% 763/2013 [1:04:20<2:00:09,  5.77s/it]loss_total_epoch 220.42672391980886
Training tokenizer:  38% 764/2013 [1:04:25<2:00:17,  5.78s/it]loss_total_epoch 220.68188101053238
Training tokenizer:  38% 765/2013 [1:04:31<2:00:18,  5.78s/it]loss_total_epoch 220.92045133188367
Training tokenizer:  38% 766/2013 [1:04:37<2:00:00,  5.77s/it]loss_total_epoch 221.16895604878664
Training tokenizer:  38% 767/2013 [1:04:43<1:59:44,  5.77s/it]loss_total_epoch 221.4251374490559
Training tokenizer:  38% 768/2013 [1:04:48<1:59:54,  5.78s/it]loss_total_epoch 221.67115624248981
Training tokenizer:  38% 769/2013 [1:04:54<1:59:31,  5.76s/it]loss_total_epoch 221.93682780116796
Training tokenizer:  38% 770/2013 [1:05:00<1:59:23,  5.76s/it]loss_total_epoch 222.18314598873258
Training tokenizer:  38% 771/2013 [1:05:06<1:59:16,  5.76s/it]loss_total_epoch 222.42646013572812
Training tokenizer:  38% 772/2013 [1:05:11<1:59:31,  5.78s/it]loss_total_epoch 222.6912119947374
Training tokenizer:  38% 773/2013 [1:05:17<1:59:22,  5.78s/it]loss_total_epoch 222.91851576790214
Training tokenizer:  38% 774/2013 [1:05:23<1:59:21,  5.78s/it]loss_total_epoch 223.15345164388418
Training tokenizer:  38% 775/2013 [1:05:29<1:58:51,  5.76s/it]loss_total_epoch 223.3819405734539
Training tokenizer:  39% 776/2013 [1:05:35<1:58:53,  5.77s/it]loss_total_epoch 223.62291080504656
Training tokenizer:  39% 777/2013 [1:05:40<1:58:42,  5.76s/it]loss_total_epoch 223.8663058616221
Training tokenizer:  39% 778/2013 [1:05:46<1:58:45,  5.77s/it]loss_total_epoch 224.11322447657585
Training tokenizer:  39% 779/2013 [1:05:52<1:58:45,  5.77s/it]loss_total_epoch 224.3317834958434
Training tokenizer:  39% 780/2013 [1:05:58<1:59:00,  5.79s/it]loss_total_epoch 224.57191411405802
Training tokenizer:  39% 781/2013 [1:06:04<1:58:59,  5.79s/it]loss_total_epoch 224.80678160116076
Training tokenizer:  39% 782/2013 [1:06:09<1:58:54,  5.80s/it]loss_total_epoch 225.06087060645223
Training tokenizer:  39% 783/2013 [1:06:15<1:58:58,  5.80s/it]loss_total_epoch 225.3198926411569
Training tokenizer:  39% 784/2013 [1:06:21<1:58:41,  5.79s/it]loss_total_epoch 225.55845722556114
Training tokenizer:  39% 785/2013 [1:06:27<1:58:45,  5.80s/it]loss_total_epoch 225.7704792059958
Training tokenizer:  39% 786/2013 [1:06:33<1:58:37,  5.80s/it]loss_total_epoch 226.0042146705091
Training tokenizer:  39% 787/2013 [1:06:38<1:58:33,  5.80s/it]loss_total_epoch 226.24968807771802
Training tokenizer:  39% 788/2013 [1:06:44<1:58:38,  5.81s/it]loss_total_epoch 226.519328892231
Training tokenizer:  39% 789/2013 [1:06:50<1:58:14,  5.80s/it]loss_total_epoch 226.7602922655642
Training tokenizer:  39% 790/2013 [1:06:56<1:58:07,  5.79s/it]loss_total_epoch 227.0137901417911
Training tokenizer:  39% 791/2013 [1:07:02<1:58:21,  5.81s/it]loss_total_epoch 227.2382145561278
Training tokenizer:  39% 792/2013 [1:07:07<1:58:05,  5.80s/it]loss_total_epoch 227.50465704128146
Training tokenizer:  39% 793/2013 [1:07:13<1:58:01,  5.80s/it]loss_total_epoch 227.7677849754691
Training tokenizer:  39% 794/2013 [1:07:19<1:58:10,  5.82s/it]loss_total_epoch 228.0363294109702
Training tokenizer:  39% 795/2013 [1:07:25<1:57:54,  5.81s/it]loss_total_epoch 228.29774671047926
Training tokenizer:  40% 796/2013 [1:07:31<1:58:03,  5.82s/it]loss_total_epoch 228.5224871300161
Training tokenizer:  40% 797/2013 [1:07:36<1:58:11,  5.83s/it]loss_total_epoch 228.7568117454648
Training tokenizer:  40% 798/2013 [1:07:42<1:58:00,  5.83s/it]loss_total_epoch 229.0014247186482
Training tokenizer:  40% 799/2013 [1:07:48<1:57:35,  5.81s/it]loss_total_epoch 229.2458913847804
Training tokenizer:  40% 800/2013 [1:07:54<1:57:22,  5.81s/it]loss_total_epoch 229.48295741155744
Training tokenizer:  40% 801/2013 [1:08:00<1:57:26,  5.81s/it]loss_total_epoch 229.73702174052596
Training tokenizer:  40% 802/2013 [1:08:06<1:57:23,  5.82s/it]loss_total_epoch 229.99251735582948
Training tokenizer:  40% 803/2013 [1:08:11<1:57:36,  5.83s/it]loss_total_epoch 230.23787475004792
Training tokenizer:  40% 804/2013 [1:08:17<1:57:45,  5.84s/it]loss_total_epoch 230.47156860679388
Training tokenizer:  40% 805/2013 [1:08:23<1:57:38,  5.84s/it]loss_total_epoch 230.71217995509505
Training tokenizer:  40% 806/2013 [1:08:29<1:58:37,  5.90s/it]loss_total_epoch 230.96778624877334
Training tokenizer:  40% 807/2013 [1:08:35<1:57:52,  5.86s/it]loss_total_epoch 231.2212488949299
Training tokenizer:  40% 808/2013 [1:08:41<1:57:37,  5.86s/it]loss_total_epoch 231.4615130610764
Training tokenizer:  40% 809/2013 [1:08:47<1:57:34,  5.86s/it]loss_total_epoch 231.70909982174635
Training tokenizer:  40% 810/2013 [1:08:52<1:57:05,  5.84s/it]loss_total_epoch 231.96285374835134
Training tokenizer:  40% 811/2013 [1:08:58<1:57:05,  5.84s/it]loss_total_epoch 232.2027412839234
Training tokenizer:  40% 812/2013 [1:09:04<1:57:00,  5.85s/it]loss_total_epoch 232.47430587932467
Training tokenizer:  40% 813/2013 [1:09:10<1:57:02,  5.85s/it]loss_total_epoch 232.71782713010907
Training tokenizer:  40% 814/2013 [1:09:16<1:56:44,  5.84s/it]loss_total_epoch 232.95636021718383
Training tokenizer:  40% 815/2013 [1:09:22<1:56:32,  5.84s/it]loss_total_epoch 233.1689464598894
Training tokenizer:  41% 816/2013 [1:09:28<1:56:39,  5.85s/it]loss_total_epoch 233.42380909621716
Training tokenizer:  41% 817/2013 [1:09:33<1:56:34,  5.85s/it]loss_total_epoch 233.66945796087384
Training tokenizer:  41% 818/2013 [1:09:39<1:56:27,  5.85s/it]loss_total_epoch 233.90738902240992
Training tokenizer:  41% 819/2013 [1:09:45<1:56:29,  5.85s/it]loss_total_epoch 234.15511202067137
Training tokenizer:  41% 820/2013 [1:09:51<1:56:22,  5.85s/it]loss_total_epoch 234.3885168917477
Training tokenizer:  41% 821/2013 [1:09:57<1:56:06,  5.84s/it]loss_total_epoch 234.63392282277346
Training tokenizer:  41% 822/2013 [1:10:03<1:56:14,  5.86s/it]loss_total_epoch 234.88216867297888
Training tokenizer:  41% 823/2013 [1:10:09<1:56:29,  5.87s/it]loss_total_epoch 235.11559142917395
Training tokenizer:  41% 824/2013 [1:10:14<1:56:20,  5.87s/it]loss_total_epoch 235.34690934419632
Training tokenizer:  41% 825/2013 [1:10:20<1:56:15,  5.87s/it]loss_total_epoch 235.60325853154063
Training tokenizer:  41% 826/2013 [1:10:26<1:55:52,  5.86s/it]loss_total_epoch 235.83339755237103
Training tokenizer:  41% 827/2013 [1:10:32<1:55:49,  5.86s/it]loss_total_epoch 236.0632038861513
Training tokenizer:  41% 828/2013 [1:10:38<1:55:40,  5.86s/it]loss_total_epoch 236.30591478198767
Training tokenizer:  41% 829/2013 [1:10:44<1:55:44,  5.86s/it]loss_total_epoch 236.54133941605687
Training tokenizer:  41% 830/2013 [1:10:50<1:55:41,  5.87s/it]loss_total_epoch 236.7999144680798
Training tokenizer:  41% 831/2013 [1:10:55<1:55:30,  5.86s/it]loss_total_epoch 237.01460471004248
Training tokenizer:  41% 832/2013 [1:11:01<1:55:27,  5.87s/it]loss_total_epoch 237.23634757846594
Training tokenizer:  41% 833/2013 [1:11:07<1:55:28,  5.87s/it]loss_total_epoch 237.49514035880566
Training tokenizer:  41% 834/2013 [1:11:13<1:55:38,  5.88s/it]loss_total_epoch 237.76662899181247
Training tokenizer:  41% 835/2013 [1:11:19<1:55:14,  5.87s/it]loss_total_epoch 238.02195258811116
Training tokenizer:  42% 836/2013 [1:11:25<1:55:02,  5.86s/it]loss_total_epoch 238.26728769764304
Training tokenizer:  42% 837/2013 [1:11:31<1:54:39,  5.85s/it]loss_total_epoch 238.52416497468948
Training tokenizer:  42% 838/2013 [1:11:37<1:54:49,  5.86s/it]loss_total_epoch 238.7665705382824
Training tokenizer:  42% 839/2013 [1:11:42<1:54:47,  5.87s/it]loss_total_epoch 239.0118060335517
Training tokenizer:  42% 840/2013 [1:11:48<1:54:57,  5.88s/it]loss_total_epoch 239.24049877002835
Training tokenizer:  42% 841/2013 [1:11:54<1:55:01,  5.89s/it]loss_total_epoch 239.4834198281169
Training tokenizer:  42% 842/2013 [1:12:00<1:54:40,  5.88s/it]loss_total_epoch 239.6926808655262
Training tokenizer:  42% 843/2013 [1:12:06<1:54:45,  5.89s/it]loss_total_epoch 239.94389045611024
Training tokenizer:  42% 844/2013 [1:12:12<1:54:24,  5.87s/it]loss_total_epoch 240.16832546144724
Training tokenizer:  42% 845/2013 [1:12:18<1:54:49,  5.90s/it]loss_total_epoch 240.41471408307552
Training tokenizer:  42% 846/2013 [1:12:24<1:54:37,  5.89s/it]loss_total_epoch 240.66881207376719
Training tokenizer:  42% 847/2013 [1:12:30<1:54:32,  5.89s/it]loss_total_epoch 240.91675294935703
Training tokenizer:  42% 848/2013 [1:12:35<1:54:16,  5.89s/it]loss_total_epoch 241.15562443062663
Training tokenizer:  42% 849/2013 [1:12:41<1:54:12,  5.89s/it]loss_total_epoch 241.38759119808674
Training tokenizer:  42% 850/2013 [1:12:47<1:54:15,  5.89s/it]loss_total_epoch 241.60665786266327
Training tokenizer:  42% 851/2013 [1:12:53<1:54:09,  5.89s/it]loss_total_epoch 241.83605493605137
Training tokenizer:  42% 852/2013 [1:12:59<1:54:17,  5.91s/it]loss_total_epoch 242.0766018331051
Training tokenizer:  42% 853/2013 [1:13:05<1:54:19,  5.91s/it]loss_total_epoch 242.31568388268352
Training tokenizer:  42% 854/2013 [1:13:11<1:54:12,  5.91s/it]loss_total_epoch 242.5765665732324
Training tokenizer:  42% 855/2013 [1:13:17<1:54:21,  5.93s/it]loss_total_epoch 242.80963656678796
Training tokenizer:  43% 856/2013 [1:13:23<1:54:07,  5.92s/it]loss_total_epoch 243.05773182213306
Training tokenizer:  43% 857/2013 [1:13:29<1:53:52,  5.91s/it]loss_total_epoch 243.28876803070307
Training tokenizer:  43% 858/2013 [1:13:35<1:53:51,  5.91s/it]loss_total_epoch 243.51783256977797
Training tokenizer:  43% 859/2013 [1:13:41<1:54:11,  5.94s/it]loss_total_epoch 243.72182825952768
Training tokenizer:  43% 860/2013 [1:13:47<1:54:22,  5.95s/it]loss_total_epoch 243.95740957185626
Training tokenizer:  43% 861/2013 [1:13:52<1:54:07,  5.94s/it]loss_total_epoch 244.18405805528164
Training tokenizer:  43% 862/2013 [1:13:58<1:54:09,  5.95s/it]loss_total_epoch 244.41384912282228
Training tokenizer:  43% 863/2013 [1:14:04<1:53:46,  5.94s/it]loss_total_epoch 244.63656717538834
Training tokenizer:  43% 864/2013 [1:14:10<1:53:43,  5.94s/it]loss_total_epoch 244.8823646865785
Training tokenizer:  43% 865/2013 [1:14:16<1:54:04,  5.96s/it]loss_total_epoch 245.1241344064474
Training tokenizer:  43% 866/2013 [1:14:22<1:53:45,  5.95s/it]loss_total_epoch 245.3406354635954
Training tokenizer:  43% 867/2013 [1:14:28<1:53:50,  5.96s/it]loss_total_epoch 245.5561435930431
Training tokenizer:  43% 868/2013 [1:14:34<1:53:52,  5.97s/it]loss_total_epoch 245.7851414233446
Training tokenizer:  43% 869/2013 [1:14:40<1:53:21,  5.95s/it]loss_total_epoch 246.00802374258637
Training tokenizer:  43% 870/2013 [1:14:46<1:53:20,  5.95s/it]loss_total_epoch 246.262785166502
Training tokenizer:  43% 871/2013 [1:14:52<1:53:08,  5.94s/it]loss_total_epoch 246.50014341250062
Training tokenizer:  43% 872/2013 [1:14:58<1:52:51,  5.94s/it]loss_total_epoch 246.69449001923203
Training tokenizer:  43% 873/2013 [1:15:04<1:52:35,  5.93s/it]loss_total_epoch 246.93457601591945
Training tokenizer:  43% 874/2013 [1:15:10<1:52:45,  5.94s/it]loss_total_epoch 247.1605735346675
Training tokenizer:  43% 875/2013 [1:15:16<1:53:12,  5.97s/it]loss_total_epoch 247.38085201755166
Training tokenizer:  44% 876/2013 [1:15:22<1:53:06,  5.97s/it]loss_total_epoch 247.62794570997357
Training tokenizer:  44% 877/2013 [1:15:28<1:52:53,  5.96s/it]loss_total_epoch 247.83098574355245
Training tokenizer:  44% 878/2013 [1:15:34<1:52:55,  5.97s/it]loss_total_epoch 248.0603867173195
Training tokenizer:  44% 879/2013 [1:15:40<1:52:47,  5.97s/it]loss_total_epoch 248.29192428290844
Training tokenizer:  44% 880/2013 [1:15:46<1:52:32,  5.96s/it]loss_total_epoch 248.49624068662524
Training tokenizer:  44% 881/2013 [1:15:52<1:52:31,  5.96s/it]loss_total_epoch 248.74434842541814
Training tokenizer:  44% 882/2013 [1:15:58<1:52:27,  5.97s/it]loss_total_epoch 248.97369273751974
Training tokenizer:  44% 883/2013 [1:16:03<1:52:05,  5.95s/it]loss_total_epoch 249.2346396893263
Training tokenizer:  44% 884/2013 [1:16:09<1:52:03,  5.96s/it]loss_total_epoch 249.45034772902727
Training tokenizer:  44% 885/2013 [1:16:15<1:52:20,  5.98s/it]loss_total_epoch 249.66171196848154
Training tokenizer:  44% 886/2013 [1:16:21<1:52:05,  5.97s/it]loss_total_epoch 249.8855840936303
Training tokenizer:  44% 887/2013 [1:16:27<1:52:11,  5.98s/it]loss_total_epoch 250.11243124306202
Training tokenizer:  44% 888/2013 [1:16:33<1:52:08,  5.98s/it]loss_total_epoch 250.32058173418045
Training tokenizer:  44% 889/2013 [1:16:39<1:52:08,  5.99s/it]loss_total_epoch 250.56156877428293
Training tokenizer:  44% 890/2013 [1:16:45<1:52:08,  5.99s/it]loss_total_epoch 250.79436768218875
Training tokenizer:  44% 891/2013 [1:16:51<1:52:12,  6.00s/it]loss_total_epoch 251.0355507172644
Training tokenizer:  44% 892/2013 [1:16:57<1:52:00,  5.99s/it]loss_total_epoch 251.24015574902296
Training tokenizer:  44% 893/2013 [1:17:03<1:51:44,  5.99s/it]loss_total_epoch 251.47511544078588
Training tokenizer:  44% 894/2013 [1:17:09<1:51:36,  5.98s/it]loss_total_epoch 251.68125588446856
Training tokenizer:  44% 895/2013 [1:17:15<1:52:06,  6.02s/it]loss_total_epoch 251.912921551615
Training tokenizer:  45% 896/2013 [1:17:21<1:51:53,  6.01s/it]loss_total_epoch 252.12627467140555
Training tokenizer:  45% 897/2013 [1:17:27<1:51:51,  6.01s/it]loss_total_epoch 252.34434498101473
Training tokenizer:  45% 898/2013 [1:17:33<1:51:30,  6.00s/it]loss_total_epoch 252.54962546005845
Training tokenizer:  45% 899/2013 [1:17:39<1:51:16,  5.99s/it]loss_total_epoch 252.78288296610117
Training tokenizer:  45% 900/2013 [1:17:45<1:51:18,  6.00s/it]loss_total_epoch 253.0097062252462
Training tokenizer:  45% 901/2013 [1:17:51<1:51:20,  6.01s/it]loss_total_epoch 253.26036327704787
Training tokenizer:  45% 902/2013 [1:17:57<1:51:05,  6.00s/it]loss_total_epoch 253.497505415231
Training tokenizer:  45% 903/2013 [1:18:03<1:51:29,  6.03s/it]loss_total_epoch 253.73319175466895
Training tokenizer:  45% 904/2013 [1:18:10<1:51:20,  6.02s/it]loss_total_epoch 253.97377739474177
Training tokenizer:  45% 905/2013 [1:18:16<1:51:06,  6.02s/it]loss_total_epoch 254.211073320359
Training tokenizer:  45% 906/2013 [1:18:22<1:51:07,  6.02s/it]loss_total_epoch 254.43173717707396
Training tokenizer:  45% 907/2013 [1:18:28<1:50:53,  6.02s/it]loss_total_epoch 254.6527489349246
Training tokenizer:  45% 908/2013 [1:18:34<1:50:48,  6.02s/it]loss_total_epoch 254.87878392264247
Training tokenizer:  45% 909/2013 [1:18:40<1:50:44,  6.02s/it]loss_total_epoch 255.09950032830238
Training tokenizer:  45% 910/2013 [1:18:46<1:50:29,  6.01s/it]loss_total_epoch 255.32095113396645
Training tokenizer:  45% 911/2013 [1:18:52<1:50:20,  6.01s/it]loss_total_epoch 255.56807596236467
Training tokenizer:  45% 912/2013 [1:18:58<1:50:18,  6.01s/it]loss_total_epoch 255.78897389024496
Training tokenizer:  45% 913/2013 [1:19:04<1:50:18,  6.02s/it]loss_total_epoch 256.03061869740486
Training tokenizer:  45% 914/2013 [1:19:10<1:50:17,  6.02s/it]loss_total_epoch 256.23729540780187
Training tokenizer:  45% 915/2013 [1:19:16<1:50:36,  6.04s/it]loss_total_epoch 256.4636705927551
Training tokenizer:  46% 916/2013 [1:19:22<1:50:08,  6.02s/it]loss_total_epoch 256.69398798421025
Training tokenizer:  46% 917/2013 [1:19:28<1:50:11,  6.03s/it]loss_total_epoch 256.9244959652424
Training tokenizer:  46% 918/2013 [1:19:34<1:49:55,  6.02s/it]loss_total_epoch 257.1412877179682
Training tokenizer:  46% 919/2013 [1:19:40<1:49:51,  6.03s/it]loss_total_epoch 257.3722642250359
Training tokenizer:  46% 920/2013 [1:19:46<1:49:34,  6.01s/it]loss_total_epoch 257.5861927382648
Training tokenizer:  46% 921/2013 [1:19:52<1:49:29,  6.02s/it]loss_total_epoch 257.84019911289215
Training tokenizer:  46% 922/2013 [1:19:58<1:49:30,  6.02s/it]loss_total_epoch 258.0629008375108
Training tokenizer:  46% 923/2013 [1:20:04<1:49:37,  6.03s/it]loss_total_epoch 258.2738276012242
Training tokenizer:  46% 924/2013 [1:20:10<1:49:53,  6.05s/it]loss_total_epoch 258.5196593441069
Training tokenizer:  46% 925/2013 [1:20:16<1:49:30,  6.04s/it]loss_total_epoch 258.7572862319648
Training tokenizer:  46% 926/2013 [1:20:22<1:49:29,  6.04s/it]loss_total_epoch 259.0135279148817
Training tokenizer:  46% 927/2013 [1:20:28<1:49:15,  6.04s/it]loss_total_epoch 259.24999099597335
Training tokenizer:  46% 928/2013 [1:20:34<1:49:19,  6.05s/it]loss_total_epoch 259.47591203823686
Training tokenizer:  46% 929/2013 [1:20:40<1:49:09,  6.04s/it]loss_total_epoch 259.6989263892174
Training tokenizer:  46% 930/2013 [1:20:46<1:49:00,  6.04s/it]loss_total_epoch 259.8977697081864
Training tokenizer:  46% 931/2013 [1:20:52<1:49:03,  6.05s/it]loss_total_epoch 260.1353055424988
Training tokenizer:  46% 932/2013 [1:20:58<1:48:43,  6.03s/it]loss_total_epoch 260.355366691947
Training tokenizer:  46% 933/2013 [1:21:04<1:48:48,  6.05s/it]loss_total_epoch 260.57735572755337
Training tokenizer:  46% 934/2013 [1:21:10<1:48:53,  6.05s/it]loss_total_epoch 260.81507355719805
Training tokenizer:  46% 935/2013 [1:21:17<1:49:00,  6.07s/it]loss_total_epoch 261.04577098041773
Training tokenizer:  46% 936/2013 [1:21:23<1:48:52,  6.07s/it]loss_total_epoch 261.26984590664506
Training tokenizer:  47% 937/2013 [1:21:29<1:48:49,  6.07s/it]loss_total_epoch 261.5146282762289
Training tokenizer:  47% 938/2013 [1:21:35<1:48:36,  6.06s/it]loss_total_epoch 261.7395118698478
Training tokenizer:  47% 939/2013 [1:21:41<1:48:38,  6.07s/it]loss_total_epoch 261.98224997892976
Training tokenizer:  47% 940/2013 [1:21:47<1:48:36,  6.07s/it]loss_total_epoch 262.2214010991156
Training tokenizer:  47% 941/2013 [1:21:53<1:48:41,  6.08s/it]loss_total_epoch 262.4380954839289
Training tokenizer:  47% 942/2013 [1:21:59<1:48:36,  6.08s/it]loss_total_epoch 262.673573538661
Training tokenizer:  47% 943/2013 [1:22:05<1:48:08,  6.06s/it]loss_total_epoch 262.87433065474033
Training tokenizer:  47% 944/2013 [1:22:11<1:47:54,  6.06s/it]loss_total_epoch 263.1034904681146
Training tokenizer:  47% 945/2013 [1:22:17<1:47:54,  6.06s/it]loss_total_epoch 263.3288040049374
Training tokenizer:  47% 946/2013 [1:22:23<1:47:46,  6.06s/it]loss_total_epoch 263.58536503463984
Training tokenizer:  47% 947/2013 [1:22:29<1:47:50,  6.07s/it]loss_total_epoch 263.8129178583622
Training tokenizer:  47% 948/2013 [1:22:35<1:47:37,  6.06s/it]loss_total_epoch 264.04185954108834
Training tokenizer:  47% 949/2013 [1:22:42<1:47:42,  6.07s/it]loss_total_epoch 264.2804540656507
Training tokenizer:  47% 950/2013 [1:22:48<1:47:19,  6.06s/it]loss_total_epoch 264.5024311430752
Training tokenizer:  47% 951/2013 [1:22:54<1:47:25,  6.07s/it]loss_total_epoch 264.7290355749428
Training tokenizer:  47% 952/2013 [1:23:00<1:47:16,  6.07s/it]loss_total_epoch 264.97416158393025
Training tokenizer:  47% 953/2013 [1:23:06<1:47:12,  6.07s/it]loss_total_epoch 265.1966297775507
Training tokenizer:  47% 954/2013 [1:23:12<1:47:22,  6.08s/it]loss_total_epoch 265.4292628057301
Training tokenizer:  47% 955/2013 [1:23:18<1:47:35,  6.10s/it]loss_total_epoch 265.67427334189415
Training tokenizer:  47% 956/2013 [1:23:24<1:47:26,  6.10s/it]loss_total_epoch 265.8860880434513
Training tokenizer:  48% 957/2013 [1:23:30<1:47:44,  6.12s/it]loss_total_epoch 266.10378312319517
Training tokenizer:  48% 958/2013 [1:23:36<1:47:21,  6.11s/it]loss_total_epoch 266.3582419641316
Training tokenizer:  48% 959/2013 [1:23:42<1:47:12,  6.10s/it]loss_total_epoch 266.56563403084874
Training tokenizer:  48% 960/2013 [1:23:49<1:47:14,  6.11s/it]loss_total_epoch 266.7841235920787
Training tokenizer:  48% 961/2013 [1:23:55<1:46:53,  6.10s/it]loss_total_epoch 267.01439809054136
Training tokenizer:  48% 962/2013 [1:24:01<1:46:34,  6.08s/it]loss_total_epoch 267.2472276352346
Training tokenizer:  48% 963/2013 [1:24:07<1:46:27,  6.08s/it]loss_total_epoch 267.4734748862684
Training tokenizer:  48% 964/2013 [1:24:13<1:46:31,  6.09s/it]loss_total_epoch 267.6805055104196
Training tokenizer:  48% 965/2013 [1:24:19<1:46:32,  6.10s/it]loss_total_epoch 267.912711057812
Training tokenizer:  48% 966/2013 [1:24:25<1:46:26,  6.10s/it]loss_total_epoch 268.1385766491294
Training tokenizer:  48% 967/2013 [1:24:31<1:45:55,  6.08s/it]loss_total_epoch 268.36507480964065
Training tokenizer:  48% 968/2013 [1:24:37<1:45:59,  6.09s/it]loss_total_epoch 268.59951090440154
Training tokenizer:  48% 969/2013 [1:24:43<1:46:05,  6.10s/it]loss_total_epoch 268.799566142261
Training tokenizer:  48% 970/2013 [1:24:49<1:45:53,  6.09s/it]loss_total_epoch 269.0241234265268
Training tokenizer:  48% 971/2013 [1:24:56<1:45:34,  6.08s/it]loss_total_epoch 269.24014787748456
Training tokenizer:  48% 972/2013 [1:25:02<1:45:53,  6.10s/it]loss_total_epoch 269.4663082771003
Training tokenizer:  48% 973/2013 [1:25:08<1:45:48,  6.10s/it]loss_total_epoch 269.6790932677686
Training tokenizer:  48% 974/2013 [1:25:14<1:45:43,  6.11s/it]loss_total_epoch 269.9131796322763
Training tokenizer:  48% 975/2013 [1:25:20<1:45:21,  6.09s/it]loss_total_epoch 270.12252885848284
Training tokenizer:  48% 976/2013 [1:25:26<1:45:22,  6.10s/it]loss_total_epoch 270.34711030870676
Training tokenizer:  49% 977/2013 [1:25:32<1:45:32,  6.11s/it]loss_total_epoch 270.56112391129136
Training tokenizer:  49% 978/2013 [1:25:38<1:45:21,  6.11s/it]loss_total_epoch 270.77003706619143
Training tokenizer:  49% 979/2013 [1:25:44<1:45:33,  6.12s/it]loss_total_epoch 270.9856180176139
Training tokenizer:  49% 980/2013 [1:25:51<1:45:36,  6.13s/it]loss_total_epoch 271.2074695266783
Training tokenizer:  49% 981/2013 [1:25:57<1:45:34,  6.14s/it]loss_total_epoch 271.42959278449416
Training tokenizer:  49% 982/2013 [1:26:03<1:45:23,  6.13s/it]loss_total_epoch 271.65512143075466
Training tokenizer:  49% 983/2013 [1:26:09<1:44:49,  6.11s/it]loss_total_epoch 271.8781368024647
Training tokenizer:  49% 984/2013 [1:26:15<1:44:54,  6.12s/it]loss_total_epoch 272.100679744035
Training tokenizer:  49% 985/2013 [1:26:21<1:44:52,  6.12s/it]loss_total_epoch 272.3468140922487
Training tokenizer:  49% 986/2013 [1:26:27<1:44:41,  6.12s/it]loss_total_epoch 272.57563546299934
Training tokenizer:  49% 987/2013 [1:26:33<1:44:51,  6.13s/it]loss_total_epoch 272.7931752689183
Training tokenizer:  49% 988/2013 [1:26:40<1:45:16,  6.16s/it]loss_total_epoch 273.0409473888576
Training tokenizer:  49% 989/2013 [1:26:46<1:44:44,  6.14s/it]loss_total_epoch 273.2466193921864
Training tokenizer:  49% 990/2013 [1:26:52<1:44:38,  6.14s/it]loss_total_epoch 273.4647158049047
Training tokenizer:  49% 991/2013 [1:26:58<1:44:42,  6.15s/it]loss_total_epoch 273.70578073710203
Training tokenizer:  49% 992/2013 [1:27:04<1:44:36,  6.15s/it]loss_total_epoch 273.91749580204487
Training tokenizer:  49% 993/2013 [1:27:10<1:44:34,  6.15s/it]loss_total_epoch 274.1325460448861
Training tokenizer:  49% 994/2013 [1:27:17<1:44:20,  6.14s/it]loss_total_epoch 274.34969644621015
Training tokenizer:  49% 995/2013 [1:27:23<1:44:33,  6.16s/it]loss_total_epoch 274.57280680537224
Training tokenizer:  49% 996/2013 [1:27:29<1:44:03,  6.14s/it]loss_total_epoch 274.7994626648724
Training tokenizer:  50% 997/2013 [1:27:35<1:43:57,  6.14s/it]loss_total_epoch 275.0562757924199
Training tokenizer:  50% 998/2013 [1:27:41<1:43:44,  6.13s/it]loss_total_epoch 275.2803688980639
Training tokenizer:  50% 999/2013 [1:27:47<1:43:57,  6.15s/it]loss_total_epoch 275.4969199895859
Training tokenizer:  50% 1000/2013 [1:27:54<1:44:15,  6.18s/it]loss_total_epoch 275.7087739184499
Training tokenizer:  50% 1001/2013 [1:28:00<1:44:05,  6.17s/it]loss_total_epoch 275.9365336038172
Training tokenizer:  50% 1002/2013 [1:28:06<1:44:03,  6.18s/it]loss_total_epoch 276.1698848903179
Training tokenizer:  50% 1003/2013 [1:28:12<1:44:31,  6.21s/it]loss_total_epoch 276.4040393680334
Training tokenizer:  50% 1004/2013 [1:28:18<1:44:18,  6.20s/it]loss_total_epoch 276.6284832842648
Training tokenizer:  50% 1005/2013 [1:28:25<1:44:15,  6.21s/it]loss_total_epoch 276.85486802458763
Training tokenizer:  50% 1006/2013 [1:28:31<1:44:02,  6.20s/it]loss_total_epoch 277.07730151712894
Training tokenizer:  50% 1007/2013 [1:28:37<1:44:05,  6.21s/it]loss_total_epoch 277.29294538870454
Training tokenizer:  50% 1008/2013 [1:28:43<1:43:50,  6.20s/it]loss_total_epoch 277.5000702999532
Training tokenizer:  50% 1009/2013 [1:28:49<1:43:30,  6.19s/it]loss_total_epoch 277.722630597651
Training tokenizer:  50% 1010/2013 [1:28:55<1:43:27,  6.19s/it]loss_total_epoch 277.9792669005692
Training tokenizer:  50% 1011/2013 [1:29:02<1:42:54,  6.16s/it]loss_total_epoch 278.1901724860072
Training tokenizer:  50% 1012/2013 [1:29:08<1:42:47,  6.16s/it]loss_total_epoch 278.42246221378446
Training tokenizer:  50% 1013/2013 [1:29:14<1:42:40,  6.16s/it]loss_total_epoch 278.6360663548112
Training tokenizer:  50% 1014/2013 [1:29:20<1:42:59,  6.19s/it]loss_total_epoch 278.87412451952696
Training tokenizer:  50% 1015/2013 [1:29:26<1:42:57,  6.19s/it]loss_total_epoch 279.0879007279873
Training tokenizer:  50% 1016/2013 [1:29:33<1:42:58,  6.20s/it]loss_total_epoch 279.28959999233484
Training tokenizer:  51% 1017/2013 [1:29:39<1:42:48,  6.19s/it]loss_total_epoch 279.50423961132765
Training tokenizer:  51% 1018/2013 [1:29:45<1:42:43,  6.19s/it]loss_total_epoch 279.7319660820067
Training tokenizer:  51% 1019/2013 [1:29:51<1:42:43,  6.20s/it]loss_total_epoch 279.95510637015104
Training tokenizer:  51% 1020/2013 [1:29:57<1:42:16,  6.18s/it]loss_total_epoch 280.17846121266484
Training tokenizer:  51% 1021/2013 [1:30:04<1:42:34,  6.20s/it]loss_total_epoch 280.38791608810425
Training tokenizer:  51% 1022/2013 [1:30:10<1:42:40,  6.22s/it]loss_total_epoch 280.60892866924405
Training tokenizer:  51% 1023/2013 [1:30:16<1:42:39,  6.22s/it]loss_total_epoch 280.8259457051754
Training tokenizer:  51% 1024/2013 [1:30:22<1:42:35,  6.22s/it]loss_total_epoch 281.0407411903143
Training tokenizer:  51% 1025/2013 [1:30:28<1:42:06,  6.20s/it]loss_total_epoch 281.23373571783304
Training tokenizer:  51% 1026/2013 [1:30:35<1:42:06,  6.21s/it]loss_total_epoch 281.4617492072284
Training tokenizer:  51% 1027/2013 [1:30:41<1:42:24,  6.23s/it]loss_total_epoch 281.6653640046716
Training tokenizer:  51% 1028/2013 [1:30:47<1:42:13,  6.23s/it]loss_total_epoch 281.87957287952304
Training tokenizer:  51% 1029/2013 [1:30:53<1:42:22,  6.24s/it]loss_total_epoch 282.10940009728074
Training tokenizer:  51% 1030/2013 [1:31:00<1:42:12,  6.24s/it]loss_total_epoch 282.3157795444131
Training tokenizer:  51% 1031/2013 [1:31:06<1:42:13,  6.25s/it]loss_total_epoch 282.544514529407
Training tokenizer:  51% 1032/2013 [1:31:12<1:42:20,  6.26s/it]loss_total_epoch 282.75245440378785
Training tokenizer:  51% 1033/2013 [1:31:18<1:42:04,  6.25s/it]loss_total_epoch 282.98914390429854
Training tokenizer:  51% 1034/2013 [1:31:25<1:42:11,  6.26s/it]loss_total_epoch 283.22056690603495
Training tokenizer:  51% 1035/2013 [1:31:31<1:41:52,  6.25s/it]loss_total_epoch 283.4298622086644
Training tokenizer:  51% 1036/2013 [1:31:37<1:41:33,  6.24s/it]loss_total_epoch 283.6697855964303
Training tokenizer:  52% 1037/2013 [1:31:43<1:41:25,  6.24s/it]loss_total_epoch 283.88899850845337
Training tokenizer:  52% 1038/2013 [1:31:50<1:41:35,  6.25s/it]loss_total_epoch 284.1303426101804
Training tokenizer:  52% 1039/2013 [1:31:56<1:41:38,  6.26s/it]loss_total_epoch 284.33649104833603
Training tokenizer:  52% 1040/2013 [1:32:02<1:41:30,  6.26s/it]loss_total_epoch 284.5837552174926
Training tokenizer:  52% 1041/2013 [1:32:08<1:41:11,  6.25s/it]loss_total_epoch 284.78073393180966
Training tokenizer:  52% 1042/2013 [1:32:15<1:41:03,  6.24s/it]loss_total_epoch 284.9962231107056
Training tokenizer:  52% 1043/2013 [1:32:21<1:40:52,  6.24s/it]loss_total_epoch 285.2194301635027
Training tokenizer:  52% 1044/2013 [1:32:27<1:40:50,  6.24s/it]loss_total_epoch 285.44132528826594
Training tokenizer:  52% 1045/2013 [1:32:33<1:40:40,  6.24s/it]loss_total_epoch 285.6593183912337
Training tokenizer:  52% 1046/2013 [1:32:40<1:40:48,  6.25s/it]loss_total_epoch 285.8889189325273
Training tokenizer:  52% 1047/2013 [1:32:46<1:40:24,  6.24s/it]loss_total_epoch 286.08176377788186
Training tokenizer:  52% 1048/2013 [1:32:52<1:40:36,  6.26s/it]loss_total_epoch 286.3148512765765
Training tokenizer:  52% 1049/2013 [1:32:58<1:40:44,  6.27s/it]loss_total_epoch 286.5651574842632
Training tokenizer:  52% 1050/2013 [1:33:05<1:40:42,  6.28s/it]loss_total_epoch 286.80282091349363
Training tokenizer:  52% 1051/2013 [1:33:11<1:40:26,  6.26s/it]loss_total_epoch 287.0253770425916
Training tokenizer:  52% 1052/2013 [1:33:17<1:40:27,  6.27s/it]loss_total_epoch 287.23343563079834
Training tokenizer:  52% 1053/2013 [1:33:24<1:40:17,  6.27s/it]loss_total_epoch 287.46550711244345
Training tokenizer:  52% 1054/2013 [1:33:30<1:40:04,  6.26s/it]loss_total_epoch 287.69775753840804
Training tokenizer:  52% 1055/2013 [1:33:36<1:39:51,  6.25s/it]loss_total_epoch 287.92067681998014
Training tokenizer:  52% 1056/2013 [1:33:42<1:39:54,  6.26s/it]loss_total_epoch 288.1534072495997
Training tokenizer:  53% 1057/2013 [1:33:49<1:39:36,  6.25s/it]loss_total_epoch 288.3762523718178
Training tokenizer:  53% 1058/2013 [1:33:55<1:39:27,  6.25s/it]loss_total_epoch 288.6012903265655
Training tokenizer:  53% 1059/2013 [1:34:01<1:39:36,  6.26s/it]loss_total_epoch 288.82132349163294
Training tokenizer:  53% 1060/2013 [1:34:07<1:39:38,  6.27s/it]loss_total_epoch 289.04306219145656
Training tokenizer:  53% 1061/2013 [1:34:14<1:39:34,  6.28s/it]loss_total_epoch 289.2582717500627
Training tokenizer:  53% 1062/2013 [1:34:20<1:39:27,  6.28s/it]loss_total_epoch 289.4665541239083
Training tokenizer:  53% 1063/2013 [1:34:26<1:39:14,  6.27s/it]loss_total_epoch 289.66716841235757
Training tokenizer:  53% 1064/2013 [1:34:32<1:39:22,  6.28s/it]loss_total_epoch 289.9011149853468
Training tokenizer:  53% 1065/2013 [1:34:39<1:38:59,  6.27s/it]loss_total_epoch 290.12420835345984
Training tokenizer:  53% 1066/2013 [1:34:45<1:39:11,  6.28s/it]loss_total_epoch 290.3429068028927
Training tokenizer:  53% 1067/2013 [1:34:51<1:39:01,  6.28s/it]loss_total_epoch 290.5666864104569
Training tokenizer:  53% 1068/2013 [1:34:58<1:38:58,  6.28s/it]loss_total_epoch 290.7930099107325
Training tokenizer:  53% 1069/2013 [1:35:04<1:38:43,  6.28s/it]loss_total_epoch 291.0539608858526
Training tokenizer:  53% 1070/2013 [1:35:10<1:38:53,  6.29s/it]loss_total_epoch 291.27388582378626
Training tokenizer:  53% 1071/2013 [1:35:16<1:38:42,  6.29s/it]loss_total_epoch 291.49732537567616
Training tokenizer:  53% 1072/2013 [1:35:23<1:39:02,  6.31s/it]loss_total_epoch 291.7075259536505
Training tokenizer:  53% 1073/2013 [1:35:29<1:38:37,  6.30s/it]loss_total_epoch 291.934796243906
Training tokenizer:  53% 1074/2013 [1:35:35<1:38:47,  6.31s/it]loss_total_epoch 292.1492748633027
Training tokenizer:  53% 1075/2013 [1:35:42<1:38:23,  6.29s/it]loss_total_epoch 292.36390996724367
Training tokenizer:  53% 1076/2013 [1:35:48<1:38:39,  6.32s/it]loss_total_epoch 292.5886086188257
Training tokenizer:  54% 1077/2013 [1:35:54<1:38:36,  6.32s/it]loss_total_epoch 292.80062567815185
Training tokenizer:  54% 1078/2013 [1:36:01<1:38:44,  6.34s/it]loss_total_epoch 293.02391949296
Training tokenizer:  54% 1079/2013 [1:36:07<1:38:35,  6.33s/it]loss_total_epoch 293.24596143886447
Training tokenizer:  54% 1080/2013 [1:36:13<1:38:10,  6.31s/it]loss_total_epoch 293.42706413567066
Training tokenizer:  54% 1081/2013 [1:36:20<1:37:58,  6.31s/it]loss_total_epoch 293.6549790240824
Training tokenizer:  54% 1082/2013 [1:36:26<1:37:53,  6.31s/it]loss_total_epoch 293.8941296823323
Training tokenizer:  54% 1083/2013 [1:36:32<1:37:57,  6.32s/it]loss_total_epoch 294.1212803609669
Training tokenizer:  54% 1084/2013 [1:36:39<1:37:45,  6.31s/it]loss_total_epoch 294.3536694981158
Training tokenizer:  54% 1085/2013 [1:36:45<1:37:38,  6.31s/it]loss_total_epoch 294.574305113405
Training tokenizer:  54% 1086/2013 [1:36:51<1:37:45,  6.33s/it]loss_total_epoch 294.7985204383731
Training tokenizer:  54% 1087/2013 [1:36:58<1:37:45,  6.33s/it]loss_total_epoch 295.0425579175353
Training tokenizer:  54% 1088/2013 [1:37:04<1:37:27,  6.32s/it]loss_total_epoch 295.24747193977237
Training tokenizer:  54% 1089/2013 [1:37:10<1:37:35,  6.34s/it]loss_total_epoch 295.45399360358715
Training tokenizer:  54% 1090/2013 [1:37:17<1:37:36,  6.35s/it]loss_total_epoch 295.65917678177357
Training tokenizer:  54% 1091/2013 [1:37:23<1:37:31,  6.35s/it]loss_total_epoch 295.849837820977
Training tokenizer:  54% 1092/2013 [1:37:29<1:37:23,  6.34s/it]loss_total_epoch 296.04355261847377
Training tokenizer:  54% 1093/2013 [1:37:36<1:37:21,  6.35s/it]loss_total_epoch 296.2525109834969
Training tokenizer:  54% 1094/2013 [1:37:42<1:36:56,  6.33s/it]loss_total_epoch 296.48709040507674
Training tokenizer:  54% 1095/2013 [1:37:48<1:36:44,  6.32s/it]loss_total_epoch 296.6872598491609
Training tokenizer:  54% 1096/2013 [1:37:55<1:36:56,  6.34s/it]loss_total_epoch 296.89337120577693
Training tokenizer:  54% 1097/2013 [1:38:01<1:36:50,  6.34s/it]loss_total_epoch 297.10972478985786
Training tokenizer:  55% 1098/2013 [1:38:07<1:37:00,  6.36s/it]loss_total_epoch 297.32094752416015
Training tokenizer:  55% 1099/2013 [1:38:14<1:37:06,  6.38s/it]loss_total_epoch 297.5546487569809
Training tokenizer:  55% 1100/2013 [1:38:20<1:37:08,  6.38s/it]loss_total_epoch 297.77707935869694
Training tokenizer:  55% 1101/2013 [1:38:27<1:36:37,  6.36s/it]loss_total_epoch 298.0076447315514
Training tokenizer:  55% 1102/2013 [1:38:33<1:36:24,  6.35s/it]loss_total_epoch 298.2258643656969
Training tokenizer:  55% 1103/2013 [1:38:39<1:36:28,  6.36s/it]loss_total_epoch 298.4612980224192
Training tokenizer:  55% 1104/2013 [1:38:46<1:36:19,  6.36s/it]loss_total_epoch 298.6930438429117
Training tokenizer:  55% 1105/2013 [1:38:52<1:36:17,  6.36s/it]loss_total_epoch 298.92830400541425
Training tokenizer:  55% 1106/2013 [1:38:58<1:36:11,  6.36s/it]loss_total_epoch 299.1585715189576
Training tokenizer:  55% 1107/2013 [1:39:05<1:36:17,  6.38s/it]loss_total_epoch 299.40186569467187
Training tokenizer:  55% 1108/2013 [1:39:11<1:36:21,  6.39s/it]loss_total_epoch 299.62253834307194
Training tokenizer:  55% 1109/2013 [1:39:18<1:36:26,  6.40s/it]loss_total_epoch 299.8444943726063
Training tokenizer:  55% 1110/2013 [1:39:24<1:36:23,  6.40s/it]loss_total_epoch 300.0990669988096
Training tokenizer:  55% 1111/2013 [1:39:30<1:36:10,  6.40s/it]loss_total_epoch 300.3627839796245
Training tokenizer:  55% 1112/2013 [1:39:37<1:36:18,  6.41s/it]loss_total_epoch 300.5868516340852
Training tokenizer:  55% 1113/2013 [1:39:43<1:36:01,  6.40s/it]loss_total_epoch 300.82928105816245
Training tokenizer:  55% 1114/2013 [1:39:50<1:36:02,  6.41s/it]loss_total_epoch 301.07993871346116
Training tokenizer:  55% 1115/2013 [1:39:56<1:35:59,  6.41s/it]loss_total_epoch 301.3109846264124
Training tokenizer:  55% 1116/2013 [1:40:02<1:35:48,  6.41s/it]loss_total_epoch 301.5180893018842
Training tokenizer:  55% 1117/2013 [1:40:09<1:35:38,  6.41s/it]loss_total_epoch 301.75458378717303
Training tokenizer:  56% 1118/2013 [1:40:15<1:35:50,  6.43s/it]loss_total_epoch 301.9744758233428
Training tokenizer:  56% 1119/2013 [1:40:22<1:35:43,  6.42s/it]loss_total_epoch 302.20383256301284
Training tokenizer:  56% 1120/2013 [1:40:28<1:35:35,  6.42s/it]loss_total_epoch 302.4017814844847
Training tokenizer:  56% 1121/2013 [1:40:35<1:35:20,  6.41s/it]loss_total_epoch 302.6376754902303
Training tokenizer:  56% 1122/2013 [1:40:41<1:35:24,  6.42s/it]loss_total_epoch 302.8825569599867
Training tokenizer:  56% 1123/2013 [1:40:47<1:35:12,  6.42s/it]loss_total_epoch 303.1058625280857
Training tokenizer:  56% 1124/2013 [1:40:54<1:35:11,  6.42s/it]loss_total_epoch 303.3213386759162
Training tokenizer:  56% 1125/2013 [1:41:00<1:35:13,  6.43s/it]loss_total_epoch 303.5339218080044
Training tokenizer:  56% 1126/2013 [1:41:07<1:34:57,  6.42s/it]loss_total_epoch 303.7268878184259
Training tokenizer:  56% 1127/2013 [1:41:13<1:35:04,  6.44s/it]loss_total_epoch 303.9166388809681
Training tokenizer:  56% 1128/2013 [1:41:20<1:35:02,  6.44s/it]loss_total_epoch 304.1555232293904
Training tokenizer:  56% 1129/2013 [1:41:26<1:34:59,  6.45s/it]loss_total_epoch 304.3785587884486
Training tokenizer:  56% 1130/2013 [1:41:33<1:34:52,  6.45s/it]loss_total_epoch 304.59413139522076
Training tokenizer:  56% 1131/2013 [1:41:39<1:34:43,  6.44s/it]loss_total_epoch 304.81085465475917
Training tokenizer:  56% 1132/2013 [1:41:45<1:34:30,  6.44s/it]loss_total_epoch 305.011800698936
Training tokenizer:  56% 1133/2013 [1:41:52<1:34:30,  6.44s/it]loss_total_epoch 305.2281193844974
Training tokenizer:  56% 1134/2013 [1:41:58<1:34:24,  6.44s/it]loss_total_epoch 305.44380974397063
Training tokenizer:  56% 1135/2013 [1:42:05<1:34:08,  6.43s/it]loss_total_epoch 305.66977502405643
Training tokenizer:  56% 1136/2013 [1:42:11<1:33:59,  6.43s/it]loss_total_epoch 305.8922230042517
Training tokenizer:  56% 1137/2013 [1:42:18<1:34:06,  6.45s/it]loss_total_epoch 306.095693487674
Training tokenizer:  57% 1138/2013 [1:42:24<1:34:05,  6.45s/it]loss_total_epoch 306.29694997891784
Training tokenizer:  57% 1139/2013 [1:42:30<1:33:36,  6.43s/it]loss_total_epoch 306.5152386575937
Training tokenizer:  57% 1140/2013 [1:42:37<1:33:35,  6.43s/it]loss_total_epoch 306.72970880940557
Training tokenizer:  57% 1141/2013 [1:42:43<1:33:41,  6.45s/it]loss_total_epoch 306.9436290860176
Training tokenizer:  57% 1142/2013 [1:42:50<1:33:38,  6.45s/it]loss_total_epoch 307.18057227507234
Training tokenizer:  57% 1143/2013 [1:42:56<1:33:48,  6.47s/it]loss_total_epoch 307.3681845329702
Training tokenizer:  57% 1144/2013 [1:43:03<1:33:46,  6.47s/it]loss_total_epoch 307.58524772152305
Training tokenizer:  57% 1145/2013 [1:43:09<1:33:42,  6.48s/it]loss_total_epoch 307.7859952189028
Training tokenizer:  57% 1146/2013 [1:43:16<1:33:26,  6.47s/it]loss_total_epoch 307.9920949153602
Training tokenizer:  57% 1147/2013 [1:43:22<1:33:22,  6.47s/it]loss_total_epoch 308.20436791330576
Training tokenizer:  57% 1148/2013 [1:43:29<1:33:11,  6.46s/it]loss_total_epoch 308.4167305268347
Training tokenizer:  57% 1149/2013 [1:43:35<1:33:04,  6.46s/it]loss_total_epoch 308.6462037973106
Training tokenizer:  57% 1150/2013 [1:43:42<1:32:40,  6.44s/it]loss_total_epoch 308.86275420337915
Training tokenizer:  57% 1151/2013 [1:43:48<1:32:47,  6.46s/it]loss_total_epoch 309.06573831290007
Training tokenizer:  57% 1152/2013 [1:43:55<1:32:58,  6.48s/it]loss_total_epoch 309.27636282518506
Training tokenizer:  57% 1153/2013 [1:44:01<1:33:04,  6.49s/it]loss_total_epoch 309.5138338841498
Training tokenizer:  57% 1154/2013 [1:44:08<1:33:03,  6.50s/it]loss_total_epoch 309.70117585733533
Training tokenizer:  57% 1155/2013 [1:44:14<1:32:39,  6.48s/it]loss_total_epoch 309.9333256781101
Training tokenizer:  57% 1156/2013 [1:44:21<1:32:38,  6.49s/it]loss_total_epoch 310.1583273559809
Training tokenizer:  57% 1157/2013 [1:44:27<1:32:19,  6.47s/it]loss_total_epoch 310.3836534805596
Training tokenizer:  58% 1158/2013 [1:44:34<1:32:34,  6.50s/it]loss_total_epoch 310.61433351784945
Training tokenizer:  58% 1159/2013 [1:44:40<1:32:41,  6.51s/it]loss_total_epoch 310.8409463316202
Training tokenizer:  58% 1160/2013 [1:44:47<1:32:36,  6.51s/it]loss_total_epoch 311.0560961179435
Training tokenizer:  58% 1161/2013 [1:44:53<1:32:29,  6.51s/it]loss_total_epoch 311.2659036666155
Training tokenizer:  58% 1162/2013 [1:45:00<1:32:22,  6.51s/it]loss_total_epoch 311.4957146458328
Training tokenizer:  58% 1163/2013 [1:45:06<1:32:24,  6.52s/it]loss_total_epoch 311.6988019347191
Training tokenizer:  58% 1164/2013 [1:45:13<1:32:19,  6.52s/it]loss_total_epoch 311.9304301328957
Training tokenizer:  58% 1165/2013 [1:45:19<1:32:13,  6.53s/it]loss_total_epoch 312.15058478713036
Training tokenizer:  58% 1166/2013 [1:45:26<1:32:18,  6.54s/it]loss_total_epoch 312.3392671905458
Training tokenizer:  58% 1167/2013 [1:45:32<1:32:09,  6.54s/it]loss_total_epoch 312.56274361908436
Training tokenizer:  58% 1168/2013 [1:45:39<1:32:01,  6.53s/it]loss_total_epoch 312.7855205051601
Training tokenizer:  58% 1169/2013 [1:45:45<1:31:54,  6.53s/it]loss_total_epoch 312.97859249264
Training tokenizer:  58% 1170/2013 [1:45:52<1:31:46,  6.53s/it]loss_total_epoch 313.19188709557056
Training tokenizer:  58% 1171/2013 [1:45:58<1:31:46,  6.54s/it]loss_total_epoch 313.3988305218518
Training tokenizer:  58% 1172/2013 [1:46:05<1:31:48,  6.55s/it]loss_total_epoch 313.60580717399716
Training tokenizer:  58% 1173/2013 [1:46:12<1:31:31,  6.54s/it]loss_total_epoch 313.8110973909497
Training tokenizer:  58% 1174/2013 [1:46:18<1:31:25,  6.54s/it]loss_total_epoch 314.0298305787146
Training tokenizer:  58% 1175/2013 [1:46:25<1:31:32,  6.55s/it]loss_total_epoch 314.2602225244045
Training tokenizer:  58% 1176/2013 [1:46:31<1:31:23,  6.55s/it]loss_total_epoch 314.4574955254793
Training tokenizer:  58% 1177/2013 [1:46:38<1:31:21,  6.56s/it]loss_total_epoch 314.6640173830092
Training tokenizer:  59% 1178/2013 [1:46:44<1:31:05,  6.55s/it]loss_total_epoch 314.8497710302472
Training tokenizer:  59% 1179/2013 [1:46:51<1:31:12,  6.56s/it]loss_total_epoch 315.0657351575792
Training tokenizer:  59% 1180/2013 [1:46:58<1:31:19,  6.58s/it]loss_total_epoch 315.27160480618477
Training tokenizer:  59% 1181/2013 [1:47:04<1:30:51,  6.55s/it]loss_total_epoch 315.5253341384232
Training tokenizer:  59% 1182/2013 [1:47:11<1:30:39,  6.55s/it]loss_total_epoch 315.75085002556443
Training tokenizer:  59% 1183/2013 [1:47:17<1:30:36,  6.55s/it]loss_total_epoch 315.9635352566838
Training tokenizer:  59% 1184/2013 [1:47:24<1:30:46,  6.57s/it]loss_total_epoch 316.1877951771021
Training tokenizer:  59% 1185/2013 [1:47:30<1:30:48,  6.58s/it]loss_total_epoch 316.4053159058094
Training tokenizer:  59% 1186/2013 [1:47:37<1:30:41,  6.58s/it]loss_total_epoch 316.62156268954277
Training tokenizer:  59% 1187/2013 [1:47:44<1:30:34,  6.58s/it]loss_total_epoch 316.8684836924076
Training tokenizer:  59% 1188/2013 [1:47:50<1:30:22,  6.57s/it]loss_total_epoch 317.112973369658
Training tokenizer:  59% 1189/2013 [1:47:57<1:30:22,  6.58s/it]loss_total_epoch 317.31623485684395
Training tokenizer:  59% 1190/2013 [1:48:03<1:30:30,  6.60s/it]loss_total_epoch 317.5235916785896
Training tokenizer:  59% 1191/2013 [1:48:10<1:30:25,  6.60s/it]loss_total_epoch 317.7537493817508
Training tokenizer:  59% 1192/2013 [1:48:17<1:30:14,  6.60s/it]loss_total_epoch 317.97227258980274
Training tokenizer:  59% 1193/2013 [1:48:23<1:30:02,  6.59s/it]loss_total_epoch 318.1947101280093
Training tokenizer:  59% 1194/2013 [1:48:30<1:30:01,  6.60s/it]loss_total_epoch 318.39452062547207
Training tokenizer:  59% 1195/2013 [1:48:36<1:29:46,  6.58s/it]loss_total_epoch 318.6146645694971
Training tokenizer:  59% 1196/2013 [1:48:43<1:29:41,  6.59s/it]loss_total_epoch 318.82813841477036
Training tokenizer:  59% 1197/2013 [1:48:49<1:29:42,  6.60s/it]loss_total_epoch 319.04855766519904
Training tokenizer:  60% 1198/2013 [1:48:56<1:29:47,  6.61s/it]loss_total_epoch 319.2750262282789
Training tokenizer:  60% 1199/2013 [1:49:03<1:29:46,  6.62s/it]loss_total_epoch 319.4629549905658
Training tokenizer:  60% 1200/2013 [1:49:09<1:29:44,  6.62s/it]loss_total_epoch 319.66432394087315
Training tokenizer:  60% 1201/2013 [1:49:16<1:29:29,  6.61s/it]loss_total_epoch 319.89323902130127
Training tokenizer:  60% 1202/2013 [1:49:23<1:29:22,  6.61s/it]loss_total_epoch 320.1273675970733
Training tokenizer:  60% 1203/2013 [1:49:29<1:29:13,  6.61s/it]loss_total_epoch 320.3391809128225
Training tokenizer:  60% 1204/2013 [1:49:36<1:29:10,  6.61s/it]loss_total_epoch 320.5650047622621
Training tokenizer:  60% 1205/2013 [1:49:42<1:29:01,  6.61s/it]loss_total_epoch 320.78880390152335
Training tokenizer:  60% 1206/2013 [1:49:49<1:29:00,  6.62s/it]loss_total_epoch 320.9980626255274
Training tokenizer:  60% 1207/2013 [1:49:56<1:28:53,  6.62s/it]loss_total_epoch 321.21703550219536
Training tokenizer:  60% 1208/2013 [1:50:02<1:28:40,  6.61s/it]loss_total_epoch 321.4124427884817
Training tokenizer:  60% 1209/2013 [1:50:09<1:28:37,  6.61s/it]loss_total_epoch 321.6245060414076
Training tokenizer:  60% 1210/2013 [1:50:15<1:28:31,  6.61s/it]loss_total_epoch 321.8390251621604
Training tokenizer:  60% 1211/2013 [1:50:22<1:28:34,  6.63s/it]loss_total_epoch 322.0579770989716
Training tokenizer:  60% 1212/2013 [1:50:29<1:28:30,  6.63s/it]loss_total_epoch 322.2571362927556
Training tokenizer:  60% 1213/2013 [1:50:35<1:28:22,  6.63s/it]loss_total_epoch 322.46180403605103
Training tokenizer:  60% 1214/2013 [1:50:42<1:28:20,  6.63s/it]loss_total_epoch 322.6688628271222
Training tokenizer:  60% 1215/2013 [1:50:49<1:28:24,  6.65s/it]loss_total_epoch 322.89432157948613
Training tokenizer:  60% 1216/2013 [1:50:55<1:28:24,  6.65s/it]loss_total_epoch 323.1177947483957
Training tokenizer:  60% 1217/2013 [1:51:02<1:28:26,  6.67s/it]loss_total_epoch 323.3395413570106
Training tokenizer:  61% 1218/2013 [1:51:09<1:28:16,  6.66s/it]loss_total_epoch 323.5472288057208
Training tokenizer:  61% 1219/2013 [1:51:15<1:27:55,  6.64s/it]loss_total_epoch 323.7652396559715
Training tokenizer:  61% 1220/2013 [1:51:22<1:27:45,  6.64s/it]loss_total_epoch 323.9886380881071
Training tokenizer:  61% 1221/2013 [1:51:29<1:27:43,  6.65s/it]loss_total_epoch 324.2104306668043
Training tokenizer:  61% 1222/2013 [1:51:35<1:27:20,  6.63s/it]loss_total_epoch 324.44658479094505
Training tokenizer:  61% 1223/2013 [1:51:42<1:27:35,  6.65s/it]loss_total_epoch 324.66026958823204
Training tokenizer:  61% 1224/2013 [1:51:49<1:27:31,  6.66s/it]loss_total_epoch 324.88868353515863
Training tokenizer:  61% 1225/2013 [1:51:55<1:27:37,  6.67s/it]loss_total_epoch 325.0757121182978
Training tokenizer:  61% 1226/2013 [1:52:02<1:27:23,  6.66s/it]loss_total_epoch 325.309044893831
Training tokenizer:  61% 1227/2013 [1:52:09<1:27:16,  6.66s/it]loss_total_epoch 325.5503150187433
Training tokenizer:  61% 1228/2013 [1:52:15<1:27:10,  6.66s/it]loss_total_epoch 325.73787536099553
Training tokenizer:  61% 1229/2013 [1:52:22<1:26:53,  6.65s/it]loss_total_epoch 325.9759425148368
Training tokenizer:  61% 1230/2013 [1:52:29<1:26:41,  6.64s/it]loss_total_epoch 326.1977879330516
Training tokenizer:  61% 1231/2013 [1:52:35<1:26:46,  6.66s/it]loss_total_epoch 326.4262433871627
Training tokenizer:  61% 1232/2013 [1:52:42<1:26:57,  6.68s/it]loss_total_epoch 326.6355349123478
Training tokenizer:  61% 1233/2013 [1:52:49<1:26:50,  6.68s/it]loss_total_epoch 326.82500040717423
Training tokenizer:  61% 1234/2013 [1:52:55<1:26:42,  6.68s/it]loss_total_epoch 327.022818652913
Training tokenizer:  61% 1235/2013 [1:53:02<1:26:36,  6.68s/it]loss_total_epoch 327.21115438081324
Training tokenizer:  61% 1236/2013 [1:53:09<1:26:20,  6.67s/it]loss_total_epoch 327.4349184911698
Training tokenizer:  61% 1237/2013 [1:53:15<1:26:04,  6.66s/it]loss_total_epoch 327.6456426884979
Training tokenizer:  62% 1238/2013 [1:53:22<1:26:20,  6.68s/it]loss_total_epoch 327.8608925137669
Training tokenizer:  62% 1239/2013 [1:53:29<1:26:14,  6.69s/it]loss_total_epoch 328.0859511401504
Training tokenizer:  62% 1240/2013 [1:53:35<1:25:58,  6.67s/it]loss_total_epoch 328.310361744836
Training tokenizer:  62% 1241/2013 [1:53:42<1:25:29,  6.64s/it]loss_total_epoch 328.5283742342144
Training tokenizer:  62% 1242/2013 [1:53:49<1:25:31,  6.66s/it]loss_total_epoch 328.7455115187913
Training tokenizer:  62% 1243/2013 [1:53:55<1:25:31,  6.66s/it]loss_total_epoch 328.98495563305914
Training tokenizer:  62% 1244/2013 [1:54:02<1:25:26,  6.67s/it]loss_total_epoch 329.21362111903727
Training tokenizer:  62% 1245/2013 [1:54:09<1:25:30,  6.68s/it]loss_total_epoch 329.4199434313923
Training tokenizer:  62% 1246/2013 [1:54:15<1:25:35,  6.70s/it]loss_total_epoch 329.6369183380157
Training tokenizer:  62% 1247/2013 [1:54:22<1:25:27,  6.69s/it]loss_total_epoch 329.84990374930203
Training tokenizer:  62% 1248/2013 [1:54:29<1:25:14,  6.69s/it]loss_total_epoch 330.0880645569414
Training tokenizer:  62% 1249/2013 [1:54:35<1:25:13,  6.69s/it]loss_total_epoch 330.31184878386557
Training tokenizer:  62% 1250/2013 [1:54:42<1:25:03,  6.69s/it]loss_total_epoch 330.5091074798256
Training tokenizer:  62% 1251/2013 [1:54:49<1:25:03,  6.70s/it]loss_total_epoch 330.747527172789
Training tokenizer:  62% 1252/2013 [1:54:56<1:25:07,  6.71s/it]loss_total_epoch 330.96672805957496
Training tokenizer:  62% 1253/2013 [1:55:02<1:24:57,  6.71s/it]loss_total_epoch 331.1666387822479
Training tokenizer:  62% 1254/2013 [1:55:09<1:24:51,  6.71s/it]loss_total_epoch 331.3767412286252
Training tokenizer:  62% 1255/2013 [1:55:16<1:24:36,  6.70s/it]loss_total_epoch 331.5623846296221
Training tokenizer:  62% 1256/2013 [1:55:22<1:24:40,  6.71s/it]loss_total_epoch 331.79256491549313
Training tokenizer:  62% 1257/2013 [1:55:29<1:25:05,  6.75s/it]loss_total_epoch 332.0153217855841
Training tokenizer:  62% 1258/2013 [1:55:36<1:24:51,  6.74s/it]loss_total_epoch 332.2344333846122
Training tokenizer:  63% 1259/2013 [1:55:43<1:24:35,  6.73s/it]loss_total_epoch 332.43932060711086
Training tokenizer:  63% 1260/2013 [1:55:49<1:24:21,  6.72s/it]loss_total_epoch 332.6467540990561
Training tokenizer:  63% 1261/2013 [1:55:56<1:24:19,  6.73s/it]loss_total_epoch 332.86191750876606
Training tokenizer:  63% 1262/2013 [1:56:03<1:24:14,  6.73s/it]loss_total_epoch 333.0754291769117
Training tokenizer:  63% 1263/2013 [1:56:10<1:24:09,  6.73s/it]loss_total_epoch 333.2679474260658
Training tokenizer:  63% 1264/2013 [1:56:16<1:24:02,  6.73s/it]loss_total_epoch 333.47336425445974
Training tokenizer:  63% 1265/2013 [1:56:23<1:24:00,  6.74s/it]loss_total_epoch 333.68786566518247
Training tokenizer:  63% 1266/2013 [1:56:30<1:23:50,  6.73s/it]loss_total_epoch 333.8887366447598
Training tokenizer:  63% 1267/2013 [1:56:37<1:23:57,  6.75s/it]loss_total_epoch 334.11640977300704
Training tokenizer:  63% 1268/2013 [1:56:43<1:23:42,  6.74s/it]loss_total_epoch 334.32665125094354
Training tokenizer:  63% 1269/2013 [1:56:50<1:23:39,  6.75s/it]loss_total_epoch 334.55355580709875
Training tokenizer:  63% 1270/2013 [1:56:57<1:23:23,  6.73s/it]loss_total_epoch 334.76567311026156
Training tokenizer:  63% 1271/2013 [1:57:04<1:23:21,  6.74s/it]loss_total_epoch 334.9832359533757
Training tokenizer:  63% 1272/2013 [1:57:10<1:23:16,  6.74s/it]loss_total_epoch 335.22239620797336
Training tokenizer:  63% 1273/2013 [1:57:17<1:23:08,  6.74s/it]loss_total_epoch 335.43194820545614
Training tokenizer:  63% 1274/2013 [1:57:24<1:23:06,  6.75s/it]loss_total_epoch 335.6295119654387
Training tokenizer:  63% 1275/2013 [1:57:31<1:23:02,  6.75s/it]loss_total_epoch 335.8424422573298
Training tokenizer:  63% 1276/2013 [1:57:37<1:23:04,  6.76s/it]loss_total_epoch 336.05463202111423
Training tokenizer:  63% 1277/2013 [1:57:44<1:23:04,  6.77s/it]loss_total_epoch 336.28687403537333
Training tokenizer:  63% 1278/2013 [1:57:51<1:22:50,  6.76s/it]loss_total_epoch 336.48190805129707
Training tokenizer:  64% 1279/2013 [1:57:58<1:22:51,  6.77s/it]loss_total_epoch 336.69203199259937
Training tokenizer:  64% 1280/2013 [1:58:04<1:22:38,  6.77s/it]loss_total_epoch 336.8979338090867
Training tokenizer:  64% 1281/2013 [1:58:11<1:22:45,  6.78s/it]loss_total_epoch 337.13111295364797
Training tokenizer:  64% 1282/2013 [1:58:18<1:22:42,  6.79s/it]loss_total_epoch 337.3385952170938
Training tokenizer:  64% 1283/2013 [1:58:25<1:22:19,  6.77s/it]loss_total_epoch 337.58514295332134
Training tokenizer:  64% 1284/2013 [1:58:32<1:22:14,  6.77s/it]loss_total_epoch 337.796430086717
Training tokenizer:  64% 1285/2013 [1:58:38<1:22:17,  6.78s/it]loss_total_epoch 338.0175838638097
Training tokenizer:  64% 1286/2013 [1:58:45<1:22:10,  6.78s/it]loss_total_epoch 338.23084770701826
Training tokenizer:  64% 1287/2013 [1:58:52<1:22:07,  6.79s/it]loss_total_epoch 338.4321684669703
Training tokenizer:  64% 1288/2013 [1:58:59<1:22:02,  6.79s/it]loss_total_epoch 338.6451031509787
Training tokenizer:  64% 1289/2013 [1:59:06<1:21:57,  6.79s/it]loss_total_epoch 338.89035255275667
Training tokenizer:  64% 1290/2013 [1:59:12<1:21:46,  6.79s/it]loss_total_epoch 339.1076062191278
Training tokenizer:  64% 1291/2013 [1:59:19<1:21:50,  6.80s/it]loss_total_epoch 339.3304195981473
Training tokenizer:  64% 1292/2013 [1:59:26<1:21:38,  6.79s/it]loss_total_epoch 339.54794110916555
Training tokenizer:  64% 1293/2013 [1:59:33<1:21:39,  6.80s/it]loss_total_epoch 339.7860146854073
Training tokenizer:  64% 1294/2013 [1:59:40<1:21:31,  6.80s/it]loss_total_epoch 340.00394331105053
Training tokenizer:  64% 1295/2013 [1:59:46<1:21:24,  6.80s/it]loss_total_epoch 340.22246625833213
Training tokenizer:  64% 1296/2013 [1:59:53<1:21:20,  6.81s/it]loss_total_epoch 340.4441383276135
Training tokenizer:  64% 1297/2013 [2:00:00<1:21:07,  6.80s/it]loss_total_epoch 340.6287850420922
Training tokenizer:  64% 1298/2013 [2:00:07<1:20:58,  6.80s/it]loss_total_epoch 340.84041163511574
Training tokenizer:  65% 1299/2013 [2:00:14<1:20:56,  6.80s/it]loss_total_epoch 341.05255247838795
Training tokenizer:  65% 1300/2013 [2:00:20<1:20:43,  6.79s/it]loss_total_epoch 341.27142178453505
Training tokenizer:  65% 1301/2013 [2:00:27<1:20:35,  6.79s/it]loss_total_epoch 341.44386263377964
Training tokenizer:  65% 1302/2013 [2:00:34<1:20:14,  6.77s/it]loss_total_epoch 341.6421844419092
Training tokenizer:  65% 1303/2013 [2:00:41<1:20:15,  6.78s/it]loss_total_epoch 341.86748309247196
Training tokenizer:  65% 1304/2013 [2:00:47<1:20:18,  6.80s/it]loss_total_epoch 342.09994206391275
Training tokenizer:  65% 1305/2013 [2:00:54<1:20:03,  6.78s/it]loss_total_epoch 342.33267757855356
Training tokenizer:  65% 1306/2013 [2:01:01<1:20:04,  6.80s/it]loss_total_epoch 342.5424056444317
Training tokenizer:  65% 1307/2013 [2:01:08<1:19:58,  6.80s/it]loss_total_epoch 342.7292168866843
Training tokenizer:  65% 1308/2013 [2:01:15<1:19:51,  6.80s/it]loss_total_epoch 342.96845583058894
Training tokenizer:  65% 1309/2013 [2:01:22<1:19:59,  6.82s/it]loss_total_epoch 343.1686536166817
Training tokenizer:  65% 1310/2013 [2:01:28<1:19:45,  6.81s/it]loss_total_epoch 343.3878132272512
Training tokenizer:  65% 1311/2013 [2:01:35<1:19:51,  6.83s/it]loss_total_epoch 343.6110582854599
Training tokenizer:  65% 1312/2013 [2:01:42<1:19:50,  6.83s/it]loss_total_epoch 343.8321352582425
Training tokenizer:  65% 1313/2013 [2:01:49<1:19:37,  6.83s/it]loss_total_epoch 344.0428360309452
Training tokenizer:  65% 1314/2013 [2:01:56<1:19:40,  6.84s/it]loss_total_epoch 344.27460678853095
Training tokenizer:  65% 1315/2013 [2:02:03<1:19:29,  6.83s/it]loss_total_epoch 344.483801914379
Training tokenizer:  65% 1316/2013 [2:02:09<1:19:35,  6.85s/it]loss_total_epoch 344.68907570280135
Training tokenizer:  65% 1317/2013 [2:02:16<1:19:27,  6.85s/it]loss_total_epoch 344.89396670274436
Training tokenizer:  65% 1318/2013 [2:02:23<1:19:35,  6.87s/it]loss_total_epoch 345.10038376040757
Training tokenizer:  66% 1319/2013 [2:02:30<1:19:22,  6.86s/it]loss_total_epoch 345.3203169833869
Training tokenizer:  66% 1320/2013 [2:02:37<1:19:32,  6.89s/it]loss_total_epoch 345.53406786732376
Training tokenizer:  66% 1321/2013 [2:02:44<1:19:23,  6.88s/it]loss_total_epoch 345.7391877155751
Training tokenizer:  66% 1322/2013 [2:02:51<1:19:22,  6.89s/it]loss_total_epoch 345.9222103524953
Training tokenizer:  66% 1323/2013 [2:02:58<1:19:13,  6.89s/it]loss_total_epoch 346.1423164065927
Training tokenizer:  66% 1324/2013 [2:03:05<1:19:01,  6.88s/it]loss_total_epoch 346.350238641724
Training tokenizer:  66% 1325/2013 [2:03:11<1:18:55,  6.88s/it]loss_total_epoch 346.56491451896727
Training tokenizer:  66% 1326/2013 [2:03:18<1:18:51,  6.89s/it]loss_total_epoch 346.77149267680943
Training tokenizer:  66% 1327/2013 [2:03:25<1:18:43,  6.88s/it]loss_total_epoch 346.9614031817764
Training tokenizer:  66% 1328/2013 [2:03:32<1:18:34,  6.88s/it]loss_total_epoch 347.15741414017975
Training tokenizer:  66% 1329/2013 [2:03:39<1:18:26,  6.88s/it]loss_total_epoch 347.37946979887784
Training tokenizer:  66% 1330/2013 [2:03:46<1:18:08,  6.87s/it]loss_total_epoch 347.5835072044283
Training tokenizer:  66% 1331/2013 [2:03:53<1:18:05,  6.87s/it]loss_total_epoch 347.7800047453493
Training tokenizer:  66% 1332/2013 [2:04:00<1:17:59,  6.87s/it]loss_total_epoch 347.9739791546017
Training tokenizer:  66% 1333/2013 [2:04:06<1:18:00,  6.88s/it]loss_total_epoch 348.1585164424032
Training tokenizer:  66% 1334/2013 [2:04:13<1:17:56,  6.89s/it]loss_total_epoch 348.36563932709396
Training tokenizer:  66% 1335/2013 [2:04:20<1:17:41,  6.87s/it]loss_total_epoch 348.57461564056575
Training tokenizer:  66% 1336/2013 [2:04:27<1:17:31,  6.87s/it]loss_total_epoch 348.76396243833005
Training tokenizer:  66% 1337/2013 [2:04:34<1:17:21,  6.87s/it]loss_total_epoch 348.96894893981516
Training tokenizer:  66% 1338/2013 [2:04:41<1:17:19,  6.87s/it]loss_total_epoch 349.1838525328785
Training tokenizer:  67% 1339/2013 [2:04:48<1:17:13,  6.88s/it]loss_total_epoch 349.3703817073256
Training tokenizer:  67% 1340/2013 [2:04:55<1:17:03,  6.87s/it]loss_total_epoch 349.58400718308985
Training tokenizer:  67% 1341/2013 [2:05:02<1:17:23,  6.91s/it]loss_total_epoch 349.7926052901894
Training tokenizer:  67% 1342/2013 [2:05:08<1:17:30,  6.93s/it]loss_total_epoch 349.99034879915416
Training tokenizer:  67% 1343/2013 [2:05:15<1:17:32,  6.94s/it]loss_total_epoch 350.19247818924487
Training tokenizer:  67% 1344/2013 [2:05:22<1:17:11,  6.92s/it]loss_total_epoch 350.40086516551673
Training tokenizer:  67% 1345/2013 [2:05:29<1:17:07,  6.93s/it]loss_total_epoch 350.5939983073622
Training tokenizer:  67% 1346/2013 [2:05:36<1:16:55,  6.92s/it]loss_total_epoch 350.8001612108201
Training tokenizer:  67% 1347/2013 [2:05:43<1:16:26,  6.89s/it]loss_total_epoch 351.00297323800623
Training tokenizer:  67% 1348/2013 [2:05:50<1:16:25,  6.90s/it]loss_total_epoch 351.1949089746922
Training tokenizer:  67% 1349/2013 [2:05:57<1:16:15,  6.89s/it]loss_total_epoch 351.4103146586567
Training tokenizer:  67% 1350/2013 [2:06:04<1:16:06,  6.89s/it]loss_total_epoch 351.6313760820776
Training tokenizer:  67% 1351/2013 [2:06:11<1:15:53,  6.88s/it]loss_total_epoch 351.8524838555604
Training tokenizer:  67% 1352/2013 [2:06:17<1:15:41,  6.87s/it]loss_total_epoch 352.03250995464623
Training tokenizer:  67% 1353/2013 [2:06:24<1:15:34,  6.87s/it]loss_total_epoch 352.23564181663096
Training tokenizer:  67% 1354/2013 [2:06:31<1:15:47,  6.90s/it]loss_total_epoch 352.4530227724463
Training tokenizer:  67% 1355/2013 [2:06:38<1:15:49,  6.91s/it]loss_total_epoch 352.6529925558716
Training tokenizer:  67% 1356/2013 [2:06:45<1:15:45,  6.92s/it]loss_total_epoch 352.8535995837301
Training tokenizer:  67% 1357/2013 [2:06:52<1:15:35,  6.91s/it]loss_total_epoch 353.05281981267035
Training tokenizer:  67% 1358/2013 [2:06:59<1:15:21,  6.90s/it]loss_total_epoch 353.25412135384977
Training tokenizer:  68% 1359/2013 [2:07:06<1:15:16,  6.91s/it]loss_total_epoch 353.46807950921357
Training tokenizer:  68% 1360/2013 [2:07:13<1:15:14,  6.91s/it]loss_total_epoch 353.6647701319307
Training tokenizer:  68% 1361/2013 [2:07:20<1:15:10,  6.92s/it]loss_total_epoch 353.87738726846874
Training tokenizer:  68% 1362/2013 [2:07:27<1:15:13,  6.93s/it]loss_total_epoch 354.0820501949638
Training tokenizer:  68% 1363/2013 [2:07:33<1:14:53,  6.91s/it]loss_total_epoch 354.284924807027
Training tokenizer:  68% 1364/2013 [2:07:40<1:14:44,  6.91s/it]loss_total_epoch 354.4941505212337
Training tokenizer:  68% 1365/2013 [2:07:47<1:14:32,  6.90s/it]loss_total_epoch 354.7028847131878
Training tokenizer:  68% 1366/2013 [2:07:54<1:14:29,  6.91s/it]loss_total_epoch 354.9083523694426
Training tokenizer:  68% 1367/2013 [2:08:01<1:14:30,  6.92s/it]loss_total_epoch 355.12776203639805
Training tokenizer:  68% 1368/2013 [2:08:08<1:14:18,  6.91s/it]loss_total_epoch 355.3423719163984
Training tokenizer:  68% 1369/2013 [2:08:15<1:14:22,  6.93s/it]loss_total_epoch 355.5333665627986
Training tokenizer:  68% 1370/2013 [2:08:22<1:14:12,  6.93s/it]loss_total_epoch 355.7350579556078
Training tokenizer:  68% 1371/2013 [2:08:29<1:14:00,  6.92s/it]loss_total_epoch 355.9452709052712
Training tokenizer:  68% 1372/2013 [2:08:36<1:14:05,  6.93s/it]loss_total_epoch 356.1515257600695
Training tokenizer:  68% 1373/2013 [2:08:43<1:14:01,  6.94s/it]loss_total_epoch 356.3786772545427
Training tokenizer:  68% 1374/2013 [2:08:50<1:13:51,  6.94s/it]loss_total_epoch 356.58073397167027
Training tokenizer:  68% 1375/2013 [2:08:57<1:13:48,  6.94s/it]loss_total_epoch 356.77613829635084
Training tokenizer:  68% 1376/2013 [2:09:04<1:13:46,  6.95s/it]loss_total_epoch 356.98953440226614
Training tokenizer:  68% 1377/2013 [2:09:11<1:13:40,  6.95s/it]loss_total_epoch 357.18177916295826
Training tokenizer:  68% 1378/2013 [2:09:18<1:13:41,  6.96s/it]loss_total_epoch 357.38082447461784
Training tokenizer:  69% 1379/2013 [2:09:24<1:13:36,  6.97s/it]loss_total_epoch 357.5782404150814
Training tokenizer:  69% 1380/2013 [2:09:31<1:13:24,  6.96s/it]loss_total_epoch 357.7861656676978
Training tokenizer:  69% 1381/2013 [2:09:38<1:13:21,  6.96s/it]loss_total_epoch 357.9992855209857
Training tokenizer:  69% 1382/2013 [2:09:45<1:13:23,  6.98s/it]loss_total_epoch 358.18613906390965
Training tokenizer:  69% 1383/2013 [2:09:52<1:13:14,  6.98s/it]loss_total_epoch 358.4007452670485
Training tokenizer:  69% 1384/2013 [2:09:59<1:13:06,  6.97s/it]loss_total_epoch 358.6161957439035
Training tokenizer:  69% 1385/2013 [2:10:06<1:13:05,  6.98s/it]loss_total_epoch 358.8356295134872
Training tokenizer:  69% 1386/2013 [2:10:13<1:12:55,  6.98s/it]loss_total_epoch 359.054259667173
Training tokenizer:  69% 1387/2013 [2:10:20<1:12:45,  6.97s/it]loss_total_epoch 359.2666202094406
Training tokenizer:  69% 1388/2013 [2:10:27<1:12:30,  6.96s/it]loss_total_epoch 359.45479432307184
Training tokenizer:  69% 1389/2013 [2:10:34<1:12:35,  6.98s/it]loss_total_epoch 359.67786558531225
Training tokenizer:  69% 1390/2013 [2:10:41<1:12:32,  6.99s/it]loss_total_epoch 359.9081356693059
Training tokenizer:  69% 1391/2013 [2:10:48<1:12:23,  6.98s/it]loss_total_epoch 360.12150058709085
Training tokenizer:  69% 1392/2013 [2:10:55<1:12:22,  6.99s/it]loss_total_epoch 360.31970856525004
Training tokenizer:  69% 1393/2013 [2:11:02<1:12:02,  6.97s/it]loss_total_epoch 360.5270708631724
Training tokenizer:  69% 1394/2013 [2:11:09<1:11:48,  6.96s/it]loss_total_epoch 360.74562443234026
Training tokenizer:  69% 1395/2013 [2:11:16<1:11:50,  6.98s/it]loss_total_epoch 360.9605372939259
Training tokenizer:  69% 1396/2013 [2:11:23<1:11:32,  6.96s/it]loss_total_epoch 361.16555498726666
Training tokenizer:  69% 1397/2013 [2:11:30<1:11:31,  6.97s/it]loss_total_epoch 361.38084605149925
Training tokenizer:  69% 1398/2013 [2:11:37<1:11:33,  6.98s/it]loss_total_epoch 361.5744605194777
Training tokenizer:  69% 1399/2013 [2:11:44<1:11:30,  6.99s/it]loss_total_epoch 361.7798945251852
Training tokenizer:  70% 1400/2013 [2:11:51<1:11:22,  6.99s/it]loss_total_epoch 361.963205056265
Training tokenizer:  70% 1401/2013 [2:11:58<1:11:16,  6.99s/it]loss_total_epoch 362.15931684337556
Training tokenizer:  70% 1402/2013 [2:12:05<1:11:11,  6.99s/it]loss_total_epoch 362.3599558528513
Training tokenizer:  70% 1403/2013 [2:12:12<1:10:57,  6.98s/it]loss_total_epoch 362.56516858004034
Training tokenizer:  70% 1404/2013 [2:12:19<1:10:57,  6.99s/it]loss_total_epoch 362.7846427541226
Training tokenizer:  70% 1405/2013 [2:12:26<1:11:00,  7.01s/it]loss_total_epoch 362.9963249322027
Training tokenizer:  70% 1406/2013 [2:12:33<1:10:44,  6.99s/it]loss_total_epoch 363.19844528473914
Training tokenizer:  70% 1407/2013 [2:12:40<1:10:32,  6.98s/it]loss_total_epoch 363.4226760510355
Training tokenizer:  70% 1408/2013 [2:12:47<1:10:18,  6.97s/it]loss_total_epoch 363.61281961761415
Training tokenizer:  70% 1409/2013 [2:12:54<1:10:28,  7.00s/it]loss_total_epoch 363.8236371446401
Training tokenizer:  70% 1410/2013 [2:13:01<1:10:08,  6.98s/it]loss_total_epoch 364.02657127566636
Training tokenizer:  70% 1411/2013 [2:13:08<1:10:10,  6.99s/it]loss_total_epoch 364.25794358737767
Training tokenizer:  70% 1412/2013 [2:13:15<1:09:59,  6.99s/it]loss_total_epoch 364.4514023605734
Training tokenizer:  70% 1413/2013 [2:13:22<1:10:08,  7.01s/it]loss_total_epoch 364.653823049739
Training tokenizer:  70% 1414/2013 [2:13:29<1:10:08,  7.03s/it]loss_total_epoch 364.8621789794415
Training tokenizer:  70% 1415/2013 [2:13:36<1:10:12,  7.04s/it]loss_total_epoch 365.0668098125607
Training tokenizer:  70% 1416/2013 [2:13:43<1:09:58,  7.03s/it]loss_total_epoch 365.2894642110914
Training tokenizer:  70% 1417/2013 [2:13:50<1:09:57,  7.04s/it]loss_total_epoch 365.4797080475837
Training tokenizer:  70% 1418/2013 [2:13:57<1:09:45,  7.03s/it]loss_total_epoch 365.6681784186512
Training tokenizer:  70% 1419/2013 [2:14:04<1:09:18,  7.00s/it]loss_total_epoch 365.85442023538053
Training tokenizer:  71% 1420/2013 [2:14:11<1:09:26,  7.03s/it]loss_total_epoch 366.07574841938913
Training tokenizer:  71% 1421/2013 [2:14:18<1:09:20,  7.03s/it]loss_total_epoch 366.25072601623833
Training tokenizer:  71% 1422/2013 [2:14:25<1:08:57,  7.00s/it]loss_total_epoch 366.4620324168354
Training tokenizer:  71% 1423/2013 [2:14:32<1:08:57,  7.01s/it]loss_total_epoch 366.6840803902596
Training tokenizer:  71% 1424/2013 [2:14:39<1:08:57,  7.02s/it]loss_total_epoch 366.921674342826
Training tokenizer:  71% 1425/2013 [2:14:46<1:08:48,  7.02s/it]loss_total_epoch 367.14988986961544
Training tokenizer:  71% 1426/2013 [2:14:53<1:08:40,  7.02s/it]loss_total_epoch 367.36159745045006
Training tokenizer:  71% 1427/2013 [2:15:00<1:08:35,  7.02s/it]loss_total_epoch 367.55957725457847
Training tokenizer:  71% 1428/2013 [2:15:07<1:08:37,  7.04s/it]loss_total_epoch 367.78024348802865
Training tokenizer:  71% 1429/2013 [2:15:14<1:08:32,  7.04s/it]loss_total_epoch 367.9409670550376
Training tokenizer:  71% 1430/2013 [2:15:22<1:08:31,  7.05s/it]loss_total_epoch 368.13976302184165
Training tokenizer:  71% 1431/2013 [2:15:29<1:08:11,  7.03s/it]loss_total_epoch 368.35370228625834
Training tokenizer:  71% 1432/2013 [2:15:36<1:08:08,  7.04s/it]loss_total_epoch 368.564736103639
Training tokenizer:  71% 1433/2013 [2:15:43<1:07:58,  7.03s/it]loss_total_epoch 368.75867006741464
Training tokenizer:  71% 1434/2013 [2:15:50<1:07:56,  7.04s/it]loss_total_epoch 368.97155992127955
Training tokenizer:  71% 1435/2013 [2:15:57<1:07:34,  7.01s/it]loss_total_epoch 369.1910748910159
Training tokenizer:  71% 1436/2013 [2:16:04<1:07:30,  7.02s/it]loss_total_epoch 369.4042834099382
Training tokenizer:  71% 1437/2013 [2:16:11<1:07:36,  7.04s/it]loss_total_epoch 369.61750713922083
Training tokenizer:  71% 1438/2013 [2:16:18<1:07:24,  7.03s/it]loss_total_epoch 369.83199204318225
Training tokenizer:  71% 1439/2013 [2:16:25<1:07:24,  7.05s/it]loss_total_epoch 370.05424210987985
Training tokenizer:  72% 1440/2013 [2:16:32<1:07:27,  7.06s/it]loss_total_epoch 370.2582110334188
Training tokenizer:  72% 1441/2013 [2:16:39<1:07:10,  7.05s/it]loss_total_epoch 370.46921873278916
Training tokenizer:  72% 1442/2013 [2:16:46<1:07:17,  7.07s/it]loss_total_epoch 370.6922216694802
Training tokenizer:  72% 1443/2013 [2:16:53<1:07:12,  7.08s/it]loss_total_epoch 370.9146438520402
Training tokenizer:  72% 1444/2013 [2:17:00<1:06:57,  7.06s/it]loss_total_epoch 371.12091595493257
Training tokenizer:  72% 1445/2013 [2:17:07<1:06:56,  7.07s/it]loss_total_epoch 371.34655267186463
Training tokenizer:  72% 1446/2013 [2:17:14<1:06:50,  7.07s/it]loss_total_epoch 371.5555478204042
Training tokenizer:  72% 1447/2013 [2:17:21<1:06:54,  7.09s/it]loss_total_epoch 371.7583906482905
Training tokenizer:  72% 1448/2013 [2:17:29<1:06:57,  7.11s/it]loss_total_epoch 371.93894080258906
Training tokenizer:  72% 1449/2013 [2:17:36<1:06:51,  7.11s/it]loss_total_epoch 372.1047810148448
Training tokenizer:  72% 1450/2013 [2:17:43<1:06:37,  7.10s/it]loss_total_epoch 372.3183306772262
Training tokenizer:  72% 1451/2013 [2:17:50<1:06:36,  7.11s/it]loss_total_epoch 372.501043247059
Training tokenizer:  72% 1452/2013 [2:17:57<1:06:32,  7.12s/it]loss_total_epoch 372.7050252687186
Training tokenizer:  72% 1453/2013 [2:18:04<1:06:02,  7.08s/it]loss_total_epoch 372.9078322444111
Training tokenizer:  72% 1454/2013 [2:18:11<1:06:08,  7.10s/it]loss_total_epoch 373.13267278485
Training tokenizer:  72% 1455/2013 [2:18:18<1:05:46,  7.07s/it]loss_total_epoch 373.34635724313557
Training tokenizer:  72% 1456/2013 [2:18:25<1:05:51,  7.09s/it]loss_total_epoch 373.5564056392759
Training tokenizer:  72% 1457/2013 [2:18:32<1:05:44,  7.09s/it]loss_total_epoch 373.76472096703947
Training tokenizer:  72% 1458/2013 [2:18:40<1:05:37,  7.10s/it]loss_total_epoch 373.9548121895641
Training tokenizer:  72% 1459/2013 [2:18:47<1:05:43,  7.12s/it]loss_total_epoch 374.18021022342145
Training tokenizer:  73% 1460/2013 [2:18:54<1:05:26,  7.10s/it]loss_total_epoch 374.37188020534813
Training tokenizer:  73% 1461/2013 [2:19:01<1:05:22,  7.11s/it]loss_total_epoch 374.56883406452835
Training tokenizer:  73% 1462/2013 [2:19:08<1:05:22,  7.12s/it]loss_total_epoch 374.75991650111973
Training tokenizer:  73% 1463/2013 [2:19:15<1:05:01,  7.09s/it]loss_total_epoch 374.957710808143
Training tokenizer:  73% 1464/2013 [2:19:22<1:04:46,  7.08s/it]loss_total_epoch 375.16983904130757
Training tokenizer:  73% 1465/2013 [2:19:29<1:04:28,  7.06s/it]loss_total_epoch 375.3860963676125
Training tokenizer:  73% 1466/2013 [2:19:36<1:04:27,  7.07s/it]loss_total_epoch 375.59727959521115
Training tokenizer:  73% 1467/2013 [2:19:43<1:04:33,  7.10s/it]loss_total_epoch 375.80571832694113
Training tokenizer:  73% 1468/2013 [2:19:51<1:04:49,  7.14s/it]loss_total_epoch 376.0343667063862
Training tokenizer:  73% 1469/2013 [2:19:58<1:04:29,  7.11s/it]loss_total_epoch 376.2507006134838
Training tokenizer:  73% 1470/2013 [2:20:05<1:04:27,  7.12s/it]loss_total_epoch 376.47256581671536
Training tokenizer:  73% 1471/2013 [2:20:12<1:04:25,  7.13s/it]loss_total_epoch 376.68989705108106
Training tokenizer:  73% 1472/2013 [2:20:19<1:04:17,  7.13s/it]loss_total_epoch 376.90776538290083
Training tokenizer:  73% 1473/2013 [2:20:26<1:04:16,  7.14s/it]loss_total_epoch 377.12262971140444
Training tokenizer:  73% 1474/2013 [2:20:33<1:04:08,  7.14s/it]loss_total_epoch 377.34204895608127
Training tokenizer:  73% 1475/2013 [2:20:40<1:03:47,  7.11s/it]loss_total_epoch 377.5468810442835
Training tokenizer:  73% 1476/2013 [2:20:48<1:03:43,  7.12s/it]loss_total_epoch 377.7509873043746
Training tokenizer:  73% 1477/2013 [2:20:55<1:03:45,  7.14s/it]loss_total_epoch 377.9564560819417
Training tokenizer:  73% 1478/2013 [2:21:02<1:03:37,  7.14s/it]loss_total_epoch 378.1438107546419
Training tokenizer:  73% 1479/2013 [2:21:09<1:03:33,  7.14s/it]loss_total_epoch 378.33607847057283
Training tokenizer:  74% 1480/2013 [2:21:16<1:03:14,  7.12s/it]loss_total_epoch 378.5431480053812
Training tokenizer:  74% 1481/2013 [2:21:23<1:02:55,  7.10s/it]loss_total_epoch 378.7451315652579
Training tokenizer:  74% 1482/2013 [2:21:30<1:02:51,  7.10s/it]loss_total_epoch 378.95592601411045
Training tokenizer:  74% 1483/2013 [2:21:37<1:02:52,  7.12s/it]loss_total_epoch 379.16237977333367
Training tokenizer:  74% 1484/2013 [2:21:45<1:02:51,  7.13s/it]loss_total_epoch 379.3827591147274
Training tokenizer:  74% 1485/2013 [2:21:52<1:02:45,  7.13s/it]loss_total_epoch 379.592193858698
Training tokenizer:  74% 1486/2013 [2:21:59<1:02:29,  7.12s/it]loss_total_epoch 379.7989465240389
Training tokenizer:  74% 1487/2013 [2:22:06<1:02:18,  7.11s/it]loss_total_epoch 380.00646097771823
Training tokenizer:  74% 1488/2013 [2:22:13<1:02:11,  7.11s/it]loss_total_epoch 380.2087692152709
Training tokenizer:  74% 1489/2013 [2:22:20<1:01:56,  7.09s/it]loss_total_epoch 380.3910787831992
Training tokenizer:  74% 1490/2013 [2:22:27<1:02:02,  7.12s/it]loss_total_epoch 380.5878078173846
Training tokenizer:  74% 1491/2013 [2:22:34<1:01:45,  7.10s/it]loss_total_epoch 380.7803861666471
Training tokenizer:  74% 1492/2013 [2:22:41<1:01:48,  7.12s/it]loss_total_epoch 380.9740909617394
Training tokenizer:  74% 1493/2013 [2:22:49<1:01:49,  7.13s/it]loss_total_epoch 381.1813803743571
Training tokenizer:  74% 1494/2013 [2:22:56<1:01:49,  7.15s/it]loss_total_epoch 381.3966584596783
Training tokenizer:  74% 1495/2013 [2:23:03<1:01:42,  7.15s/it]loss_total_epoch 381.5952034238726
Training tokenizer:  74% 1496/2013 [2:23:10<1:01:40,  7.16s/it]loss_total_epoch 381.7904643844813
Training tokenizer:  74% 1497/2013 [2:23:17<1:01:33,  7.16s/it]loss_total_epoch 381.9834428485483
Training tokenizer:  74% 1498/2013 [2:23:24<1:01:05,  7.12s/it]loss_total_epoch 382.192092852667
Training tokenizer:  74% 1499/2013 [2:23:31<1:00:46,  7.09s/it]loss_total_epoch 382.4084834102541
Training tokenizer:  75% 1500/2013 [2:23:39<1:00:49,  7.11s/it]loss_total_epoch 382.6037151757628
Training tokenizer:  75% 1501/2013 [2:23:46<1:00:51,  7.13s/it]loss_total_epoch 382.7979224231094
Training tokenizer:  75% 1502/2013 [2:23:53<1:00:50,  7.14s/it]loss_total_epoch 382.9927367102355
Training tokenizer:  75% 1503/2013 [2:24:00<1:00:25,  7.11s/it]loss_total_epoch 383.1721603255719
Training tokenizer:  75% 1504/2013 [2:24:07<1:00:13,  7.10s/it]loss_total_epoch 383.3871866520494
Training tokenizer:  75% 1505/2013 [2:24:14<1:00:12,  7.11s/it]loss_total_epoch 383.6020200122148
Training tokenizer:  75% 1506/2013 [2:24:21<1:00:17,  7.14s/it]loss_total_epoch 383.8229213748127
Training tokenizer:  75% 1507/2013 [2:24:29<1:00:22,  7.16s/it]loss_total_epoch 384.04213591851294
Training tokenizer:  75% 1508/2013 [2:24:36<1:00:14,  7.16s/it]loss_total_epoch 384.24670002423227
Training tokenizer:  75% 1509/2013 [2:24:43<1:00:04,  7.15s/it]loss_total_epoch 384.4511015396565
Training tokenizer:  75% 1510/2013 [2:24:50<1:00:03,  7.16s/it]loss_total_epoch 384.6647658441216
Training tokenizer:  75% 1511/2013 [2:24:57<59:47,  7.15s/it]  loss_total_epoch 384.8469456862658
Training tokenizer:  75% 1512/2013 [2:25:04<59:56,  7.18s/it]loss_total_epoch 385.0410305913538
Training tokenizer:  75% 1513/2013 [2:25:12<59:50,  7.18s/it]loss_total_epoch 385.27208459191024
Training tokenizer:  75% 1514/2013 [2:25:19<59:44,  7.18s/it]loss_total_epoch 385.448067182675
Training tokenizer:  75% 1515/2013 [2:25:26<59:31,  7.17s/it]loss_total_epoch 385.6619191970676
Training tokenizer:  75% 1516/2013 [2:25:33<59:29,  7.18s/it]loss_total_epoch 385.8725578729063
Training tokenizer:  75% 1517/2013 [2:25:40<59:26,  7.19s/it]loss_total_epoch 386.0650310199708
Training tokenizer:  75% 1518/2013 [2:25:48<59:22,  7.20s/it]loss_total_epoch 386.2706888448447
Training tokenizer:  75% 1519/2013 [2:25:55<59:17,  7.20s/it]loss_total_epoch 386.4233985450119
Training tokenizer:  76% 1520/2013 [2:26:02<58:55,  7.17s/it]loss_total_epoch 386.6180477757007
Training tokenizer:  76% 1521/2013 [2:26:09<58:50,  7.18s/it]loss_total_epoch 386.81547184474766
Training tokenizer:  76% 1522/2013 [2:26:16<58:40,  7.17s/it]loss_total_epoch 386.99274962209165
Training tokenizer:  76% 1523/2013 [2:26:23<58:47,  7.20s/it]loss_total_epoch 387.186336947605
Training tokenizer:  76% 1524/2013 [2:26:31<58:42,  7.20s/it]loss_total_epoch 387.36226820386946
Training tokenizer:  76% 1525/2013 [2:26:38<58:33,  7.20s/it]loss_total_epoch 387.5642448756844
Training tokenizer:  76% 1526/2013 [2:26:45<58:30,  7.21s/it]loss_total_epoch 387.7519091870636
Training tokenizer:  76% 1527/2013 [2:26:52<58:20,  7.20s/it]loss_total_epoch 387.93875353224576
Training tokenizer:  76% 1528/2013 [2:26:59<58:15,  7.21s/it]loss_total_epoch 388.1391044575721
Training tokenizer:  76% 1529/2013 [2:27:07<58:17,  7.23s/it]loss_total_epoch 388.31434229947627
Training tokenizer:  76% 1530/2013 [2:27:14<58:04,  7.21s/it]loss_total_epoch 388.5108448397368
Training tokenizer:  76% 1531/2013 [2:27:21<58:01,  7.22s/it]loss_total_epoch 388.7062525805086
Training tokenizer:  76% 1532/2013 [2:27:28<57:42,  7.20s/it]loss_total_epoch 388.91057464294136
Training tokenizer:  76% 1533/2013 [2:27:36<57:45,  7.22s/it]loss_total_epoch 389.11039668135345
Training tokenizer:  76% 1534/2013 [2:27:43<57:35,  7.21s/it]loss_total_epoch 389.32157420553267
Training tokenizer:  76% 1535/2013 [2:27:50<57:25,  7.21s/it]loss_total_epoch 389.5238556358963
Training tokenizer:  76% 1536/2013 [2:27:57<57:24,  7.22s/it]loss_total_epoch 389.71728968434036
Training tokenizer:  76% 1537/2013 [2:28:05<57:21,  7.23s/it]loss_total_epoch 389.92497226409614
Training tokenizer:  76% 1538/2013 [2:28:12<57:05,  7.21s/it]loss_total_epoch 390.1527855899185
Training tokenizer:  76% 1539/2013 [2:28:19<57:11,  7.24s/it]loss_total_epoch 390.3754902984947
Training tokenizer:  77% 1540/2013 [2:28:26<57:01,  7.23s/it]loss_total_epoch 390.55591469816864
Training tokenizer:  77% 1541/2013 [2:28:34<57:08,  7.26s/it]loss_total_epoch 390.7393631171435
Training tokenizer:  77% 1542/2013 [2:28:41<57:05,  7.27s/it]loss_total_epoch 390.9304197411984
Training tokenizer:  77% 1543/2013 [2:28:48<57:01,  7.28s/it]loss_total_epoch 391.1329206582159
Training tokenizer:  77% 1544/2013 [2:28:55<57:05,  7.30s/it]loss_total_epoch 391.337444068864
Training tokenizer:  77% 1545/2013 [2:29:03<56:51,  7.29s/it]loss_total_epoch 391.54017050750554
Training tokenizer:  77% 1546/2013 [2:29:10<56:42,  7.29s/it]loss_total_epoch 391.73759840615094
Training tokenizer:  77% 1547/2013 [2:29:17<56:34,  7.28s/it]loss_total_epoch 391.9366283547133
Training tokenizer:  77% 1548/2013 [2:29:25<56:26,  7.28s/it]loss_total_epoch 392.12796251289546
Training tokenizer:  77% 1549/2013 [2:29:32<56:19,  7.28s/it]loss_total_epoch 392.326015079394
Training tokenizer:  77% 1550/2013 [2:29:39<56:04,  7.27s/it]loss_total_epoch 392.52855239622295
Training tokenizer:  77% 1551/2013 [2:29:46<55:56,  7.26s/it]loss_total_epoch 392.7532616276294
Training tokenizer:  77% 1552/2013 [2:29:54<55:47,  7.26s/it]loss_total_epoch 392.97964709065855
Training tokenizer:  77% 1553/2013 [2:30:01<55:42,  7.27s/it]loss_total_epoch 393.221276788041
Training tokenizer:  77% 1554/2013 [2:30:08<55:26,  7.25s/it]loss_total_epoch 393.41856922023
Training tokenizer:  77% 1555/2013 [2:30:15<55:27,  7.26s/it]loss_total_epoch 393.61990495584905
Training tokenizer:  77% 1556/2013 [2:30:23<55:16,  7.26s/it]loss_total_epoch 393.8298872541636
Training tokenizer:  77% 1557/2013 [2:30:30<55:15,  7.27s/it]loss_total_epoch 394.03349761478603
Training tokenizer:  77% 1558/2013 [2:30:37<55:09,  7.27s/it]loss_total_epoch 394.25009391643107
Training tokenizer:  77% 1559/2013 [2:30:45<55:12,  7.30s/it]loss_total_epoch 394.44141043163836
Training tokenizer:  77% 1560/2013 [2:30:52<54:53,  7.27s/it]loss_total_epoch 394.6311999056488
Training tokenizer:  78% 1561/2013 [2:30:59<54:46,  7.27s/it]loss_total_epoch 394.8225282449275
Training tokenizer:  78% 1562/2013 [2:31:06<54:39,  7.27s/it]loss_total_epoch 395.02795566804707
Training tokenizer:  78% 1563/2013 [2:31:14<54:36,  7.28s/it]loss_total_epoch 395.2395593803376
Training tokenizer:  78% 1564/2013 [2:31:21<54:35,  7.29s/it]loss_total_epoch 395.4390636254102
Training tokenizer:  78% 1565/2013 [2:31:28<54:12,  7.26s/it]loss_total_epoch 395.61446647904813
Training tokenizer:  78% 1566/2013 [2:31:35<54:09,  7.27s/it]loss_total_epoch 395.80257555283606
Training tokenizer:  78% 1567/2013 [2:31:43<53:52,  7.25s/it]loss_total_epoch 396.0019501838833
Training tokenizer:  78% 1568/2013 [2:31:50<53:36,  7.23s/it]loss_total_epoch 396.1870888005942
Training tokenizer:  78% 1569/2013 [2:31:57<53:37,  7.25s/it]loss_total_epoch 396.3696578349918
Training tokenizer:  78% 1570/2013 [2:32:04<53:33,  7.25s/it]loss_total_epoch 396.5523277502507
Training tokenizer:  78% 1571/2013 [2:32:12<53:24,  7.25s/it]loss_total_epoch 396.7630212176591
Training tokenizer:  78% 1572/2013 [2:32:19<53:29,  7.28s/it]loss_total_epoch 396.9423666689545
Training tokenizer:  78% 1573/2013 [2:32:26<53:27,  7.29s/it]loss_total_epoch 397.1454357188195
Training tokenizer:  78% 1574/2013 [2:32:34<53:25,  7.30s/it]loss_total_epoch 397.3537912387401
Training tokenizer:  78% 1575/2013 [2:32:41<53:16,  7.30s/it]loss_total_epoch 397.55259779281914
Training tokenizer:  78% 1576/2013 [2:32:48<53:14,  7.31s/it]loss_total_epoch 397.75015881471336
Training tokenizer:  78% 1577/2013 [2:32:55<52:57,  7.29s/it]loss_total_epoch 397.95978003181517
Training tokenizer:  78% 1578/2013 [2:33:03<53:01,  7.31s/it]loss_total_epoch 398.1722293738276
Training tokenizer:  78% 1579/2013 [2:33:10<52:46,  7.30s/it]loss_total_epoch 398.3871385436505
Training tokenizer:  78% 1580/2013 [2:33:17<52:37,  7.29s/it]loss_total_epoch 398.5719004776329
Training tokenizer:  79% 1581/2013 [2:33:25<52:30,  7.29s/it]loss_total_epoch 398.7765782382339
Training tokenizer:  79% 1582/2013 [2:33:32<52:36,  7.32s/it]loss_total_epoch 398.98566873557866
Training tokenizer:  79% 1583/2013 [2:33:39<52:22,  7.31s/it]loss_total_epoch 399.18661793507636
Training tokenizer:  79% 1584/2013 [2:33:47<52:14,  7.31s/it]loss_total_epoch 399.40432589687407
Training tokenizer:  79% 1585/2013 [2:33:54<52:04,  7.30s/it]loss_total_epoch 399.5829920079559
Training tokenizer:  79% 1586/2013 [2:34:01<51:59,  7.31s/it]loss_total_epoch 399.79361216537654
Training tokenizer:  79% 1587/2013 [2:34:09<51:51,  7.30s/it]loss_total_epoch 399.97324567846954
Training tokenizer:  79% 1588/2013 [2:34:16<51:50,  7.32s/it]loss_total_epoch 400.16684082336724
Training tokenizer:  79% 1589/2013 [2:34:23<51:49,  7.33s/it]loss_total_epoch 400.35530144535005
Training tokenizer:  79% 1590/2013 [2:34:31<51:39,  7.33s/it]loss_total_epoch 400.5550385732204
Training tokenizer:  79% 1591/2013 [2:34:38<51:31,  7.33s/it]loss_total_epoch 400.7273340281099
Training tokenizer:  79% 1592/2013 [2:34:45<51:21,  7.32s/it]loss_total_epoch 400.9090012293309
Training tokenizer:  79% 1593/2013 [2:34:53<51:18,  7.33s/it]loss_total_epoch 401.110217185691
Training tokenizer:  79% 1594/2013 [2:35:00<50:59,  7.30s/it]loss_total_epoch 401.2867512758821
Training tokenizer:  79% 1595/2013 [2:35:07<51:01,  7.32s/it]loss_total_epoch 401.4882373381406
Training tokenizer:  79% 1596/2013 [2:35:14<50:50,  7.32s/it]loss_total_epoch 401.677846794948
Training tokenizer:  79% 1597/2013 [2:35:22<50:51,  7.33s/it]loss_total_epoch 401.8856381047517
Training tokenizer:  79% 1598/2013 [2:35:29<50:46,  7.34s/it]loss_total_epoch 402.10124946944416
Training tokenizer:  79% 1599/2013 [2:35:37<50:41,  7.35s/it]loss_total_epoch 402.29353847913444
Training tokenizer:  79% 1600/2013 [2:35:44<50:34,  7.35s/it]loss_total_epoch 402.49007293768227
Training tokenizer:  80% 1601/2013 [2:35:51<50:19,  7.33s/it]loss_total_epoch 402.66237824223936
Training tokenizer:  80% 1602/2013 [2:35:59<50:12,  7.33s/it]loss_total_epoch 402.85385868139565
Training tokenizer:  80% 1603/2013 [2:36:06<50:10,  7.34s/it]loss_total_epoch 403.05122797749937
Training tokenizer:  80% 1604/2013 [2:36:13<50:04,  7.34s/it]loss_total_epoch 403.2453721035272
Training tokenizer:  80% 1605/2013 [2:36:21<49:50,  7.33s/it]loss_total_epoch 403.4511222895235
Training tokenizer:  80% 1606/2013 [2:36:28<49:49,  7.34s/it]loss_total_epoch 403.661344723776
Training tokenizer:  80% 1607/2013 [2:36:35<49:38,  7.34s/it]loss_total_epoch 403.8588881921023
Training tokenizer:  80% 1608/2013 [2:36:43<49:32,  7.34s/it]loss_total_epoch 404.06328038685024
Training tokenizer:  80% 1609/2013 [2:36:50<49:29,  7.35s/it]loss_total_epoch 404.24191417731345
Training tokenizer:  80% 1610/2013 [2:36:57<49:23,  7.35s/it]loss_total_epoch 404.43199435807765
Training tokenizer:  80% 1611/2013 [2:37:05<49:12,  7.35s/it]loss_total_epoch 404.6054328735918
Training tokenizer:  80% 1612/2013 [2:37:12<49:15,  7.37s/it]loss_total_epoch 404.78604864142835
Training tokenizer:  80% 1613/2013 [2:37:19<49:07,  7.37s/it]loss_total_epoch 404.96517389826477
Training tokenizer:  80% 1614/2013 [2:37:27<48:43,  7.33s/it]loss_total_epoch 405.1479922775179
Training tokenizer:  80% 1615/2013 [2:37:34<48:43,  7.35s/it]loss_total_epoch 405.325941240415
Training tokenizer:  80% 1616/2013 [2:37:41<48:36,  7.35s/it]loss_total_epoch 405.50142709724605
Training tokenizer:  80% 1617/2013 [2:37:49<48:27,  7.34s/it]loss_total_epoch 405.7015133667737
Training tokenizer:  80% 1618/2013 [2:37:56<48:22,  7.35s/it]loss_total_epoch 405.88826522789896
Training tokenizer:  80% 1619/2013 [2:38:03<48:20,  7.36s/it]loss_total_epoch 406.09163294918835
Training tokenizer:  80% 1620/2013 [2:38:11<48:23,  7.39s/it]loss_total_epoch 406.2977166865021
Training tokenizer:  81% 1621/2013 [2:38:18<48:24,  7.41s/it]loss_total_epoch 406.50785627402365
Training tokenizer:  81% 1622/2013 [2:38:26<48:13,  7.40s/it]loss_total_epoch 406.7301821317524
Training tokenizer:  81% 1623/2013 [2:38:33<47:49,  7.36s/it]loss_total_epoch 406.943304868415
Training tokenizer:  81% 1624/2013 [2:38:40<47:41,  7.36s/it]loss_total_epoch 407.1475317981094
Training tokenizer:  81% 1625/2013 [2:38:48<47:36,  7.36s/it]loss_total_epoch 407.3532386366278
Training tokenizer:  81% 1626/2013 [2:38:55<47:25,  7.35s/it]loss_total_epoch 407.55480787344277
Training tokenizer:  81% 1627/2013 [2:39:02<47:16,  7.35s/it]loss_total_epoch 407.78094807453454
Training tokenizer:  81% 1628/2013 [2:39:10<47:20,  7.38s/it]loss_total_epoch 407.9953820053488
Training tokenizer:  81% 1629/2013 [2:39:17<47:13,  7.38s/it]loss_total_epoch 408.20133890770376
Training tokenizer:  81% 1630/2013 [2:39:25<46:54,  7.35s/it]loss_total_epoch 408.40112358517945
Training tokenizer:  81% 1631/2013 [2:39:32<46:38,  7.32s/it]loss_total_epoch 408.61174713261425
Training tokenizer:  81% 1632/2013 [2:39:39<46:41,  7.35s/it]loss_total_epoch 408.80389756150544
Training tokenizer:  81% 1633/2013 [2:39:47<46:36,  7.36s/it]loss_total_epoch 409.02971980534494
Training tokenizer:  81% 1634/2013 [2:39:54<46:35,  7.38s/it]loss_total_epoch 409.2352435346693
Training tokenizer:  81% 1635/2013 [2:40:01<46:31,  7.38s/it]loss_total_epoch 409.434567598626
Training tokenizer:  81% 1636/2013 [2:40:09<46:29,  7.40s/it]loss_total_epoch 409.6066238824278
Training tokenizer:  81% 1637/2013 [2:40:16<46:20,  7.40s/it]loss_total_epoch 409.80060585029423
Training tokenizer:  81% 1638/2013 [2:40:24<46:17,  7.41s/it]loss_total_epoch 410.004358259961
Training tokenizer:  81% 1639/2013 [2:40:31<45:57,  7.37s/it]loss_total_epoch 410.19778977893293
Training tokenizer:  81% 1640/2013 [2:40:38<46:05,  7.41s/it]loss_total_epoch 410.4008806105703
Training tokenizer:  82% 1641/2013 [2:40:46<45:54,  7.41s/it]loss_total_epoch 410.5878750514239
Training tokenizer:  82% 1642/2013 [2:40:53<45:47,  7.41s/it]loss_total_epoch 410.78737633116543
Training tokenizer:  82% 1643/2013 [2:41:01<45:43,  7.41s/it]loss_total_epoch 410.9881080891937
Training tokenizer:  82% 1644/2013 [2:41:08<45:32,  7.41s/it]loss_total_epoch 411.17657211236656
Training tokenizer:  82% 1645/2013 [2:41:16<45:27,  7.41s/it]loss_total_epoch 411.3691142555326
Training tokenizer:  82% 1646/2013 [2:41:23<45:30,  7.44s/it]loss_total_epoch 411.558893578127
Training tokenizer:  82% 1647/2013 [2:41:30<45:24,  7.44s/it]loss_total_epoch 411.7507079374045
Training tokenizer:  82% 1648/2013 [2:41:38<45:22,  7.46s/it]loss_total_epoch 411.92283359356225
Training tokenizer:  82% 1649/2013 [2:41:45<44:56,  7.41s/it]loss_total_epoch 412.09896531142294
Training tokenizer:  82% 1650/2013 [2:41:53<44:48,  7.41s/it]loss_total_epoch 412.30074618570507
Training tokenizer:  82% 1651/2013 [2:42:00<44:44,  7.42s/it]loss_total_epoch 412.48353748209774
Training tokenizer:  82% 1652/2013 [2:42:08<44:44,  7.44s/it]loss_total_epoch 412.69109667278826
Training tokenizer:  82% 1653/2013 [2:42:15<44:38,  7.44s/it]loss_total_epoch 412.86723231337965
Training tokenizer:  82% 1654/2013 [2:42:23<44:35,  7.45s/it]loss_total_epoch 413.0814458820969
Training tokenizer:  82% 1655/2013 [2:42:30<44:29,  7.46s/it]loss_total_epoch 413.27447626926005
Training tokenizer:  82% 1656/2013 [2:42:37<44:15,  7.44s/it]loss_total_epoch 413.4936917889863
Training tokenizer:  82% 1657/2013 [2:42:45<44:14,  7.46s/it]loss_total_epoch 413.6706780064851
Training tokenizer:  82% 1658/2013 [2:42:52<44:12,  7.47s/it]loss_total_epoch 413.88938410021365
Training tokenizer:  82% 1659/2013 [2:43:00<43:55,  7.45s/it]loss_total_epoch 414.0820269305259
Training tokenizer:  82% 1660/2013 [2:43:07<43:52,  7.46s/it]loss_total_epoch 414.30098821781576
Training tokenizer:  83% 1661/2013 [2:43:15<43:46,  7.46s/it]loss_total_epoch 414.51983586139977
Training tokenizer:  83% 1662/2013 [2:43:22<43:40,  7.47s/it]loss_total_epoch 414.7394007463008
Training tokenizer:  83% 1663/2013 [2:43:30<43:29,  7.45s/it]loss_total_epoch 414.9376154523343
Training tokenizer:  83% 1664/2013 [2:43:37<43:18,  7.44s/it]loss_total_epoch 415.14889202825725
Training tokenizer:  83% 1665/2013 [2:43:45<43:11,  7.45s/it]loss_total_epoch 415.34713848493993
Training tokenizer:  83% 1666/2013 [2:43:52<43:09,  7.46s/it]loss_total_epoch 415.56731534190476
Training tokenizer:  83% 1667/2013 [2:43:59<43:01,  7.46s/it]loss_total_epoch 415.793296514079
Training tokenizer:  83% 1668/2013 [2:44:07<42:55,  7.47s/it]loss_total_epoch 415.99527308158576
Training tokenizer:  83% 1669/2013 [2:44:14<42:46,  7.46s/it]loss_total_epoch 416.1959280986339
Training tokenizer:  83% 1670/2013 [2:44:22<42:45,  7.48s/it]loss_total_epoch 416.37782385759056
Training tokenizer:  83% 1671/2013 [2:44:29<42:31,  7.46s/it]loss_total_epoch 416.5794934909791
Training tokenizer:  83% 1672/2013 [2:44:37<42:28,  7.47s/it]loss_total_epoch 416.7819517683238
Training tokenizer:  83% 1673/2013 [2:44:44<42:13,  7.45s/it]loss_total_epoch 416.96242203749716
Training tokenizer:  83% 1674/2013 [2:44:52<42:05,  7.45s/it]loss_total_epoch 417.14510777406394
Training tokenizer:  83% 1675/2013 [2:44:59<42:02,  7.46s/it]loss_total_epoch 417.3371523935348
Training tokenizer:  83% 1676/2013 [2:45:07<41:42,  7.43s/it]loss_total_epoch 417.5282878745347
Training tokenizer:  83% 1677/2013 [2:45:14<41:34,  7.42s/it]loss_total_epoch 417.710494922474
Training tokenizer:  83% 1678/2013 [2:45:21<41:40,  7.46s/it]loss_total_epoch 417.9171937163919
Training tokenizer:  83% 1679/2013 [2:45:29<41:41,  7.49s/it]loss_total_epoch 418.11947131343186
Training tokenizer:  83% 1680/2013 [2:45:37<41:38,  7.50s/it]loss_total_epoch 418.31449205614626
Training tokenizer:  84% 1681/2013 [2:45:44<41:29,  7.50s/it]loss_total_epoch 418.50930690579116
Training tokenizer:  84% 1682/2013 [2:45:52<41:20,  7.49s/it]loss_total_epoch 418.70449112541974
Training tokenizer:  84% 1683/2013 [2:45:59<41:14,  7.50s/it]loss_total_epoch 418.91438124887645
Training tokenizer:  84% 1684/2013 [2:46:07<41:06,  7.50s/it]loss_total_epoch 419.11556603200734
Training tokenizer:  84% 1685/2013 [2:46:14<40:58,  7.50s/it]loss_total_epoch 419.3196254950017
Training tokenizer:  84% 1686/2013 [2:46:22<40:55,  7.51s/it]loss_total_epoch 419.50813198648393
Training tokenizer:  84% 1687/2013 [2:46:29<40:49,  7.51s/it]loss_total_epoch 419.67917767725885
Training tokenizer:  84% 1688/2013 [2:46:37<40:37,  7.50s/it]loss_total_epoch 419.8867512252182
Training tokenizer:  84% 1689/2013 [2:46:44<40:32,  7.51s/it]loss_total_epoch 420.0631624441594
Training tokenizer:  84% 1690/2013 [2:46:52<40:24,  7.51s/it]loss_total_epoch 420.2626440208405
Training tokenizer:  84% 1691/2013 [2:46:59<40:07,  7.48s/it]loss_total_epoch 420.45388402603567
Training tokenizer:  84% 1692/2013 [2:47:06<40:01,  7.48s/it]loss_total_epoch 420.6532357838005
Training tokenizer:  84% 1693/2013 [2:47:14<40:03,  7.51s/it]loss_total_epoch 420.83813611231744
Training tokenizer:  84% 1694/2013 [2:47:21<39:44,  7.48s/it]loss_total_epoch 421.0531051065773
Training tokenizer:  84% 1695/2013 [2:47:29<39:43,  7.50s/it]loss_total_epoch 421.2420492116362
Training tokenizer:  84% 1696/2013 [2:47:37<39:41,  7.51s/it]loss_total_epoch 421.44745885394514
Training tokenizer:  84% 1697/2013 [2:47:44<39:27,  7.49s/it]loss_total_epoch 421.63416390679777
Training tokenizer:  84% 1698/2013 [2:47:52<39:22,  7.50s/it]loss_total_epoch 421.84659329615533
Training tokenizer:  84% 1699/2013 [2:47:59<39:18,  7.51s/it]loss_total_epoch 422.06188458763063
Training tokenizer:  84% 1700/2013 [2:48:07<39:19,  7.54s/it]loss_total_epoch 422.26052788458765
Training tokenizer:  85% 1701/2013 [2:48:14<39:08,  7.53s/it]loss_total_epoch 422.45949604548514
Training tokenizer:  85% 1702/2013 [2:48:22<39:05,  7.54s/it]loss_total_epoch 422.6734279152006
Training tokenizer:  85% 1703/2013 [2:48:29<38:50,  7.52s/it]loss_total_epoch 422.88382358290255
Training tokenizer:  85% 1704/2013 [2:48:37<38:35,  7.49s/it]loss_total_epoch 423.0909714009613
Training tokenizer:  85% 1705/2013 [2:48:44<38:28,  7.49s/it]loss_total_epoch 423.26827586628497
Training tokenizer:  85% 1706/2013 [2:48:52<38:26,  7.51s/it]loss_total_epoch 423.44731170497835
Training tokenizer:  85% 1707/2013 [2:48:59<38:15,  7.50s/it]loss_total_epoch 423.6304745916277
Training tokenizer:  85% 1708/2013 [2:49:07<38:02,  7.48s/it]loss_total_epoch 423.837570829317
Training tokenizer:  85% 1709/2013 [2:49:14<38:00,  7.50s/it]loss_total_epoch 424.02301473356783
Training tokenizer:  85% 1710/2013 [2:49:22<37:59,  7.52s/it]loss_total_epoch 424.24533840827644
Training tokenizer:  85% 1711/2013 [2:49:29<37:55,  7.53s/it]loss_total_epoch 424.445874305442
Training tokenizer:  85% 1712/2013 [2:49:37<37:49,  7.54s/it]loss_total_epoch 424.6346022877842
Training tokenizer:  85% 1713/2013 [2:49:44<37:42,  7.54s/it]loss_total_epoch 424.832423010841
Training tokenizer:  85% 1714/2013 [2:49:52<37:39,  7.56s/it]loss_total_epoch 425.02343785203993
Training tokenizer:  85% 1715/2013 [2:50:00<37:30,  7.55s/it]loss_total_epoch 425.2295221295208
Training tokenizer:  85% 1716/2013 [2:50:07<37:27,  7.57s/it]loss_total_epoch 425.42567430250347
Training tokenizer:  85% 1717/2013 [2:50:15<37:23,  7.58s/it]loss_total_epoch 425.61562366224825
Training tokenizer:  85% 1718/2013 [2:50:22<37:16,  7.58s/it]loss_total_epoch 425.79812688566744
Training tokenizer:  85% 1719/2013 [2:50:30<37:11,  7.59s/it]loss_total_epoch 425.99755428917706
Training tokenizer:  85% 1720/2013 [2:50:37<36:59,  7.58s/it]loss_total_epoch 426.18503344245255
Training tokenizer:  85% 1721/2013 [2:50:45<36:55,  7.59s/it]loss_total_epoch 426.37448108382523
Training tokenizer:  86% 1722/2013 [2:50:53<36:46,  7.58s/it]loss_total_epoch 426.5816830191761
Training tokenizer:  86% 1723/2013 [2:51:00<36:39,  7.59s/it]loss_total_epoch 426.78677545301616
Training tokenizer:  86% 1724/2013 [2:51:08<36:30,  7.58s/it]loss_total_epoch 426.9880363289267
Training tokenizer:  86% 1725/2013 [2:51:15<36:22,  7.58s/it]loss_total_epoch 427.19061281718314
Training tokenizer:  86% 1726/2013 [2:51:23<36:18,  7.59s/it]loss_total_epoch 427.37553189881146
Training tokenizer:  86% 1727/2013 [2:51:31<36:11,  7.59s/it]loss_total_epoch 427.5625473279506
Training tokenizer:  86% 1728/2013 [2:51:38<36:01,  7.59s/it]loss_total_epoch 427.75897398404777
Training tokenizer:  86% 1729/2013 [2:51:46<35:48,  7.57s/it]loss_total_epoch 427.9459638055414
Training tokenizer:  86% 1730/2013 [2:51:53<35:48,  7.59s/it]loss_total_epoch 428.13368795625865
Training tokenizer:  86% 1731/2013 [2:52:01<35:42,  7.60s/it]loss_total_epoch 428.3147196713835
Training tokenizer:  86% 1732/2013 [2:52:09<35:39,  7.61s/it]loss_total_epoch 428.49961978010833
Training tokenizer:  86% 1733/2013 [2:52:16<35:34,  7.62s/it]loss_total_epoch 428.69691703282297
Training tokenizer:  86% 1734/2013 [2:52:24<35:25,  7.62s/it]loss_total_epoch 428.8952531758696
Training tokenizer:  86% 1735/2013 [2:52:31<35:12,  7.60s/it]loss_total_epoch 429.08121316321194
Training tokenizer:  86% 1736/2013 [2:52:39<35:08,  7.61s/it]loss_total_epoch 429.27457038499415
Training tokenizer:  86% 1737/2013 [2:52:47<34:58,  7.61s/it]loss_total_epoch 429.46641230769455
Training tokenizer:  86% 1738/2013 [2:52:54<34:53,  7.61s/it]loss_total_epoch 429.64850769378245
Training tokenizer:  86% 1739/2013 [2:53:02<34:49,  7.62s/it]loss_total_epoch 429.82420124299824
Training tokenizer:  86% 1740/2013 [2:53:10<34:41,  7.62s/it]loss_total_epoch 430.02357309125364
Training tokenizer:  86% 1741/2013 [2:53:17<34:40,  7.65s/it]loss_total_epoch 430.2109201904386
Training tokenizer:  87% 1742/2013 [2:53:25<34:31,  7.64s/it]loss_total_epoch 430.40727648697793
Training tokenizer:  87% 1743/2013 [2:53:33<34:25,  7.65s/it]loss_total_epoch 430.60659003816545
Training tokenizer:  87% 1744/2013 [2:53:40<34:19,  7.66s/it]loss_total_epoch 430.79880707897246
Training tokenizer:  87% 1745/2013 [2:53:48<34:08,  7.64s/it]loss_total_epoch 430.9737101625651
Training tokenizer:  87% 1746/2013 [2:53:55<33:58,  7.64s/it]loss_total_epoch 431.1641789842397
Training tokenizer:  87% 1747/2013 [2:54:03<34:08,  7.70s/it]loss_total_epoch 431.3464626464993
Training tokenizer:  87% 1748/2013 [2:54:11<33:52,  7.67s/it]loss_total_epoch 431.54049888439476
Training tokenizer:  87% 1749/2013 [2:54:18<33:36,  7.64s/it]loss_total_epoch 431.73606751300395
Training tokenizer:  87% 1750/2013 [2:54:26<33:27,  7.63s/it]loss_total_epoch 431.9276279453188
Training tokenizer:  87% 1751/2013 [2:54:34<33:25,  7.65s/it]loss_total_epoch 432.11934248544276
Training tokenizer:  87% 1752/2013 [2:54:41<33:12,  7.64s/it]loss_total_epoch 432.31620200537145
Training tokenizer:  87% 1753/2013 [2:54:49<33:04,  7.63s/it]loss_total_epoch 432.50858004949987
Training tokenizer:  87% 1754/2013 [2:54:57<32:58,  7.64s/it]loss_total_epoch 432.7041618768126
Training tokenizer:  87% 1755/2013 [2:55:04<32:50,  7.64s/it]loss_total_epoch 432.90593797154725
Training tokenizer:  87% 1756/2013 [2:55:12<32:42,  7.63s/it]loss_total_epoch 433.1071525309235
Training tokenizer:  87% 1757/2013 [2:55:20<32:42,  7.67s/it]loss_total_epoch 433.28731154836714
Training tokenizer:  87% 1758/2013 [2:55:27<32:31,  7.65s/it]loss_total_epoch 433.4553113114089
Training tokenizer:  87% 1759/2013 [2:55:35<32:24,  7.66s/it]loss_total_epoch 433.6273088734597
Training tokenizer:  87% 1760/2013 [2:55:43<32:17,  7.66s/it]loss_total_epoch 433.8179256822914
Training tokenizer:  87% 1761/2013 [2:55:50<32:13,  7.67s/it]loss_total_epoch 434.0084567721933
Training tokenizer:  88% 1762/2013 [2:55:58<32:04,  7.67s/it]loss_total_epoch 434.1877168659121
Training tokenizer:  88% 1763/2013 [2:56:06<31:54,  7.66s/it]loss_total_epoch 434.3627129290253
Training tokenizer:  88% 1764/2013 [2:56:13<31:46,  7.66s/it]loss_total_epoch 434.55670134164393
Training tokenizer:  88% 1765/2013 [2:56:21<31:41,  7.67s/it]loss_total_epoch 434.7309507969767
Training tokenizer:  88% 1766/2013 [2:56:29<31:36,  7.68s/it]loss_total_epoch 434.9260294865817
Training tokenizer:  88% 1767/2013 [2:56:36<31:29,  7.68s/it]loss_total_epoch 435.1180909629911
Training tokenizer:  88% 1768/2013 [2:56:44<31:22,  7.69s/it]loss_total_epoch 435.3290692809969
Training tokenizer:  88% 1769/2013 [2:56:52<31:12,  7.68s/it]loss_total_epoch 435.51048237271607
Training tokenizer:  88% 1770/2013 [2:56:59<31:06,  7.68s/it]loss_total_epoch 435.716443663463
Training tokenizer:  88% 1771/2013 [2:57:07<30:58,  7.68s/it]loss_total_epoch 435.9006822090596
Training tokenizer:  88% 1772/2013 [2:57:15<30:47,  7.67s/it]loss_total_epoch 436.0982156340033
Training tokenizer:  88% 1773/2013 [2:57:22<30:40,  7.67s/it]loss_total_epoch 436.290840016678
Training tokenizer:  88% 1774/2013 [2:57:30<30:32,  7.67s/it]loss_total_epoch 436.47669966332614
Training tokenizer:  88% 1775/2013 [2:57:38<30:31,  7.70s/it]loss_total_epoch 436.6779707390815
Training tokenizer:  88% 1776/2013 [2:57:46<30:26,  7.71s/it]loss_total_epoch 436.852497426793
Training tokenizer:  88% 1777/2013 [2:57:53<30:18,  7.71s/it]loss_total_epoch 437.0383887793869
Training tokenizer:  88% 1778/2013 [2:58:01<30:09,  7.70s/it]loss_total_epoch 437.2338673416525
Training tokenizer:  88% 1779/2013 [2:58:09<29:57,  7.68s/it]loss_total_epoch 437.4387001078576
Training tokenizer:  88% 1780/2013 [2:58:16<29:53,  7.70s/it]loss_total_epoch 437.6281667742878
Training tokenizer:  88% 1781/2013 [2:58:24<29:46,  7.70s/it]loss_total_epoch 437.84600951336324
Training tokenizer:  89% 1782/2013 [2:58:32<29:37,  7.70s/it]loss_total_epoch 438.05124614946544
Training tokenizer:  89% 1783/2013 [2:58:39<29:35,  7.72s/it]loss_total_epoch 438.224113015458
Training tokenizer:  89% 1784/2013 [2:58:47<29:25,  7.71s/it]loss_total_epoch 438.4199217427522
Training tokenizer:  89% 1785/2013 [2:58:55<29:20,  7.72s/it]loss_total_epoch 438.62305690534413
Training tokenizer:  89% 1786/2013 [2:59:02<29:02,  7.68s/it]loss_total_epoch 438.807087643072
Training tokenizer:  89% 1787/2013 [2:59:10<28:55,  7.68s/it]loss_total_epoch 439.01406285725534
Training tokenizer:  89% 1788/2013 [2:59:18<28:53,  7.70s/it]loss_total_epoch 439.22882954217494
Training tokenizer:  89% 1789/2013 [2:59:26<28:47,  7.71s/it]loss_total_epoch 439.42202978767455
Training tokenizer:  89% 1790/2013 [2:59:33<28:47,  7.75s/it]loss_total_epoch 439.61749116145074
Training tokenizer:  89% 1791/2013 [2:59:41<28:34,  7.72s/it]loss_total_epoch 439.8220458198339
Training tokenizer:  89% 1792/2013 [2:59:49<28:27,  7.72s/it]loss_total_epoch 440.0062480401248
Training tokenizer:  89% 1793/2013 [2:59:57<28:15,  7.71s/it]loss_total_epoch 440.19823451526463
Training tokenizer:  89% 1794/2013 [3:00:04<28:00,  7.67s/it]loss_total_epoch 440.4021011982113
Training tokenizer:  89% 1795/2013 [3:00:12<27:54,  7.68s/it]loss_total_epoch 440.6046757120639
Training tokenizer:  89% 1796/2013 [3:00:20<27:49,  7.69s/it]loss_total_epoch 440.7978343684226
Training tokenizer:  89% 1797/2013 [3:00:27<27:45,  7.71s/it]loss_total_epoch 441.0012068655342
Training tokenizer:  89% 1798/2013 [3:00:35<27:37,  7.71s/it]loss_total_epoch 441.16858670674264
Training tokenizer:  89% 1799/2013 [3:00:43<27:32,  7.72s/it]loss_total_epoch 441.35107467137277
Training tokenizer:  89% 1800/2013 [3:00:50<27:23,  7.71s/it]loss_total_epoch 441.537979638204
Training tokenizer:  89% 1801/2013 [3:00:58<27:14,  7.71s/it]loss_total_epoch 441.75049607269466
Training tokenizer:  90% 1802/2013 [3:01:06<27:08,  7.72s/it]loss_total_epoch 441.9423726256937
Training tokenizer:  90% 1803/2013 [3:01:14<27:00,  7.72s/it]loss_total_epoch 442.15826445259154
Training tokenizer:  90% 1804/2013 [3:01:21<26:59,  7.75s/it]loss_total_epoch 442.333613736555
Training tokenizer:  90% 1805/2013 [3:01:29<26:43,  7.71s/it]loss_total_epoch 442.5226833727211
Training tokenizer:  90% 1806/2013 [3:01:37<26:37,  7.72s/it]loss_total_epoch 442.7194837462157
Training tokenizer:  90% 1807/2013 [3:01:45<26:36,  7.75s/it]loss_total_epoch 442.91387495212257
Training tokenizer:  90% 1808/2013 [3:01:52<26:32,  7.77s/it]loss_total_epoch 443.1053991895169
Training tokenizer:  90% 1809/2013 [3:02:00<26:24,  7.77s/it]loss_total_epoch 443.288167392835
Training tokenizer:  90% 1810/2013 [3:02:08<26:12,  7.75s/it]loss_total_epoch 443.49078805185854
Training tokenizer:  90% 1811/2013 [3:02:16<25:59,  7.72s/it]loss_total_epoch 443.69556565769017
Training tokenizer:  90% 1812/2013 [3:02:23<25:56,  7.74s/it]loss_total_epoch 443.8824494984001
Training tokenizer:  90% 1813/2013 [3:02:31<25:51,  7.76s/it]loss_total_epoch 444.07668807543814
Training tokenizer:  90% 1814/2013 [3:02:39<25:43,  7.75s/it]loss_total_epoch 444.26512294076383
Training tokenizer:  90% 1815/2013 [3:02:47<25:38,  7.77s/it]loss_total_epoch 444.4591191839427
Training tokenizer:  90% 1816/2013 [3:02:54<25:32,  7.78s/it]loss_total_epoch 444.6359877195209
Training tokenizer:  90% 1817/2013 [3:03:02<25:19,  7.75s/it]loss_total_epoch 444.81913184933364
Training tokenizer:  90% 1818/2013 [3:03:10<25:13,  7.76s/it]loss_total_epoch 445.00892817787826
Training tokenizer:  90% 1819/2013 [3:03:18<25:02,  7.74s/it]loss_total_epoch 445.1968596186489
Training tokenizer:  90% 1820/2013 [3:03:25<24:57,  7.76s/it]loss_total_epoch 445.4038561116904
Training tokenizer:  90% 1821/2013 [3:03:33<24:50,  7.76s/it]loss_total_epoch 445.5761225204915
Training tokenizer:  91% 1822/2013 [3:03:41<24:44,  7.77s/it]loss_total_epoch 445.7739868257195
Training tokenizer:  91% 1823/2013 [3:03:49<24:34,  7.76s/it]loss_total_epoch 445.9518392328173
Training tokenizer:  91% 1824/2013 [3:03:56<24:24,  7.75s/it]loss_total_epoch 446.11609611846507
Training tokenizer:  91% 1825/2013 [3:04:04<24:16,  7.75s/it]loss_total_epoch 446.2883214559406
Training tokenizer:  91% 1826/2013 [3:04:12<24:08,  7.75s/it]loss_total_epoch 446.48023174889386
Training tokenizer:  91% 1827/2013 [3:04:20<24:03,  7.76s/it]loss_total_epoch 446.6720886323601
Training tokenizer:  91% 1828/2013 [3:04:27<23:55,  7.76s/it]loss_total_epoch 446.8615301642567
Training tokenizer:  91% 1829/2013 [3:04:35<23:48,  7.76s/it]loss_total_epoch 447.02900810725987
Training tokenizer:  91% 1830/2013 [3:04:43<23:42,  7.77s/it]loss_total_epoch 447.216923257336
Training tokenizer:  91% 1831/2013 [3:04:51<23:36,  7.78s/it]loss_total_epoch 447.39884516038
Training tokenizer:  91% 1832/2013 [3:04:59<23:30,  7.79s/it]loss_total_epoch 447.57521593384445
Training tokenizer:  91% 1833/2013 [3:05:06<23:22,  7.79s/it]loss_total_epoch 447.76313635520637
Training tokenizer:  91% 1834/2013 [3:05:14<23:10,  7.77s/it]loss_total_epoch 447.960624800995
Training tokenizer:  91% 1835/2013 [3:05:22<23:04,  7.78s/it]loss_total_epoch 448.1339102741331
Training tokenizer:  91% 1836/2013 [3:05:30<22:56,  7.77s/it]loss_total_epoch 448.3394040148705
Training tokenizer:  91% 1837/2013 [3:05:37<22:45,  7.76s/it]loss_total_epoch 448.5515646059066
Training tokenizer:  91% 1838/2013 [3:05:45<22:40,  7.78s/it]loss_total_epoch 448.7376690711826
Training tokenizer:  91% 1839/2013 [3:05:53<22:36,  7.80s/it]loss_total_epoch 448.9334383215755
Training tokenizer:  91% 1840/2013 [3:06:01<22:29,  7.80s/it]loss_total_epoch 449.1217886004597
Training tokenizer:  91% 1841/2013 [3:06:09<22:20,  7.79s/it]loss_total_epoch 449.3067151810974
Training tokenizer:  92% 1842/2013 [3:06:17<22:13,  7.80s/it]loss_total_epoch 449.4971710238606
Training tokenizer:  92% 1843/2013 [3:06:24<22:05,  7.80s/it]loss_total_epoch 449.7006072420627
Training tokenizer:  92% 1844/2013 [3:06:32<21:57,  7.80s/it]loss_total_epoch 449.888462850824
Training tokenizer:  92% 1845/2013 [3:06:40<21:52,  7.81s/it]loss_total_epoch 450.07355687208474
Training tokenizer:  92% 1846/2013 [3:06:48<21:46,  7.82s/it]loss_total_epoch 450.2593990396708
Training tokenizer:  92% 1847/2013 [3:06:56<21:37,  7.82s/it]loss_total_epoch 450.4510338474065
Training tokenizer:  92% 1848/2013 [3:07:03<21:31,  7.83s/it]loss_total_epoch 450.6206868831068
Training tokenizer:  92% 1849/2013 [3:07:11<21:24,  7.83s/it]loss_total_epoch 450.80376247502863
Training tokenizer:  92% 1850/2013 [3:07:19<21:15,  7.83s/it]loss_total_epoch 450.9944625515491
Training tokenizer:  92% 1851/2013 [3:07:27<21:03,  7.80s/it]loss_total_epoch 451.20249563269317
Training tokenizer:  92% 1852/2013 [3:07:35<20:57,  7.81s/it]loss_total_epoch 451.39850836806
Training tokenizer:  92% 1853/2013 [3:07:43<20:49,  7.81s/it]loss_total_epoch 451.57865927554667
Training tokenizer:  92% 1854/2013 [3:07:50<20:36,  7.77s/it]loss_total_epoch 451.76743024773896
Training tokenizer:  92% 1855/2013 [3:07:58<20:29,  7.78s/it]loss_total_epoch 451.95634345524013
Training tokenizer:  92% 1856/2013 [3:08:06<20:23,  7.79s/it]loss_total_epoch 452.1503402274102
Training tokenizer:  92% 1857/2013 [3:08:14<20:16,  7.80s/it]loss_total_epoch 452.354166822508
Training tokenizer:  92% 1858/2013 [3:08:21<20:09,  7.80s/it]loss_total_epoch 452.55137316323817
Training tokenizer:  92% 1859/2013 [3:08:29<20:04,  7.82s/it]loss_total_epoch 452.74498606286943
Training tokenizer:  92% 1860/2013 [3:08:37<19:52,  7.79s/it]loss_total_epoch 452.92582717351615
Training tokenizer:  92% 1861/2013 [3:08:45<19:44,  7.79s/it]loss_total_epoch 453.12490285746753
Training tokenizer:  92% 1862/2013 [3:08:53<19:35,  7.79s/it]loss_total_epoch 453.31587434001267
Training tokenizer:  93% 1863/2013 [3:09:00<19:28,  7.79s/it]loss_total_epoch 453.51859799958766
Training tokenizer:  93% 1864/2013 [3:09:08<19:24,  7.82s/it]loss_total_epoch 453.7096101064235
Training tokenizer:  93% 1865/2013 [3:09:16<19:16,  7.81s/it]loss_total_epoch 453.8904835898429
Training tokenizer:  93% 1866/2013 [3:09:24<19:08,  7.81s/it]loss_total_epoch 454.07879483141005
Training tokenizer:  93% 1867/2013 [3:09:32<19:00,  7.81s/it]loss_total_epoch 454.26461210660636
Training tokenizer:  93% 1868/2013 [3:09:40<18:54,  7.82s/it]loss_total_epoch 454.4514679927379
Training tokenizer:  93% 1869/2013 [3:09:47<18:45,  7.82s/it]loss_total_epoch 454.65417000837624
Training tokenizer:  93% 1870/2013 [3:09:55<18:40,  7.83s/it]loss_total_epoch 454.8347280677408
Training tokenizer:  93% 1871/2013 [3:10:03<18:30,  7.82s/it]loss_total_epoch 455.02831769548357
Training tokenizer:  93% 1872/2013 [3:10:11<18:20,  7.81s/it]loss_total_epoch 455.2187557872385
Training tokenizer:  93% 1873/2013 [3:10:19<18:15,  7.82s/it]loss_total_epoch 455.4232372250408
Training tokenizer:  93% 1874/2013 [3:10:27<18:10,  7.85s/it]loss_total_epoch 455.61993610300124
Training tokenizer:  93% 1875/2013 [3:10:34<18:01,  7.84s/it]loss_total_epoch 455.7945616673678
Training tokenizer:  93% 1876/2013 [3:10:42<17:54,  7.84s/it]loss_total_epoch 455.97087243385613
Training tokenizer:  93% 1877/2013 [3:10:50<17:46,  7.84s/it]loss_total_epoch 456.15620042942464
Training tokenizer:  93% 1878/2013 [3:10:58<17:40,  7.86s/it]loss_total_epoch 456.34483000449836
Training tokenizer:  93% 1879/2013 [3:11:06<17:31,  7.85s/it]loss_total_epoch 456.5212798509747
Training tokenizer:  93% 1880/2013 [3:11:14<17:23,  7.84s/it]loss_total_epoch 456.7237633410841
Training tokenizer:  93% 1881/2013 [3:11:21<17:16,  7.86s/it]loss_total_epoch 456.90403177402914
Training tokenizer:  93% 1882/2013 [3:11:29<17:10,  7.86s/it]loss_total_epoch 457.0888089183718
Training tokenizer:  94% 1883/2013 [3:11:37<17:02,  7.86s/it]loss_total_epoch 457.2841512132436
Training tokenizer:  94% 1884/2013 [3:11:45<16:53,  7.86s/it]loss_total_epoch 457.4753990340978
Training tokenizer:  94% 1885/2013 [3:11:53<16:44,  7.85s/it]loss_total_epoch 457.66761248372495
Training tokenizer:  94% 1886/2013 [3:12:01<16:38,  7.86s/it]loss_total_epoch 457.8633960355073
Training tokenizer:  94% 1887/2013 [3:12:09<16:31,  7.87s/it]loss_total_epoch 458.0437470395118
Training tokenizer:  94% 1888/2013 [3:12:16<16:21,  7.85s/it]loss_total_epoch 458.2309269737452
Training tokenizer:  94% 1889/2013 [3:12:24<16:08,  7.81s/it]loss_total_epoch 458.4183426890522
Training tokenizer:  94% 1890/2013 [3:12:32<16:03,  7.83s/it]loss_total_epoch 458.5895993504673
Training tokenizer:  94% 1891/2013 [3:12:40<15:54,  7.82s/it]loss_total_epoch 458.77404079400003
Training tokenizer:  94% 1892/2013 [3:12:48<15:47,  7.83s/it]loss_total_epoch 458.9666400272399
Training tokenizer:  94% 1893/2013 [3:12:56<15:41,  7.85s/it]loss_total_epoch 459.1453609559685
Training tokenizer:  94% 1894/2013 [3:13:04<15:35,  7.86s/it]loss_total_epoch 459.32522799260914
Training tokenizer:  94% 1895/2013 [3:13:11<15:28,  7.87s/it]loss_total_epoch 459.4985033068806
Training tokenizer:  94% 1896/2013 [3:13:19<15:20,  7.87s/it]loss_total_epoch 459.6996268276125
Training tokenizer:  94% 1897/2013 [3:13:27<15:13,  7.88s/it]loss_total_epoch 459.87960972078145
Training tokenizer:  94% 1898/2013 [3:13:35<15:06,  7.88s/it]loss_total_epoch 460.0621286351234
Training tokenizer:  94% 1899/2013 [3:13:43<14:58,  7.88s/it]loss_total_epoch 460.24347259290516
Training tokenizer:  94% 1900/2013 [3:13:51<14:49,  7.87s/it]loss_total_epoch 460.42689439095557
Training tokenizer:  94% 1901/2013 [3:13:59<14:40,  7.86s/it]loss_total_epoch 460.6190772037953
Training tokenizer:  94% 1902/2013 [3:14:06<14:27,  7.81s/it]loss_total_epoch 460.80044061876833
Training tokenizer:  95% 1903/2013 [3:14:14<14:23,  7.85s/it]loss_total_epoch 460.96877838484943
Training tokenizer:  95% 1904/2013 [3:14:22<14:11,  7.82s/it]loss_total_epoch 461.1546110343188
Training tokenizer:  95% 1905/2013 [3:14:30<14:05,  7.83s/it]loss_total_epoch 461.34137164242566
Training tokenizer:  95% 1906/2013 [3:14:38<13:58,  7.84s/it]loss_total_epoch 461.5296424943954
Training tokenizer:  95% 1907/2013 [3:14:46<13:51,  7.84s/it]loss_total_epoch 461.72395647503436
Training tokenizer:  95% 1908/2013 [3:14:53<13:44,  7.86s/it]loss_total_epoch 461.9112452287227
Training tokenizer:  95% 1909/2013 [3:15:01<13:37,  7.86s/it]loss_total_epoch 462.104393677786
Training tokenizer:  95% 1910/2013 [3:15:09<13:30,  7.87s/it]loss_total_epoch 462.2889459412545
Training tokenizer:  95% 1911/2013 [3:15:17<13:23,  7.88s/it]loss_total_epoch 462.47189871035516
Training tokenizer:  95% 1912/2013 [3:15:25<13:16,  7.89s/it]loss_total_epoch 462.6740223560482
Training tokenizer:  95% 1913/2013 [3:15:33<13:06,  7.87s/it]loss_total_epoch 462.87336684577167
Training tokenizer:  95% 1914/2013 [3:15:41<12:55,  7.84s/it]loss_total_epoch 463.04480330087245
Training tokenizer:  95% 1915/2013 [3:15:49<12:49,  7.85s/it]loss_total_epoch 463.2210528049618
Training tokenizer:  95% 1916/2013 [3:15:56<12:43,  7.87s/it]loss_total_epoch 463.40051707811654
Training tokenizer:  95% 1917/2013 [3:16:04<12:35,  7.87s/it]loss_total_epoch 463.57736538909376
Training tokenizer:  95% 1918/2013 [3:16:12<12:27,  7.87s/it]loss_total_epoch 463.7669671867043
Training tokenizer:  95% 1919/2013 [3:16:20<12:21,  7.89s/it]loss_total_epoch 463.96237015910447
Training tokenizer:  95% 1920/2013 [3:16:28<12:14,  7.90s/it]loss_total_epoch 464.1435655001551
Training tokenizer:  95% 1921/2013 [3:16:36<12:07,  7.91s/it]loss_total_epoch 464.34174157492816
Training tokenizer:  95% 1922/2013 [3:16:44<11:59,  7.91s/it]loss_total_epoch 464.5483387950808
Training tokenizer:  96% 1923/2013 [3:16:52<11:50,  7.89s/it]loss_total_epoch 464.72021895088255
Training tokenizer:  96% 1924/2013 [3:17:00<11:41,  7.88s/it]loss_total_epoch 464.92188203521073
Training tokenizer:  96% 1925/2013 [3:17:07<11:33,  7.88s/it]loss_total_epoch 465.1198385115713
Training tokenizer:  96% 1926/2013 [3:17:15<11:26,  7.89s/it]loss_total_epoch 465.3272219803184
Training tokenizer:  96% 1927/2013 [3:17:23<11:19,  7.90s/it]loss_total_epoch 465.5190856214613
Training tokenizer:  96% 1928/2013 [3:17:31<11:11,  7.90s/it]loss_total_epoch 465.70762513019145
Training tokenizer:  96% 1929/2013 [3:17:39<11:03,  7.90s/it]loss_total_epoch 465.88063828088343
Training tokenizer:  96% 1930/2013 [3:17:47<10:54,  7.88s/it]loss_total_epoch 466.0928822327405
Training tokenizer:  96% 1931/2013 [3:17:55<10:47,  7.90s/it]loss_total_epoch 466.26256870664656
Training tokenizer:  96% 1932/2013 [3:18:03<10:40,  7.91s/it]loss_total_epoch 466.45266927592456
Training tokenizer:  96% 1933/2013 [3:18:11<10:31,  7.89s/it]loss_total_epoch 466.6572154182941
Training tokenizer:  96% 1934/2013 [3:18:19<10:24,  7.90s/it]loss_total_epoch 466.8578798677772
Training tokenizer:  96% 1935/2013 [3:18:26<10:14,  7.87s/it]loss_total_epoch 467.0763719845563
Training tokenizer:  96% 1936/2013 [3:18:34<10:07,  7.88s/it]loss_total_epoch 467.2587436977774
Training tokenizer:  96% 1937/2013 [3:18:42<10:00,  7.91s/it]loss_total_epoch 467.4420397821814
Training tokenizer:  96% 1938/2013 [3:18:50<09:54,  7.93s/it]loss_total_epoch 467.6230022702366
Training tokenizer:  96% 1939/2013 [3:18:58<09:47,  7.94s/it]loss_total_epoch 467.80002854578197
Training tokenizer:  96% 1940/2013 [3:19:06<09:37,  7.92s/it]loss_total_epoch 467.97846907936037
Training tokenizer:  96% 1941/2013 [3:19:14<09:30,  7.92s/it]loss_total_epoch 468.1541172694415
Training tokenizer:  96% 1942/2013 [3:19:22<09:23,  7.93s/it]loss_total_epoch 468.3684135172516
Training tokenizer:  97% 1943/2013 [3:19:30<09:13,  7.90s/it]loss_total_epoch 468.56267410330474
Training tokenizer:  97% 1944/2013 [3:19:38<09:05,  7.91s/it]loss_total_epoch 468.73986520431936
Training tokenizer:  97% 1945/2013 [3:19:46<08:59,  7.94s/it]loss_total_epoch 468.9218601491302
Training tokenizer:  97% 1946/2013 [3:19:54<08:51,  7.93s/it]loss_total_epoch 469.10735471360385
Training tokenizer:  97% 1947/2013 [3:20:02<08:42,  7.92s/it]loss_total_epoch 469.2917923871428
Training tokenizer:  97% 1948/2013 [3:20:09<08:36,  7.94s/it]loss_total_epoch 469.4767585899681
Training tokenizer:  97% 1949/2013 [3:20:17<08:28,  7.94s/it]loss_total_epoch 469.65128688700497
Training tokenizer:  97% 1950/2013 [3:20:25<08:20,  7.95s/it]loss_total_epoch 469.82662313245237
Training tokenizer:  97% 1951/2013 [3:20:33<08:12,  7.94s/it]loss_total_epoch 470.02339173294604
Training tokenizer:  97% 1952/2013 [3:20:41<08:04,  7.94s/it]loss_total_epoch 470.2014852333814
Training tokenizer:  97% 1953/2013 [3:20:49<07:56,  7.94s/it]loss_total_epoch 470.40526468865573
Training tokenizer:  97% 1954/2013 [3:20:57<07:49,  7.95s/it]loss_total_epoch 470.56254139728844
Training tokenizer:  97% 1955/2013 [3:21:06<07:53,  8.17s/it]loss_total_epoch 470.76475331000984
Training tokenizer:  97% 1956/2013 [3:21:14<07:41,  8.09s/it]loss_total_epoch 470.94509013555944
Training tokenizer:  97% 1957/2013 [3:21:22<07:31,  8.06s/it]loss_total_epoch 471.14344060234725
Training tokenizer:  97% 1958/2013 [3:21:30<07:22,  8.04s/it]loss_total_epoch 471.3371307644993
Training tokenizer:  97% 1959/2013 [3:21:38<07:12,  8.01s/it]loss_total_epoch 471.5150130931288
Training tokenizer:  97% 1960/2013 [3:21:46<07:03,  8.00s/it]loss_total_epoch 471.7054036092013
Training tokenizer:  97% 1961/2013 [3:21:54<06:55,  7.99s/it]loss_total_epoch 471.8859017621726
Training tokenizer:  97% 1962/2013 [3:22:02<06:47,  7.98s/it]loss_total_epoch 472.07886821962893
Training tokenizer:  98% 1963/2013 [3:22:09<06:36,  7.93s/it]loss_total_epoch 472.27093338035047
Training tokenizer:  98% 1964/2013 [3:22:18<06:32,  8.00s/it]loss_total_epoch 472.43568543531
Training tokenizer:  98% 1965/2013 [3:22:26<06:23,  7.99s/it]loss_total_epoch 472.60938133485615
Training tokenizer:  98% 1966/2013 [3:22:34<06:15,  7.98s/it]loss_total_epoch 472.80605085007846
Training tokenizer:  98% 1967/2013 [3:22:41<06:05,  7.95s/it]loss_total_epoch 472.97784614004195
Training tokenizer:  98% 1968/2013 [3:22:49<05:56,  7.93s/it]loss_total_epoch 473.1710789632052
Training tokenizer:  98% 1969/2013 [3:22:57<05:49,  7.94s/it]loss_total_epoch 473.3534298893064
Training tokenizer:  98% 1970/2013 [3:23:05<05:42,  7.96s/it]loss_total_epoch 473.54232618398964
Training tokenizer:  98% 1971/2013 [3:23:13<05:34,  7.98s/it]loss_total_epoch 473.73839648626745
Training tokenizer:  98% 1972/2013 [3:23:21<05:26,  7.97s/it]loss_total_epoch 473.9135264325887
Training tokenizer:  98% 1973/2013 [3:23:29<05:17,  7.94s/it]loss_total_epoch 474.11175431869924
Training tokenizer:  98% 1974/2013 [3:23:37<05:09,  7.93s/it]loss_total_epoch 474.2916791047901
Training tokenizer:  98% 1975/2013 [3:23:45<05:01,  7.94s/it]loss_total_epoch 474.46980431862175
Training tokenizer:  98% 1976/2013 [3:23:53<04:54,  7.97s/it]loss_total_epoch 474.6418262068182
Training tokenizer:  98% 1977/2013 [3:24:01<04:46,  7.96s/it]loss_total_epoch 474.84221438132226
Training tokenizer:  98% 1978/2013 [3:24:09<04:38,  7.96s/it]loss_total_epoch 475.03227252326906
Training tokenizer:  98% 1979/2013 [3:24:17<04:30,  7.96s/it]loss_total_epoch 475.21906591020525
Training tokenizer:  98% 1980/2013 [3:24:25<04:22,  7.97s/it]loss_total_epoch 475.4076101500541
Training tokenizer:  98% 1981/2013 [3:24:33<04:15,  7.98s/it]loss_total_epoch 475.6005096528679
Training tokenizer:  98% 1982/2013 [3:24:41<04:07,  7.98s/it]loss_total_epoch 475.776089431718
Training tokenizer:  99% 1983/2013 [3:24:49<03:59,  7.97s/it]loss_total_epoch 475.9424986485392
Training tokenizer:  99% 1984/2013 [3:24:57<03:51,  7.98s/it]loss_total_epoch 476.127938779071
Training tokenizer:  99% 1985/2013 [3:25:05<03:42,  7.96s/it]loss_total_epoch 476.3079022746533
Training tokenizer:  99% 1986/2013 [3:25:13<03:34,  7.93s/it]loss_total_epoch 476.51070812903345
Training tokenizer:  99% 1987/2013 [3:25:21<03:26,  7.95s/it]loss_total_epoch 476.7114998716861
Training tokenizer:  99% 1988/2013 [3:25:28<03:18,  7.94s/it]loss_total_epoch 476.9028930980712
Training tokenizer:  99% 1989/2013 [3:25:36<03:10,  7.95s/it]loss_total_epoch 477.09529610164464
Training tokenizer:  99% 1990/2013 [3:25:44<03:02,  7.95s/it]loss_total_epoch 477.26372932083905
Training tokenizer:  99% 1991/2013 [3:25:52<02:55,  7.97s/it]loss_total_epoch 477.45273909159005
Training tokenizer:  99% 1992/2013 [3:26:00<02:47,  7.95s/it]loss_total_epoch 477.6251867879182
Training tokenizer:  99% 1993/2013 [3:26:08<02:38,  7.94s/it]loss_total_epoch 477.79210846312344
Training tokenizer:  99% 1994/2013 [3:26:16<02:31,  7.97s/it]loss_total_epoch 477.97443861700594
Training tokenizer:  99% 1995/2013 [3:26:24<02:23,  7.96s/it]loss_total_epoch 478.12753594107926
Training tokenizer:  99% 1996/2013 [3:26:32<02:15,  7.96s/it]loss_total_epoch 478.3107420001179
Training tokenizer:  99% 1997/2013 [3:26:41<02:09,  8.08s/it]loss_total_epoch 478.47916325367987
Training tokenizer:  99% 1998/2013 [3:26:48<01:59,  8.00s/it]loss_total_epoch 478.6668765004724
Training tokenizer:  99% 1999/2013 [3:26:56<01:51,  7.99s/it]loss_total_epoch 478.850787775591
Training tokenizer:  99% 2000/2013 [3:27:04<01:43,  7.99s/it]loss_total_epoch 479.0246789138764
Training tokenizer:  99% 2001/2013 [3:27:12<01:36,  8.02s/it]loss_total_epoch 479.20210059173405
Training tokenizer:  99% 2002/2013 [3:27:20<01:28,  8.01s/it]loss_total_epoch 479.38326034881175
Training tokenizer: 100% 2003/2013 [3:27:28<01:19,  7.99s/it]loss_total_epoch 479.5667597372085
Training tokenizer: 100% 2004/2013 [3:27:36<01:11,  7.99s/it]loss_total_epoch 479.7458416353911
Training tokenizer: 100% 2005/2013 [3:27:44<01:03,  7.99s/it]loss_total_epoch 479.931274401024
Training tokenizer: 100% 2006/2013 [3:27:52<00:55,  7.98s/it]loss_total_epoch 480.1163123715669
Training tokenizer: 100% 2007/2013 [3:28:00<00:47,  7.97s/it]loss_total_epoch 480.3113480117172
Training tokenizer: 100% 2008/2013 [3:28:08<00:39,  7.98s/it]loss_total_epoch 480.49957182072103
Training tokenizer: 100% 2009/2013 [3:28:16<00:32,  8.00s/it]loss_total_epoch 480.67135097272694
Training tokenizer: 100% 2010/2013 [3:28:24<00:24,  8.03s/it]loss_total_epoch 480.8542884904891
Training tokenizer: 100% 2011/2013 [3:28:32<00:16,  8.02s/it]loss_total_epoch 481.0182816814631
Training tokenizer: 100% 2012/2013 [3:28:40<00:08,  8.01s/it]loss_total_epoch 481.1873915847391
Training tokenizer: 100% 2013/2013 [3:28:48<00:00,  8.02s/it]Training tokenizer: 100% 2013/2013 [3:28:48<00:00,  6.22s/it]

Epoch 2 / 200

32183 3493 3560
Training tokenizer:   0% 0/2013 [00:00<?, ?it/s]loss_total_epoch 0.1886669658124447
Training tokenizer:   0% 1/2013 [00:07<4:28:05,  7.99s/it]loss_total_epoch 0.3518422767519951
Training tokenizer:   0% 2/2013 [00:16<4:29:18,  8.04s/it]loss_total_epoch 0.5500195063650608
Training tokenizer:   0% 3/2013 [00:24<4:29:15,  8.04s/it]loss_total_epoch 0.7459658086299896
Training tokenizer:   0% 4/2013 [00:32<4:28:01,  8.00s/it]loss_total_epoch 0.9171322397887707
Training tokenizer:   0% 5/2013 [00:40<4:28:28,  8.02s/it]loss_total_epoch 1.0824977941811085
Training tokenizer:   0% 6/2013 [00:48<4:28:02,  8.01s/it]loss_total_epoch 1.2624211683869362
Training tokenizer:   0% 7/2013 [00:56<4:28:35,  8.03s/it]loss_total_epoch 1.4214574694633484
Training tokenizer:   0% 8/2013 [01:04<4:26:24,  7.97s/it]loss_total_epoch 1.6158168502151966
Training tokenizer:   0% 9/2013 [01:12<4:27:42,  8.02s/it]loss_total_epoch 1.8016636818647385
Training tokenizer:   0% 10/2013 [01:20<4:28:52,  8.05s/it]loss_total_epoch 1.9809441529214382
Training tokenizer:   1% 11/2013 [01:28<4:28:15,  8.04s/it]loss_total_epoch 2.164944961667061
Training tokenizer:   1% 12/2013 [01:36<4:27:44,  8.03s/it]loss_total_epoch 2.3493104986846447
Training tokenizer:   1% 13/2013 [01:44<4:27:03,  8.01s/it]loss_total_epoch 2.5164067447185516
Training tokenizer:   1% 14/2013 [01:52<4:28:12,  8.05s/it]loss_total_epoch 2.6921365968883038
Training tokenizer:   1% 15/2013 [02:00<4:27:43,  8.04s/it]loss_total_epoch 2.858886331319809
Training tokenizer:   1% 16/2013 [02:08<4:28:12,  8.06s/it]loss_total_epoch 3.060742449015379
Training tokenizer:   1% 17/2013 [02:16<4:27:50,  8.05s/it]loss_total_epoch 3.2247201167047024
Training tokenizer:   1% 18/2013 [02:24<4:26:21,  8.01s/it]loss_total_epoch 3.415261521935463
Training tokenizer:   1% 19/2013 [02:32<4:25:00,  7.97s/it]loss_total_epoch 3.5746454410254955
Training tokenizer:   1% 20/2013 [02:40<4:25:22,  7.99s/it]loss_total_epoch 3.7524548918008804
Training tokenizer:   1% 21/2013 [02:48<4:25:11,  7.99s/it]loss_total_epoch 3.9378519244492054
Training tokenizer:   1% 22/2013 [02:56<4:25:16,  7.99s/it]loss_total_epoch 4.136445466428995
Training tokenizer:   1% 23/2013 [03:04<4:25:13,  8.00s/it]loss_total_epoch 4.317534901201725
Training tokenizer:   1% 24/2013 [03:12<4:25:35,  8.01s/it]loss_total_epoch 4.483613688498735
Training tokenizer:   1% 25/2013 [03:20<4:25:42,  8.02s/it]loss_total_epoch 4.667439140379429
Training tokenizer:   1% 26/2013 [03:28<4:25:40,  8.02s/it]loss_total_epoch 4.863346271216869
Training tokenizer:   1% 27/2013 [03:36<4:25:58,  8.04s/it]loss_total_epoch 5.054015606641769
Training tokenizer:   1% 28/2013 [03:44<4:25:34,  8.03s/it]loss_total_epoch 5.236870959401131
Training tokenizer:   1% 29/2013 [03:52<4:26:02,  8.05s/it]loss_total_epoch 5.3850178848952055
Training tokenizer:   1% 30/2013 [04:00<4:26:15,  8.06s/it]loss_total_epoch 5.585590375587344
Training tokenizer:   2% 31/2013 [04:08<4:25:52,  8.05s/it]loss_total_epoch 5.747616915032268
Training tokenizer:   2% 32/2013 [04:16<4:25:47,  8.05s/it]loss_total_epoch 5.946186954155564
Training tokenizer:   2% 33/2013 [04:24<4:24:59,  8.03s/it]loss_total_epoch 6.12638240493834
Training tokenizer:   2% 34/2013 [04:32<4:25:39,  8.05s/it]loss_total_epoch 6.312308629974723
Training tokenizer:   2% 35/2013 [04:40<4:25:00,  8.04s/it]loss_total_epoch 6.489151392132044
Training tokenizer:   2% 36/2013 [04:48<4:24:37,  8.03s/it]loss_total_epoch 6.677349869161844
Training tokenizer:   2% 37/2013 [04:56<4:24:33,  8.03s/it]loss_total_epoch 6.859385676681995
Training tokenizer:   2% 38/2013 [05:05<4:25:10,  8.06s/it]loss_total_epoch 7.042228028178215
Training tokenizer:   2% 39/2013 [05:13<4:25:49,  8.08s/it]loss_total_epoch 7.2106986455619335
Training tokenizer:   2% 40/2013 [05:21<4:24:53,  8.06s/it]loss_total_epoch 7.38648471981287
Training tokenizer:   2% 41/2013 [05:29<4:25:01,  8.06s/it]loss_total_epoch 7.574419561773539
Training tokenizer:   2% 42/2013 [05:37<4:24:00,  8.04s/it]loss_total_epoch 7.77456422150135
Training tokenizer:   2% 43/2013 [05:45<4:23:37,  8.03s/it]loss_total_epoch 7.958157621324062
Training tokenizer:   2% 44/2013 [05:53<4:23:38,  8.03s/it]loss_total_epoch 8.11794301494956
Training tokenizer:   2% 45/2013 [06:01<4:23:43,  8.04s/it]loss_total_epoch 8.297485616058111
Training tokenizer:   2% 46/2013 [06:09<4:23:47,  8.05s/it]loss_total_epoch 8.47281926870346
Training tokenizer:   2% 47/2013 [06:17<4:23:13,  8.03s/it]loss_total_epoch 8.6630474999547
Training tokenizer:   2% 48/2013 [06:25<4:22:07,  8.00s/it]loss_total_epoch 8.861713003367186
Training tokenizer:   2% 49/2013 [06:33<4:20:52,  7.97s/it]loss_total_epoch 9.060032606124878
Training tokenizer:   2% 50/2013 [06:41<4:21:54,  8.01s/it]loss_total_epoch 9.232913874089718
Training tokenizer:   3% 51/2013 [06:49<4:21:53,  8.01s/it]loss_total_epoch 9.412112776190042
Training tokenizer:   3% 52/2013 [06:57<4:22:01,  8.02s/it]loss_total_epoch 9.604324173182249
Training tokenizer:   3% 53/2013 [07:05<4:21:47,  8.01s/it]loss_total_epoch 9.764024667441845
Training tokenizer:   3% 54/2013 [07:13<4:21:40,  8.01s/it]loss_total_epoch 9.9356694445014
Training tokenizer:   3% 55/2013 [07:21<4:22:02,  8.03s/it]loss_total_epoch 10.119989592581987
Training tokenizer:   3% 56/2013 [07:29<4:22:05,  8.04s/it]loss_total_epoch 10.303818110376596
Training tokenizer:   3% 57/2013 [07:37<4:21:50,  8.03s/it]loss_total_epoch 10.461878415197134
Training tokenizer:   3% 58/2013 [07:45<4:22:44,  8.06s/it]loss_total_epoch 10.658102467656136
Training tokenizer:   3% 59/2013 [07:53<4:22:59,  8.08s/it]loss_total_epoch 10.841516099870205
Training tokenizer:   3% 60/2013 [08:01<4:22:31,  8.07s/it]loss_total_epoch 11.045507483184338
Training tokenizer:   3% 61/2013 [08:09<4:20:47,  8.02s/it]loss_total_epoch 11.224611107259989
Training tokenizer:   3% 62/2013 [08:17<4:21:08,  8.03s/it]loss_total_epoch 11.421261444687843
Training tokenizer:   3% 63/2013 [08:25<4:21:17,  8.04s/it]loss_total_epoch 11.58730560168624
Training tokenizer:   3% 64/2013 [08:33<4:19:53,  8.00s/it]loss_total_epoch 11.769346494227648
Training tokenizer:   3% 65/2013 [08:41<4:19:50,  8.00s/it]loss_total_epoch 11.94435516372323
Training tokenizer:   3% 66/2013 [08:49<4:19:58,  8.01s/it]loss_total_epoch 12.13240147009492
Training tokenizer:   3% 67/2013 [08:57<4:20:00,  8.02s/it]loss_total_epoch 12.326970968395472
Training tokenizer:   3% 68/2013 [09:05<4:20:15,  8.03s/it]loss_total_epoch 12.50940142199397
Training tokenizer:   3% 69/2013 [09:13<4:20:44,  8.05s/it]loss_total_epoch 12.70261538028717
Training tokenizer:   3% 70/2013 [09:21<4:18:54,  8.00s/it]loss_total_epoch 12.885001439601183
Training tokenizer:   4% 71/2013 [09:29<4:19:11,  8.01s/it]loss_total_epoch 13.094685047864914
Training tokenizer:   4% 72/2013 [09:37<4:17:22,  7.96s/it]loss_total_epoch 13.289915341883898
Training tokenizer:   4% 73/2013 [09:45<4:18:05,  7.98s/it]loss_total_epoch 13.479644052684307
Training tokenizer:   4% 74/2013 [09:53<4:18:01,  7.98s/it]loss_total_epoch 13.676574684679508
Training tokenizer:   4% 75/2013 [10:01<4:18:07,  7.99s/it]loss_total_epoch 13.87945019826293
Training tokenizer:   4% 76/2013 [10:09<4:18:02,  7.99s/it]loss_total_epoch 14.086534436792135
Training tokenizer:   4% 77/2013 [10:17<4:17:04,  7.97s/it]loss_total_epoch 14.267142735421658
Training tokenizer:   4% 78/2013 [10:25<4:18:09,  8.01s/it]loss_total_epoch 14.443618971854448
Training tokenizer:   4% 79/2013 [10:33<4:18:18,  8.01s/it]loss_total_epoch 14.623695943504572
Training tokenizer:   4% 80/2013 [10:41<4:18:27,  8.02s/it]loss_total_epoch 14.814374566078186
Training tokenizer:   4% 81/2013 [10:49<4:18:27,  8.03s/it]loss_total_epoch 15.01848528161645
Training tokenizer:   4% 82/2013 [10:57<4:19:02,  8.05s/it]loss_total_epoch 15.190027415752411
Training tokenizer:   4% 83/2013 [11:05<4:18:24,  8.03s/it]loss_total_epoch 15.368065781891346
Training tokenizer:   4% 84/2013 [11:14<4:18:46,  8.05s/it]loss_total_epoch 15.555138107389212
Training tokenizer:   4% 85/2013 [11:22<4:18:44,  8.05s/it]loss_total_epoch 15.732633292675018
Training tokenizer:   4% 86/2013 [11:30<4:18:37,  8.05s/it]loss_total_epoch 15.886209391057491
Training tokenizer:   4% 87/2013 [11:38<4:18:15,  8.05s/it]loss_total_epoch 16.06175659224391
Training tokenizer:   4% 88/2013 [11:46<4:18:02,  8.04s/it]loss_total_epoch 16.242879297584295
Training tokenizer:   4% 89/2013 [11:54<4:16:52,  8.01s/it]loss_total_epoch 16.42700804769993
Training tokenizer:   4% 90/2013 [12:02<4:17:28,  8.03s/it]loss_total_epoch 16.629487190395594
Training tokenizer:   5% 91/2013 [12:10<4:16:28,  8.01s/it]loss_total_epoch 16.79896468669176
Training tokenizer:   5% 92/2013 [12:18<4:16:18,  8.01s/it]loss_total_epoch 16.961226373910904
Training tokenizer:   5% 93/2013 [12:26<4:16:45,  8.02s/it]loss_total_epoch 17.13053385913372
Training tokenizer:   5% 94/2013 [12:34<4:17:09,  8.04s/it]loss_total_epoch 17.301266469061375
Training tokenizer:   5% 95/2013 [12:42<4:17:49,  8.07s/it]loss_total_epoch 17.44801202788949
Training tokenizer:   5% 96/2013 [12:50<4:17:28,  8.06s/it]loss_total_epoch 17.6331904605031
Training tokenizer:   5% 97/2013 [12:58<4:17:07,  8.05s/it]loss_total_epoch 17.812188792973757
Training tokenizer:   5% 98/2013 [13:06<4:17:24,  8.07s/it]loss_total_epoch 17.993826508522034
Training tokenizer:   5% 99/2013 [13:14<4:15:57,  8.02s/it]loss_total_epoch 18.156056568026543
Training tokenizer:   5% 100/2013 [13:22<4:18:46,  8.12s/it]loss_total_epoch 18.33444084227085
Training tokenizer:   5% 101/2013 [13:30<4:17:55,  8.09s/it]loss_total_epoch 18.50496916845441
Training tokenizer:   5% 102/2013 [13:39<4:19:52,  8.16s/it]loss_total_epoch 18.680653870105743
Training tokenizer:   5% 103/2013 [13:47<4:18:04,  8.11s/it]loss_total_epoch 18.85593891143799
Training tokenizer:   5% 104/2013 [13:55<4:16:33,  8.06s/it]wandb: ERROR Error while calling W&B API: graphql: panic occurred: runtime error: invalid memory address or nil pointer dereference (<Response [500]>)
wandb: ERROR Error while calling W&B API: graphql: panic occurred: runtime error: invalid memory address or nil pointer dereference (<Response [500]>)
loss_total_epoch 19.055575158447027
Training tokenizer:   5% 105/2013 [14:03<4:16:04,  8.05s/it]loss_total_epoch 19.235236555337906
Training tokenizer:   5% 106/2013 [14:11<4:15:36,  8.04s/it]wandb: ERROR Error while calling W&B API: graphql: panic occurred: runtime error: invalid memory address or nil pointer dereference (<Response [500]>)
loss_total_epoch 19.437492955476046
Training tokenizer:   5% 107/2013 [14:19<4:14:52,  8.02s/it]loss_total_epoch 19.620708912611008
Training tokenizer:   5% 108/2013 [14:27<4:14:52,  8.03s/it]loss_total_epoch 19.79800596833229
Training tokenizer:   5% 109/2013 [14:35<4:14:43,  8.03s/it]loss_total_epoch 19.96977063268423
Training tokenizer:   5% 110/2013 [14:43<4:13:41,  8.00s/it]wandb: ERROR Error while calling W&B API: graphql: panic occurred: runtime error: invalid memory address or nil pointer dereference (<Response [500]>)
loss_total_epoch 20.142903976142406
Training tokenizer:   6% 111/2013 [14:51<4:12:54,  7.98s/it]wandb: ERROR Error while calling W&B API: graphql: panic occurred: runtime error: invalid memory address or nil pointer dereference (<Response [500]>)
wandb: ERROR Error while calling W&B API: graphql: panic occurred: runtime error: invalid memory address or nil pointer dereference (<Response [500]>)
loss_total_epoch 20.311446744948626
Training tokenizer:   6% 112/2013 [14:59<4:13:04,  7.99s/it]wandb: ERROR Error while calling W&B API: graphql: panic occurred: runtime error: invalid memory address or nil pointer dereference (<Response [500]>)
loss_total_epoch 20.49107415601611
Training tokenizer:   6% 113/2013 [15:07<4:13:15,  8.00s/it]wandb: ERROR Error while calling W&B API: graphql: panic occurred: runtime error: invalid memory address or nil pointer dereference (<Response [500]>)
loss_total_epoch 20.686810716986656
Training tokenizer:   6% 114/2013 [15:15<4:13:43,  8.02s/it]loss_total_epoch 20.86848447844386
Training tokenizer:   6% 115/2013 [15:23<4:13:54,  8.03s/it]wandb: ERROR Error while calling W&B API: graphql: panic occurred: runtime error: invalid memory address or nil pointer dereference (<Response [500]>)
loss_total_epoch 21.0403960570693
Training tokenizer:   6% 116/2013 [15:31<4:13:30,  8.02s/it]loss_total_epoch 21.225685372948647
Training tokenizer:   6% 117/2013 [15:39<4:12:58,  8.01s/it]loss_total_epoch 21.411252696067095
Training tokenizer:   6% 118/2013 [15:47<4:13:07,  8.01s/it]loss_total_epoch 21.59794833138585
Training tokenizer:   6% 119/2013 [15:55<4:13:15,  8.02s/it]wandb: ERROR Error while calling W&B API: graphql: panic occurred: runtime error: invalid memory address or nil pointer dereference (<Response [500]>)
loss_total_epoch 21.780701138079166
Training tokenizer:   6% 120/2013 [16:03<4:12:31,  8.00s/it]loss_total_epoch 21.988591510802507
Training tokenizer:   6% 121/2013 [16:11<4:12:39,  8.01s/it]loss_total_epoch 22.17063445970416
Training tokenizer:   6% 122/2013 [16:19<4:13:18,  8.04s/it]loss_total_epoch 22.372675374150276
Training tokenizer:   6% 123/2013 [16:27<4:12:54,  8.03s/it]loss_total_epoch 22.582721702754498
Training tokenizer:   6% 124/2013 [16:35<4:13:03,  8.04s/it]loss_total_epoch 22.76252043992281
Training tokenizer:   6% 125/2013 [16:43<4:12:59,  8.04s/it]loss_total_epoch 22.944531079381704
Training tokenizer:   6% 126/2013 [16:51<4:13:26,  8.06s/it]loss_total_epoch 23.09785745292902
Training tokenizer:   6% 127/2013 [16:59<4:12:58,  8.05s/it]loss_total_epoch 23.28326664865017
Training tokenizer:   6% 128/2013 [17:07<4:12:19,  8.03s/it]loss_total_epoch 23.462446443736553
Training tokenizer:   6% 129/2013 [17:15<4:12:44,  8.05s/it]wandb: ERROR Error while calling W&B API: graphql: panic occurred: runtime error: invalid memory address or nil pointer dereference (<Response [500]>)
loss_total_epoch 23.63885184004903
Training tokenizer:   6% 130/2013 [17:23<4:12:36,  8.05s/it]loss_total_epoch 23.812053475528955
Training tokenizer:   7% 131/2013 [17:31<4:12:11,  8.04s/it]loss_total_epoch 23.993448860943317
Training tokenizer:   7% 132/2013 [17:39<4:12:14,  8.05s/it]loss_total_epoch 24.177135821431875
Training tokenizer:   7% 133/2013 [17:47<4:12:04,  8.05s/it]loss_total_epoch 24.335494056344032
Training tokenizer:   7% 134/2013 [17:55<4:11:55,  8.04s/it]loss_total_epoch 24.51536274701357
Training tokenizer:   7% 135/2013 [18:04<4:11:47,  8.04s/it]loss_total_epoch 24.687723077833652
Training tokenizer:   7% 136/2013 [18:12<4:11:07,  8.03s/it]loss_total_epoch 24.868575144559145
Training tokenizer:   7% 137/2013 [18:20<4:10:29,  8.01s/it]loss_total_epoch 25.043481152504683
Training tokenizer:   7% 138/2013 [18:27<4:10:09,  8.00s/it]loss_total_epoch 25.234055768698454
Training tokenizer:   7% 139/2013 [18:36<4:10:06,  8.01s/it]loss_total_epoch 25.390793204307556
Training tokenizer:   7% 140/2013 [18:44<4:10:18,  8.02s/it]loss_total_epoch 25.566355377435684
Training tokenizer:   7% 141/2013 [18:52<4:10:49,  8.04s/it]loss_total_epoch 25.748540207743645
Training tokenizer:   7% 142/2013 [19:00<4:10:35,  8.04s/it]loss_total_epoch 25.918872833251953
Training tokenizer:   7% 143/2013 [19:08<4:10:33,  8.04s/it]loss_total_epoch 26.075576093047857
Training tokenizer:   7% 144/2013 [19:16<4:10:21,  8.04s/it]loss_total_epoch 26.25176829099655
Training tokenizer:   7% 145/2013 [19:24<4:10:10,  8.04s/it]loss_total_epoch 26.433608565479517
Training tokenizer:   7% 146/2013 [19:32<4:10:12,  8.04s/it]loss_total_epoch 26.60441568121314
Training tokenizer:   7% 147/2013 [19:40<4:10:23,  8.05s/it]loss_total_epoch 26.782322395592928
Training tokenizer:   7% 148/2013 [19:48<4:10:27,  8.06s/it]loss_total_epoch 26.949431240558624
Training tokenizer:   7% 149/2013 [19:56<4:10:34,  8.07s/it]loss_total_epoch 27.121523659676313
Training tokenizer:   7% 150/2013 [20:04<4:10:06,  8.05s/it]loss_total_epoch 27.29038713313639
Training tokenizer:   8% 151/2013 [20:12<4:09:56,  8.05s/it]loss_total_epoch 27.453552400693297
Training tokenizer:   8% 152/2013 [20:20<4:08:29,  8.01s/it]loss_total_epoch 27.630389841273427
Training tokenizer:   8% 153/2013 [20:28<4:08:49,  8.03s/it]loss_total_epoch 27.806502616032958
Training tokenizer:   8% 154/2013 [20:36<4:07:52,  8.00s/it]loss_total_epoch 27.962345821782947
Training tokenizer:   8% 155/2013 [20:45<4:14:11,  8.21s/it]loss_total_epoch 28.158086394891143
Training tokenizer:   8% 156/2013 [20:53<4:11:30,  8.13s/it]loss_total_epoch 28.333608439192176
Training tokenizer:   8% 157/2013 [21:01<4:10:46,  8.11s/it]loss_total_epoch 28.509824765846133
Training tokenizer:   8% 158/2013 [21:09<4:09:10,  8.06s/it]loss_total_epoch 28.697247808799148
Training tokenizer:   8% 159/2013 [21:17<4:08:35,  8.05s/it]loss_total_epoch 28.877666940912604
Training tokenizer:   8% 160/2013 [21:25<4:08:07,  8.03s/it]loss_total_epoch 29.051325036212802
Training tokenizer:   8% 161/2013 [21:33<4:08:11,  8.04s/it]loss_total_epoch 29.23086025007069
Training tokenizer:   8% 162/2013 [21:41<4:07:59,  8.04s/it]loss_total_epoch 29.420498544350266
Training tokenizer:   8% 163/2013 [21:49<4:07:58,  8.04s/it]loss_total_epoch 29.59785665012896
Training tokenizer:   8% 164/2013 [21:57<4:08:04,  8.05s/it]loss_total_epoch 29.790747506543994
Training tokenizer:   8% 165/2013 [22:05<4:07:39,  8.04s/it]loss_total_epoch 29.968632461503148
Training tokenizer:   8% 166/2013 [22:13<4:06:45,  8.02s/it]loss_total_epoch 30.145758910104632
Training tokenizer:   8% 167/2013 [22:21<4:05:53,  7.99s/it]loss_total_epoch 30.318066181614995
Training tokenizer:   8% 168/2013 [22:29<4:06:22,  8.01s/it]loss_total_epoch 30.506470357999206
Training tokenizer:   8% 169/2013 [22:37<4:06:53,  8.03s/it]loss_total_epoch 30.689615769311786
Training tokenizer:   8% 170/2013 [22:45<4:06:29,  8.02s/it]loss_total_epoch 30.861251963302493
Training tokenizer:   8% 171/2013 [22:53<4:06:58,  8.04s/it]loss_total_epoch 31.057481000199914
Training tokenizer:   9% 172/2013 [23:01<4:06:40,  8.04s/it]loss_total_epoch 31.23036408610642
Training tokenizer:   9% 173/2013 [23:09<4:05:44,  8.01s/it]loss_total_epoch 31.41908106766641
Training tokenizer:   9% 174/2013 [23:17<4:04:32,  7.98s/it]loss_total_epoch 31.599187595769763
Training tokenizer:   9% 175/2013 [23:25<4:05:18,  8.01s/it]loss_total_epoch 31.782272597774863
Training tokenizer:   9% 176/2013 [23:33<4:05:03,  8.00s/it]loss_total_epoch 31.94474501349032
Training tokenizer:   9% 177/2013 [23:41<4:05:46,  8.03s/it]loss_total_epoch 32.110201966017485
Training tokenizer:   9% 178/2013 [23:49<4:05:30,  8.03s/it]loss_total_epoch 32.295232214033604
Training tokenizer:   9% 179/2013 [23:57<4:05:30,  8.03s/it]loss_total_epoch 32.480741303414106
Training tokenizer:   9% 180/2013 [24:05<4:05:07,  8.02s/it]loss_total_epoch 32.68406871706247
Training tokenizer:   9% 181/2013 [24:13<4:05:02,  8.03s/it]loss_total_epoch 32.87902005016804
Training tokenizer:   9% 182/2013 [24:21<4:03:53,  7.99s/it]loss_total_epoch 33.068201903253794
Training tokenizer:   9% 183/2013 [24:29<4:04:20,  8.01s/it]loss_total_epoch 33.275793470442295
Training tokenizer:   9% 184/2013 [24:37<4:04:27,  8.02s/it]loss_total_epoch 33.48589799180627
Training tokenizer:   9% 185/2013 [24:45<4:04:10,  8.01s/it]loss_total_epoch 33.6866857893765
Training tokenizer:   9% 186/2013 [24:53<4:03:50,  8.01s/it]loss_total_epoch 33.888036232441664
Training tokenizer:   9% 187/2013 [25:01<4:03:53,  8.01s/it]loss_total_epoch 34.05840392038226
Training tokenizer:   9% 188/2013 [25:09<4:04:43,  8.05s/it]loss_total_epoch 34.24963263422251
Training tokenizer:   9% 189/2013 [25:17<4:04:08,  8.03s/it]loss_total_epoch 34.4357413649559
Training tokenizer:   9% 190/2013 [25:25<4:03:49,  8.03s/it]loss_total_epoch 34.619908690452576
Training tokenizer:   9% 191/2013 [25:33<4:03:22,  8.01s/it]loss_total_epoch 34.785611890256405
Training tokenizer:  10% 192/2013 [25:41<4:03:56,  8.04s/it]loss_total_epoch 34.98474212735891
Training tokenizer:  10% 193/2013 [25:50<4:04:21,  8.06s/it]loss_total_epoch 35.141658417880535
Training tokenizer:  10% 194/2013 [25:58<4:03:35,  8.03s/it]loss_total_epoch 35.291995238512754
Training tokenizer:  10% 195/2013 [26:06<4:03:21,  8.03s/it]loss_total_epoch 35.48189692944288
Training tokenizer:  10% 196/2013 [26:14<4:02:40,  8.01s/it]loss_total_epoch 35.641750045120716
Training tokenizer:  10% 197/2013 [26:22<4:02:29,  8.01s/it]loss_total_epoch 35.82931316643953
Training tokenizer:  10% 198/2013 [26:30<4:02:39,  8.02s/it]loss_total_epoch 35.98850705847144
Training tokenizer:  10% 199/2013 [26:38<4:02:55,  8.03s/it]loss_total_epoch 36.15592907369137
Training tokenizer:  10% 200/2013 [26:46<4:02:44,  8.03s/it]loss_total_epoch 36.33527284488082
Training tokenizer:  10% 201/2013 [26:54<4:02:17,  8.02s/it]loss_total_epoch 36.52054878696799
Training tokenizer:  10% 202/2013 [27:02<4:02:40,  8.04s/it]loss_total_epoch 36.683226495981216
Training tokenizer:  10% 203/2013 [27:10<4:01:29,  8.01s/it]loss_total_epoch 36.85043339058757
Training tokenizer:  10% 204/2013 [27:18<4:01:42,  8.02s/it]loss_total_epoch 37.03953256830573
Training tokenizer:  10% 205/2013 [27:26<4:02:00,  8.03s/it]loss_total_epoch 37.214997582137585
Training tokenizer:  10% 206/2013 [27:34<4:01:46,  8.03s/it]loss_total_epoch 37.403190683573484
Training tokenizer:  10% 207/2013 [27:42<4:02:01,  8.04s/it]loss_total_epoch 37.588811829686165
Training tokenizer:  10% 208/2013 [27:50<4:01:33,  8.03s/it]loss_total_epoch 37.76058467105031
Training tokenizer:  10% 209/2013 [27:58<4:01:51,  8.04s/it]loss_total_epoch 37.957297902554274
Training tokenizer:  10% 210/2013 [28:06<4:00:57,  8.02s/it]loss_total_epoch 38.13079974800348
Training tokenizer:  10% 211/2013 [28:14<4:01:24,  8.04s/it]loss_total_epoch 38.30803906917572
Training tokenizer:  11% 212/2013 [28:22<4:01:03,  8.03s/it]loss_total_epoch 38.482202637940645
Training tokenizer:  11% 213/2013 [28:30<4:00:49,  8.03s/it]loss_total_epoch 38.6493685990572
Training tokenizer:  11% 214/2013 [28:38<4:00:39,  8.03s/it]loss_total_epoch 38.81571224331856
Training tokenizer:  11% 215/2013 [28:46<4:00:49,  8.04s/it]loss_total_epoch 38.98984206467867
Training tokenizer:  11% 216/2013 [28:54<4:00:00,  8.01s/it]loss_total_epoch 39.159184679389
Training tokenizer:  11% 217/2013 [29:02<4:00:11,  8.02s/it]loss_total_epoch 39.33287224173546
Training tokenizer:  11% 218/2013 [29:10<4:00:00,  8.02s/it]loss_total_epoch 39.51046882197261
Training tokenizer:  11% 219/2013 [29:18<3:59:34,  8.01s/it]loss_total_epoch 39.68716663494706
Training tokenizer:  11% 220/2013 [29:26<3:59:36,  8.02s/it]loss_total_epoch 39.86381360888481
Training tokenizer:  11% 221/2013 [29:34<4:00:21,  8.05s/it]loss_total_epoch 40.052139442414045
Training tokenizer:  11% 222/2013 [29:42<3:59:52,  8.04s/it]loss_total_epoch 40.23044053092599
Training tokenizer:  11% 223/2013 [29:50<4:00:01,  8.05s/it]loss_total_epoch 40.38070556893945
Training tokenizer:  11% 224/2013 [29:58<3:59:56,  8.05s/it]loss_total_epoch 40.56758996844292
Training tokenizer:  11% 225/2013 [30:06<3:59:33,  8.04s/it]loss_total_epoch 40.7290792632848
Training tokenizer:  11% 226/2013 [30:14<3:59:03,  8.03s/it]loss_total_epoch 40.91145277582109
Training tokenizer:  11% 227/2013 [30:22<3:59:12,  8.04s/it]loss_total_epoch 41.07352099008858
Training tokenizer:  11% 228/2013 [30:30<3:57:39,  7.99s/it]loss_total_epoch 41.244168462231755
Training tokenizer:  11% 229/2013 [30:38<3:58:33,  8.02s/it]loss_total_epoch 41.41971174813807
Training tokenizer:  11% 230/2013 [30:46<3:58:06,  8.01s/it]loss_total_epoch 41.592065116390586
Training tokenizer:  11% 231/2013 [30:54<3:57:36,  8.00s/it]loss_total_epoch 41.769855277612805
Training tokenizer:  12% 232/2013 [31:02<3:57:53,  8.01s/it]loss_total_epoch 41.9509187694639
Training tokenizer:  12% 233/2013 [31:10<3:56:24,  7.97s/it]loss_total_epoch 42.14121535234153
Training tokenizer:  12% 234/2013 [31:18<3:56:20,  7.97s/it]loss_total_epoch 42.32231290824711
Training tokenizer:  12% 235/2013 [31:26<3:57:33,  8.02s/it]loss_total_epoch 42.523649821057916
Training tokenizer:  12% 236/2013 [31:34<3:56:35,  7.99s/it]loss_total_epoch 42.72384894452989
Training tokenizer:  12% 237/2013 [31:42<3:57:00,  8.01s/it]loss_total_epoch 42.89878178574145
Training tokenizer:  12% 238/2013 [31:50<3:57:08,  8.02s/it]loss_total_epoch 43.075214905664325
Training tokenizer:  12% 239/2013 [31:58<3:56:48,  8.01s/it]loss_total_epoch 43.26883572153747
Training tokenizer:  12% 240/2013 [32:06<3:56:22,  8.00s/it]loss_total_epoch 43.44965292327106
Training tokenizer:  12% 241/2013 [32:14<3:56:21,  8.00s/it]loss_total_epoch 43.63732721097767
Training tokenizer:  12% 242/2013 [32:22<3:56:09,  8.00s/it]loss_total_epoch 43.81627727113664
Training tokenizer:  12% 243/2013 [32:31<3:56:47,  8.03s/it]loss_total_epoch 44.0058034863323
Training tokenizer:  12% 244/2013 [32:39<3:57:20,  8.05s/it]loss_total_epoch 44.17025185935199
Training tokenizer:  12% 245/2013 [32:47<3:57:27,  8.06s/it]loss_total_epoch 44.35159535892308
Training tokenizer:  12% 246/2013 [32:55<3:56:59,  8.05s/it]loss_total_epoch 44.539068253710866
Training tokenizer:  12% 247/2013 [33:03<3:56:45,  8.04s/it]loss_total_epoch 44.72286428697407
Training tokenizer:  12% 248/2013 [33:11<3:56:18,  8.03s/it]loss_total_epoch 44.90739153511822
Training tokenizer:  12% 249/2013 [33:19<3:55:41,  8.02s/it]loss_total_epoch 45.09324131719768
Training tokenizer:  12% 250/2013 [33:27<3:55:31,  8.02s/it]loss_total_epoch 45.26729220338166
Training tokenizer:  12% 251/2013 [33:35<3:56:32,  8.05s/it]loss_total_epoch 45.45728203468025
Training tokenizer:  13% 252/2013 [33:43<3:56:20,  8.05s/it]loss_total_epoch 45.62927962653339
Training tokenizer:  13% 253/2013 [33:51<3:56:13,  8.05s/it]loss_total_epoch 45.82675668038428
Training tokenizer:  13% 254/2013 [33:59<3:55:54,  8.05s/it]loss_total_epoch 45.97512243874371
Training tokenizer:  13% 255/2013 [34:07<3:55:46,  8.05s/it]loss_total_epoch 46.14004865847528
Training tokenizer:  13% 256/2013 [34:15<3:55:45,  8.05s/it]loss_total_epoch 46.306986259296536
Training tokenizer:  13% 257/2013 [34:23<3:56:04,  8.07s/it]loss_total_epoch 46.478021236136556
Training tokenizer:  13% 258/2013 [34:31<3:55:34,  8.05s/it]loss_total_epoch 46.6499539706856
Training tokenizer:  13% 259/2013 [34:39<3:55:14,  8.05s/it]loss_total_epoch 46.82905078865588
Training tokenizer:  13% 260/2013 [34:47<3:54:08,  8.01s/it]loss_total_epoch 47.00050978176296
Training tokenizer:  13% 261/2013 [34:55<3:53:39,  8.00s/it]loss_total_epoch 47.15425025485456
Training tokenizer:  13% 262/2013 [35:03<3:53:34,  8.00s/it]loss_total_epoch 47.32614187896252
Training tokenizer:  13% 263/2013 [35:11<3:54:50,  8.05s/it]loss_total_epoch 47.50493028387427
Training tokenizer:  13% 264/2013 [35:20<3:55:29,  8.08s/it]loss_total_epoch 47.67255217209458
Training tokenizer:  13% 265/2013 [35:28<3:55:18,  8.08s/it]loss_total_epoch 47.83848816715181
Training tokenizer:  13% 266/2013 [35:36<3:54:50,  8.07s/it]loss_total_epoch 48.02439840696752
Training tokenizer:  13% 267/2013 [35:44<3:54:42,  8.07s/it]loss_total_epoch 48.19830005802214
Training tokenizer:  13% 268/2013 [35:52<3:54:10,  8.05s/it]loss_total_epoch 48.377196153625846
Training tokenizer:  13% 269/2013 [36:00<3:54:35,  8.07s/it]loss_total_epoch 48.54260833375156
Training tokenizer:  13% 270/2013 [36:08<3:53:27,  8.04s/it]loss_total_epoch 48.71362101845443
Training tokenizer:  13% 271/2013 [36:16<3:53:29,  8.04s/it]loss_total_epoch 48.895069705322385
Training tokenizer:  14% 272/2013 [36:24<3:53:16,  8.04s/it]loss_total_epoch 49.06017313338816
Training tokenizer:  14% 273/2013 [36:32<3:53:17,  8.04s/it]loss_total_epoch 49.226227106526494
Training tokenizer:  14% 274/2013 [36:40<3:53:08,  8.04s/it]loss_total_epoch 49.40296323411167
Training tokenizer:  14% 275/2013 [36:48<3:55:36,  8.13s/it]loss_total_epoch 49.5907558593899
Training tokenizer:  14% 276/2013 [36:56<3:55:27,  8.13s/it]loss_total_epoch 49.7719116974622
Training tokenizer:  14% 277/2013 [37:04<3:54:26,  8.10s/it]loss_total_epoch 49.94479734636843
Training tokenizer:  14% 278/2013 [37:13<3:53:47,  8.09s/it]loss_total_epoch 50.12343622930348
Training tokenizer:  14% 279/2013 [37:20<3:52:22,  8.04s/it]loss_total_epoch 50.304667403921485
Training tokenizer:  14% 280/2013 [37:28<3:51:51,  8.03s/it]loss_total_epoch 50.454235000535846
Training tokenizer:  14% 281/2013 [37:36<3:50:31,  7.99s/it]loss_total_epoch 50.638913011178374
Training tokenizer:  14% 282/2013 [37:44<3:50:51,  8.00s/it]loss_total_epoch 50.80928800441325
Training tokenizer:  14% 283/2013 [37:52<3:50:21,  7.99s/it]loss_total_epoch 50.9897413123399
Training tokenizer:  14% 284/2013 [38:00<3:51:30,  8.03s/it]loss_total_epoch 51.16980974562466
Training tokenizer:  14% 285/2013 [38:08<3:50:53,  8.02s/it]loss_total_epoch 51.33060744218528
Training tokenizer:  14% 286/2013 [38:17<3:51:16,  8.03s/it]loss_total_epoch 51.50885813497007
Training tokenizer:  14% 287/2013 [38:25<3:50:40,  8.02s/it]loss_total_epoch 51.70204625464976
Training tokenizer:  14% 288/2013 [38:32<3:50:05,  8.00s/it]loss_total_epoch 51.8651968780905
Training tokenizer:  14% 289/2013 [38:41<3:50:38,  8.03s/it]loss_total_epoch 52.022222170606256
Training tokenizer:  14% 290/2013 [38:49<3:51:31,  8.06s/it]loss_total_epoch 52.186099618673325
Training tokenizer:  14% 291/2013 [38:57<3:50:15,  8.02s/it]loss_total_epoch 52.35936103016138
Training tokenizer:  15% 292/2013 [39:05<3:51:19,  8.06s/it]loss_total_epoch 52.53084936738014
Training tokenizer:  15% 293/2013 [39:13<3:50:20,  8.04s/it]loss_total_epoch 52.70194689184427
Training tokenizer:  15% 294/2013 [39:21<3:49:50,  8.02s/it]loss_total_epoch 52.888973135501146
Training tokenizer:  15% 295/2013 [39:29<3:50:17,  8.04s/it]loss_total_epoch 53.06196180358529
Training tokenizer:  15% 296/2013 [39:37<3:51:45,  8.10s/it]loss_total_epoch 53.23377373814583
Training tokenizer:  15% 297/2013 [39:45<3:51:31,  8.10s/it]loss_total_epoch 53.41174575313926
Training tokenizer:  15% 298/2013 [39:53<3:51:50,  8.11s/it]loss_total_epoch 53.57071044109762
Training tokenizer:  15% 299/2013 [40:01<3:51:33,  8.11s/it]loss_total_epoch 53.7586824093014
Training tokenizer:  15% 300/2013 [40:10<3:51:34,  8.11s/it]loss_total_epoch 53.91306332685053
Training tokenizer:  15% 301/2013 [40:18<3:51:58,  8.13s/it]loss_total_epoch 54.08441982232034
Training tokenizer:  15% 302/2013 [40:26<3:50:22,  8.08s/it]loss_total_epoch 54.267176857218146
Training tokenizer:  15% 303/2013 [40:34<3:49:51,  8.06s/it]loss_total_epoch 54.44048632495105
Training tokenizer:  15% 304/2013 [40:42<3:49:46,  8.07s/it]loss_total_epoch 54.61770471744239
Training tokenizer:  15% 305/2013 [40:50<3:49:52,  8.08s/it]loss_total_epoch 54.79810713790357
Training tokenizer:  15% 306/2013 [40:58<3:49:04,  8.05s/it]loss_total_epoch 54.980657083913684
Training tokenizer:  15% 307/2013 [41:06<3:47:56,  8.02s/it]loss_total_epoch 55.17359504662454
Training tokenizer:  15% 308/2013 [41:14<3:48:34,  8.04s/it]loss_total_epoch 55.35054007358849
Training tokenizer:  15% 309/2013 [41:22<3:48:12,  8.04s/it]loss_total_epoch 55.53602205403149
Training tokenizer:  15% 310/2013 [41:30<3:48:37,  8.06s/it]loss_total_epoch 55.71719909273088
Training tokenizer:  15% 311/2013 [41:38<3:47:17,  8.01s/it]loss_total_epoch 55.90191460214555
Training tokenizer:  15% 312/2013 [41:46<3:48:07,  8.05s/it]loss_total_epoch 56.0683439578861
Training tokenizer:  16% 313/2013 [41:54<3:48:30,  8.06s/it]loss_total_epoch 56.241510389372706
Training tokenizer:  16% 314/2013 [42:02<3:48:27,  8.07s/it]loss_total_epoch 56.413885498419404
Training tokenizer:  16% 315/2013 [42:10<3:48:14,  8.07s/it]loss_total_epoch 56.57551316358149
Training tokenizer:  16% 316/2013 [42:18<3:48:36,  8.08s/it]loss_total_epoch 56.7642239164561
Training tokenizer:  16% 317/2013 [42:27<3:49:15,  8.11s/it]loss_total_epoch 56.93999263457954
Training tokenizer:  16% 318/2013 [42:35<3:49:18,  8.12s/it]loss_total_epoch 57.13017593510449
Training tokenizer:  16% 319/2013 [42:43<3:48:24,  8.09s/it]loss_total_epoch 57.30607439391315
Training tokenizer:  16% 320/2013 [42:51<3:47:56,  8.08s/it]loss_total_epoch 57.47729297913611
Training tokenizer:  16% 321/2013 [42:59<3:47:59,  8.09s/it]loss_total_epoch 57.66667236573994
Training tokenizer:  16% 322/2013 [43:07<3:47:48,  8.08s/it]loss_total_epoch 57.82449417375028
Training tokenizer:  16% 323/2013 [43:15<3:47:19,  8.07s/it]loss_total_epoch 57.98028617724776
Training tokenizer:  16% 324/2013 [43:23<3:47:17,  8.07s/it]loss_total_epoch 58.168326288461685
Training tokenizer:  16% 325/2013 [43:31<3:46:09,  8.04s/it]loss_total_epoch 58.3508379124105
Training tokenizer:  16% 326/2013 [43:39<3:46:09,  8.04s/it]loss_total_epoch 58.51928774639964
Training tokenizer:  16% 327/2013 [43:47<3:46:19,  8.05s/it]loss_total_epoch 58.69792537763715
Training tokenizer:  16% 328/2013 [43:55<3:46:08,  8.05s/it]loss_total_epoch 58.8754575625062
Training tokenizer:  16% 329/2013 [44:03<3:46:17,  8.06s/it]loss_total_epoch 59.056215185672045
Training tokenizer:  16% 330/2013 [44:11<3:46:24,  8.07s/it]loss_total_epoch 59.22281647846103
Training tokenizer:  16% 331/2013 [44:19<3:45:19,  8.04s/it]loss_total_epoch 59.400409407913685
Training tokenizer:  16% 332/2013 [44:27<3:44:43,  8.02s/it]loss_total_epoch 59.57469503208995
Training tokenizer:  17% 333/2013 [44:35<3:43:44,  7.99s/it]loss_total_epoch 59.731109756976366
Training tokenizer:  17% 334/2013 [44:43<3:44:00,  8.00s/it]loss_total_epoch 59.9149681776762
Training tokenizer:  17% 335/2013 [44:51<3:44:42,  8.03s/it]loss_total_epoch 60.07320982217789
Training tokenizer:  17% 336/2013 [45:00<3:44:57,  8.05s/it]loss_total_epoch 60.234100084751844
Training tokenizer:  17% 337/2013 [45:08<3:45:24,  8.07s/it]loss_total_epoch 60.390386920422316
Training tokenizer:  17% 338/2013 [45:16<3:44:36,  8.05s/it]loss_total_epoch 60.57536229863763
Training tokenizer:  17% 339/2013 [45:24<3:44:52,  8.06s/it]loss_total_epoch 60.72563077881932
Training tokenizer:  17% 340/2013 [45:32<3:45:05,  8.07s/it]loss_total_epoch 60.892736330628395
Training tokenizer:  17% 341/2013 [45:40<3:45:25,  8.09s/it]loss_total_epoch 61.05749258026481
Training tokenizer:  17% 342/2013 [45:48<3:45:26,  8.09s/it]loss_total_epoch 61.24514043703675
Training tokenizer:  17% 343/2013 [45:56<3:45:09,  8.09s/it]loss_total_epoch 61.412932835519314
Training tokenizer:  17% 344/2013 [46:04<3:45:04,  8.09s/it]loss_total_epoch 61.585422951728106
Training tokenizer:  17% 345/2013 [46:12<3:45:06,  8.10s/it]loss_total_epoch 61.76009254902601
Training tokenizer:  17% 346/2013 [46:20<3:45:14,  8.11s/it]loss_total_epoch 61.91631139814854
Training tokenizer:  17% 347/2013 [46:29<3:45:35,  8.12s/it]loss_total_epoch 62.08211565390229
Training tokenizer:  17% 348/2013 [46:37<3:45:20,  8.12s/it]loss_total_epoch 62.25190494954586
Training tokenizer:  17% 349/2013 [46:45<3:44:43,  8.10s/it]loss_total_epoch 62.43020673841238
Training tokenizer:  17% 350/2013 [46:53<3:43:01,  8.05s/it]loss_total_epoch 62.61064039170742
Training tokenizer:  17% 351/2013 [47:01<3:42:14,  8.02s/it]loss_total_epoch 62.789087038487196
Training tokenizer:  17% 352/2013 [47:09<3:43:03,  8.06s/it]loss_total_epoch 62.97568430379033
Training tokenizer:  18% 353/2013 [47:17<3:43:35,  8.08s/it]loss_total_epoch 63.14872718974948
Training tokenizer:  18% 354/2013 [47:25<3:43:18,  8.08s/it]loss_total_epoch 63.3397898003459
Training tokenizer:  18% 355/2013 [47:33<3:43:02,  8.07s/it]loss_total_epoch 63.496857304126024
Training tokenizer:  18% 356/2013 [47:41<3:43:03,  8.08s/it]loss_total_epoch 63.69449718296528
Training tokenizer:  18% 357/2013 [47:49<3:43:28,  8.10s/it]loss_total_epoch 63.87905302271247
Training tokenizer:  18% 358/2013 [47:57<3:42:14,  8.06s/it]loss_total_epoch 64.03416201844811
Training tokenizer:  18% 359/2013 [48:05<3:42:47,  8.08s/it]loss_total_epoch 64.21629786118865
Training tokenizer:  18% 360/2013 [48:13<3:41:31,  8.04s/it]loss_total_epoch 64.3837297335267
Training tokenizer:  18% 361/2013 [48:21<3:40:41,  8.02s/it]loss_total_epoch 64.55959698557854
Training tokenizer:  18% 362/2013 [48:29<3:39:36,  7.98s/it]loss_total_epoch 64.73506623134017
Training tokenizer:  18% 363/2013 [48:37<3:40:32,  8.02s/it]loss_total_epoch 64.91476490721107
Training tokenizer:  18% 364/2013 [48:46<3:41:42,  8.07s/it]loss_total_epoch 65.08980542048812
Training tokenizer:  18% 365/2013 [48:54<3:41:37,  8.07s/it]loss_total_epoch 65.25086201354861
Training tokenizer:  18% 366/2013 [49:02<3:42:00,  8.09s/it]loss_total_epoch 65.42464866861701
Training tokenizer:  18% 367/2013 [49:10<3:41:20,  8.07s/it]loss_total_epoch 65.59886809810996
Training tokenizer:  18% 368/2013 [49:18<3:39:51,  8.02s/it]loss_total_epoch 65.76721194013953
Training tokenizer:  18% 369/2013 [49:26<3:39:09,  8.00s/it]loss_total_epoch 65.95023458823562
Training tokenizer:  18% 370/2013 [49:34<3:39:07,  8.00s/it]loss_total_epoch 66.11721643060446
Training tokenizer:  18% 371/2013 [49:42<3:38:30,  7.98s/it]loss_total_epoch 66.27755884453654
Training tokenizer:  18% 372/2013 [49:50<3:39:31,  8.03s/it]loss_total_epoch 66.45512463152409
Training tokenizer:  19% 373/2013 [49:58<3:39:52,  8.04s/it]loss_total_epoch 66.6235719025135
Training tokenizer:  19% 374/2013 [50:06<3:40:20,  8.07s/it]loss_total_epoch 66.80628718435764
Training tokenizer:  19% 375/2013 [50:14<3:39:47,  8.05s/it]loss_total_epoch 66.96972443163395
Training tokenizer:  19% 376/2013 [50:22<3:39:42,  8.05s/it]loss_total_epoch 67.14058097079396
Training tokenizer:  19% 377/2013 [50:30<3:39:34,  8.05s/it]loss_total_epoch 67.3085756264627
Training tokenizer:  19% 378/2013 [50:38<3:39:54,  8.07s/it]loss_total_epoch 67.4801048450172
Training tokenizer:  19% 379/2013 [50:46<3:39:54,  8.07s/it]loss_total_epoch 67.65922316536307
Training tokenizer:  19% 380/2013 [50:54<3:39:32,  8.07s/it]loss_total_epoch 67.82353282347322
Training tokenizer:  19% 381/2013 [51:02<3:38:52,  8.05s/it]Training tokenizer:  19% 381/2013 [51:06<3:38:53,  8.05s/it]
Error executing job with overrides: []
Traceback (most recent call last):
  File "/space/zboucher/iris/src/main.py", line 10, in main
    trainer.run()
  File "/space/zboucher/iris/src/trainer.py", line 128, in run
    to_log += self.train_agent(epoch, nb_train_batches_per_epoch, training_data)
  File "/space/zboucher/iris/src/trainer.py", line 163, in train_agent
    _, index =self.train_collector.get_next_batch(epoch, self.batch_size, data_index, training_data)
  File "/space/zboucher/iris/src/collector.py", line 133, in get_next_batch
    for i, images in enumerate(training_data, start= start_index):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 633, in __next__
    data = self._next_data()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1325, in _next_data
    return self._process_data(data)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torch/_utils.py", line 644, in reraise
    raise exception
BlockingIOError: Caught BlockingIOError in DataLoader worker process 3.
Original Traceback (most recent call last):
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/dataset.py", line 243, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File "/space/zboucher/iris/src/collector.py", line 204, in __getitem__
    image = np.array(h5py.File(path)['image1']['image_data'])
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/h5py/_hl/files.py", line 567, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)
  File "/space/zboucher/anaconda3/envs/project_env/lib/python3.9/site-packages/h5py/_hl/files.py", line 231, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 106, in h5py.h5f.open
BlockingIOError: [Errno 11] Unable to open file (unable to lock file, errno = 11, error message = 'Resource temporarily unavailable')


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: Waiting for W&B process to finish, PID 2948410... (failed 1). Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.24MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.24MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.24MB uploaded (0.00MB deduped)wandb: | 0.24MB of 0.24MB uploaded (0.00MB deduped)wandb: / 0.24MB of 0.24MB uploaded (0.00MB deduped)wandb: - 0.24MB of 0.24MB uploaded (0.00MB deduped)wandb: \ 0.24MB of 0.24MB uploaded (0.00MB deduped)wandb: | 0.24MB of 0.24MB uploaded (0.00MB deduped)wandb: / 0.24MB of 0.24MB uploaded (0.00MB deduped)wandb: - 0.24MB of 0.24MB uploaded (0.00MB deduped)wandb: \ 0.24MB of 0.24MB uploaded (0.00MB deduped)wandb: | 0.24MB of 0.24MB uploaded (0.00MB deduped)wandb: / 0.24MB of 0.24MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:                              duration 
wandb:                                 epoch 
wandb:       tokenizer/train/commitment_loss 
wandb:       tokenizer/train/perceptual_loss 
wandb:   tokenizer/train/reconstruction_loss 
wandb:            tokenizer/train/total_loss 
wandb: 
wandb: Run summary:
wandb:                              duration 3.48048
wandb:                                 epoch 1
wandb:       tokenizer/train/commitment_loss 0.0046
wandb:       tokenizer/train/perceptual_loss 0.15207
wandb:   tokenizer/train/reconstruction_loss 0.01244
wandb:            tokenizer/train/total_loss 481.18739
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Synced iconic-butterfly-76: https://wandb.ai/zeina/iris/runs/1wbcnbjv
wandb: Find logs at: ./outputs/2023-09-25/16-58-37/wandb/run-20230925_165838-1wbcnbjv/logs/debug.log
wandb: 

